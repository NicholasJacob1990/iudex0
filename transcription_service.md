Vou adaptar toda a arquitetura para **RunPod como backend principal** e preparar para migra√ß√£o futura para AWS/GCP. A estrutura ser√° multi-cloud desde o in√≠cio. [ppl-ai-file-upload.s3.amazonaws](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/13046215/ee3d856b-85d2-4958-ba9f-964771cfd4bf/mlx_vomo.py)

## 1. Estrutura do projeto (adaptada)

```
transcription-service/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ docker-compose.prod.yml
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transcribe.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jobs.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sse.py
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îî‚îÄ‚îÄ schemas.py
‚îÇ
‚îú‚îÄ‚îÄ worker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ worker.py               # Orquestrador de jobs
‚îÇ   ‚îú‚îÄ‚îÄ backends/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py            # Interface abstrata
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ runpod_backend.py  # Backend principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aws_backend.py     # GPU EC2 (futuro)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gcp_backend.py     # GPU GCE (futuro)
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ storage.py
‚îÇ       ‚îî‚îÄ‚îÄ audio.py
‚îÇ
‚îú‚îÄ‚îÄ runpod_worker/              # Worker que roda NO RunPod
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ handler.py              # Handler RunPod
‚îÇ   ‚îú‚îÄ‚îÄ transcriber.py          # Baseado no seu mlx_vomo.py
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ cloud_workers/              # Workers para AWS/GCP (futuro)
‚îÇ   ‚îú‚îÄ‚îÄ aws/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ start.sh
‚îÇ   ‚îî‚îÄ‚îÄ gcp/
‚îÇ       ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ       ‚îî‚îÄ‚îÄ start.sh
‚îÇ
‚îî‚îÄ‚îÄ shared/
    ‚îú‚îÄ‚îÄ database.py
    ‚îú‚îÄ‚îÄ queue.py
    ‚îú‚îÄ‚îÄ config.py
    ‚îú‚îÄ‚îÄ metrics.py             # Sistema de m√©tricas unificado
    ‚îî‚îÄ‚îÄ storage_adapters.py    # Abstra√ß√£o S3/GCS/Azure

```

## 2. Vari√°veis de ambiente (multi-cloud)

```bash
# .env.example
# ========================================
# DATABASE
# ========================================
POSTGRES_PASSWORD=seu_password_forte
DATABASE_URL=postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/transcription_db

# ========================================
# STORAGE (Multi-cloud)
# ========================================
STORAGE_BACKEND=s3  # s3, gcs, azure, minio
STORAGE_REGION=us-east-1

# AWS S3
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
S3_BUCKET_NAME=transcriptions
S3_ENDPOINT=  # deixe vazio para AWS, ou http://minio:9000 para local

# Google Cloud Storage (opcional)
GCS_BUCKET_NAME=
GCS_PROJECT_ID=
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json

# Azure Blob (opcional)
AZURE_STORAGE_CONNECTION_STRING=
AZURE_CONTAINER_NAME=

# MinIO (desenvolvimento local)
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin

# ========================================
# BACKENDS DE PROCESSAMENTO
# ========================================
# Backend principal (runpod, aws_gpu, gcp_gpu)
PRIMARY_BACKEND=runpod

# RunPod
RUNPOD_API_KEY=
RUNPOD_ENDPOINT_ID=
RUNPOD_ACTIVE_WORKERS=1  # Manter 1+ worker ativo para evitar cold start

# AWS GPU (futuro)
AWS_GPU_ENABLED=false
AWS_GPU_INSTANCE_TYPE=g5.xlarge
AWS_GPU_REGION=us-east-1
AWS_GPU_AMI_ID=

# GCP GPU (futuro)
GCP_GPU_ENABLED=false
GCP_GPU_MACHINE_TYPE=n1-standard-4-t4
GCP_GPU_ZONE=us-central1-a

# ========================================
# FALLBACK/SECONDARY BACKENDS
# ========================================
# OpenAI Whisper API (fallback quando RunPod offline)
OPENAI_API_KEY=
OPENAI_ENABLED=true
OPENAI_AS_FALLBACK=true

# pyannoteAI (para diariza√ß√£o sem GPU)
PYANNOTE_API_KEY=
PYANNOTE_ENABLED=false

# ========================================
# HUGGINGFACE (para workers GPU)
# ========================================
HUGGINGFACE_TOKEN=hf_...

# ========================================
# APP
# ========================================
SECRET_KEY=seu_secret_key
ENVIRONMENT=development
MAX_UPLOAD_SIZE_MB=500
MAX_CONCURRENT_JOBS=20
REDIS_URL=redis://redis:6379

# ========================================
# WORKER
# ========================================
WORKER_CONCURRENCY=2
JOB_TIMEOUT_SECONDS=7200  # 2h para aulas longas
QUEUE_NAME=transcription_jobs

# ========================================
# MONITORAMENTO
# ========================================
PROMETHEUS_ENABLED=true
DATADOG_ENABLED=false
DATADOG_API_KEY=
GRAFANA_PASSWORD=admin

# ========================================
# FEATURES
# ========================================
ENABLE_DIARIZATION=true
ENABLE_WORD_TIMESTAMPS=true
DEFAULT_LANGUAGE=pt
```

## 3. Abstra√ß√£o de Storage (multi-cloud)

```python
# shared/storage_adapters.py
from abc import ABC, abstractmethod
import boto3
from google.cloud import storage as gcs_storage
from azure.storage.blob import BlobServiceClient
import os

class StorageAdapter(ABC):
    """Interface abstrata para storage"""
    
    @abstractmethod
    async def upload(self, file_obj, key: str) -> str:
        """Upload e retorna URL"""
        pass
    
    @abstractmethod
    async def download(self, key: str, local_path: str):
        """Download para arquivo local"""
        pass
    
    @abstractmethod
    def generate_presigned_url(self, key: str, expires_in: int = 3600) -> str:
        """Gera URL pr√©-assinada"""
        pass
    
    @abstractmethod
    async def delete(self, key: str):
        """Deleta arquivo"""
        pass


class S3Adapter(StorageAdapter):
    """AWS S3 ou MinIO"""
    
    def __init__(self):
        self.client = boto3.client(
            's3',
            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
            endpoint_url=os.getenv('S3_ENDPOINT'),  # None para AWS, URL para MinIO
            region_name=os.getenv('STORAGE_REGION', 'us-east-1')
        )
        self.bucket = os.getenv('S3_BUCKET_NAME')
    
    async def upload(self, file_obj, key: str) -> str:
        self.client.upload_fileobj(file_obj, self.bucket, key)
        return f"s3://{self.bucket}/{key}"
    
    async def download(self, key: str, local_path: str):
        self.client.download_file(self.bucket, key, local_path)
    
    def generate_presigned_url(self, key: str, expires_in: int = 3600) -> str:
        return self.client.generate_presigned_url(
            'get_object',
            Params={'Bucket': self.bucket, 'Key': key},
            ExpiresIn=expires_in
        )
    
    async def delete(self, key: str):
        self.client.delete_object(Bucket=self.bucket, Key=key)


class GCSAdapter(StorageAdapter):
    """Google Cloud Storage"""
    
    def __init__(self):
        self.client = gcs_storage.Client(
            project=os.getenv('GCS_PROJECT_ID')
        )
        self.bucket = self.client.bucket(os.getenv('GCS_BUCKET_NAME'))
    
    async def upload(self, file_obj, key: str) -> str:
        blob = self.bucket.blob(key)
        blob.upload_from_file(file_obj)
        return f"gs://{self.bucket.name}/{key}"
    
    async def download(self, key: str, local_path: str):
        blob = self.bucket.blob(key)
        blob.download_to_filename(local_path)
    
    def generate_presigned_url(self, key: str, expires_in: int = 3600) -> str:
        blob = self.bucket.blob(key)
        return blob.generate_signed_url(expiration=expires_in)
    
    async def delete(self, key: str):
        blob = self.bucket.blob(key)
        blob.delete()


class AzureAdapter(StorageAdapter):
    """Azure Blob Storage"""
    
    def __init__(self):
        self.client = BlobServiceClient.from_connection_string(
            os.getenv('AZURE_STORAGE_CONNECTION_STRING')
        )
        self.container = os.getenv('AZURE_CONTAINER_NAME')
    
    async def upload(self, file_obj, key: str) -> str:
        blob_client = self.client.get_blob_client(self.container, key)
        blob_client.upload_blob(file_obj, overwrite=True)
        return f"azure://{self.container}/{key}"
    
    async def download(self, key: str, local_path: str):
        blob_client = self.client.get_blob_client(self.container, key)
        with open(local_path, "wb") as f:
            f.write(blob_client.download_blob().readall())
    
    def generate_presigned_url(self, key: str, expires_in: int = 3600) -> str:
        blob_client = self.client.get_blob_client(self.container, key)
        # Azure SAS token
        from datetime import datetime, timedelta
        sas_token = blob_client.generate_shared_access_signature(
            permission="r",
            expiry=datetime.utcnow() + timedelta(seconds=expires_in)
        )
        return f"{blob_client.url}?{sas_token}"
    
    async def delete(self, key: str):
        blob_client = self.client.get_blob_client(self.container, key)
        blob_client.delete_blob()


# Factory
def get_storage_adapter() -> StorageAdapter:
    """Retorna adapter baseado em configura√ß√£o"""
    backend = os.getenv('STORAGE_BACKEND', 's3').lower()
    
    if backend == 's3':
        return S3Adapter()
    elif backend == 'gcs':
        return GCSAdapter()
    elif backend == 'azure':
        return AzureAdapter()
    else:
        raise ValueError(f"Storage backend n√£o suportado: {backend}")

# Inst√¢ncia global
storage = get_storage_adapter()
```

## 4. Interface abstrata de backends

```python
# worker/backends/base.py
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
from dataclasses import dataclass

@dataclass
class TranscriptionResult:
    """Resultado padronizado de transcri√ß√£o"""
    segments: list
    text: str
    language: str
    duration: float
    metadata: Dict[str, Any]
    cost: Optional[float] = None
    backend: Optional[str] = None


class TranscriptionBackend(ABC):
    """Interface abstrata para backends de transcri√ß√£o"""
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Nome do backend (runpod, aws_gpu, gcp_gpu, openai)"""
        pass
    
    @abstractmethod
    async def transcribe(
        self,
        audio_path: str,
        language: str = "pt",
        enable_diarization: bool = False,
        **kwargs
    ) -> TranscriptionResult:
        """Transcreve √°udio e retorna resultado padronizado"""
        pass
    
    @abstractmethod
    async def health_check(self) -> bool:
        """Verifica se backend est√° dispon√≠vel"""
        pass
    
    @abstractmethod
    def estimate_cost(self, audio_duration_seconds: float, enable_diarization: bool) -> float:
        """Estima custo em USD"""
        pass
    
    @abstractmethod
    def supports_diarization(self) -> bool:
        """Backend suporta diariza√ß√£o nativa?"""
        pass
```

## 5. Backend RunPod (principal)

```python
# worker/backends/runpod_backend.py
from .base import TranscriptionBackend, TranscriptionResult
import requests
import time
import os
from typing import Optional

class RunPodBackend(TranscriptionBackend):
    """Backend RunPod Serverless"""
    
    def __init__(self):
        self.api_key = os.getenv('RUNPOD_API_KEY')
        self.endpoint_id = os.getenv('RUNPOD_ENDPOINT_ID')
        self.base_url = f"https://api.runpod.ai/v2/{self.endpoint_id}"
        
        # Pricing (24GB tier active)
        self.cost_per_second = 0.468 / 3600  # $0.468/h ‚Üí $/s
    
    @property
    def name(self) -> str:
        return "runpod"
    
    async def transcribe(
        self,
        audio_path: str,
        language: str = "pt",
        enable_diarization: bool = False,
        **kwargs
    ) -> TranscriptionResult:
        """
        Transcreve via RunPod worker
        """
        from shared.storage_adapters import storage
        
        # 1. Upload para storage e gerar URL
        s3_key = f"temp/{os.path.basename(audio_path)}"
        with open(audio_path, 'rb') as f:
            await storage.upload(f, s3_key)
        
        audio_url = storage.generate_presigned_url(s3_key, expires_in=7200)
        
        # 2. Disparar job no RunPod
        start_time = time.time()
        
        response = requests.post(
            f"{self.base_url}/run",
            json={
                "input": {
                    "audio_url": audio_url,
                    "language": language,
                    "enable_diarization": enable_diarization,
                    "model": "large-v3-turbo",  # faster-whisper model
                    "vad_filter": True,
                    "word_timestamps": kwargs.get('word_timestamps', True)
                }
            },
            headers={"Authorization": f"Bearer {self.api_key}"}
        )
        
        if response.status_code != 200:
            raise Exception(f"RunPod error: {response.text}")
        
        job_id = response.json()["id"]
        
        # 3. Polling at√© completar
        result = await self._poll_job(job_id)
        
        # 4. Calcular custo
        execution_time = result.get("executionTime", 0) / 1000  # ms ‚Üí s
        cost = execution_time * self.cost_per_second
        
        # 5. Limpar storage tempor√°rio
        await storage.delete(s3_key)
        
        # 6. Converter para formato padronizado
        output = result.get("output", {})
        
        return TranscriptionResult(
            segments=output.get("segments", []),
            text=output.get("text", ""),
            language=output.get("language", language),
            duration=output.get("duration", 0),
            metadata={
                "model": "large-v3-turbo",
                "execution_time": execution_time,
                "cold_start_time": result.get("cold_start_time", 0),
                "rtf": output.get("rtf", 0)
            },
            cost=cost,
            backend=self.name
        )
    
    async def _poll_job(self, job_id: str, max_wait: int = 7200) -> dict:
        """
        Aguarda conclus√£o do job
        """
        start = time.time()
        
        while time.time() - start < max_wait:
            response = requests.get(
                f"{self.base_url}/status/{job_id}",
                headers={"Authorization": f"Bearer {self.api_key}"}
            )
            
            data = response.json()
            status = data.get("status")
            
            if status == "COMPLETED":
                return data
            elif status == "FAILED":
                raise Exception(f"RunPod job failed: {data.get('error')}")
            
            await asyncio.sleep(3)
        
        raise TimeoutError(f"Job {job_id} timeout ap√≥s {max_wait}s")
    
    async def health_check(self) -> bool:
        """Verifica se endpoint RunPod est√° online"""
        try:
            response = requests.get(
                f"{self.base_url}/health",
                headers={"Authorization": f"Bearer {self.api_key}"},
                timeout=5
            )
            return response.status_code == 200
        except:
            return False
    
    def estimate_cost(self, audio_duration_seconds: float, enable_diarization: bool) -> float:
        """
        Estima custo baseado em RTF hist√≥rico
        """
        # RTF m√©dio: 0.2 (transcription) + 0.1 (diarization se habilitado)
        rtf = 0.2
        if enable_diarization:
            rtf += 0.1
        
        processing_time = audio_duration_seconds * rtf
        return processing_time * self.cost_per_second
    
    def supports_diarization(self) -> bool:
        return True
```

## 6. Worker RunPod (roda NO RunPod)

```python
# runpod_worker/handler.py
"""
Handler que roda NO RunPod Serverless
Baseado no seu mlx_vomo.py
"""
import runpod
import os
import time
from transcriber import FasterWhisperTranscriber

# Inicializar transcriber FORA do handler (evita cold start)
print("üîß Inicializando transcriber...")
transcriber = FasterWhisperTranscriber(
    model_name="large-v3-turbo",
    device="cuda",
    compute_type="float16",
    enable_vad=True
)
print("‚úì Transcriber pronto")

def handler(job):
    """
    Handler principal do RunPod
    
    Input esperado:
    {
        "audio_url": "https://...",
        "language": "pt",
        "enable_diarization": false,
        "model": "large-v3-turbo",
        "vad_filter": true,
        "word_timestamps": true
    }
    """
    input_data = job["input"]
    
    try:
        start_time = time.time()
        
        # 1. Download do √°udio
        audio_url = input_data["audio_url"]
        audio_path = download_audio(audio_url)
        
        download_time = time.time() - start_time
        
        # 2. Transcrever
        transcribe_start = time.time()
        
        result = transcriber.transcribe(
            audio_path=audio_path,
            language=input_data.get("language", "pt"),
            enable_diarization=input_data.get("enable_diarization", False),
            word_timestamps=input_data.get("word_timestamps", True)
        )
        
        transcribe_time = time.time() - transcribe_start
        
        # 3. Limpar arquivo tempor√°rio
        os.remove(audio_path)
        
        # 4. Adicionar m√©tricas
        total_time = time.time() - start_time
        result["metadata"]["execution_time"] = transcribe_time
        result["metadata"]["download_time"] = download_time
        result["metadata"]["total_time"] = total_time
        
        if result.get("duration", 0) > 0:
            result["metadata"]["rtf"] = transcribe_time / result["duration"]
        
        return result
        
    except Exception as e:
        return {"error": str(e), "type": type(e).__name__}


def download_audio(url: str) -> str:
    """Download de √°udio via URL"""
    import requests
    import tempfile
    
    response = requests.get(url, stream=True)
    response.raise_for_status()
    
    # Salvar em arquivo tempor√°rio
    suffix = url.split('.')[-1].split('?')[0]  # extens√£o
    fd, path = tempfile.mkstemp(suffix=f".{suffix}")
    
    with os.fdopen(fd, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
    
    return path


# Iniciar RunPod serverless
if __name__ == "__main__":
    runpod.serverless.start({"handler": handler})
```

```python
# runpod_worker/transcriber.py
"""
Transcriber baseado no seu mlx_vomo.py
Adaptado para faster-whisper + pyannote
"""
from faster_whisper import WhisperModel
from pyannote.audio import Pipeline
import torch
import os
from typing import Optional, List, Dict
from intervaltree import IntervalTree

class FasterWhisperTranscriber:
    """
    Transcriber com faster-whisper + pyannote (opcional)
    """
    
    def __init__(
        self,
        model_name: str = "large-v3-turbo",
        device: str = "cuda",
        compute_type: str = "float16",
        enable_vad: bool = True
    ):
        self.model_name = model_name
        self.device = device
        
        # Carregar Whisper
        print(f"üì• Carregando Whisper {model_name}...")
        self.whisper_model = WhisperModel(
            model_name,
            device=device,
            compute_type=compute_type
        )
        print("‚úì Whisper carregado")
        
        # Diarization pipeline (lazy load)
        self.diarization_pipeline = None
        self.enable_vad = enable_vad
    
    def transcribe(
        self,
        audio_path: str,
        language: str = "pt",
        enable_diarization: bool = False,
        word_timestamps: bool = True
    ) -> Dict:
        """
        Transcreve √°udio com faster-whisper
        """
        # 1. Transcri√ß√£o
        segments, info = self.whisper_model.transcribe(
            audio_path,
            language=language,
            beam_size=1,
            vad_filter=self.enable_vad,
            word_timestamps=word_timestamps,
            condition_on_previous_text=False  # melhor para √°udios longos
        )
        
        # Converter generator para lista
        segments_list = []
        full_text = []
        
        for segment in segments:
            seg_dict = {
                "start": segment.start,
                "end": segment.end,
                "text": segment.text.strip()
            }
            
            if word_timestamps and hasattr(segment, 'words'):
                seg_dict["words"] = [
                    {
                        "start": w.start,
                        "end": w.end,
                        "word": w.word,
                        "probability": w.probability
                    }
                    for w in segment.words
                ]
            
            segments_list.append(seg_dict)
            full_text.append(segment.text.strip())
        
        result = {
            "segments": segments_list,
            "text": " ".join(full_text),
            "language": info.language,
            "duration": info.duration,
            "metadata": {
                "model": self.model_name,
                "language_probability": info.language_probability,
                "vad_enabled": self.enable_vad
            }
        }
        
        # 2. Diariza√ß√£o (opcional)
        if enable_diarization:
            diarization_output = self._diarize(audio_path)
            result["segments"] = self._assign_speakers(
                segments_list,
                diarization_output
            )
            result["metadata"]["diarization_enabled"] = True
        
        return result
    
    def _diarize(self, audio_path: str):
        """
        Diariza√ß√£o com pyannote
        """
        if self.diarization_pipeline is None:
            print("üì• Carregando pyannote...")
            token = os.getenv("HUGGINGFACE_TOKEN")
            
            if not token:
                raise ValueError("HUGGINGFACE_TOKEN n√£o configurado")
            
            self.diarization_pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=token
            )
            
            if self.device == "cuda":
                self.diarization_pipeline.to(torch.device("cuda"))
            
            print("‚úì pyannote carregado")
        
        return self.diarization_pipeline(audio_path)
    
    def _assign_speakers(self, segments: List[Dict], diarization) -> List[Dict]:
        """
        Alinha segmentos de transcri√ß√£o com speakers
        Baseado no seu assignDiarizationLabels
        """
        tree = IntervalTree()
        
        # Construir √°rvore de intervalos
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            tree[turn.start:turn.end] = speaker
        
        # Atribuir speakers aos segmentos
        for segment in segments:
            start = segment["start"]
            end = segment["end"]
            
            overlaps = tree[start:end]
            
            if overlaps:
                # Pegar speaker com maior overlap
                best_overlap = max(
                    overlaps,
                    key=lambda interval: min(end, interval.end) - max(start, interval.begin)
                )
                segment["speaker"] = best_overlap.data
            else:
                segment["speaker"] = "SPEAKER_UNKNOWN"
        
        return segments
```

```dockerfile
# runpod_worker/Dockerfile
FROM runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel

WORKDIR /app

# Instalar depend√™ncias do sistema
RUN apt-get update && \
    apt-get install -y ffmpeg && \
    rm -rf /var/lib/apt/lists/*

# Instalar depend√™ncias Python
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar c√≥digo
COPY handler.py transcriber.py ./

# Pre-download dos modelos (otimiza cold start)
RUN python -c "from faster_whisper import WhisperModel; WhisperModel('large-v3-turbo', device='cpu', compute_type='int8')"

CMD ["python", "-u", "handler.py"]
```

```txt
# runpod_worker/requirements.txt
runpod==1.5.0
faster-whisper==0.10.0
pyannote.audio==3.1.1
torch==2.1.0
torchaudio==2.1.0
intervaltree==3.1.0
requests==2.31.0
```

## 7. Worker orquestrador (seu servidor)

```python
# worker/worker.py
"""
Worker orquestrador que roda no SEU servidor
Gerencia fila e roteia jobs para backends apropriados
"""
import asyncio
import json
import os
import sys
import time
from datetime import datetime
from typing import Optional
import redis.asyncio as redis

sys.path.append('/app')
from backends.runpod_backend import RunPodBackend
from backends.aws_backend import AWSGPUBackend
from backends.gcp_backend import GCPGPUBackend
from backends.openai_backend import OpenAIBackend
from shared.database import get_db_sync as get_db
from shared.storage_adapters import storage
from shared.metrics import MetricsCollector
from utils.audio import get_audio_duration, optimize_audio

class BackendRouter:
    """
    Roteia jobs para backend mais apropriado
    """
    
    def __init__(self):
        self.backends = {}
        self.primary = os.getenv('PRIMARY_BACKEND', 'runpod')
        
        # Inicializar backends dispon√≠veis
        self._init_backends()
    
    def _init_backends(self):
        """Inicializa backends baseado em configura√ß√£o"""
        
        # RunPod (sempre dispon√≠vel se configurado)
        if os.getenv('RUNPOD_API_KEY'):
            self.backends['runpod'] = RunPodBackend()
            print("‚úì RunPod backend dispon√≠vel")
        
        # AWS GPU (opcional)
        if os.getenv('AWS_GPU_ENABLED', 'false').lower() == 'true':
            self.backends['aws_gpu'] = AWSGPUBackend()
            print("‚úì AWS GPU backend dispon√≠vel")
        
        # GCP GPU (opcional)
        if os.getenv('GCP_GPU_ENABLED', 'false').lower() == 'true':
            self.backends['gcp_gpu'] = GCPGPUBackend()
            print("‚úì GCP GPU backend dispon√≠vel")
        
        # OpenAI (fallback)
        if os.getenv('OPENAI_API_KEY'):
            self.backends['openai'] = OpenAIBackend()
            print("‚úì OpenAI backend dispon√≠vel (fallback)")
    
    async def select_backend(
        self,
        audio_duration: float,
        enable_diarization: bool,
        priority: str = "cost"
    ) -> str:
        """
        Seleciona melhor backend baseado em crit√©rios
        
        priority: "cost", "latency", "quality"
        """
        # 1. Verificar sa√∫de do backend prim√°rio
        primary_backend = self.backends.get(self.primary)
        
        if primary_backend and await primary_backend.health_check():
            return self.primary
        
        # 2. Fallback: tentar outros backends GPU
        for name in ['runpod', 'aws_gpu', 'gcp_gpu']:
            backend = self.backends.get(name)
            if backend and await backend.health_check():
                print(f"‚ö†Ô∏è Usando {name} como fallback")
                return name
        
        # 3. √öltimo recurso: OpenAI (mas com limite de tamanho)
        if 'openai' in self.backends and audio_duration < 3600:
            print("‚ö†Ô∏è Usando OpenAI como fallback (√°udio curto)")
            return 'openai'
        
        raise Exception("Nenhum backend dispon√≠vel")


class TranscriptionWorker:
    """
    Worker principal que processa jobs da fila
    """
    
    def __init__(self):
        self.redis_client = None
        self.router = BackendRouter()
        self.metrics = MetricsCollector()
        self.running = True
    
    async def connect(self):
        """Conecta ao Redis"""
        self.redis_client = await redis.from_url(os.getenv("REDIS_URL"))
        print("‚úì Worker conectado ao Redis")
    
    async def process_job(self, job_data: dict):
        """
        Processa um job individual
        """
        job_id = job_data["job_id"]
        tenant_id = job_data["tenant_id"]
        
        print(f"\n{'='*60}")
        print(f"üìù Processando job: {job_id}")
        print(f"   Arquivo: {job_data['filename']}")
        print(f"   Tenant: {tenant_id}")
        print(f"{'='*60}")
        
        db = get_db()
        
        try:
            # 1. Atualizar status: processing
            db.jobs.update_one(
                {"job_id": job_id},
                {
                    "$set": {
                        "status": "processing",
                        "started_at": datetime.utcnow(),
                        "worker_id": os.getenv("HOSTNAME", "worker-1")
                    }
                }
            )
            
            await self._notify_client(tenant_id, job_id, "processing")
            
            # 2. Download do √°udio
            print(f"‚¨áÔ∏è  Baixando √°udio do storage...")
            download_start = time.time()
            
            local_path = f"/tmp/{job_id}.audio"
            await storage.download(job_data["s3_key"], local_path)
            
            download_time = time.time() - download_start
            print(f"‚úì Download conclu√≠do em {download_time:.2f}s")
            
            # 3. Otimizar √°udio (comprimir se muito grande)
            optimized_path = await optimize_audio(local_path)
            audio_duration = get_audio_duration(optimized_path)
            audio_size = os.path.getsize(optimized_path)
            
            print(f"üìä Dura√ß√£o: {audio_duration/60:.1f} min | Tamanho: {audio_size/1024/1024:.1f} MB")
            
            # 4. Selecionar backend
            backend_name = await self.router.select_backend(
                audio_duration,
                job_data["enable_diarization"],
                priority="cost"
            )
            
            backend = self.router.backends[backend_name]
            print(f"üéØ Backend selecionado: {backend_name}")
            
            # 5. Estimar custo
            estimated_cost = backend.estimate_cost(
                audio_duration,
                job_data["enable_diarization"]
            )
            print(f"üí∞ Custo estimado: ${estimated_cost:.4f}")
            
            # 6. Processar com m√©tricas
            with self.metrics.track_job(
                job_id=job_id,
                tenant_id=tenant_id,
                backend=backend_name,
                audio_duration=audio_duration,
                audio_size=audio_size
            ) as metrics:
                
                process_start = time.time()
                
                result = await backend.transcribe(
                    audio_path=optimized_path,
                    language=job_data.get("language", "pt"),
                    enable_diarization=job_data["enable_diarization"],
                    word_timestamps=job_data.get("word_timestamps", True)
                )
                
                process_time = time.time() - process_start
                
                # Preencher m√©tricas
                metrics["processing_time"] = process_time
                metrics["cost"] = result.cost or estimated_cost
                metrics["segments_count"] = len(result.segments)
                metrics["rtf"] = result.metadata.get("rtf", 0)
                metrics["backend_metadata"] = result.metadata
            
            # 7. Salvar resultado no banco
            print(f"üíæ Salvando resultado...")
            db.transcriptions.insert_one({
                "job_id": job_id,
                "tenant_id": tenant_id,
                "text": result.text,
                "segments": result.segments,
                "language": result.language,
                "duration": result.duration,
                "backend": backend_name,
                "cost": result.cost,
                "metadata": result.metadata,
                "created_at": datetime.utcnow()
            })
            
            # 8. Atualizar status: completed
            db.jobs.update_one(
                {"job_id": job_id},
                {
                    "$set": {
                        "status": "completed",
                        "completed_at": datetime.utcnow(),
                        "cost": result.cost,
                        "backend": backend_name,
                        "segments_count": len(result.segments),
                        "rtf": result.metadata.get("rtf", 0)
                    }
                }
            )
            
            await self._notify_client(tenant_id, job_id, "completed", {
                "segments_count": len(result.segments),
                "duration": result.duration,
                "cost": result.cost
            })
            
            # 9. Limpar arquivos tempor√°rios
            os.remove(local_path)
            if optimized_path != local_path:
                os.remove(optimized_path)
            
            print(f"‚úÖ Job {job_id} conclu√≠do com sucesso!")
            print(f"   Segmentos: {len(result.segments)}")
            print(f"   RTF: {result.metadata.get('rtf', 0):.3f}")
            print(f"   Custo: ${result.cost:.4f}")
            
        except Exception as e:
            print(f"‚ùå Erro no job {job_id}: {e}")
            
            # Salvar erro no banco
            db.jobs.update_one(
                {"job_id": job_id},
                {
                    "$set": {
                        "status": "failed",
                        "error": str(e),
                        "error_type": type(e).__name__,
                        "failed_at": datetime.utcnow()
                    }
                }
            )
            
            await self._notify_client(tenant_id, job_id, "failed", {
                "error": str(e)
            })
            
            # Limpar arquivos se existirem
            if os.path.exists(local_path):
                os.remove(local_path)
    
    async def _notify_client(
        self,
        tenant_id: str,
        job_id: str,
        status: str,
        data: dict = None
    ):
        """Notifica cliente via Redis Pub/Sub"""
        message = {
            "job_id": job_id,
            "status": status,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        if data:
            message.update(data)
        
        await self.redis_client.publish(
            f"jobs:{tenant_id}",
            json.dumps(message)
        )
    
    async def run(self):
        """
        Loop principal do worker
        """
        await self.connect()
        
        queue_name = os.getenv('QUEUE_NAME', 'transcription_jobs')
        print(f"\n{'='*60}")
        print(f"üöÄ Worker iniciado")
        print(f"   Queue: {queue_name}")
        print(f"   Backend prim√°rio: {self.router.primary}")
        print(f"   Backends dispon√≠veis: {list(self.router.backends.keys())}")
        print(f"{'='*60}\n")
        
        while self.running:
            try:
                # Bloqueia at√© ter job (timeout 5s)
                result = await self.redis_client.blpop(queue_name, timeout=5)
                
                if result:
                    _, job_json = result
                    job_data = json.loads(job_json)
                    
                    await self.process_job(job_data)
                
            except KeyboardInterrupt:
                print("\n‚ö†Ô∏è  Recebido sinal de interrup√ß√£o. Finalizando...")
                self.running = False
                break
                
            except Exception as e:
                print(f"‚ùå Erro no loop do worker: {e}")
                await asyncio.sleep(5)
        
        print("üëã Worker finalizado")
    
    async def shutdown(self):
        """Graceful shutdown"""
        self.running = False
        if self.redis_client:
            await self.redis_client.close()


if __name__ == "__main__":
    worker = TranscriptionWorker()
    
    try:
        asyncio.run(worker.run())
    except KeyboardInterrupt:
        asyncio.run(worker.shutdown())
```

## 8. Backend AWS GPU (futuro)

```python
# worker/backends/aws_backend.py
"""
Backend para inst√¢ncias GPU na AWS (g5.xlarge com spot)
Conecta via SSH e executa transcription no worker remoto
"""
from .base import TranscriptionBackend, TranscriptionResult
import boto3
import paramiko
import os
import time
from typing import Optional

class AWSGPUBackend(TranscriptionBackend):
    """Backend AWS EC2 com GPU"""
    
    def __init__(self):
        self.ec2 = boto3.client(
            'ec2',
            region_name=os.getenv('AWS_GPU_REGION', 'us-east-1')
        )
        
        self.instance_type = os.getenv('AWS_GPU_INSTANCE_TYPE', 'g5.xlarge')
        self.ami_id = os.getenv('AWS_GPU_AMI_ID')  # AMI com CUDA + faster-whisper
        
        # Pricing (g5.xlarge spot m√©dio)
        self.cost_per_hour = 0.47
    
    @property
    def name(self) -> str:
        return "aws_gpu"
    
    async def transcribe(
        self,
        audio_path: str,
        language: str = "pt",
        enable_diarization: bool = False,
        **kwargs
    ) -> TranscriptionResult:
        """
        Transcreve via inst√¢ncia AWS
        """
        # 1. Garantir que h√° inst√¢ncia dispon√≠vel
        instance = await self._get_or_create_instance()
        
        # 2. Upload √°udio para inst√¢ncia
        instance_ip = instance['PublicIpAddress']
        remote_path = f"/tmp/{os.path.basename(audio_path)}"
        
        await self._upload_file(instance_ip, audio_path, remote_path)
        
        # 3. Executar transcri√ß√£o remotamente
        start_time = time.time()
        
        command = f"""
        python3 /opt/transcriber/transcribe.py \
            --audio {remote_path} \
            --language {language} \
            --diarization {enable_diarization} \
            --output-json
        """
        
        output = await self._execute_command(instance_ip, command)
        
        execution_time = time.time() - start_time
        
        # 4. Parse resultado
        import json
        result_data = json.loads(output)
        
        # 5. Calcular custo
        cost = (execution_time / 3600) * self.cost_per_hour
        
        return TranscriptionResult(
            segments=result_data["segments"],
            text=result_data["text"],
            language=result_data["language"],
            duration=result_data["duration"],
            metadata={
                "instance_type": self.instance_type,
                "execution_time": execution_time,
                "instance_id": instance['InstanceId']
            },
            cost=cost,
            backend=self.name
        )
    
    async def _get_or_create_instance(self):
        """
        Busca inst√¢ncia spot running ou cria nova
        """
        # Buscar inst√¢ncias running com tag espec√≠fica
        response = self.ec2.describe_instances(
            Filters=[
                {'Name': 'instance-state-name', 'Values': ['running']},
                {'Name': 'tag:Purpose', 'Values': ['transcription-worker']},
                {'Name': 'instance-type', 'Values': [self.instance_type]}
            ]
        )
        
        if response['Reservations']:
            return response['Reservations'][0]['Instances'][0]
        
        # Criar nova inst√¢ncia spot
        print("üöÄ Criando nova inst√¢ncia AWS GPU...")
        
        response = self.ec2.request_spot_instances(
            InstanceCount=1,
            Type='one-time',
            LaunchSpecification={
                'ImageId': self.ami_id,
                'InstanceType': self.instance_type,
                'KeyName': os.getenv('AWS_KEY_PAIR_NAME'),
                'SecurityGroupIds': [os.getenv('AWS_SECURITY_GROUP_ID')],
                'UserData': self._get_user_data(),
                'TagSpecifications': [{
                    'ResourceType': 'instance',
                    'Tags': [
                        {'Key': 'Purpose', 'Value': 'transcription-worker'},
                        {'Key': 'ManagedBy', 'Value': 'transcription-service'}
                    ]
                }]
            }
        )
        
        # Aguardar inst√¢ncia ficar running
        # (implementa√ß√£o simplificada - produ√ß√£o precisa polling)
        time.sleep(60)
        
        return await self._get_or_create_instance()
    
    def _get_user_data(self) -> str:
        """Script de inicializa√ß√£o da inst√¢ncia"""
        return """#!/bin/bash
        # Instalar depend√™ncias
        apt-get update
        apt-get install -y python3-pip ffmpeg
        
        # Instalar faster-whisper + pyannote
        pip3 install faster-whisper pyannote.audio
        
        # Baixar modelo
        python3 -c "from faster_whisper import WhisperModel; WhisperModel('large-v3-turbo')"
        
        # Criar script de transcri√ß√£o
        mkdir -p /opt/transcriber
        cat > /opt/transcriber/transcribe.py << 'EOF'
        # Script Python que faz a transcri√ß√£o
        # (mesmo c√≥digo do transcriber.py do RunPod)
        EOF
        
        echo "‚úì Worker pronto"
        """
    
    async def _upload_file(self, host: str, local_path: str, remote_path: str):
        """Upload via SFTP"""
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        
        ssh.connect(
            host,
            username='ubuntu',
            key_filename=os.getenv('AWS_SSH_KEY_PATH')
        )
        
        sftp = ssh.open_sftp()
        sftp.put(local_path, remote_path)
        sftp.close()
        ssh.close()
    
    async def _execute_command(self, host: str, command: str) -> str:
        """Executa comando via SSH"""
        ssh = paramiko.SSHClient()
        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        
        ssh.connect(
            host,
            username='ubuntu',
            key_filename=os.getenv('AWS_SSH_KEY_PATH')
        )
        
        stdin, stdout, stderr = ssh.exec_command(command)
        output = stdout.read().decode()
        error = stderr.read().decode()
        
        ssh.close()
        
        if error:
            raise Exception(f"AWS GPU error: {error}")
        
        return output
    
    async def health_check(self) -> bool:
        """Verifica se h√° inst√¢ncias dispon√≠veis"""
        try:
            response = self.ec2.describe_instances(
                Filters=[
                    {'Name': 'instance-state-name', 'Values': ['running']},
                    {'Name': 'tag:Purpose', 'Values': ['transcription-worker']}
                ]
            )
            return len(response['Reservations']) > 0
        except:
            return False
    
    def estimate_cost(self, audio_duration_seconds: float, enable_diarization: bool) -> float:
        """Estima custo baseado em RTF"""
        rtf = 0.2 if not enable_diarization else 0.3
        processing_time_hours = (audio_duration_seconds * rtf) / 3600
        return processing_time_hours * self.cost_per_hour
    
    def supports_diarization(self) -> bool:
        return True
```

## 9. Backend GCP GPU (futuro)

```python
# worker/backends/gcp_backend.py
"""
Backend para inst√¢ncias GPU no Google Cloud
Similar ao AWS mas usando GCP Compute Engine
"""
from .base import TranscriptionBackend, TranscriptionResult
from google.cloud import compute_v1
import os
import time

class GCPGPUBackend(TranscriptionBackend):
    """Backend GCP Compute Engine com GPU"""
    
    def __init__(self):
        self.compute = compute_v1.InstancesClient()
        self.project_id = os.getenv('GCP_PROJECT_ID')
        self.zone = os.getenv('GCP_GPU_ZONE', 'us-central1-a')
        self.machine_type = os.getenv('GCP_GPU_MACHINE_TYPE', 'n1-standard-4-t4')
        
        # Pricing (T4 preemptible m√©dio)
        self.cost_per_hour = 0.35
    
    @property
    def name(self) -> str:
        return "gcp_gpu"
    
    async def transcribe(
        self,
        audio_path: str,
        language: str = "pt",
        enable_diarization: bool = False,
        **kwargs
    ) -> TranscriptionResult:
        """
        Transcreve via inst√¢ncia GCP
        (Implementa√ß√£o similar ao AWS)
        """
        # TODO: implementar l√≥gica similar ao AWSGPUBackend
        pass
    
    async def health_check(self) -> bool:
        """Verifica inst√¢ncias GCP dispon√≠veis"""
        try:
            request = compute_v1.ListInstancesRequest(
                project=self.project_id,
                zone=self.zone,
                filter='labels.purpose=transcription-worker AND status=RUNNING'
            )
            instances = list(self.compute.list(request=request))
            return len(instances) > 0
        except:
            return False
    
    def estimate_cost(self, audio_duration_seconds: float, enable_diarization: bool) -> float:
        rtf = 0.2 if not enable_diarization else 0.3
        processing_time_hours = (audio_duration_seconds * rtf) / 3600
        return processing_time_hours * self.cost_per_hour
    
    def supports_diarization(self) -> bool:
        return True
```

## 10. Sistema de m√©tricas unificado

```python
# shared/metrics.py
"""
Sistema de m√©tricas unificado que suporta m√∫ltiplos exporters
"""
from contextlib import contextmanager
import time
import os
from typing import Dict, Any, Optional
from datetime import datetime

class MetricsCollector:
    """
    Coleta m√©tricas e exporta para m√∫ltiplos sistemas
    """
    
    def __init__(self):
        self.exporters = []
        self._init_exporters()
    
    def _init_exporters(self):
        """Inicializa exporters baseado em configura√ß√£o"""
        
        # PostgreSQL (sempre habilitado)
        from .exporters.postgres_exporter import PostgreSQLExporter
        self.exporters.append(PostgreSQLExporter())
        
        # Prometheus (opcional)
        if os.getenv('PROMETHEUS_ENABLED', 'false').lower() == 'true':
            from .exporters.prometheus_exporter import PrometheusExporter
            self.exporters.append(PrometheusExporter())
        
        # Datadog (opcional)
        if os.getenv('DATADOG_ENABLED', 'false').lower() == 'true':
            from .exporters.datadog_exporter import DatadogExporter
            self.exporters.append(DatadogExporter())
    
    @contextmanager
    def track_job(
        self,
        job_id: str,
        tenant_id: str,
        backend: str,
        audio_duration: float,
        audio_size: int
    ):
        """
        Context manager para tracking de m√©tricas de um job
        """
        metrics = {
            "job_id": job_id,
            "tenant_id": tenant_id,
            "backend": backend,
            "audio_duration": audio_duration,
            "audio_size": audio_size,
            "start_time": time.time(),
            "status": "processing",
            "worker_id": os.getenv("HOSTNAME", "worker-unknown")
        }
        
        try:
            yield metrics
            metrics["status"] = "success"
            
        except Exception as e:
            metrics["status"] = "failed"
            metrics["error"] = str(e)
            metrics["error_type"] = type(e).__name__
            raise
            
        finally:
            metrics["end_time"] = time.time()
            metrics["total_time"] = metrics["end_time"] - metrics["start_time"]
            
            # Calcular m√©tricas derivadas
            if audio_duration > 0:
                metrics["rtf"] = metrics.get("processing_time", metrics["total_time"]) / audio_duration
            
            # Exportar para todos os sistemas
            for exporter in self.exporters:
                try:
                    exporter.export(metrics)
                except Exception as e:
                    print(f"Erro ao exportar m√©tricas para {exporter.__class__.__name__}: {e}")
```

```python
# shared/exporters/postgres_exporter.py
"""
Exporta m√©tricas para PostgreSQL
"""
from shared.database import get_db_sync

class PostgreSQLExporter:
    """Exporta para tabela job_metrics"""
    
    def export(self, metrics: dict):
        """Salva m√©tricas no PostgreSQL"""
        db = get_db_sync()
        
        db.execute("""
            INSERT INTO job_metrics (
                job_id, tenant_id, backend,
                audio_duration_seconds, audio_size_bytes,
                total_processing_time, total_latency,
                rtf, cost_total, status, error_type,
                worker_id, created_at
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
            )
        """, (
            metrics["job_id"],
            metrics["tenant_id"],
            metrics["backend"],
            metrics.get("audio_duration"),
            metrics.get("audio_size"),
            metrics.get("processing_time"),
            metrics.get("total_time"),
            metrics.get("rtf"),
            metrics.get("cost"),
            metrics.get("status"),
            metrics.get("error_type"),
            metrics.get("worker_id"),
            datetime.utcnow()
        ))
        
        db.commit()
```

```python
# shared/exporters/prometheus_exporter.py
"""
Exporta m√©tricas para Prometheus
"""
from prometheus_client import Counter, Histogram, Gauge

# M√©tricas Prometheus
jobs_total = Counter('transcription_jobs_total', 'Total jobs', ['backend', 'status'])
job_duration = Histogram('transcription_duration_seconds', 'Job duration', ['backend'])
rtf_histogram = Histogram('transcription_rtf', 'RTF', ['backend'])
cost_total = Counter('transcription_cost_total', 'Cost USD', ['backend'])

class PrometheusExporter:
    """Exporta para Prometheus"""
    
    def export(self, metrics: dict):
        """Registra m√©tricas no Prometheus"""
        backend = metrics["backend"]
        status = metrics["status"]
        
        jobs_total.labels(backend=backend, status=status).inc()
        
        if "total_time" in metrics:
            job_duration.labels(backend=backend).observe(metrics["total_time"])
        
        if "rtf" in metrics:
            rtf_histogram.labels(backend=backend).observe(metrics["rtf"])
        
        if "cost" in metrics:
            cost_total.labels(backend=backend).inc(metrics["cost"])
```

## 11. Utilit√°rios de √°udio

```python
# worker/utils/audio.py
"""
Utilit√°rios para processamento de √°udio
"""
import subprocess
import os
from pydub import AudioSegment

def get_audio_duration(audio_path: str) -> float:
    """Retorna dura√ß√£o em segundos usando ffprobe"""
    result = subprocess.run(
        [
            'ffprobe',
            '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1',
            audio_path
        ],
        capture_output=True,
        text=True
    )
    
    return float(result.stdout.strip())


async def optimize_audio(audio_path: str, target_size_mb: int = 100) -> str:
    """
    Otimiza √°udio para reduzir tamanho se necess√°rio
    Retorna path do arquivo otimizado
    """
    size_mb = os.path.getsize(audio_path) / (1024 * 1024)
    
    # Se menor que target, retorna original
    if size_mb <= target_size_mb:
        return audio_path
    
    print(f"üîß Otimizando √°udio ({size_mb:.1f}MB ‚Üí ~{target_size_mb}MB)...")
    
    # Comprimir para mono 16kHz (padr√£o Whisper)
    audio = AudioSegment.from_file(audio_path)
    audio = audio.set_channels(1).set_frame_rate(16000)
    
    # Ajustar bitrate baseado no target
    duration_minutes = len(audio) / 1000 / 60
    target_bitrate = int((target_size_mb * 8 * 1024) / duration_minutes)
    target_bitrate = min(max(target_bitrate, 32), 128)  # entre 32k e 128k
    
    optimized_path = audio_path.replace('.', '_optimized.')
    audio.export(
        optimized_path,
        format="mp3",
        bitrate=f"{target_bitrate}k",
        parameters=["-ar", "16000", "-ac", "1"]
    )
    
    new_size_mb = os.path.getsize(optimized_path) / (1024 * 1024)
    print(f"‚úì √Åudio otimizado: {new_size_mb:.1f}MB (bitrate: {target_bitrate}kbps)")
    
    return optimized_path
```

## 12. Docker Compose para produ√ß√£o (multi-cloud)

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  # API (escal√°vel)
  api:
    image: ${REGISTRY}/transcription-api:${VERSION}
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - STORAGE_BACKEND=${STORAGE_BACKEND}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - PRIMARY_BACKEND=${PRIMARY_BACKEND}
      - RUNPOD_API_KEY=${RUNPOD_API_KEY}
      - RUNPOD_ENDPOINT_ID=${RUNPOD_ENDPOINT_ID}
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - transcription-net

  # Workers (escal√°vel baseado em carga)
  worker:
    image: ${REGISTRY}/transcription-worker:${VERSION}
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - STORAGE_BACKEND=${STORAGE_BACKEND}
      - PRIMARY_BACKEND=${PRIMARY_BACKEND}
      - RUNPOD_API_KEY=${RUNPOD_API_KEY}
      - AWS_GPU_ENABLED=${AWS_GPU_ENABLED}
      - GCP_GPU_ENABLED=${GCP_GPU_ENABLED}
      - PROMETHEUS_ENABLED=true
    deploy:
      replicas: 5
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
      restart_policy:
        condition: on-failure
    volumes:
      - /tmp:/tmp
    networks:
      - transcription-net

  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.retention.time=90d'
    ports:
      - "9090:9090"
    networks:
      - transcription-net

  # Grafana
  grafana:
    image: grafana/grafana:latest
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    ports:
      - "3000:3000"
    networks:
      - transcription-net

volumes:
  prometheus_data:
  grafana_data:

networks:
  transcription-net:
    driver: overlay
```

## 13. Deploy no RunPod

```bash
# scripts/deploy_runpod_worker.sh
#!/bin/bash
set -e

echo "üöÄ Deploying RunPod Worker"

# 1. Build imagem Docker
cd runpod_worker
docker build -t transcription-runpod:latest .

# 2. Push para registry
docker tag transcription-runpod:latest $DOCKER_REGISTRY/transcription-runpod:latest
docker push $DOCKER_REGISTRY/transcription-runpod:latest

# 3. Criar endpoint no RunPod via API
curl -X POST https://api.runpod.ai/v2/$RUNPOD_ENDPOINT_ID/deploy \
  -H "Authorization: Bearer $RUNPOD_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "image": "'$DOCKER_REGISTRY'/transcription-runpod:latest",
    "gpu_type": "NVIDIA L4",
    "workers": {
      "min": 1,
      "max": 10
    },
    "env": {
      "HUGGINGFACE_TOKEN": "'$HUGGINGFACE_TOKEN'"
    }
  }'

echo "‚úÖ RunPod worker deployed"
```

## 14. Deploy AWS (ECS + RDS + ElastiCache)

### 14.1 Infraestrutura como C√≥digo (Terraform)

```hcl
# terraform/aws/main.tf
terraform {
  required_version = ">= 1.0"
  
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  
  backend "s3" {
    bucket = "transcription-service-terraform-state"
    key    = "prod/terraform.tfstate"
    region = "us-east-1"
  }
}

provider "aws" {
  region = var.aws_region
}

# ========================================
# VPC e Networking
# ========================================
module "vpc" {
  source = "terraform-aws-modules/vpc/aws"
  
  name = "transcription-vpc"
  cidr = "10.0.0.0/16"
  
  azs             = ["us-east-1a", "us-east-1b", "us-east-1c"]
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]
  
  enable_nat_gateway = true
  enable_vpn_gateway = false
  
  tags = {
    Terraform   = "true"
    Environment = var.environment
  }
}

# ========================================
# RDS PostgreSQL
# ========================================
resource "aws_db_instance" "postgres" {
  identifier     = "transcription-db-${var.environment}"
  engine         = "postgres"
  engine_version = "15.4"
  instance_class = var.db_instance_class
  
  allocated_storage     = 100
  max_allocated_storage = 500
  storage_type          = "gp3"
  storage_encrypted     = true
  
  db_name  = "transcription_db"
  username = "postgres"
  password = var.db_password
  
  vpc_security_group_ids = [aws_security_group.rds.id]
  db_subnet_group_name   = aws_db_subnet_group.main.name
  
  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "mon:04:00-mon:05:00"
  
  skip_final_snapshot       = false
  final_snapshot_identifier = "transcription-db-final-${var.environment}"
  
  performance_insights_enabled = true
  
  tags = {
    Name        = "transcription-db"
    Environment = var.environment
  }
}

resource "aws_db_subnet_group" "main" {
  name       = "transcription-db-subnet-${var.environment}"
  subnet_ids = module.vpc.private_subnets
  
  tags = {
    Name = "transcription-db-subnet"
  }
}

# ========================================
# ElastiCache Redis
# ========================================
resource "aws_elasticache_cluster" "redis" {
  cluster_id           = "transcription-redis-${var.environment}"
  engine               = "redis"
  engine_version       = "7.0"
  node_type            = var.redis_node_type
  num_cache_nodes      = 1
  parameter_group_name = "default.redis7"
  port                 = 6379
  
  subnet_group_name    = aws_elasticache_subnet_group.main.name
  security_group_ids   = [aws_security_group.redis.id]
  
  snapshot_retention_limit = 5
  snapshot_window         = "03:00-05:00"
  
  tags = {
    Name        = "transcription-redis"
    Environment = var.environment
  }
}

resource "aws_elasticache_subnet_group" "main" {
  name       = "transcription-redis-subnet-${var.environment}"
  subnet_ids = module.vpc.private_subnets
}

# ========================================
# S3 Bucket (Storage)
# ========================================
resource "aws_s3_bucket" "storage" {
  bucket = "transcription-storage-${var.environment}-${data.aws_caller_identity.current.account_id}"
  
  tags = {
    Name        = "transcription-storage"
    Environment = var.environment
  }
}

resource "aws_s3_bucket_versioning" "storage" {
  bucket = aws_s3_bucket.storage.id
  
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_lifecycle_configuration" "storage" {
  bucket = aws_s3_bucket.storage.id
  
  rule {
    id     = "delete-old-audio"
    status = "Enabled"
    
    filter {
      prefix = "audio/"
    }
    
    expiration {
      days = 30
    }
  }
  
  rule {
    id     = "archive-transcriptions"
    status = "Enabled"
    
    filter {
      prefix = "transcriptions/"
    }
    
    transition {
      days          = 90
      storage_class = "GLACIER"
    }
  }
}

resource "aws_s3_bucket_public_access_block" "storage" {
  bucket = aws_s3_bucket.storage.id
  
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# ========================================
# ECS Cluster
# ========================================
resource "aws_ecs_cluster" "main" {
  name = "transcription-cluster-${var.environment}"
  
  setting {
    name  = "containerInsights"
    value = "enabled"
  }
  
  tags = {
    Name        = "transcription-cluster"
    Environment = var.environment
  }
}

resource "aws_ecs_cluster_capacity_providers" "main" {
  cluster_name = aws_ecs_cluster.main.name
  
  capacity_providers = ["FARGATE", "FARGATE_SPOT"]
  
  default_capacity_provider_strategy {
    capacity_provider = "FARGATE_SPOT"
    weight            = 1
    base              = 0
  }
}

# ========================================
# API Task Definition
# ========================================
resource "aws_ecs_task_definition" "api" {
  family                   = "transcription-api"
  requires_compatibilities = ["FARGATE"]
  network_mode            = "awsvpc"
  cpu                     = 2048
  memory                  = 4096
  execution_role_arn      = aws_iam_role.ecs_execution.arn
  task_role_arn           = aws_iam_role.ecs_task.arn
  
  container_definitions = jsonencode([
    {
      name  = "api"
      image = "${var.ecr_registry}/transcription-api:${var.image_tag}"
      
      portMappings = [
        {
          containerPort = 8000
          protocol      = "tcp"
        }
      ]
      
      environment = [
        {
          name  = "ENVIRONMENT"
          value = var.environment
        },
        {
          name  = "DATABASE_URL"
          value = "postgresql://postgres:${var.db_password}@${aws_db_instance.postgres.endpoint}/transcription_db"
        },
        {
          name  = "REDIS_URL"
          value = "redis://${aws_elasticache_cluster.redis.cache_nodes[0].address}:6379"
        },
        {
          name  = "S3_BUCKET_NAME"
          value = aws_s3_bucket.storage.id
        },
        {
          name  = "PRIMARY_BACKEND"
          value = "runpod"
        }
      ]
      
      secrets = [
        {
          name      = "RUNPOD_API_KEY"
          valueFrom = aws_secretsmanager_secret.runpod_api_key.arn
        },
        {
          name      = "OPENAI_API_KEY"
          valueFrom = aws_secretsmanager_secret.openai_api_key.arn
        }
      ]
      
      logConfiguration = {
        logDriver = "awslogs"
        options = {
          awslogs-group         = aws_cloudwatch_log_group.api.name
          awslogs-region        = var.aws_region
          awslogs-stream-prefix = "api"
        }
      }
      
      healthCheck = {
        command     = ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
        interval    = 30
        timeout     = 5
        retries     = 3
        startPeriod = 60
      }
    }
  ])
}

# ========================================
# Worker Task Definition
# ========================================
resource "aws_ecs_task_definition" "worker" {
  family                   = "transcription-worker"
  requires_compatibilities = ["FARGATE"]
  network_mode            = "awsvpc"
  cpu                     = 4096
  memory                  = 8192
  execution_role_arn      = aws_iam_role.ecs_execution.arn
  task_role_arn           = aws_iam_role.ecs_task.arn
  
  container_definitions = jsonencode([
    {
      name  = "worker"
      image = "${var.ecr_registry}/transcription-worker:${var.image_tag}"
      
      environment = [
        {
          name  = "ENVIRONMENT"
          value = var.environment
        },
        {
          name  = "DATABASE_URL"
          value = "postgresql://postgres:${var.db_password}@${aws_db_instance.postgres.endpoint}/transcription_db"
        },
        {
          name  = "REDIS_URL"
          value = "redis://${aws_elasticache_cluster.redis.cache_nodes[0].address}:6379"
        },
        {
          name  = "PRIMARY_BACKEND"
          value = "runpod"
        },
        {
          name  = "WORKER_CONCURRENCY"
          value = "2"
        }
      ]
      
      secrets = [
        {
          name      = "RUNPOD_API_KEY"
          valueFrom = aws_secretsmanager_secret.runpod_api_key.arn
        }
      ]
      
      logConfiguration = {
        logDriver = "awslogs"
        options = {
          awslogs-group         = aws_cloudwatch_log_group.worker.name
          awslogs-region        = var.aws_region
          awslogs-stream-prefix = "worker"
        }
      }
      
      mountPoints = [
        {
          sourceVolume  = "tmp"
          containerPath = "/tmp"
        }
      ]
    }
  ])
  
  volume {
    name = "tmp"
  }
}

# ========================================
# ECS Services
# ========================================
resource "aws_ecs_service" "api" {
  name            = "transcription-api"
  cluster         = aws_ecs_cluster.main.id
  task_definition = aws_ecs_task_definition.api.arn
  desired_count   = 3
  launch_type     = "FARGATE"
  
  network_configuration {
    subnets          = module.vpc.private_subnets
    security_groups  = [aws_security_group.ecs_tasks.id]
    assign_public_ip = false
  }
  
  load_balancer {
    target_group_arn = aws_lb_target_group.api.arn
    container_name   = "api"
    container_port   = 8000
  }
  
  deployment_configuration {
    maximum_percent         = 200
    minimum_healthy_percent = 100
  }
  
  depends_on = [aws_lb_listener.api]
}

resource "aws_ecs_service" "worker" {
  name            = "transcription-worker"
  cluster         = aws_ecs_cluster.main.id
  task_definition = aws_ecs_task_definition.worker.arn
  desired_count   = 5
  launch_type     = "FARGATE"
  
  network_configuration {
    subnets          = module.vpc.private_subnets
    security_groups  = [aws_security_group.ecs_tasks.id]
    assign_public_ip = false
  }
  
  deployment_configuration {
    maximum_percent         = 200
    minimum_healthy_percent = 50
  }
}

# ========================================
# Auto Scaling (Workers baseado em fila)
# ========================================
resource "aws_appautoscaling_target" "worker" {
  max_capacity       = 20
  min_capacity       = 3
  resource_id        = "service/${aws_ecs_cluster.main.name}/${aws_ecs_service.worker.name}"
  scalable_dimension = "ecs:service:DesiredCount"
  service_namespace  = "ecs"
}

resource "aws_appautoscaling_policy" "worker_scale_up" {
  name               = "transcription-worker-scale-up"
  policy_type        = "TargetTrackingScaling"
  resource_id        = aws_appautoscaling_target.worker.resource_id
  scalable_dimension = aws_appautoscaling_target.worker.scalable_dimension
  service_namespace  = aws_appautoscaling_target.worker.service_namespace
  
  target_tracking_scaling_policy_configuration {
    target_value = 70.0
    
    customized_metric_specification {
      metric_name = "QueueSize"
      namespace   = "Transcription"
      statistic   = "Average"
      
      dimensions {
        name  = "QueueName"
        value = "transcription_jobs"
      }
    }
    
    scale_in_cooldown  = 300
    scale_out_cooldown = 60
  }
}

# ========================================
# Application Load Balancer
# ========================================
resource "aws_lb" "main" {
  name               = "transcription-alb-${var.environment}"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.alb.id]
  subnets            = module.vpc.public_subnets
  
  enable_deletion_protection = var.environment == "prod"
  
  tags = {
    Name        = "transcription-alb"
    Environment = var.environment
  }
}

resource "aws_lb_target_group" "api" {
  name        = "transcription-api-tg-${var.environment}"
  port        = 8000
  protocol    = "HTTP"
  vpc_id      = module.vpc.vpc_id
  target_type = "ip"
  
  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher             = "200"
    path                = "/health"
    port                = "traffic-port"
    protocol            = "HTTP"
    timeout             = 5
    unhealthy_threshold = 3
  }
  
  deregistration_delay = 30
}

resource "aws_lb_listener" "api" {
  load_balancer_arn = aws_lb.main.arn
  port              = "443"
  protocol          = "HTTPS"
  ssl_policy        = "ELBSecurityPolicy-TLS-1-2-2017-01"
  certificate_arn   = var.acm_certificate_arn
  
  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.api.arn
  }
}

# ========================================
# CloudWatch Logs
# ========================================
resource "aws_cloudwatch_log_group" "api" {
  name              = "/ecs/transcription-api-${var.environment}"
  retention_in_days = 30
}

resource "aws_cloudwatch_log_group" "worker" {
  name              = "/ecs/transcription-worker-${var.environment}"
  retention_in_days = 30
}

# ========================================
# Secrets Manager
# ========================================
resource "aws_secretsmanager_secret" "runpod_api_key" {
  name = "transcription/${var.environment}/runpod-api-key"
}

resource "aws_secretsmanager_secret" "openai_api_key" {
  name = "transcription/${var.environment}/openai-api-key"
}

# ========================================
# IAM Roles
# ========================================
resource "aws_iam_role" "ecs_execution" {
  name = "transcription-ecs-execution-${var.environment}"
  
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "ecs-tasks.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "ecs_execution" {
  role       = aws_iam_role.ecs_execution.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy"
}

resource "aws_iam_role_policy" "ecs_execution_secrets" {
  name = "secrets-access"
  role = aws_iam_role.ecs_execution.id
  
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "secretsmanager:GetSecretValue"
        ]
        Resource = [
          aws_secretsmanager_secret.runpod_api_key.arn,
          aws_secretsmanager_secret.openai_api_key.arn
        ]
      }
    ]
  })
}

resource "aws_iam_role" "ecs_task" {
  name = "transcription-ecs-task-${var.environment}"
  
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "ecs-tasks.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy" "ecs_task_s3" {
  name = "s3-access"
  role = aws_iam_role.ecs_task.id
  
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject",
          "s3:PutObject",
          "s3:DeleteObject",
          "s3:ListBucket"
        ]
        Resource = [
          aws_s3_bucket.storage.arn,
          "${aws_s3_bucket.storage.arn}/*"
        ]
      }
    ]
  })
}

# ========================================
# Security Groups
# ========================================
resource "aws_security_group" "alb" {
  name        = "transcription-alb-${var.environment}"
  description = "Security group for ALB"
  vpc_id      = module.vpc.vpc_id
  
  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_security_group" "ecs_tasks" {
  name        = "transcription-ecs-tasks-${var.environment}"
  description = "Security group for ECS tasks"
  vpc_id      = module.vpc.vpc_id
  
  ingress {
    from_port       = 8000
    to_port         = 8000
    protocol        = "tcp"
    security_groups = [aws_security_group.alb.id]
  }
  
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_security_group" "rds" {
  name        = "transcription-rds-${var.environment}"
  description = "Security group for RDS"
  vpc_id      = module.vpc.vpc_id
  
  ingress {
    from_port       = 5432
    to_port         = 5432
    protocol        = "tcp"
    security_groups = [aws_security_group.ecs_tasks.id]
  }
}

resource "aws_security_group" "redis" {
  name        = "transcription-redis-${var.environment}"
  description = "Security group for Redis"
  vpc_id      = module.vpc.vpc_id
  
  ingress {
    from_port       = 6379
    to_port         = 6379
    protocol        = "tcp"
    security_groups = [aws_security_group.ecs_tasks.id]
  }
}

# ========================================
# Outputs
# ========================================
output "alb_dns_name" {
  value = aws_lb.main.dns_name
}

output "rds_endpoint" {
  value = aws_db_instance.postgres.endpoint
}

output "redis_endpoint" {
  value = aws_elasticache_cluster.redis.cache_nodes[0].address
}

output "s3_bucket" {
  value = aws_s3_bucket.storage.id
}
```

```hcl
# terraform/aws/variables.tf
variable "aws_region" {
  default = "us-east-1"
}

variable "environment" {
  type = string
}

variable "db_instance_class" {
  default = "db.t4g.large"
}

variable "redis_node_type" {
  default = "cache.t4g.medium"
}

variable "db_password" {
  type      = string
  sensitive = true
}

variable "ecr_registry" {
  type = string
}

variable "image_tag" {
  type = string
}

variable "acm_certificate_arn" {
  type = string
}

data "aws_caller_identity" "current" {}
```

### 14.2 Scripts de Deploy AWS

```bash
# scripts/deploy_aws.sh
#!/bin/bash
set -e

ENVIRONMENT=${1:-staging}
AWS_REGION=${2:-us-east-1}
IMAGE_TAG=${3:-latest}

echo "üöÄ Deploying to AWS ECS - Environment: $ENVIRONMENT"

# 1. Build e push de imagens para ECR
echo "üì¶ Building Docker images..."

ECR_REGISTRY=$(aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin)
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
ECR_BASE="$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com"

# Build API
docker build -t transcription-api:$IMAGE_TAG ./api
docker tag transcription-api:$IMAGE_TAG $ECR_BASE/transcription-api:$IMAGE_TAG
docker push $ECR_BASE/transcription-api:$IMAGE_TAG

# Build Worker
docker build -t transcription-worker:$IMAGE_TAG ./worker
docker tag transcription-worker:$IMAGE_TAG $ECR_BASE/transcription-worker:$IMAGE_TAG
docker push $ECR_BASE/transcription-worker:$IMAGE_TAG

echo "‚úÖ Images pushed to ECR"

# 2. Aplicar Terraform
echo "üèóÔ∏è  Applying Terraform..."
cd terraform/aws

terraform init
terraform workspace select $ENVIRONMENT || terraform workspace new $ENVIRONMENT

terraform apply \
  -var="environment=$ENVIRONMENT" \
  -var="image_tag=$IMAGE_TAG" \
  -var="ecr_registry=$ECR_BASE" \
  -auto-approve

echo "‚úÖ Infrastructure deployed"

# 3. Update ECS services (force new deployment)
echo "üîÑ Updating ECS services..."

aws ecs update-service \
  --cluster transcription-cluster-$ENVIRONMENT \
  --service transcription-api \
  --force-new-deployment \
  --region $AWS_REGION

aws ecs update-service \
  --cluster transcription-cluster-$ENVIRONMENT \
  --service transcription-worker \
  --force-new-deployment \
  --region $AWS_REGION

echo "‚úÖ Services updated"

# 4. Wait for services to stabilize
echo "‚è≥ Waiting for services to stabilize..."

aws ecs wait services-stable \
  --cluster transcription-cluster-$ENVIRONMENT \
  --services transcription-api transcription-worker \
  --region $AWS_REGION

echo "‚úÖ Deployment completed successfully!"

# 5. Show ALB URL
ALB_DNS=$(terraform output -raw alb_dns_name)
echo ""
echo "üåê Application URL: https://$ALB_DNS"
```

## 15. Deploy GCP (Cloud Run + Cloud SQL + Memorystore)

```hcl
# terraform/gcp/main.tf
terraform {
  required_version = ">= 1.0"
  
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "~> 5.0"
    }
  }
  
  backend "gcs" {
    bucket = "transcription-terraform-state"
    prefix = "prod"
  }
}

provider "google" {
  project = var.project_id
  region  = var.region
}

# ========================================
# Cloud SQL (PostgreSQL)
# ========================================
resource "google_sql_database_instance" "postgres" {
  name             = "transcription-db-${var.environment}"
  database_version = "POSTGRES_15"
  region           = var.region
  
  settings {
    tier              = var.db_tier
    availability_type = "REGIONAL"
    disk_size         = 100
    disk_autoresize   = true
    
    backup_configuration {
      enabled                        = true
      point_in_time_recovery_enabled = true
      start_time                     = "03:00"
      backup_retention_settings {
        retained_backups = 7
      }
    }
    
    ip_configuration {
      ipv4_enabled    = false
      private_network = google_compute_network.main.id
    }
    
    insights_config {
      query_insights_enabled = true
    }
  }
  
  deletion_protection = var.environment == "prod"
}

resource "google_sql_database" "main" {
  name     = "transcription_db"
  instance = google_sql_database_instance.postgres.name
}

resource "google_sql_user" "main" {
  name     = "postgres"
  instance = google_sql_database_instance.postgres.name
  password = var.db_password
}

# ========================================
# Memorystore Redis
# ========================================
resource "google_redis_instance" "redis" {
  name           = "transcription-redis-${var.environment}"
  tier           = "STANDARD_HA"
  memory_size_gb = 5
  region         = var.region
  
  redis_version     = "REDIS_7_0"
  authorized_network = google_compute_network.main.id
  
  maintenance_policy {
    weekly_maintenance_window {
      day = "SUNDAY"
      start_time {
        hours   = 3
        minutes = 0
      }
    }
  }
}

# ========================================
# Cloud Storage
# ========================================
resource "google_storage_bucket" "storage" {
  name          = "transcription-storage-${var.environment}-${var.project_id}"
  location      = var.region
  force_destroy = false
  
  lifecycle_rule {
    condition {
      age            = 30
      matches_prefix = ["audio/"]
    }
    action {
      type = "Delete"
    }
  }
  
  lifecycle_rule {
    condition {
      age = 90
    }
    action {
      type          = "SetStorageClass"
      storage_class = "NEARLINE"
    }
  }
  
  uniform_bucket_level_access = true
}

# ========================================
# Cloud Run (API)
# ========================================
resource "google_cloud_run_service" "api" {
  name     = "transcription-api"
  location = var.region
  
  template {
    spec {
      containers {
        image = "${var.artifact_registry}/${var.project_id}/transcription-api:${var.image_tag}"
        
        ports {
          container_port = 8000
        }
        
        env {
          name  = "ENVIRONMENT"
          value = var.environment
        }
        
        env {
          name  = "DATABASE_URL"
          value = "postgresql://postgres:${var.db_password}@/transcription_db?host=/cloudsql/${google_sql_database_instance.postgres.connection_name}"
        }
        
        env {
          name  = "REDIS_URL"
          value = "redis://${google_redis_instance.redis.host}:${google_redis_instance.redis.port}"
        }
        
        env {
          name  = "STORAGE_BACKEND"
          value = "gcs"
        }
        
        env {
          name  = "GCS_BUCKET_NAME"
          value = google_storage_bucket.storage.name
        }
        
        env {
          name = "RUNPOD_API_KEY"
          value_from {
            secret_key_ref {
              name = google_secret_manager_secret.runpod_api_key.secret_id
              key  = "latest"
            }
          }
        }
        
        resources {
          limits = {
            cpu    = "2"
            memory = "4Gi"
          }
        }
      }
      
      service_account_name = google_service_account.api.email
    }
    
    metadata {
      annotations = {
        "autoscaling.knative.dev/minScale"        = "3"
        "autoscaling.knative.dev/maxScale"        = "10"
        "run.googleapis.com/cloudsql-instances"   = google_sql_database_instance.postgres.connection_name
        "run.googleapis.com/vpc-access-connector" = google_vpc_access_connector.main.name
      }
    }
  }
  
  traffic {
    percent         = 100
    latest_revision = true
  }
}

# ========================================
# Cloud Run Jobs (Workers)
# ========================================
resource "google_cloud_run_v2_job" "worker" {
  name     = "transcription-worker"
  location = var.region
  
  template {
    template {
      containers {
        image = "${var.artifact_registry}/${var.project_id}/transcription-worker:${var.image_tag}"
        
        env {
          name  = "ENVIRONMENT"
          value = var.environment
        }
        
        env {
          name  = "DATABASE_URL"
          value = "postgresql://postgres:${var.db_password}@/transcription_db?host=/cloudsql/${google_sql_database_instance.postgres.connection_name}"
        }
        
        env {
          name  = "REDIS_URL"
          value = "redis://${google_redis_instance.redis.host}:${google_redis_instance.redis.port}"
        }
        
        resources {
          limits = {
            cpu    = "4"
            memory = "8Gi"
          }
        }
      }
      
      service_account = google_service_account.worker.email
      
      vpc_access {
        connector = google_vpc_access_connector.main.name
      }
    }
    
    parallelism = 5
    task_count  = 1
  }
}

# ========================================
# VPC Networking
# ========================================
resource "google_compute_network" "main" {
  name                    = "transcription-network"
  auto_create_subnetworks = false
}

resource "google_compute_subnetwork" "main" {
  name          = "transcription-subnet"
  ip_cidr_range = "10.0.0.0/24"
  region        = var.region
  network       = google_compute_network.main.id
}

resource "google_vpc_access_connector" "main" {
  name          = "transcription-connector"
  region        = var.region
  ip_cidr_range = "10.8.0.0/28"
  network       = google_compute_network.main.name
}

# ========================================
# Service Accounts
# ========================================
resource "google_service_account" "api" {
  account_id   = "transcription-api"
  display_name = "Transcription API Service Account"
}

resource "google_service_account" "worker" {
  account_id   = "transcription-worker"
  display_name = "Transcription Worker Service Account"
}

# IAM bindings
resource "google_storage_bucket_iam_member" "api_storage" {
  bucket = google_storage_bucket.storage.name
  role   = "roles/storage.objectAdmin"
  member = "serviceAccount:${google_service_account.api.email}"
}

resource "google_storage_bucket_iam_member" "worker_storage" {
  bucket = google_storage_bucket.storage.name
  role   = "roles/storage.objectAdmin"
  member = "serviceAccount:${google_service_account.worker.email}"
}

# ========================================
# Secret Manager
# ========================================
resource "google_secret_manager_secret" "runpod_api_key" {
  secret_id = "runpod-api-key"
  
  replication {
    automatic = true
  }
}

# ========================================
# Outputs
# ========================================
output "api_url" {
  value = google_cloud_run_service.api.status[0].url
}

output "db_connection_name" {
  value = google_sql_database_instance.postgres.connection_name
}
```

```bash
# scripts/deploy_gcp.sh
#!/bin/bash
set -e

ENVIRONMENT=${1:-staging}
PROJECT_ID=${2:-your-project-id}
REGION=${3:-us-central1}
IMAGE_TAG=${4:-latest}

echo "üöÄ Deploying to GCP Cloud Run - Environment: $ENVIRONMENT"

# 1. Build e push para Artifact Registry
echo "üì¶ Building Docker images..."

gcloud auth configure-docker ${REGION}-docker.pkg.dev

AR_BASE="${REGION}-docker.pkg.dev/${PROJECT_ID}/transcription"

# Build e push API
docker build -t transcription-api:$IMAGE_TAG ./api
docker tag transcription-api:$IMAGE_TAG $AR_BASE/transcription-api:$IMAGE_TAG
docker push $AR_BASE/transcription-api:$IMAGE_TAG

# Build e push Worker
docker build -t transcription-worker:$IMAGE_TAG ./worker
docker tag transcription-worker:$IMAGE_TAG $AR_BASE/transcription-worker:$IMAGE_TAG
docker push $AR_BASE/transcription-worker:$IMAGE_TAG

echo "‚úÖ Images pushed to Artifact Registry"

# 2. Aplicar Terraform
echo "üèóÔ∏è  Applying Terraform..."
cd terraform/gcp

terraform init
terraform workspace select $ENVIRONMENT || terraform workspace new $ENVIRONMENT

terraform apply \
  -var="environment=$ENVIRONMENT" \
  -var="project_id=$PROJECT_ID" \
  -var="region=$REGION" \
  -var="image_tag=$IMAGE_TAG" \
  -auto-approve

echo "‚úÖ Infrastructure deployed"

# 3. Deploy Cloud Run service
echo "üîÑ Deploying Cloud Run service..."

gcloud run services update transcription-api \
  --platform managed \
  --region $REGION \
  --image $AR_BASE/transcription-api:$IMAGE_TAG

echo "‚úÖ Deployment completed successfully!"

# 4. Show service URL
SERVICE_URL=$(gcloud run services describe transcription-api \
  --platform managed \
  --region $REGION \
  --format 'value(status.url)')

echo ""
echo "üåê Application URL: $SERVICE_URL"
```

## 16. CI/CD Pipeline (GitHub Actions)

```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  AWS_REGION: us-east-1
  GCP_REGION: us-central1
  GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}

jobs:
  # ========================================
  # Tests e Lint
  # ========================================
  test:
    name: Tests & Linting
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov black flake8
      
      - name: Lint with black
        run: black --check api/ worker/ shared/
      
      - name: Lint with flake8
        run: flake8 api/ worker/ shared/ --max-line-length=100
      
      - name: Run tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379
        run: |
          pytest tests/ -v --cov=. --cov-report=xml
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml
  
  # ========================================
  # Build Docker Images
  # ========================================
  build:
    name: Build Docker Images
    runs-on: ubuntu-latest
    needs: test
    
    strategy:
      matrix:
        service: [api, worker, runpod_worker]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      
      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: ./${{ matrix.service }}
          push: true
          tags: |
            ${{ secrets.DOCKERHUB_USERNAME }}/transcription-${{ matrix.service }}:${{ github.sha }}
            ${{ secrets.DOCKERHUB_USERNAME }}/transcription-${{ matrix.service }}:latest
          cache-from: type=registry,ref=${{ secrets.DOCKERHUB_USERNAME }}/transcription-${{ matrix.service }}:buildcache
          cache-to: type=registry,ref=${{ secrets.DOCKERHUB_USERNAME }}/transcription-${{ matrix.service }}:buildcache,mode=max
  
  # ========================================
  # Deploy to RunPod
  # ========================================
  deploy-runpod:
    name: Deploy RunPod Worker
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Deploy to RunPod
        env:
          RUNPOD_API_KEY: ${{ secrets.RUNPOD_API_KEY }}
          RUNPOD_ENDPOINT_ID: ${{ secrets.RUNPOD_ENDPOINT_ID }}
        run: |
          bash scripts/deploy_runpod_worker.sh
  
  # ========================================
  # Deploy to AWS (Staging)
  # ========================================
  deploy-aws-staging:
    name: Deploy to AWS Staging
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/develop'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2
      
      - name: Deploy to staging
        run: |
          bash scripts/deploy_aws.sh staging ${{ env.AWS_REGION }} ${{ github.sha }}
  
  # ========================================
  # Deploy to AWS (Production)
  # ========================================
  deploy-aws-prod:
    name: Deploy to AWS Production
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Deploy to production
        run: |
          bash scripts/deploy_aws.sh prod ${{ env.AWS_REGION }} ${{ github.sha }}
  
  # ========================================
  # Deploy to GCP
  # ========================================
  deploy-gcp:
    name: Deploy to GCP
    runs-on: ubuntu-latest
    needs: build
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
      
      - name: Deploy to GCP
        run: |
          bash scripts/deploy_gcp.sh prod ${{ env.GCP_PROJECT_ID }} ${{ env.GCP_REGION }} ${{ github.sha }}
```

## 17. Scripts de Manuten√ß√£o e Opera√ß√£o

### 17.1 Scripts de Backup

```bash
# scripts/backup.sh
#!/bin/bash
set -e

ENVIRONMENT=${1:-prod}
BACKUP_TYPE=${2:-full}  # full, incremental
DATE=$(date +%Y%m%d_%H%M%S)

echo "üîí Starting backup - Environment: $ENVIRONMENT, Type: $BACKUP_TYPE"

# ========================================
# Backup PostgreSQL
# ========================================
backup_postgres() {
    echo "üì¶ Backing up PostgreSQL..."
    
    if [[ "$ENVIRONMENT" == "aws" ]]; then
        # AWS RDS snapshot
        DB_INSTANCE="transcription-db-$ENVIRONMENT"
        SNAPSHOT_ID="transcription-db-manual-$DATE"
        
        aws rds create-db-snapshot \
            --db-instance-identifier $DB_INSTANCE \
            --db-snapshot-identifier $SNAPSHOT_ID \
            --tags Key=Type,Value=manual Key=Date,Value=$DATE
        
        echo "‚úì RDS snapshot created: $SNAPSHOT_ID"
        
    elif [[ "$ENVIRONMENT" == "gcp" ]]; then
        # GCP Cloud SQL backup
        INSTANCE_NAME="transcription-db-$ENVIRONMENT"
        
        gcloud sql backups create \
            --instance=$INSTANCE_NAME \
            --description="Manual backup $DATE"
        
        echo "‚úì Cloud SQL backup created"
        
    else
        # Manual pg_dump
        BACKUP_FILE="backups/postgres_${ENVIRONMENT}_${DATE}.sql.gz"
        
        pg_dump $DATABASE_URL | gzip > $BACKUP_FILE
        
        # Upload para S3/GCS
        if command -v aws &> /dev/null; then
            aws s3 cp $BACKUP_FILE s3://transcription-backups/postgres/
        elif command -v gsutil &> /dev/null; then
            gsutil cp $BACKUP_FILE gs://transcription-backups/postgres/
        fi
        
        echo "‚úì Database backup saved: $BACKUP_FILE"
    fi
}

# ========================================
# Backup Redis (se necess√°rio)
# ========================================
backup_redis() {
    echo "üì¶ Backing up Redis..."
    
    if [[ "$ENVIRONMENT" == "aws" ]]; then
        CLUSTER_ID="transcription-redis-$ENVIRONMENT"
        SNAPSHOT_NAME="redis-manual-$DATE"
        
        aws elasticache create-snapshot \
            --cache-cluster-id $CLUSTER_ID \
            --snapshot-name $SNAPSHOT_NAME
        
        echo "‚úì Redis snapshot created: $SNAPSHOT_NAME"
    fi
}

# ========================================
# Backup Storage (metadata)
# ========================================
backup_storage_metadata() {
    echo "üì¶ Backing up storage metadata..."
    
    # Export metadata de arquivos do S3/GCS
    METADATA_FILE="backups/storage_metadata_${ENVIRONMENT}_${DATE}.json"
    
    if [[ "$STORAGE_BACKEND" == "s3" ]]; then
        aws s3api list-objects-v2 \
            --bucket $S3_BUCKET_NAME \
            --output json > $METADATA_FILE
    elif [[ "$STORAGE_BACKEND" == "gcs" ]]; then
        gsutil ls -L gs://$GCS_BUCKET_NAME/** > $METADATA_FILE
    fi
    
    echo "‚úì Storage metadata saved: $METADATA_FILE"
}

# ========================================
# Executar backups
# ========================================
mkdir -p backups

if [[ "$BACKUP_TYPE" == "full" ]]; then
    backup_postgres
    backup_redis
    backup_storage_metadata
else
    backup_postgres
fi

# ========================================
# Cleanup backups antigos (manter √∫ltimos 30 dias)
# ========================================
echo "üßπ Cleaning up old backups..."
find backups/ -name "*.sql.gz" -mtime +30 -delete
find backups/ -name "*.json" -mtime +30 -delete

echo "‚úÖ Backup completed successfully!"
```

### 17.2 Script de Rollback

```bash
# scripts/rollback.sh
#!/bin/bash
set -e

ENVIRONMENT=${1:-staging}
TARGET_VERSION=${2:-previous}  # previous, ou tag espec√≠fico

echo "‚ö†Ô∏è  Rolling back - Environment: $ENVIRONMENT to version: $TARGET_VERSION"

# ========================================
# Get previous version
# ========================================
get_previous_version() {
    if [[ "$ENVIRONMENT" == "aws" ]]; then
        # Get previous task definition
        CLUSTER="transcription-cluster-$ENVIRONMENT"
        SERVICE="transcription-api"
        
        CURRENT_TASK_DEF=$(aws ecs describe-services \
            --cluster $CLUSTER \
            --services $SERVICE \
            --query 'services[0].taskDefinition' \
            --output text)
        
        # Get previous revision
        FAMILY=$(echo $CURRENT_TASK_DEF | sed 's/:.*//')
        CURRENT_REV=$(echo $CURRENT_TASK_DEF | sed 's/.*://')
        PREVIOUS_REV=$((CURRENT_REV - 1))
        
        echo "$FAMILY:$PREVIOUS_REV"
        
    elif [[ "$ENVIRONMENT" == "gcp" ]]; then
        # Get previous Cloud Run revision
        SERVICE_NAME="transcription-api"
        REGION="us-central1"
        
        gcloud run revisions list \
            --service=$SERVICE_NAME \
            --region=$REGION \
            --format='value(name)' \
            --limit=2 | tail -n 1
    fi
}

# ========================================
# Rollback AWS ECS
# ========================================
rollback_aws() {
    echo "üîÑ Rolling back AWS ECS..."
    
    CLUSTER="transcription-cluster-$ENVIRONMENT"
    PREVIOUS_TASK_DEF=$(get_previous_version)
    
    echo "Previous task definition: $PREVIOUS_TASK_DEF"
    
    # Rollback API
    aws ecs update-service \
        --cluster $CLUSTER \
        --service transcription-api \
        --task-definition $PREVIOUS_TASK_DEF \
        --force-new-deployment
    
    # Rollback Worker
    aws ecs update-service \
        --cluster $CLUSTER \
        --service transcription-worker \
        --task-definition $PREVIOUS_TASK_DEF \
        --force-new-deployment
    
    # Wait for stable
    echo "‚è≥ Waiting for services to stabilize..."
    aws ecs wait services-stable \
        --cluster $CLUSTER \
        --services transcription-api transcription-worker
    
    echo "‚úì AWS rollback completed"
}

# ========================================
# Rollback GCP Cloud Run
# ========================================
rollback_gcp() {
    echo "üîÑ Rolling back GCP Cloud Run..."
    
    SERVICE_NAME="transcription-api"
    REGION="us-central1"
    PREVIOUS_REVISION=$(get_previous_version)
    
    echo "Previous revision: $PREVIOUS_REVISION"
    
    # Update traffic to previous revision
    gcloud run services update-traffic $SERVICE_NAME \
        --region=$REGION \
        --to-revisions=$PREVIOUS_REVISION=100
    
    echo "‚úì GCP rollback completed"
}

# ========================================
# Verificar health ap√≥s rollback
# ========================================
verify_health() {
    echo "üè• Verifying health..."
    
    if [[ "$ENVIRONMENT" == "aws" ]]; then
        ALB_DNS=$(terraform -chdir=terraform/aws output -raw alb_dns_name)
        HEALTH_URL="https://$ALB_DNS/health"
    elif [[ "$ENVIRONMENT" == "gcp" ]]; then
        SERVICE_URL=$(gcloud run services describe transcription-api \
            --region us-central1 \
            --format 'value(status.url)')
        HEALTH_URL="$SERVICE_URL/health"
    fi
    
    # Wait 30s for deployment
    sleep 30
    
    # Check health
    for i in {1..5}; do
        if curl -f -s $HEALTH_URL > /dev/null; then
            echo "‚úì Health check passed"
            return 0
        fi
        echo "Attempt $i/5 failed, retrying..."
        sleep 10
    done
    
    echo "‚ùå Health check failed!"
    return 1
}

# ========================================
# Executar rollback
# ========================================
if [[ "$ENVIRONMENT" == "aws" ]]; then
    rollback_aws
elif [[ "$ENVIRONMENT" == "gcp" ]]; then
    rollback_gcp
else
    echo "‚ùå Unknown environment: $ENVIRONMENT"
    exit 1
fi

verify_health

echo "‚úÖ Rollback completed successfully!"
```

### 17.3 Script de Monitoramento

```bash
# scripts/monitor.sh
#!/bin/bash

ENVIRONMENT=${1:-prod}
CHECK_TYPE=${2:-all}  # all, health, metrics, costs

echo "üìä Monitoring - Environment: $ENVIRONMENT"

# ========================================
# Health Checks
# ========================================
check_health() {
    echo "üè• Checking service health..."
    
    # API health
    if curl -f -s $API_URL/health > /dev/null; then
        echo "‚úì API: healthy"
    else
        echo "‚ùå API: unhealthy"
        send_alert "API_DOWN" "API health check failed"
    fi
    
    # Database
    if pg_isready -d $DATABASE_URL > /dev/null 2>&1; then
        echo "‚úì Database: connected"
    else
        echo "‚ùå Database: connection failed"
        send_alert "DB_DOWN" "Database connection failed"
    fi
    
    # Redis
    if redis-cli -u $REDIS_URL ping > /dev/null 2>&1; then
        echo "‚úì Redis: connected"
    else
        echo "‚ùå Redis: connection failed"
        send_alert "REDIS_DOWN" "Redis connection failed"
    fi
    
    # RunPod endpoint
    RUNPOD_HEALTH=$(curl -s -H "Authorization: Bearer $RUNPOD_API_KEY" \
        https://api.runpod.ai/v2/$RUNPOD_ENDPOINT_ID/health)
    
    if [[ "$RUNPOD_HEALTH" == *"healthy"* ]]; then
        echo "‚úì RunPod: online"
    else
        echo "‚ö†Ô∏è  RunPod: degraded"
    fi
}

# ========================================
# Metrics Check
# ========================================
check_metrics() {
    echo "üìà Checking metrics..."
    
    # Query PostgreSQL metrics
    psql $DATABASE_URL -c "
        SELECT 
            status,
            COUNT(*) as count,
            AVG(EXTRACT(EPOCH FROM (completed_at - started_at))) as avg_duration
        FROM jobs
        WHERE created_at >= NOW() - INTERVAL '1 hour'
        GROUP BY status;
    "
    
    # Queue size
    QUEUE_SIZE=$(redis-cli -u $REDIS_URL LLEN transcription_jobs)
    echo "Queue size: $QUEUE_SIZE"
    
    if [[ $QUEUE_SIZE -gt 100 ]]; then
        echo "‚ö†Ô∏è  Large queue detected"
        send_alert "LARGE_QUEUE" "Queue size: $QUEUE_SIZE"
    fi
    
    # Failed jobs (last hour)
    FAILED_JOBS=$(psql $DATABASE_URL -t -c "
        SELECT COUNT(*) FROM jobs 
        WHERE status = 'failed' 
        AND created_at >= NOW() - INTERVAL '1 hour';
    ")
    
    FAILED_JOBS=$(echo $FAILED_JOBS | xargs)  # trim whitespace
    
    echo "Failed jobs (last hour): $FAILED_JOBS"
    
    if [[ $FAILED_JOBS -gt 10 ]]; then
        echo "‚ùå High failure rate detected"
        send_alert "HIGH_FAILURE_RATE" "Failed jobs: $FAILED_JOBS"
    fi
}

# ========================================
# Cost Check
# ========================================
check_costs() {
    echo "üí∞ Checking costs..."
    
    # Query cost metrics (last 24h)
    psql $DATABASE_URL -c "
        SELECT 
            backend,
            COUNT(*) as jobs,
            SUM(cost_total) as total_cost,
            AVG(cost_per_audio_hour) as avg_cost_per_hour
        FROM job_metrics
        WHERE created_at >= NOW() - INTERVAL '24 hours'
        GROUP BY backend
        ORDER BY total_cost DESC;
    "
    
    # Check if costs exceeded threshold
    DAILY_COST=$(psql $DATABASE_URL -t -c "
        SELECT SUM(cost_total) FROM job_metrics 
        WHERE created_at >= NOW() - INTERVAL '24 hours';
    ")
    
    DAILY_COST=$(echo $DAILY_COST | xargs)
    
    echo "Total cost (24h): \$$DAILY_COST"
    
    # Alert if > $100/day
    if (( $(echo "$DAILY_COST > 100" | bc -l) )); then
        echo "‚ö†Ô∏è  High daily cost"
        send_alert "HIGH_COST" "Daily cost: \$$DAILY_COST"
    fi
}

# ========================================
# Send Alert
# ========================================
send_alert() {
    ALERT_TYPE=$1
    MESSAGE=$2
    
    # Slack webhook
    if [[ -n "$SLACK_WEBHOOK_URL" ]]; then
        curl -X POST $SLACK_WEBHOOK_URL \
            -H 'Content-Type: application/json' \
            -d "{
                \"text\": \"üö® [$ENVIRONMENT] $ALERT_TYPE\",
                \"blocks\": [
                    {
                        \"type\": \"section\",
                        \"text\": {
                            \"type\": \"mrkdwn\",
                            \"text\": \"*Environment:* $ENVIRONMENT\\n*Alert:* $ALERT_TYPE\\n*Message:* $MESSAGE\"
                        }
                    }
                ]
            }"
    fi
    
    # PagerDuty (para alertas cr√≠ticos)
    if [[ "$ALERT_TYPE" == "API_DOWN" ]] || [[ "$ALERT_TYPE" == "DB_DOWN" ]]; then
        if [[ -n "$PAGERDUTY_INTEGRATION_KEY" ]]; then
            curl -X POST https://events.pagerduty.com/v2/enqueue \
                -H 'Content-Type: application/json' \
                -d "{
                    \"routing_key\": \"$PAGERDUTY_INTEGRATION_KEY\",
                    \"event_action\": \"trigger\",
                    \"payload\": {
                        \"summary\": \"$ALERT_TYPE: $MESSAGE\",
                        \"severity\": \"critical\",
                        \"source\": \"$ENVIRONMENT\"
                    }
                }"
        fi
    fi
}

# ========================================
# Executar checks
# ========================================
case $CHECK_TYPE in
    health)
        check_health
        ;;
    metrics)
        check_metrics
        ;;
    costs)
        check_costs
        ;;
    all)
        check_health
        echo ""
        check_metrics
        echo ""
        check_costs
        ;;
    *)
        echo "Unknown check type: $CHECK_TYPE"
        exit 1
        ;;
esac

echo ""
echo "‚úÖ Monitoring completed"
```

### 17.4 Cron Jobs para Manuten√ß√£o

```bash
# scripts/setup_cron.sh
#!/bin/bash

echo "‚è∞ Setting up cron jobs..."

# Criar crontab
cat > /tmp/transcription_cron << 'EOF'
# Backup di√°rio (3 AM)
0 3 * * * /app/scripts/backup.sh prod full >> /var/log/transcription/backup.log 2>&1

# Monitoramento a cada 5 minutos
*/5 * * * * /app/scripts/monitor.sh prod all >> /var/log/transcription/monitor.log 2>&1

# Limpeza de jobs antigos (di√°rio √†s 2 AM)
0 2 * * * /app/scripts/cleanup.sh prod >> /var/log/transcription/cleanup.log 2>&1

# Relat√≥rio de custos (di√°rio √†s 9 AM)
0 9 * * * /app/scripts/cost_report.sh prod >> /var/log/transcription/cost_report.log 2>&1

# Otimiza√ß√£o de DB (semanal, domingo 4 AM)
0 4 * * 0 /app/scripts/db_maintenance.sh prod >> /var/log/transcription/db_maintenance.log 2>&1
EOF

# Instalar crontab
crontab /tmp/transcription_cron

echo "‚úÖ Cron jobs installed"
```

### 17.5 Script de Limpeza

```bash
# scripts/cleanup.sh
#!/bin/bash
set -e

ENVIRONMENT=${1:-prod}
DRY_RUN=${2:-false}

echo "üßπ Cleanup - Environment: $ENVIRONMENT, Dry run: $DRY_RUN"

# ========================================
# Cleanup jobs antigos (> 90 dias)
# ========================================
cleanup_old_jobs() {
    echo "Cleaning up old jobs..."
    
    QUERY="DELETE FROM jobs WHERE status = 'completed' AND completed_at < NOW() - INTERVAL '90 days'"
    
    if [[ "$DRY_RUN" == "true" ]]; then
        COUNT=$(psql $DATABASE_URL -t -c "SELECT COUNT(*) FROM jobs WHERE status = 'completed' AND completed_at < NOW() - INTERVAL '90 days'")
        echo "Would delete $COUNT jobs"
    else
        psql $DATABASE_URL -c "$QUERY"
        echo "‚úì Old jobs deleted"
    fi
}

# ========================================
# Cleanup storage (√°udios tempor√°rios > 30 dias)
# ========================================
cleanup_old_audio() {
    echo "Cleaning up old audio files..."
    
    if [[ "$STORAGE_BACKEND" == "s3" ]]; then
        if [[ "$DRY_RUN" == "true" ]]; then
            aws s3 ls s3://$S3_BUCKET_NAME/temp/ --recursive | \
                awk '$1 <= "'$(date -d '30 days ago' +%Y-%m-%d)'" {print $4}' | wc -l
        else
            # S3 lifecycle j√° cuida disso, mas podemos for√ßar
            aws s3 rm s3://$S3_BUCKET_NAME/temp/ --recursive \
                --exclude "*" \
                --include "*" \
                --older-than 30d
        fi
    fi
}

# ========================================
# Cleanup m√©tricas antigas (> 180 dias)
# ========================================
cleanup_old_metrics() {
    echo "Cleaning up old metrics..."
    
    QUERY="DELETE FROM job_metrics WHERE created_at < NOW() - INTERVAL '180 days'"
    
    if [[ "$DRY_RUN" == "true" ]]; then
        COUNT=$(psql $DATABASE_URL -t -c "SELECT COUNT(*) FROM job_metrics WHERE created_at < NOW() - INTERVAL '180 days'")
        echo "Would delete $COUNT metric records"
    else
        psql $DATABASE_URL -c "$QUERY"
        echo "‚úì Old metrics deleted"
    fi
}

# ========================================
# Vacuum database
# ========================================
vacuum_database() {
    echo "Vacuuming database..."
    
    if [[ "$DRY_RUN" == "false" ]]; then
        psql $DATABASE_URL -c "VACUUM ANALYZE jobs;"
        psql $DATABASE_URL -c "VACUUM ANALYZE transcriptions;"
        psql $DATABASE_URL -c "VACUUM ANALYZE job_metrics;"
        echo "‚úì Database vacuumed"
    fi
}

# ========================================
# Executar limpeza
# ========================================
cleanup_old_jobs
cleanup_old_audio
cleanup_old_metrics
vacuum_database

echo "‚úÖ Cleanup completed"
```

### 17.6 Relat√≥rio de Custos

```bash
# scripts/cost_report.sh
#!/bin/bash

ENVIRONMENT=${1:-prod}
PERIOD=${2:-7}  # dias

echo "üí∞ Cost Report - Environment: $ENVIRONMENT, Period: $PERIOD days"

# ========================================
# Generate HTML report
# ========================================
REPORT_FILE="/tmp/cost_report_$(date +%Y%m%d).html"

cat > $REPORT_FILE << EOF
<!DOCTYPE html>
<html>
<head>
    <title>Transcription Service - Cost Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        table { border-collapse: collapse; width: 100%; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }
        th { background-color: #4CAF50; color: white; }
        tr:nth-child(even) { background-color: #f2f2f2; }
        .summary { background-color: #e7f3fe; padding: 15px; margin: 20px 0; border-left: 6px solid #2196F3; }
        .warning { color: #ff9800; }
        .critical { color: #f44336; }
    </style>
</head>
<body>
    <h1>Cost Report - $ENVIRONMENT</h1>
    <p>Period: Last $PERIOD days | Generated: $(date)</p>
    
    <div class="summary">
        <h2>Summary</h2>
EOF

# Query total costs
TOTAL_COST=$(psql $DATABASE_URL -t -c "
    SELECT ROUND(SUM(cost_total)::numeric, 2) 
    FROM job_metrics 
    WHERE created_at >= NOW() - INTERVAL '$PERIOD days'
")

TOTAL_JOBS=$(psql $DATABASE_URL -t -c "
    SELECT COUNT(*) 
    FROM job_metrics 
    WHERE created_at >= NOW() - INTERVAL '$PERIOD days'
")

TOTAL_HOURS=$(psql $DATABASE_URL -t -c "
    SELECT ROUND((SUM(audio_duration_seconds) / 3600)::numeric, 2) 
    FROM job_metrics 
    WHERE created_at >= NOW() - INTERVAL '$PERIOD days'
")

cat >> $REPORT_FILE << EOF
        <p><strong>Total Cost:</strong> \$$TOTAL_COST</p>
        <p><strong>Total Jobs:</strong> $TOTAL_JOBS</p>
        <p><strong>Total Audio Hours:</strong> $TOTAL_HOURS</p>
        <p><strong>Average Cost/Hour:</strong> \$$(echo "scale=4; $TOTAL_COST / $TOTAL_HOURS" | bc)</p>
    </div>
    
    <h2>Cost by Backend</h2>
    <table>
        <tr>
            <th>Backend</th>
            <th>Jobs</th>
            <th>Audio Hours</th>
            <th>Total Cost</th>
            <th>Avg Cost/Hour</th>
            <th>% of Total</th>
        </tr>
EOF

# Query by backend
psql $DATABASE_URL -H -c "
    SELECT 
        backend,
        COUNT(*) as jobs,
        ROUND((SUM(audio_duration_seconds) / 3600)::numeric, 2) as audio_hours,
        ROUND(SUM(cost_total)::numeric, 2) as total_cost,
        ROUND(AVG(cost_per_audio_hour)::numeric, 4) as avg_cost_per_hour,
        ROUND((SUM(cost_total) / (SELECT SUM(cost_total) FROM job_metrics WHERE created_at >= NOW() - INTERVAL '$PERIOD days') * 100)::numeric, 1) as pct_of_total
    FROM job_metrics
    WHERE created_at >= NOW() - INTERVAL '$PERIOD days'
    GROUP BY backend
    ORDER BY total_cost DESC
" | tail -n +4 | head -n -1 >> $REPORT_FILE

cat >> $REPORT_FILE << EOF
    </table>
    
    <h2>Daily Breakdown</h2>
    <table>
        <tr>
            <th>Date</th>
            <th>Jobs</th>
            <th>Cost</th>
        </tr>
EOF

# Daily breakdown
psql $DATABASE_URL -H -c "
    SELECT 
        DATE(created_at) as date,
        COUNT(*) as jobs,
        ROUND(SUM(cost_total)::numeric, 2) as cost
    FROM job_metrics
    WHERE created_at >= NOW() - INTERVAL '$PERIOD days'
    GROUP BY DATE(created_at)
    ORDER BY date DESC
" | tail -n +4 | head -n -1 >> $REPORT_FILE

cat >> $REPORT_FILE << EOF
    </table>
    
    <h2>Top 10 Most Expensive Jobs</h2>
    <table>
        <tr>
            <th>Job ID</th>
            <th>Tenant</th>
            <th>Backend</th>
            <th>Duration (min)</th>
            <th>Cost</th>
            <th>Date</th>
        </tr>
EOF

# Top expensive jobs
psql $DATABASE_URL -H -c "
    SELECT 
        job_id,
        tenant_id,
        backend,
        ROUND((audio_duration_seconds / 60)::numeric, 1) as duration_min,
        ROUND(cost_total::numeric, 4) as cost,
        created_at::date as date
    FROM job_metrics
    WHERE created_at >= NOW() - INTERVAL '$PERIOD days'
    ORDER BY cost_total DESC
    LIMIT 10
" | tail -n +4 | head -n -1 >> $REPORT_FILE

cat >> $REPORT_FILE << EOF
    </table>
    
    <h2>Recommendations</h2>
    <div class="summary">
EOF

# Generate recommendations
AVG_COST_OPENAI=$(psql $DATABASE_URL -t -c "
    SELECT ROUND(AVG(cost_per_audio_hour)::numeric, 4) 
    FROM job_metrics 
    WHERE backend = 'openai' 
    AND created_at >= NOW() - INTERVAL '$PERIOD days'
")

AVG_COST_RUNPOD=$(psql $DATABASE_URL -t -c "
    SELECT ROUND(AVG(cost_per_audio_hour)::numeric, 4) 
    FROM job_metrics 
    WHERE backend = 'runpod' 
    AND created_at >= NOW() - INTERVAL '$PERIOD days'
")

if (( $(echo "$AVG_COST_OPENAI > $AVG_COST_RUNPOD * 1.5" | bc -l) )); then
    cat >> $REPORT_FILE << EOF
        <p class="warning">‚ö†Ô∏è OpenAI average cost (\$$AVG_COST_OPENAI/h) is significantly higher than RunPod (\$$AVG_COST_RUNPOD/h). Consider migrating more traffic to RunPod.</p>
EOF
fi

# Check if daily cost > threshold
DAILY_AVG=$(echo "scale=2; $TOTAL_COST / $PERIOD" | bc)
if (( $(echo "$DAILY_AVG > 50" | bc -l) )); then
    cat >> $REPORT_FILE << EOF
        <p class="critical">üö® Daily average cost (\$$DAILY_AVG) exceeds \$50. Review usage patterns.</p>
EOF
fi

cat >> $REPORT_FILE << EOF
    </div>
</body>
</html>
EOF

echo "‚úÖ Report generated: $REPORT_FILE"

# ========================================
# Send report via email (opcional)
# ========================================
if [[ -n "$REPORT_EMAIL" ]]; then
    echo "üìß Sending report to $REPORT_EMAIL..."
    
    # Using AWS SES
    aws ses send-email \
        --from "reports@transcription-service.com" \
        --to "$REPORT_EMAIL" \
        --subject "Cost Report - $ENVIRONMENT ($PERIOD days)" \
        --html file://$REPORT_FILE
    
    echo "‚úì Report sent"
fi

# Upload to S3
if [[ -n "$S3_BUCKET_NAME" ]]; then
    aws s3 cp $REPORT_FILE s3://$S3_BUCKET_NAME/reports/
    echo "‚úì Report uploaded to S3"
fi
```

## 18. Dashboard Grafana Completo

```json
// grafana/dashboards/transcription-complete.json
{
  "dashboard": {
    "title": "Transcription Service - Complete Overview",
    "tags": ["transcription", "production"],
    "timezone": "browser",
    "refresh": "30s",
    "panels": [
      {
        "id": 1,
        "title": "System Health",
        "type": "stat",
        "gridPos": {"x": 0, "y": 0, "w": 6, "h": 4},
        "targets": [
          {
            "expr": "up{job=\"api\"}",
            "legendFormat": "API Status"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "mappings": [
              {"value": 1, "text": "Healthy", "color": "green"},
              {"value": 0, "text": "Down", "color": "red"}
            ]
          }
        }
      },
      {
        "id": 2,
        "title": "Jobs/min (by Backend)",
        "type": "graph",
        "gridPos": {"x": 6, "y": 0, "w": 12, "h": 8},
        "targets": [
          {
            "expr": "sum by (backend) (rate(transcription_jobs_total[1m]) * 60)",
            "legendFormat": "{{backend}}"
          }
        ],
        "yaxes": [
          {"format": "short", "label": "Jobs/min"}
        ]
      },
      {
        "id": 3,
        "title": "Success Rate",
        "type": "gauge",
        "gridPos": {"x": 18, "y": 0, "w": 6, "h": 4},
        "targets": [
          {
            "expr": "sum(rate(transcription_jobs_total{status=\"success\"}[5m])) / sum(rate(transcription_jobs_total[5m])) * 100"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "steps": [
                {"value": 0, "color": "red"},
                {"value": 95, "color": "yellow"},
                {"value": 99, "color": "green"}
              ]
            }
          }
        }
      },
      {
        "id": 4,
        "title": "Latency P50/P95/P99",
        "type": "graph",
        "gridPos": {"x": 0, "y": 8, "w": 12, "h": 8},
        "targets": [
          {
            "expr": "histogram_quantile(0.50, sum by (backend, le) (rate(transcription_job_duration_seconds_bucket[5m])))",
            "legendFormat": "{{backend}} P50"
          },
          {
            "expr": "histogram_quantile(0.95, sum by (backend, le) (rate(transcription_job_duration_seconds_bucket[5m])))",
            "legendFormat": "{{backend}} P95"
          },
          {
            "expr": "histogram_quantile(0.99, sum by (backend, le) (rate(transcription_job_duration_seconds_bucket[5m])))",
            "legendFormat": "{{backend}} P99"
          }
        ],
        "yaxes": [
          {"format": "s", "label": "Latency"}
        ]
      },
      {
        "id": 5,
        "title": "Queue Size",
        "type": "graph",
        "gridPos": {"x": 12, "y": 8, "w": 12, "h": 8},
        "targets": [
          {
            "expr": "transcription_queue_size"
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": {"params": [100], "type": "gt"},
              "query": {"params": ["A", "5m", "now"]},
              "type": "query"
            }
          ],
          "frequency": "1m",
          "handler": 1,
          "name": "Large Queue Alert"
        }
      },
      {
        "id": 6,
        "title": "Cost per Hour (by Backend)",
        "type": "graph",
        "gridPos": {"x": 0, "y": 16, "w": 12, "h": 8},
        "targets": [
          {
            "expr": "sum by (backend) (rate(transcription_cost_usd_total[1h]))",
            "legendFormat": "{{backend}}"
          }
        ],
        "yaxes": [
          {"format": "currencyUSD", "label": "USD/hour"}
        ]
      },
      {
        "id": 7,
        "title": "RTF Distribution",
        "type": "heatmap",
        "gridPos": {"x": 12, "y": 16, "w": 12, "h": 8},
        "targets": [
          {
            "expr": "sum by (le) (rate(transcription_rtf_bucket[5m]))",
            "format": "heatmap"
          }
        ],
        "dataFormat": "tsbuckets"
      },
      {
        "id": 8,
        "title": "Active Workers",
        "type": "stat",
        "gridPos": {"x": 0, "y": 24, "w": 6, "h": 4},
        "targets": [
          {
            "expr": "sum(transcription_active_workers)"
          }
        ]
      },
      {
        "id": 9,
        "title": "Error Rate by Type",
        "type": "piechart",
        "gridPos": {"x": 6, "y": 24, "w": 6, "h": 8},
        "targets": [
          {
            "expr": "sum by (error_type) (rate(transcription_jobs_failed_total[1h]))"
          }
        ]
      },
      {
        "id": 10,
        "title": "Cost Efficiency (Cost per Audio Hour)",
        "type": "table",
        "gridPos": {"x": 12, "y": 24, "w": 12, "h": 8},
        "targets": [
          {
            "expr": "avg by (backend) (transcription_cost_per_audio_hour)",
            "format": "table",
            "instant": true
          }
        ],
        "transformations": [
          {
            "id": "organize",
            "options": {
              "renameByName": {
                "backend": "Backend",
                "Value": "Cost per Audio Hour (USD)"
              }
            }
          }
        ]
      }
    ],
    "time": {
      "from": "now-6h",
      "to": "now"
    },
    "timepicker": {
      "refresh_intervals": ["10s", "30s", "1m", "5m", "15m"]
    },
    "annotations": {
      "list": [
        {
          "datasource": "Prometheus",
          "enable": true,
          "expr": "ALERTS{alertname=~\".*Transcription.*\"}",
          "iconColor": "red",
          "name": "Alerts",
          "step": "60s",
          "tagKeys": "alertname",
          "textFormat": "{{alertname}}",
          "titleFormat": "Alert"
        }
      ]
    }
  }
}
```

## 19. Troubleshooting Guide

```bash
# scripts/troubleshoot.sh
#!/bin/bash

ISSUE_TYPE=${1:-all}

echo "üîç Troubleshooting - Issue: $ISSUE_TYPE"

# ========================================
# High latency
# ========================================
troubleshoot_latency() {
    echo "Analyzing latency issues..."
    
    # Check recent slow jobs
    psql $DATABASE_URL -c "
        SELECT 
            job_id,
            backend,
            audio_duration_seconds / 60 as duration_min,
            total_processing_time as processing_sec,
            rtf,
            created_at
        FROM job_metrics
        WHERE created_at >= NOW() - INTERVAL '1 hour'
        AND rtf > 0.5
        ORDER BY rtf DESC
        LIMIT 10;
    "
    
    # Check if RunPod has cold start issues
    echo "Checking RunPod cold starts..."
    psql $DATABASE_URL -c "
        SELECT 
            AVG(cold_start_time) as avg_cold_start,
            COUNT(*) as count
        FROM job_metrics
        WHERE backend = 'runpod'
        AND cold_start_time > 10
        AND created_at >= NOW() - INTERVAL '1 hour';
    "
    
    echo "Recommendation: If cold starts are high, increase RUNPOD_ACTIVE_WORKERS"
}

# ========================================
# High failure rate
# ========================================
troubleshoot_failures() {
    echo "Analyzing failure patterns..."
    
    # Group by error type
    psql $DATABASE_URL -c "
        SELECT 
            error_type,
            COUNT(*) as count,
            array_agg(DISTINCT error LIMIT 3) as sample_errors
        FROM job_metrics
        WHERE status = 'failed'
        AND created_at >= NOW() - INTERVAL '1 hour'
        GROUP BY error_type
        ORDER BY count DESC;
    "
    
    # Check if specific backend is failing
    psql $DATABASE_URL -c "
        SELECT 
            backend,
            COUNT(*) FILTER (WHERE status = 'failed') as failed,
            COUNT(*) as total,
            ROUND((COUNT(*) FILTER (WHERE status = 'failed')::numeric / COUNT(*)) * 100, 2) as failure_rate
        FROM job_metrics
        WHERE created_at >= NOW() - INTERVAL '1 hour'
        GROUP BY backend;
    "
}

# ========================================
# High costs
# ========================================
troubleshoot_costs() {
    echo "Analyzing cost issues..."
    
    # Find expensive jobs
    psql $DATABASE_URL -c "
        SELECT 
            job_id,
            tenant_id,
            backend,
            audio_duration_seconds / 60 as duration_min,
            cost_total,
            rtf,
            created_at
        FROM job_metrics
        WHERE created_at >= NOW() - INTERVAL '24 hours'
        AND cost_total > 1.0
        ORDER BY cost_total DESC
        LIMIT 10;
    "
    
    # Compare backend efficiency
    psql $DATABASE_URL -c "
        SELECT 
            backend,
            AVG(cost_per_audio_hour) as avg_cost_per_hour,
            AVG(rtf) as avg_rtf
        FROM job_metrics
        WHERE created_at >= NOW() - INTERVAL '24 hours'
        GROUP BY backend;
    "
}

# ========================================
# Queue issues
# ========================================
troubleshoot_queue() {
    echo "Analyzing queue issues..."
    
    QUEUE_SIZE=$(redis-cli -u $REDIS_URL LLEN transcription_jobs)
    echo "Current queue size: $QUEUE_SIZE"
    
    # Check stuck jobs
    psql $DATABASE_URL -c "
        SELECT 
            job_id,
            status,
            started_at,
            NOW() - started_at as stuck_duration
        FROM jobs
        WHERE status = 'processing'
        AND started_at < NOW() - INTERVAL '1 hour';
    "
    
    # Check worker count
    ACTIVE_WORKERS=$(psql $DATABASE_URL -t -c "
        SELECT COUNT(DISTINCT worker_id)
        FROM job_metrics
        WHERE created_at >= NOW() - INTERVAL '5 minutes'
    ")
    
    echo "Active workers: $ACTIVE_WORKERS"
    
    if [[ $QUEUE_SIZE -gt 50 ]] && [[ $ACTIVE_WORKERS -lt 5 ]]; then
        echo "‚ö†Ô∏è Recommendation: Scale up workers"
    fi
}

# ========================================
# Execute troubleshooting
# ========================================
case $ISSUE_TYPE in
    latency)
        troubleshoot_latency
        ;;
    failures)
        troubleshoot_failures
        ;;
    costs)
        troubleshoot_costs
        ;;
    queue)
        troubleshoot_queue
        ;;
    all)
        troubleshoot_latency
        echo ""
        troubleshoot_failures
        echo ""
        troubleshoot_costs
        echo ""
        troubleshoot_queue
        ;;
    *)
        echo "Unknown issue type: $ISSUE_TYPE"
        exit 1
        ;;
esac

echo ""
echo "‚úÖ Troubleshooting completed"
```

Pronto! Arquitetura completa com deploy multi-cloud, monitoramento e manuten√ß√£o. Quer que eu crie um README final consolidando tudo, ou algum componente espec√≠fico precisa de mais detalhes?

## 20. README.md Principal

```markdown
# Transcription Service - Arquitetura Multi-Cloud

Sistema de transcri√ß√£o de √°udio/v√≠deo em lote com suporte a diariza√ß√£o, processamento distribu√≠do e m√∫ltiplos backends (RunPod, AWS, GCP).

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-ready-brightgreen.svg)](https://www.docker.com/)

---

## üìã √çndice

- [Vis√£o Geral](#vis√£o-geral)
- [Arquitetura](#arquitetura)
- [Features](#features)
- [In√≠cio R√°pido](#in√≠cio-r√°pido)
- [Deploy](#deploy)
- [Configura√ß√£o](#configura√ß√£o)
- [Uso](#uso)
- [Monitoramento](#monitoramento)
- [Manuten√ß√£o](#manuten√ß√£o)
- [Troubleshooting](#troubleshooting)
- [Contribuindo](#contribuindo)

---

## üéØ Vis√£o Geral

Sistema empresarial de transcri√ß√£o autom√°tica de √°udio/v√≠deo otimizado para:

- **Aulas longas** (at√© 6 horas)
- **Processamento em lote** (m√∫ltiplos arquivos simult√¢neos)
- **Diariza√ß√£o de speakers** (identifica√ß√£o de quem falou)
- **Multi-cloud** (RunPod, AWS, GCP)
- **Custo otimizado** (escolha autom√°tica do backend mais econ√¥mico)

### Casos de Uso

- üìö **Educa√ß√£o**: Transcri√ß√£o de aulas gravadas
- ‚öñÔ∏è **Jur√≠dico**: Audi√™ncias e reuni√µes
- üèõÔ∏è **√ìrg√£os P√∫blicos**: Atas e sess√µes
- üéôÔ∏è **Podcasts**: Transcri√ß√£o para SEO e acessibilidade

---

## üèóÔ∏è Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         CLIENTE                                 ‚îÇ
‚îÇ            (Upload em lote + WebSocket/SSE)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      API (FastAPI)                              ‚îÇ
‚îÇ  -  Upload handling                                              ‚îÇ
‚îÇ  -  Job management                                               ‚îÇ
‚îÇ  -  Authentication                                               ‚îÇ
‚îÇ  -  SSE/WebSocket para status em tempo real                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   STORAGE (S3/GCS/Azure)                        ‚îÇ
‚îÇ  -  √Åudio/v√≠deo original (TTL 30 dias)                           ‚îÇ
‚îÇ  -  Transcri√ß√µes (permanente)                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     FILA (Redis)                                ‚îÇ
‚îÇ  -  Jobs pendentes                                               ‚îÇ
‚îÇ  -  Pub/Sub para notifica√ß√µes                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  WORKERS (Escal√°vel)                            ‚îÇ
‚îÇ  -  Orquestra√ß√£o de jobs                                         ‚îÇ
‚îÇ  -  Sele√ß√£o de backend                                           ‚îÇ
‚îÇ  -  Retry logic                                                  ‚îÇ
‚îÇ  -  M√©tricas                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ            ‚îÇ            ‚îÇ
        ‚ñº            ‚ñº            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RUNPOD  ‚îÇ  ‚îÇ   AWS    ‚îÇ  ‚îÇ   GCP    ‚îÇ
‚îÇ (Primary)‚îÇ  ‚îÇ  (GPU)   ‚îÇ  ‚îÇ  (GPU)   ‚îÇ
‚îÇ          ‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ          ‚îÇ
‚îÇ faster-  ‚îÇ  ‚îÇ EC2 g5   ‚îÇ  ‚îÇ GCE T4   ‚îÇ
‚îÇ whisper  ‚îÇ  ‚îÇ spot     ‚îÇ  ‚îÇ preempt  ‚îÇ
‚îÇ +pyannote‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ            ‚îÇ            ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              DATABASE (PostgreSQL)                              ‚îÇ
‚îÇ  -  Jobs & status                                                ‚îÇ
‚îÇ  -  Transcri√ß√µes                                                 ‚îÇ
‚îÇ  -  M√©tricas detalhadas                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          MONITORAMENTO (Prometheus + Grafana)                   ‚îÇ
‚îÇ  -  Lat√™ncia P50/P95/P99                                         ‚îÇ
‚îÇ  -  Custos por backend                                           ‚îÇ
‚îÇ  -  RTF (Real-Time Factor)                                       ‚îÇ
‚îÇ  -  Queue size & worker count                                    ‚îÇ
‚îÇ  -  Alertas (Slack, PagerDuty)                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Backends de Processamento

| Backend | Custo/hora √°udio* | RTF | Lat√™ncia | Limite |
|---------|-------------------|-----|----------|---------|
| **RunPod** (primary) | $0.10 - $0.14 | 0.2-0.3 | 5-30s cold start | Ilimitado |
| AWS EC2 g5.xlarge spot | $0.09 - $0.14 | 0.2-0.3 | Zero (sempre on) | Manual scaling |
| GCP T4 preemptible | $0.07 - $0.10 | 0.2-0.3 | Zero (sempre on) | Manual scaling |
| OpenAI Whisper API | $2.16 (fallback) | N/A | <5s | 25 MB por arquivo |

*Com diariza√ß√£o. Sem diariza√ß√£o: -30%

---

## ‚ú® Features

### Core

- ‚úÖ **Transcri√ß√£o**: Faster-whisper (large-v3-turbo) com VAD
- ‚úÖ **Diariza√ß√£o**: pyannote 3.1 (speaker identification)
- ‚úÖ **Processamento em lote**: Upload de m√∫ltiplos arquivos
- ‚úÖ **Arquivos longos**: Suporte a aulas de 6+ horas
- ‚úÖ **Multi-idioma**: Detec√ß√£o autom√°tica ou especifica√ß√£o manual
- ‚úÖ **Word timestamps**: Timestamps palavra por palavra

### Operacional

- ‚úÖ **Multi-cloud**: RunPod, AWS, GCP com failover autom√°tico
- ‚úÖ **Auto-scaling**: Workers escalam baseado no tamanho da fila
- ‚úÖ **Backend selection**: Escolha autom√°tica do backend mais econ√¥mico
- ‚úÖ **Retry logic**: Tentativas autom√°ticas em caso de falha
- ‚úÖ **TTL autom√°tico**: Limpeza de arquivos antigos

### Monitoramento

- ‚úÖ **M√©tricas detalhadas**: Lat√™ncia, RTF, custos, taxa de erro
- ‚úÖ **Dashboards Grafana**: Vis√£o em tempo real
- ‚úÖ **Alertas**: Slack, PagerDuty, webhooks
- ‚úÖ **Cost tracking**: Custo por job, tenant, backend
- ‚úÖ **Relat√≥rios**: Relat√≥rios di√°rios/semanais autom√°ticos

### Integra√ß√£o

- ‚úÖ **REST API**: FastAPI com OpenAPI docs
- ‚úÖ **SSE/WebSocket**: Notifica√ß√µes em tempo real
- ‚úÖ **Webhooks**: Notifica√ß√£o quando job completa
- ‚úÖ **Multi-tenant**: Isolamento de dados por tenant

---

## üöÄ In√≠cio R√°pido

### Pr√©-requisitos

- Docker & Docker Compose
- Python 3.11+
- Conta RunPod (para produ√ß√£o)
- Conta AWS/GCP (opcional)

### Desenvolvimento Local

```bash
# 1. Clone o reposit√≥rio
git clone https://github.com/seu-usuario/transcription-service.git
cd transcription-service

# 2. Configure vari√°veis de ambiente
cp .env.example .env
# Edite .env com suas credenciais

# 3. Inicie os servi√ßos
docker-compose up -d

# 4. Verifique health
curl http://localhost:8000/health

# 5. Acesse dashboards
# API: http://localhost:8000/docs
# Redis Commander: http://localhost:8081
# Grafana: http://localhost:3000 (admin/admin)
```

### Upload de Teste

```bash
# Upload via curl
curl -X POST http://localhost:8000/api/batch/transcribe \
  -F "files=@aula1.mp4" \
  -F "files=@aula2.mp4" \
  -F "tenant_id=test-tenant" \
  -F "enable_diarization=true"

# Resposta
{
  "batch_id": "uuid-do-batch",
  "total_jobs": 2,
  "jobs": [
    {
      "job_id": "uuid-1",
      "filename": "aula1.mp4",
      "status": "queued"
    },
    ...
  ]
}

# Consultar status
curl http://localhost:8000/api/jobs/uuid-1/status

# Baixar resultado
curl http://localhost:8000/api/jobs/uuid-1/result
```

### Client Python

```python
import requests

class TranscriptionClient:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
    
    def upload(self, files, tenant_id, enable_diarization=False):
        """Upload em lote"""
        files_data = [('files', open(f, 'rb')) for f in files]
        
        response = requests.post(
            f"{self.base_url}/api/batch/transcribe",
            files=files_data,
            data={
                'tenant_id': tenant_id,
                'enable_diarization': enable_diarization
            }
        )
        return response.json()
    
    def get_status(self, job_id):
        """Consultar status"""
        response = requests.get(
            f"{self.base_url}/api/jobs/{job_id}/status"
        )
        return response.json()
    
    def get_result(self, job_id):
        """Baixar resultado"""
        response = requests.get(
            f"{self.base_url}/api/jobs/{job_id}/result"
        )
        return response.json()

# Uso
client = TranscriptionClient()

# Upload
result = client.upload(
    files=['aula1.mp4', 'aula2.mp4'],
    tenant_id='meu-tenant',
    enable_diarization=True
)

# Aguardar
import time
for job in result['jobs']:
    while True:
        status = client.get_status(job['job_id'])
        if status['status'] == 'completed':
            transcription = client.get_result(job['job_id'])
            print(f"Transcri√ß√£o: {transcription['text'][:100]}...")
            break
        time.sleep(5)
```

---

## üì¶ Deploy

### Deploy RunPod Worker

```bash
# 1. Build e push Docker image
cd runpod_worker
docker build -t transcription-runpod:latest .
docker tag transcription-runpod:latest $REGISTRY/transcription-runpod:latest
docker push $REGISTRY/transcription-runpod:latest

# 2. Deploy via script
bash scripts/deploy_runpod_worker.sh

# 3. Configurar endpoint no .env
# RUNPOD_ENDPOINT_ID=seu-endpoint-id
```

### Deploy AWS (ECS)

```bash
# 1. Configurar AWS credentials
aws configure

# 2. Criar secrets
aws secretsmanager create-secret \
  --name transcription/prod/runpod-api-key \
  --secret-string "sua-api-key"

# 3. Deploy com Terraform
cd terraform/aws
terraform init
terraform workspace new prod
terraform apply \
  -var="environment=prod" \
  -var="db_password=SENHA_SEGURA"

# 4. Deploy aplica√ß√£o
bash scripts/deploy_aws.sh prod us-east-1 v1.0.0

# Output: ALB DNS name para acesso
```

### Deploy GCP (Cloud Run)

```bash
# 1. Autenticar
gcloud auth login
gcloud config set project SEU_PROJECT_ID

# 2. Criar secrets
gcloud secrets create runpod-api-key \
  --data-file=- <<< "sua-api-key"

# 3. Deploy com Terraform
cd terraform/gcp
terraform init
terraform workspace new prod
terraform apply \
  -var="environment=prod" \
  -var="project_id=SEU_PROJECT_ID"

# 4. Deploy aplica√ß√£o
bash scripts/deploy_gcp.sh prod SEU_PROJECT_ID us-central1 v1.0.0
```

### Deploy via CI/CD (GitHub Actions)

```bash
# 1. Configurar secrets no GitHub
# Settings ‚Üí Secrets ‚Üí Actions:
# - RUNPOD_API_KEY
# - AWS_ACCESS_KEY_ID
# - AWS_SECRET_ACCESS_KEY
# - GCP_SA_KEY (service account JSON)
# - DOCKERHUB_USERNAME
# - DOCKERHUB_TOKEN

# 2. Push para branch
git checkout -b develop
git push origin develop  # Deploy staging autom√°tico

git checkout main
git push origin main     # Deploy production (requer approval)
```

---

## ‚öôÔ∏è Configura√ß√£o

### Vari√°veis de Ambiente Essenciais

```bash
# Backend
PRIMARY_BACKEND=runpod  # runpod, aws_gpu, gcp_gpu
RUNPOD_API_KEY=your-key
RUNPOD_ENDPOINT_ID=your-endpoint-id
RUNPOD_ACTIVE_WORKERS=1  # Manter 1+ worker para evitar cold start

# Storage
STORAGE_BACKEND=s3  # s3, gcs, azure
S3_BUCKET_NAME=transcriptions
AWS_ACCESS_KEY_ID=your-key
AWS_SECRET_ACCESS_KEY=your-secret

# Database
DATABASE_URL=postgresql://user:pass@host:5432/db
REDIS_URL=redis://host:6379

# Features
ENABLE_DIARIZATION=true
DEFAULT_LANGUAGE=pt
MAX_UPLOAD_SIZE_MB=500

# Monitoramento
PROMETHEUS_ENABLED=true
DATADOG_ENABLED=false
SLACK_WEBHOOK_URL=https://hooks.slack.com/...
```

### Configura√ß√£o de Custos

```bash
# Limites de custo (alertas)
DAILY_COST_THRESHOLD=50  # USD
MONTHLY_COST_THRESHOLD=1000  # USD

# Backend selection strategy
BACKEND_SELECTION_PRIORITY=cost  # cost, latency, quality
```

### Auto-scaling Workers

```bash
# Desenvolvimento
WORKER_CONCURRENCY=2
MIN_WORKERS=1
MAX_WORKERS=5

# Produ√ß√£o
WORKER_CONCURRENCY=4
MIN_WORKERS=3
MAX_WORKERS=20

# Scale baseado em fila
SCALE_UP_THRESHOLD=50  # jobs na fila
SCALE_DOWN_THRESHOLD=10
```

---

## üìä Monitoramento

### M√©tricas Principais

**Lat√™ncia**
```promql
# P95 latency por backend
histogram_quantile(0.95, 
  sum by (backend, le) (rate(transcription_job_duration_seconds_bucket[5m]))
)
```

**Taxa de Sucesso**
```promql
# Success rate √∫ltimos 5 minutos
sum(rate(transcription_jobs_total{status="success"}[5m])) / 
sum(rate(transcription_jobs_total[5m])) * 100
```

**Custo**
```promql
# Custo por hora (√∫ltimas 24h)
sum by (backend) (increase(transcription_cost_usd_total[24h]))
```

**RTF (Real-Time Factor)**
```promql
# RTF m√©dio por backend
avg by (backend) (transcription_rtf)
```

### Dashboards

- **Grafana**: http://grafana.seu-dominio.com
  - Overview: Sa√∫de geral do sistema
  - Performance: Lat√™ncia, RTF, throughput
  - Costs: Custos por backend, tenant, per√≠odo
  - Errors: Taxa de erro, tipos de erro

- **Prometheus**: http://prometheus.seu-dominio.com
  - Queries ad-hoc
  - Alertas configurados

### Alertas

Configurados no Alertmanager (Prometheus) e enviados para:

- **Slack** (#alerts-transcription): Warnings e info
- **PagerDuty**: Cr√≠ticos (API down, DB down)
- **Email**: Relat√≥rios di√°rios

**Alertas Configurados**:
- HighFailureRate: Taxa de falha >10% por 5 min
- HighLatency: P95 >5 min por 10 min
- HighCost: >$10/hora por 30 min
- LargeQueue: >100 jobs na fila por 10 min
- NoActiveWorkers: 0 workers por 5 min

---

## üîß Manuten√ß√£o

### Backup

```bash
# Backup manual completo
bash scripts/backup.sh prod full

# Backup autom√°tico (cron)
# Di√°rio √†s 3 AM
0 3 * * * /app/scripts/backup.sh prod full >> /var/log/backup.log 2>&1
```

### Limpeza

```bash
# Limpar jobs antigos (>90 dias)
bash scripts/cleanup.sh prod

# Dry-run (ver o que seria deletado)
bash scripts/cleanup.sh prod true
```

### Manuten√ß√£o de DB

```bash
# Vacuum e analyze
bash scripts/db_maintenance.sh prod

# Reindex
psql $DATABASE_URL -c "REINDEX DATABASE transcription_db;"
```

### Relat√≥rios

```bash
# Relat√≥rio de custos (√∫ltimos 7 dias)
bash scripts/cost_report.sh prod 7

# Envia por email automaticamente se configurado
```

### Rollback

```bash
# Rollback para vers√£o anterior
bash scripts/rollback.sh prod

# Rollback para vers√£o espec√≠fica
bash scripts/rollback.sh prod v1.2.3
```

---

## üêõ Troubleshooting

### Alta Lat√™ncia

```bash
# Diagnosticar
bash scripts/troubleshoot.sh latency

# Solu√ß√µes comuns:
# 1. Cold start alto no RunPod ‚Üí Aumentar RUNPOD_ACTIVE_WORKERS
# 2. RTF alto ‚Üí Verificar carga da GPU
# 3. Queue grande ‚Üí Escalar workers
```

### Alta Taxa de Falhas

```bash
# Diagnosticar
bash scripts/troubleshoot.sh failures

# Verificar logs
docker-compose logs -f --tail=100 worker

# Logs espec√≠ficos de um job
psql $DATABASE_URL -c "SELECT error FROM jobs WHERE job_id='uuid';"
```

### Custos Altos

```bash
# Diagnosticar
bash scripts/troubleshoot.sh costs

# A√ß√µes:
# 1. Verificar se backend mais barato est√° sendo usado
# 2. Verificar jobs muito longos (RTF alto)
# 3. Considerar migrar de OpenAI para RunPod
```

### Queue Travada

```bash
# Diagnosticar
bash scripts/troubleshoot.sh queue

# Limpar jobs stuck
psql $DATABASE_URL -c "
  UPDATE jobs SET status='failed', error='Timeout' 
  WHERE status='processing' 
  AND started_at < NOW() - INTERVAL '2 hours';"

# Restart workers
docker-compose restart worker
```

### Logs √öteis

```bash
# API logs
docker-compose logs -f api

# Worker logs
docker-compose logs -f worker

# Logs de um job espec√≠fico
grep "job_id" /var/log/transcription/worker.log | grep "uuid"

# Redis queue
redis-cli -u $REDIS_URL
> LLEN transcription_jobs
> LRANGE transcription_jobs 0 10
```

---

## üìà Performance

### Benchmarks

**Hardware**: RunPod L4 GPU (24GB VRAM)

| √Åudio | Dura√ß√£o | RTF | Lat√™ncia | Custo |
|-------|---------|-----|----------|-------|
| Aula curta | 30 min | 0.15 | 4.5 min | $0.035 |
| Aula padr√£o | 2h | 0.20 | 24 min | $0.19 |
| Aula longa | 6h | 0.25 | 90 min | $0.70 |

*Com diariza√ß√£o habilitada. Sem diariza√ß√£o: RTF -30%, custo -30%*

### Otimiza√ß√µes

**1. Reduzir Cold Start (RunPod)**
```bash
# Manter 1+ worker ativo
RUNPOD_ACTIVE_WORKERS=1

# Trade-off: paga $0.468/h mas elimina 10-30s de cold start
```

**2. Batch Processing**
```python
# Processar m√∫ltiplos arquivos do mesmo tenant juntos
# Reduz overhead de setup
```

**3. VAD (Voice Activity Detection)**
```python
# J√° habilitado por padr√£o
# Pula sil√™ncio, reduz tempo de processamento em ~20%
```

**4. Escolha de Modelo**
```bash
# large-v3-turbo: √≥timo balan√ßo velocidade/qualidade
# large-v3: +10% mais preciso, +50% mais lento
# medium: -20% qualidade, +40% mais r√°pido
```

---

## üîê Seguran√ßa

### Boas Pr√°ticas

1. **Secrets Management**
   - Use AWS Secrets Manager / GCP Secret Manager
   - Nunca commite .env no git
   - Rotacione keys regularmente

2. **Network**
   - API atr√°s de ALB/Load Balancer com SSL
   - VPC isolada para RDS/Redis
   - Security Groups restritivos

3. **Storage**
   - S3/GCS com bucket policies restritivos
   - Encryption at rest habilitado
   - TTL autom√°tico para arquivos tempor√°rios

4. **Database**
   - Connections via SSL
   - Backups autom√°ticos di√°rios
   - Point-in-time recovery habilitado

5. **Logs**
   - N√£o logar dados sens√≠veis
   - Centralize logs (CloudWatch/Stackdriver)
   - Retention policy configurado

### Compliance

- **LGPD/GDPR**: Modo privado processa dados sem sair do ambiente
- **SOC 2**: Logs de auditoria completos
- **Data Retention**: TTL configur√°vel por tenant

---

## ü§ù Contribuindo

```bash
# 1. Fork o projeto
git clone https://github.com/seu-usuario/transcription-service.git

# 2. Crie uma branch
git checkout -b feature/nova-feature

# 3. Fa√ßa suas altera√ß√µes e teste
docker-compose up -d
pytest tests/

# 4. Commit com mensagem descritiva
git commit -m "feat: adiciona suporte a Azure backend"

# 5. Push e abra PR
git push origin feature/nova-feature
```

### Conven√ß√µes

- **Commits**: Conventional Commits (feat, fix, docs, refactor)
- **Code Style**: Black (line-length=100) + Flake8
- **Tests**: Cobertura m√≠nima 80%
- **Docs**: Docstrings em todas as fun√ß√µes p√∫blicas

---

## üìù License

MIT License - veja [LICENSE](LICENSE) para detalhes.

---

## üôã Suporte

- **Documenta√ß√£o**: [docs.transcription-service.com](https://docs.transcription-service.com)
- **Issues**: [GitHub Issues](https://github.com/seu-usuario/transcription-service/issues)
- **Discuss√µes**: [GitHub Discussions](https://github.com/seu-usuario/transcription-service/discussions)
- **Email**: support@transcription-service.com
- **Slack**: [Workspace](https://transcription-service.slack.com)

---

## üìö Recursos Adicionais

### Documenta√ß√£o T√©cnica

- [Guia de Arquitetura](docs/ARCHITECTURE.md)
- [API Reference](docs/API.md)
- [Deployment Guide](docs/DEPLOYMENT.md)
- [Monitoring Guide](docs/MONITORING.md)
- [Cost Optimization](docs/COST_OPTIMIZATION.md)

### Tutoriais

- [Primeiros Passos](docs/tutorials/getting-started.md)
- [Deploy em Produ√ß√£o](docs/tutorials/production-deployment.md)
- [Configurando Diariza√ß√£o](docs/tutorials/diarization-setup.md)
- [Integrando com sua Aplica√ß√£o](docs/tutorials/integration.md)

### Compara√ß√µes

- [RunPod vs AWS vs GCP](docs/comparisons/cloud-providers.md)
- [Whisper Models](docs/comparisons/whisper-models.md)
- [Diarization Options](docs/comparisons/diarization.md)

---

## üó∫Ô∏è Roadmap

### Q1 2026 ‚úÖ
- [x] Backend RunPod (primary)
- [x] Diariza√ß√£o com pyannote
- [x] Multi-cloud storage (S3/GCS)
- [x] Monitoramento completo
- [x] Auto-scaling workers

### Q2 2026 üöß
- [ ] Suporte a streaming (real-time transcription)
- [ ] Fine-tuning de modelos espec√≠ficos por dom√≠nio
- [ ] Suporte a mais idiomas (20+)
- [ ] API GraphQL
- [ ] Mobile SDK (iOS/Android)

### Q3 2026 üìã
- [ ] Speaker recognition (identifica√ß√£o por voz)
- [ ] Translation autom√°tica (multilingual)
- [ ] Summarization com LLMs
- [ ] Sentiment analysis
- [ ] Highlights autom√°ticos

### Q4 2026 üí°
- [ ] On-premise deployment
- [ ] Kubernetes operators
- [ ] Video processing pipeline
- [ ] Subtitle generation (SRT/VTT)
- [ ] Integra√ß√£o com LMS (Moodle, Canvas)

---

## üìä Estat√≠sticas

![GitHub Stars](https://img.shields.io/github/stars/seu-usuario/transcription-service?style=social)
![GitHub Forks](https://img.shields.io/github/forks/seu-usuario/transcription-service?style=social)
![GitHub Issues](https://img.shields.io/github/issues/seu-usuario/transcription-service)
![GitHub Pull Requests](https://img.shields.io/github/issues-pr/seu-usuario/transcription-service)

**M√©tricas de Produ√ß√£o** (atualizado mensalmente):
- üéØ Uptime: 99.9%
- ‚ö° Lat√™ncia P95: <3min para √°udios de 1h
- üí∞ Custo m√©dio: $0.12/hora de √°udio
- üé§ Precis√£o: >95% WER
- üìà Jobs processados: 1M+

---

## üåü Reconhecimentos

Constru√≠do com tecnologias open-source incr√≠veis:

- [Faster-Whisper](https://github.com/guillaumekln/faster-whisper) - Transcri√ß√£o otimizada
- [pyannote.audio](https://github.com/pyannote/pyannote-audio) - Diariza√ß√£o
- [FastAPI](https://fastapi.tiangolo.com/) - Framework web
- [RunPod](https://runpod.io/) - GPU serverless
- [Prometheus](https://prometheus.io/) + [Grafana](https://grafana.com/) - Monitoramento

---

**Made with ‚ù§Ô∏è in Brazil üáßüá∑**

```

## 21. Guias Complementares

### ARCHITECTURE.md

```markdown
# Guia de Arquitetura Detalhado

## Decis√µes de Design

### Por que RunPod como Primary Backend?

1. **Custo-benef√≠cio**: ~74% mais barato que OpenAI API para volume m√©dio/alto
2. **Serverless**: Paga s√≥ pelo que usa, escala autom√°tico
3. **Zero ops**: N√£o precisa gerenciar GPUs/drivers
4. **Cold start aceit√°vel**: 5-30s vs 0s do "always-on", mas compensado pelo custo

### Por que PostgreSQL ao inv√©s de MongoDB?

1. **Transa√ß√µes ACID**: Jobs precisam de consist√™ncia forte
2. **Queries anal√≠ticas**: Agrega√ß√µes complexas para m√©tricas
3. **Relacionamentos**: Jobs ‚Üî Transcriptions ‚Üî Tenants
4. **Time-series**: PostgreSQL + TimescaleDB para m√©tricas

### Por que Redis para Fila?

1. **Performance**: >100K ops/sec
2. **Pub/Sub nativo**: Para SSE/WebSocket
3. **Simplicidade**: BLPOP blocking queue √© trivial
4. **Observabilidade**: Redis Commander para debug

### Alternativas Consideradas

**Celery** (descartado)
- ‚ùå Overhead desnecess√°rio para caso de uso simples
- ‚ùå Serialization issues com objetos grandes
- ‚úÖ Redis raw √© mais direto

**RabbitMQ** (descartado)
- ‚ùå Mais complexo de operar
- ‚ùå Mem√≥ria maior que Redis
- ‚úÖ Redis suficiente para escala

**SQS** (considerado para AWS-only)
- ‚úÖ Serverless, sem gest√£o
- ‚ùå Lock na AWS
- ‚ùå Lat√™ncia maior que Redis

## Padr√µes Implementados

### Event-Driven Architecture
- Pub/Sub para notifica√ß√µes
- Workers reativos √† fila
- Decoupling API ‚Üî Processing

### Backend Pattern
- Interface abstrata `TranscriptionBackend`
- Factory pattern para sele√ß√£o
- Strategy pattern para fallback

### Multi-tenancy
- Row-level isolation no DB
- Tenant ID em todas as m√©tricas
- Storage isolado por tenant

## Trade-offs

| Decis√£o | Pro | Con |
|---------|-----|-----|
| RunPod serverless | Custo vari√°vel baixo | Cold start ocasional |
| Fila Redis | Simples, r√°pido | Single point of failure |
| PostgreSQL | ACID, queries ricas | N√£o-horizontal scaling |
| Fargate | Serverless, escala f√°cil | Custo maior que EC2 |
```

### COST_OPTIMIZATION.md

```markdown
# Guia de Otimiza√ß√£o de Custos

## Estrat√©gias de Redu√ß√£o

### 1. Backend Selection Inteligente

```python
# Implementado em BackendRouter
def select_backend(audio_duration, enable_diarization):
    # √Åudio < 10min ‚Üí OpenAI (lat√™ncia melhor)
    if audio_duration < 600:
        return "openai"
    
    # √Åudio longo + diariza√ß√£o ‚Üí RunPod (mais barato)
    if enable_diarization:
        return "runpod"
    
    # Check health e escolhe mais barato dispon√≠vel
    return cheapest_available_backend()
```

### 2. Batch Processing

- Processe m√∫ltiplos arquivos do mesmo tenant juntos
- Reduz overhead de cold start
- Amortiza custo de setup

### 3. Storage Lifecycle

```terraform
lifecycle_rule {
  condition { age = 30 }
  action { type = "Delete" }  # √Åudio tempor√°rio
}

lifecycle_rule {
  condition { age = 90 }
  action { 
    type = "SetStorageClass"
    storage_class = "GLACIER"  # Archive transcri√ß√µes antigas
  }
}
```

### 4. Worker Scaling Inteligente

- Min workers = 1 (sempre um dispon√≠vel)
- Scale up agressivo quando queue > 50
- Scale down conservador (evita thrashing)

### 5. Spot/Preemptible Instances

Para AWS/GCP pr√≥prio:
- Use spot/preemptible (50-90% desconto)
- Retry autom√°tico se preempted
- Combine com RunPod para redund√¢ncia

## C√°lculo de ROI

### Migra√ß√£o OpenAI ‚Üí RunPod

```
Volume mensal: 1000h √°udio
Custo OpenAI: 1000h √ó $2.16 = $2,160/m√™s
Custo RunPod: 1000h √ó $0.10 = $100/m√™s (RTF 0.2, sem diariza√ß√£o)

Economia: $2,060/m√™s ($24,720/ano)
Setup cost: ~$500 (dev + testes)
Payback: 7 dias
```

### AWS Spot vs RunPod

```
Volume: 1000h/m√™s processadas
RTF m√©dio: 0.25

RunPod: 250h GPU √ó $0.468 = $117/m√™s
AWS spot (sempre on): 730h √ó $0.47 = $343/m√™s
Economia RunPod: $226/m√™s

Break-even: RunPod ganha se uso < 36% do tempo
```

## Alertas de Custo

Configurados no sistema:

- Daily threshold: $50
- Monthly threshold: $1000
- Per-tenant threshold: configur√°vel
- Automatic email + Slack quando excedido
```

Pronto! Documenta√ß√£o completa consolidada. Quer que eu crie mais algum documento espec√≠fico (ex: API reference detalhada, guia de contribui√ß√£o, ou exemplos de integra√ß√£o)?