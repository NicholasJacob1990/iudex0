import os
import sys
import time
import argparse
import subprocess
import traceback
import hashlib
import shutil
from urllib.parse import urlparse
from pathlib import Path
from typing import Optional
try:
    from colorama import init, Fore, Style
except ImportError:
    class MockColor:
        def __getattr__(self, name): return ""
    init = lambda *a, **k: None
    Fore = MockColor()
    Style = MockColor()

try:
    from tqdm import tqdm
except ImportError:
    def tqdm(iterable, *args, **kwargs):
        return iterable
import re
import difflib
import json
import asyncio
from google import genai
from google.genai import types
from openai import OpenAI, AsyncOpenAI
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import threading
import random
from collections import deque
from time import sleep # Added for RateLimiter fallback if needed
import logging

# Carrega .env no in√≠cio do m√≥dulo para garantir vari√°veis dispon√≠veis
try:
    from dotenv import load_dotenv
    load_dotenv(override=False)
except ImportError:
    pass

init(autoreset=True)

try:
    from app.services.api_call_tracker import record_api_call as _record_api_call
except Exception:
    _record_api_call = None


def _safe_int(value):
    try:
        return int(value)
    except (TypeError, ValueError):
        return None


def _safe_float(value):
    try:
        return float(value)
    except (TypeError, ValueError):
        return None


def _env_truthy(name: str, default: Optional[bool] = None) -> Optional[bool]:
    value = os.getenv(name)
    if value is None:
        return default
    value_norm = str(value).strip().lower()
    if value_norm in ("1", "true", "yes", "y", "on", "enable", "enabled"):
        return True
    if value_norm in ("0", "false", "no", "n", "off", "disable", "disabled"):
        return False
    return default


def _record_llm_usage(
    *,
    provider: str,
    model: Optional[str],
    tokens_in: Optional[int] = None,
    tokens_out: Optional[int] = None,
    cached_tokens_in: Optional[int] = None,
    seconds_audio: Optional[float] = None,
    seconds_video: Optional[float] = None,
):
    if not _record_api_call or not model:
        return
    meta = {}
    if tokens_in is not None:
        meta["tokens_in"] = int(tokens_in)
        meta["context_tokens"] = int(tokens_in)
    if tokens_out is not None:
        meta["tokens_out"] = int(tokens_out)
    if cached_tokens_in is not None:
        meta["cached_tokens_in"] = int(cached_tokens_in)
    if seconds_audio is not None:
        meta["seconds_audio"] = float(seconds_audio)
    if seconds_video is not None:
        meta["seconds_video"] = float(seconds_video)
    try:
        _record_api_call(kind="llm", provider=provider, model=model, success=True, meta=meta)
    except Exception:
        pass


def _record_openai_usage(response, *, model: Optional[str], provider: str = "openai"):
    usage = getattr(response, "usage", None)
    tokens_in = _safe_int(getattr(usage, "prompt_tokens", None) or getattr(usage, "input_tokens", None))
    tokens_out = _safe_int(getattr(usage, "completion_tokens", None) or getattr(usage, "output_tokens", None))
    details = getattr(usage, "prompt_tokens_details", None) or getattr(usage, "input_tokens_details", None)
    cached_tokens = _safe_int(getattr(details, "cached_tokens", None) or getattr(details, "cached", None))
    _record_llm_usage(
        provider=provider,
        model=model,
        tokens_in=tokens_in,
        tokens_out=tokens_out,
        cached_tokens_in=cached_tokens,
    )


def _record_genai_usage(response, *, model: Optional[str], provider: str = "gemini"):
    usage = getattr(response, "usage_metadata", None)
    tokens_in = _safe_int(
        getattr(usage, "prompt_token_count", None) or getattr(usage, "input_tokens", None)
    )
    tokens_out = _safe_int(
        getattr(usage, "candidates_token_count", None) or getattr(usage, "output_tokens", None)
    )
    cached_tokens = _safe_int(
        getattr(usage, "cached_content_token_count", None)
        or getattr(usage, "cached_token_count", None)
        or getattr(usage, "cached_tokens", None)
    )
    seconds_audio = _safe_float(
        getattr(usage, "prompt_audio_duration_seconds", None)
        or getattr(usage, "audio_duration_seconds", None)
        or getattr(usage, "audio_seconds", None)
    )
    seconds_video = _safe_float(
        getattr(usage, "prompt_video_duration_seconds", None)
        or getattr(usage, "video_duration_seconds", None)
        or getattr(usage, "video_seconds", None)
    )
    _record_llm_usage(
        provider=provider,
        model=model,
        tokens_in=tokens_in,
        tokens_out=tokens_out,
        cached_tokens_in=cached_tokens,
        seconds_audio=seconds_audio,
        seconds_video=seconds_video,
    )

class HILCheckpointException(Exception):
    """Interrompe o pipeline quando a revis√£o humana √© obrigat√≥ria."""
    pass

# v2.18: Import do m√≥dulo de auditoria jur√≠dica (desativado por padr√£o)
LEGAL_AUDIT_ENABLED = os.getenv("ENABLE_LEGAL_AUDIT", "").lower() in ("1", "true", "yes", "on")
if LEGAL_AUDIT_ENABLED:
    try:
        from audit_module import auditar_consistencia_legal
        AUDIT_AVAILABLE = True
    except ImportError:
        AUDIT_AVAILABLE = False
        print("‚ö†Ô∏è audit_module n√£o encontrado. Auditoria jur√≠dica desabilitada.")
else:
    AUDIT_AVAILABLE = False

# v2.27: Auditoria preventiva de fidelidade (n√£o limitada √† autoria)
FIDELITY_AUDIT_ENABLED = os.getenv("ENABLE_FIDELITY_AUDIT", "1").lower() in ("1", "true", "yes", "on")
# Backup opcional da valida√ß√£o full-context
FIDELITY_BACKUP_ENABLED = os.getenv("ENABLE_FIDELITY_BACKUP", "1").lower() in ("1", "true", "yes", "on")
try:
    from audit_fidelity_preventive import (
        auditar_fidelidade_preventiva,
        gerar_relatorio_markdown_completo,
    )
    FIDELITY_AUDIT_AVAILABLE = True
except ImportError as e:
    FIDELITY_AUDIT_AVAILABLE = False
    print(f"‚ö†Ô∏è audit_fidelity_preventive n√£o encontrado ou erro de importa√ß√£o: {e}. Auditoria preventiva desabilitada.")

# v2.27: Auditoria de fontes integrada (controla inclus√£o no relat√≥rio preventivo)
SOURCES_AUDIT_ENABLED = os.getenv("ENABLE_SOURCES_AUDIT", "1").lower() in ("1", "true", "yes", "on")

# v2.24: Import auto_fix_apostilas for post-processing
try:
    from auto_fix_apostilas import analyze_structural_issues, apply_structural_fixes_to_file
    AUTO_FIX_AVAILABLE = True
except ImportError:
    AUTO_FIX_AVAILABLE = False

# v3.0: Relat√≥rio unificado (cross-referencing entre camadas)
try:
    from audit_unified import UnifiedAuditEngine, generate_unified_markdown, UnifiedReport, compare_reports
    UNIFIED_AUDIT_AVAILABLE = True
except ImportError:
    UNIFIED_AUDIT_AVAILABLE = False

try:
    import mlx_whisper
except ImportError:
    mlx_whisper = None

# Faster-Whisper (Beam Search Backend)
try:
    from faster_whisper import WhisperModel
    FASTER_WHISPER_AVAILABLE = True
except ImportError:
    FASTER_WHISPER_AVAILABLE = False

# Imports Pyannote e Torch
try:
    from pyannote.audio import Pipeline
    import torch
    
    HF_TOKEN = os.getenv("HUGGING_FACE_TOKEN")
    if not HF_TOKEN:
        pass
    
except ImportError:
    Pipeline = None
    torch = None

# ==================== CONTEXT CACHING (v2.2) ====================
# S√≥ usa cache para documentos grandes (economia de tokens)
MIN_CHARS_PARA_CACHE = 150000  # 150k chars (~37k tokens)
CACHE_TTL = '7200s'            # 2 horas (padr√£o)

def criar_cache_contexto(client, transcricao_completa, system_prompt, estrutura_global=None, model_name="gemini-3-flash-preview"):
    """
    v2.2: Cria cache de contexto com hash est√°vel para reutiliza√ß√£o.
    
    Args:
        client: Cliente Gemini
        transcricao_completa: Texto completo (usado para calcular TTL din√¢mico)
        system_prompt: Prompt de formata√ß√£o (PROMPT_APOSTILA ou PROMPT_FIDELIDADE)
        estrutura_global: Estrutura mapeada (opcional)
        model_name: Nome do modelo Gemini
    
    Returns:
        Cache object ou None se falhar/n√£o necess√°rio
    """
    # Cache s√≥ vale a pena para documentos grandes
    if len(transcricao_completa) < MIN_CHARS_PARA_CACHE:
        print(f"{Fore.YELLOW}üì¶ Documento pequeno ({len(transcricao_completa):,} chars), cache n√£o necess√°rio{Style.RESET_ALL}")
        return None
    
    try:
        # Hash do prompt + estrutura para garantir unicidade por documento
        combined_content = system_prompt + (estrutura_global or "")
        prompt_hash = hashlib.sha256(combined_content.encode()).hexdigest()[:16]
        cache_name = f"vomo_{prompt_hash}"
        
        # Tenta encontrar cache existente v√°lido
        try:
            for c in client.caches.list(page_size=100):
                if c.display_name == cache_name:
                    print(f"{Fore.GREEN}‚ôªÔ∏è  Reusando cache existente: {cache_name}{Style.RESET_ALL}")
                    return c
        except Exception as e:
            print(f"{Fore.YELLOW}‚ö†Ô∏è Cache lookup warning: {e}{Style.RESET_ALL}")

        # Adiciona a estrutura global se dispon√≠vel
        estrutura_text = f"\n\n## ESTRUTURA GLOBAL (GUIA):\n{estrutura_global}" if estrutura_global else ""
        
        cache_content = f"""{system_prompt}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìö CONTEXTO GLOBAL
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
{estrutura_text}
"""
        
        # TTL Din√¢mico: 1 hora a cada 500k chars + 1h margem
        tempo_estimado_segundos = int((len(transcricao_completa) / 500000) * 3600) + 3600
        dinamico_ttl = f"{max(3600, tempo_estimado_segundos)}s"
        
        # Cria cache usando a API do google-genai
        cache = client.caches.create(
            model=model_name,
            config=types.CreateCachedContentConfig(
                contents=[cache_content],
                ttl=dinamico_ttl,
                display_name=cache_name
            )
        )
        
        print(f"{Fore.GREEN}‚úÖ Cache criado: {cache_name} (hash: {prompt_hash}, TTL: {dinamico_ttl}){Style.RESET_ALL}")
        return cache
        
    except Exception as e:
        print(f"{Fore.YELLOW}‚ö†Ô∏è Falha ao criar cache: {e}. Continuando sem cache.{Style.RESET_ALL}")
        return None

# ==================== RATE LIMITER ====================
class RateLimiter:
    """Controla requisi√ß√µes por minuto para n√£o estourar rate limit da API"""
    def __init__(self, max_requests_per_minute=60): # Vertex AI Limit
        self.max_rpm = max_requests_per_minute
        self._window_seconds = 60.0
        self._requests_sync = deque()
        self._requests_async = deque()
        self._lock = threading.Lock()
        self._async_lock = asyncio.Lock()

    def _prune_requests(self, requests, now):
        cutoff = now - self._window_seconds
        while requests and requests[0] <= cutoff:
            requests.popleft()
    
    def wait_if_needed(self):
        while True:
            with self._lock:
                now = time.monotonic()
                self._prune_requests(self._requests_sync, now)

                if len(self._requests_sync) < self.max_rpm:
                    self._requests_sync.append(now)
                    return

                oldest = self._requests_sync[0]
                wait_time = self._window_seconds - (now - oldest) + 0.5

            if wait_time > 0:
                print(f"{Fore.YELLOW}‚è±Ô∏è  Rate limit: aguardando {wait_time:.1f}s...")
                sleep(wait_time)

    async def wait_if_needed_async(self):
        """Vers√£o async do rate limiter para n√£o bloquear o event loop"""
        while True:
            async with self._async_lock:
                now = time.monotonic()
                self._prune_requests(self._requests_async, now)

                if len(self._requests_async) < self.max_rpm:
                    self._requests_async.append(now)
                    return

                oldest = self._requests_async[0]
                wait_time = self._window_seconds - (now - oldest) + 0.5

            if wait_time > 0:
                print(f"{Fore.YELLOW}‚è±Ô∏è  Rate limit (Async): aguardando {wait_time:.1f}s...")
                await asyncio.sleep(wait_time)

# Inst√¢ncia global
rate_limiter = RateLimiter(max_requests_per_minute=60)

def remover_overlap_duplicado(resultados, mode="APOSTILA"):
    """Remove duplica√ß√£o causada pelo overlap entre chunks usando detec√ß√£o ROBUSTA de conte√∫do
    
    v2.17: Usa LIMIAR_7DIFF diferenciado por modo.
    - FIDELIDADE: 0.85 (mais conservador)
    - APOSTILA: 0.80 (mais agressivo - overlaps s√£o quase sempre erros)
    """
    # Limiares adaptativos por camada de deduplica√ß√£o
    LIMIAR_7DIFF = 0.85 if mode == "FIDELIDADE" else 0.80
    if len(resultados) <= 1:
        return resultados[0] if resultados else ""
    
    import re
    from difflib import SequenceMatcher
    
    # === FUN√á√ïES AUXILIARES DA ESTRAT√âGIA ROBUSTA (Portadas de clean_redundancy.py) ===
    
    def normalize_text(text):
        if not text: return ""
        text = re.sub(r'[#*-]', ' ', text)
        text = re.sub(r'[^\w\s]', '', text)
        return ' '.join(text.lower().split())

    def calculate_similarity(text1, text2):
        if not text1 or not text2:
            return 0.0
        if len(text1) < 50:
            return 1.0 if text1 in text2 or text2 in text1 else 0.0
        return SequenceMatcher(None, text1, text2).quick_ratio()

    def extract_unique_paragraphs(sec_curr_content, sec_prev_content):
        if not sec_curr_content: return []
        unique = []
        paras_curr = sec_curr_content.split('\n\n')
        paras_prev_norm = [normalize_text(p) for p in sec_prev_content.split('\n\n')]
        
        for p in paras_curr:
            p_clean = p.strip()
            if not p_clean or len(p_clean) < 20: continue
            
            p_norm = normalize_text(p_clean)
            is_present = False
            for pp_norm in paras_prev_norm:
                if calculate_similarity(p_norm, pp_norm) > 0.85:
                    is_present = True
                    break
            
            if not is_present:
                unique.append(p_clean)
        return unique
    
    # ==================================================================================

    print("üßπ Iniciando deduplica√ß√£o robusta (7-DIFF Strategy)...")

    # 1. Junta e Parseia
    texto_bruto = '\n\n'.join(resultados)
    lines = texto_bruto.split('\n')
    
    sections = []
    current_section = None
    header_pattern = re.compile(r'^(#{1,6})\s+(.+)$')
    
    captured_lines = []
    intro_lines = []
    has_started = False
    
    for i, line in enumerate(lines):
        match = header_pattern.match(line)
        if match:
            has_started = True
            if current_section:
                current_section['content'] = '\n'.join(captured_lines).strip()
                sections.append(current_section)
                captured_lines = []
            
            title_text = match.group(2).strip()
            # Remove numera√ß√£o original para compara√ß√£o agn√≥stica
            title_clean = re.sub(r'^\d+(?:\.\d+)*\.?\s*', '', title_text)
            
            current_section = {
                'title_clean': title_clean,
                'level': len(match.group(1)),
                'full_header': line,
                'content': ""
            }
        else:
            if has_started:
                captured_lines.append(line)
            else:
                intro_lines.append(line)
    
    if current_section:
        current_section['content'] = '\n'.join(captured_lines).strip()
        sections.append(current_section)
        
    print(f"   üìä Analisando {len(sections)} se√ß√µes...")

    # 2. Detec√ß√£o e Remo√ß√£o
    indices_to_remove = set()
    MAX_WINDOW = 20 # Olha at√© 20 se√ß√µes para tr√°s (cobre overlaps grandes)
    
    for i in range(len(sections)):
        if i in indices_to_remove: continue
        sec_curr = sections[i]
        
        # Janela deslizante
        start_check = max(0, i - MAX_WINDOW)
        for j in range(start_check, i):
            if j in indices_to_remove: continue
            sec_prev = sections[j]
            
            if sec_curr['level'] != sec_prev['level']: continue
            
            sim_title = calculate_similarity(normalize_text(sec_curr['title_clean']), normalize_text(sec_prev['title_clean']))
            sim_content = calculate_similarity(normalize_text(sec_curr['content']), normalize_text(sec_prev['content']))
            
            is_duplicate = False
            
            if sim_title > 0.9 and sim_content > 0.6:
                is_duplicate = True
            elif sim_content > LIMIAR_7DIFF:  # Usa limiar 7-DIFF (0.85 Fidelidade / 0.80 Apostila)
                 is_duplicate = True
            elif sim_title > 0.95 and len(sec_curr['content']) < 100:
                 is_duplicate = True
            
            if is_duplicate:
                print(f"   üóëÔ∏è  Detectado: '{sec_curr['title_clean'][:30]}...' duplica se√ß√£o anterior")
                
                # Mescla conte√∫do √∫nico antes de excluir
                unique_paras = extract_unique_paragraphs(sec_curr['content'], sec_prev['content'])
                if unique_paras:
                    sections[j]['content'] += '\n\n' + '\n\n'.join(unique_paras)
                
                indices_to_remove.add(i)
                break
    
    # 3. Reconstru√ß√£o
    final_lines = list(intro_lines)
    for i, sec in enumerate(sections):
        if i in indices_to_remove: continue
        final_lines.append(sec['full_header'])
        if sec['content']:
            final_lines.append(sec['content'])
            final_lines.append("")
            
    texto_limpo = '\n'.join(final_lines)
    print(f"   ‚úÖ Removidas {len(indices_to_remove)} se√ß√µes duplicadas.")

    return texto_limpo


# ==================== v2.28: VALIDA√á√ÉO E SANITIZA√á√ÉO DE MARKDOWN ====================

class TruncamentoError(Exception):
    """Exce√ß√£o levantada quando truncamento cr√≠tico √© detectado."""
    pass


def corrigir_headings_duplicados(texto: str) -> str:
    """
    v2.28: Corrige headings duplicados como '#### #### T√≠tulo' ‚Üí '#### T√≠tulo'

    Tamb√©m normaliza varia√ß√µes como '## ## #' ‚Üí '##'
    """
    # Padr√£o: m√∫ltiplos grupos de # separados por espa√ßos (ex: "#### #### T√≠tulo")
    # Importante: exige pelo menos 1 espa√ßo entre os grupos para n√£o degradar headings normais ("#### T√≠tulo").
    pattern = r'^(#{1,6})(?:\s+#{1,6})+\s*(.*)$'

    def fix_heading(match):
        level = match.group(1)  # Primeiro conjunto de #
        title = (match.group(2) or "").strip()
        return f"{level} {title}" if title else level

    linhas = texto.split('\n')
    linhas_corrigidas = []
    correcoes = 0

    for linha in linhas:
        if re.match(r'^#{1,6}\s+#{1,6}', linha):
            linha_corrigida = re.sub(pattern, fix_heading, linha)
            if linha_corrigida != linha:
                correcoes += 1
                print(f"{Fore.YELLOW}   üîß Heading corrigido: '{linha[:50]}...' ‚Üí '{linha_corrigida[:50]}...'")
            linhas_corrigidas.append(linha_corrigida)
        else:
            linhas_corrigidas.append(linha)

    if correcoes > 0:
        print(f"{Fore.GREEN}   ‚úÖ Corrigidos {correcoes} headings duplicados")

    return '\n'.join(linhas_corrigidas)


def padronizar_separadores(texto: str, estilo: str = "remover") -> str:
    """
    v2.28: Padroniza separadores horizontais (---, ***, ___).

    Args:
        texto: Texto markdown
        estilo: 'remover' (remove todos), 'padronizar' (usa --- apenas), 'manter' (n√£o altera)

    Returns:
        Texto com separadores padronizados
    """
    if estilo == "manter":
        return texto

    # Padr√£o: linha contendo apenas h√≠fens, asteriscos ou underscores (3+)
    pattern = r'^[\s]*[-*_]{3,}[\s]*$'

    linhas = texto.split('\n')
    linhas_novas = []
    removidos = 0

    for linha in linhas:
        if re.match(pattern, linha):
            if estilo == "remover":
                removidos += 1
                continue  # Pula a linha
            elif estilo == "padronizar":
                linhas_novas.append("---")
                continue
        linhas_novas.append(linha)

    if removidos > 0:
        print(f"{Fore.CYAN}   üîß Removidos {removidos} separadores horizontais")

    return '\n'.join(linhas_novas)


def detectar_tabelas_em_par(texto: str) -> list:
    """
    v2.28: Detecta pares de tabelas (Quadro-s√≠ntese + Pegadinhas).

    Padr√£o esperado (flex√≠vel):
    - #### üìã [t√≠tulo contextual]
    - Tabela 5 colunas
    - #### üéØ [t√≠tulo contextual]
    - Tabela 3 colunas

    Returns:
        Lista de dicts com informa√ß√µes sobre cada par de tabelas
    """
    pares = []
    linhas = texto.split('\n')

    i = 0
    while i < len(linhas):
        linha = linhas[i].strip()

        # Detectar in√≠cio de quadro-s√≠ntese / tabela principal (t√≠tulo contextual com üìã)
        if re.match(r'^#{3,5}\s*üìã', linha):
            par = {
                'quadro_titulo': linha,
                'quadro_linha': i,
                'quadro_tabela_inicio': None,
                'quadro_tabela_linhas': 0,
                'pegadinha_titulo': None,
                'pegadinha_linha': None,
                'pegadinha_tabela_inicio': None,
                'pegadinha_tabela_linhas': 0,
                'completo': False
            }

            # Procurar tabela do quadro
            j = i + 1
            while j < len(linhas) and j < i + 20:
                if linhas[j].strip().startswith('|'):
                    par['quadro_tabela_inicio'] = j
                    # Contar linhas da tabela
                    k = j
                    while k < len(linhas) and linhas[k].strip().startswith('|'):
                        par['quadro_tabela_linhas'] += 1
                        k += 1
                    break
                elif re.match(r'^#{1,5}\s', linhas[j]):
                    break  # Novo heading, tabela ausente
                j += 1

            # Procurar tabela de pegadinhas
            j = (
                par['quadro_tabela_inicio'] + par['quadro_tabela_linhas']
                if par['quadro_tabela_inicio'] is not None
                else i + 1
            )
            while j < len(linhas) and j < i + 50:
                if re.match(r'^#{3,5}\s*üéØ', linhas[j]):
                    par['pegadinha_titulo'] = linhas[j].strip()
                    par['pegadinha_linha'] = j

                    # Procurar tabela de pegadinhas
                    k = j + 1
                    while k < len(linhas) and k < j + 15:
                        if linhas[k].strip().startswith('|'):
                            par['pegadinha_tabela_inicio'] = k
                            m = k
                            while m < len(linhas) and linhas[m].strip().startswith('|'):
                                par['pegadinha_tabela_linhas'] += 1
                                m += 1
                            break
                        k += 1
                    break
                elif re.match(r'^#{1,2}\s', linhas[j]):
                    break  # Novo bloco tem√°tico
                j += 1

            # Verificar se par est√° completo
            par['completo'] = (
                par['quadro_tabela_linhas'] >= 3 and  # Header + separador + pelo menos 1 dado
                par['pegadinha_tabela_linhas'] >= 3
            )

            pares.append(par)
            i = j if j > i else i + 1
        else:
            i += 1

    # Log de diagn√≥stico
    completos = sum(1 for p in pares if p['completo'])
    print(f"{Fore.CYAN}   üìä Pares de tabelas detectados: {len(pares)} ({completos} completos)")

    for p in pares:
        if not p['completo']:
            print(f"{Fore.YELLOW}   ‚ö†Ô∏è  Par incompleto: {p['quadro_titulo'][:40]}... "
                  f"(Quadro: {p['quadro_tabela_linhas']} linhas, Pegadinha: {p['pegadinha_tabela_linhas']} linhas)")

    return pares


def validar_celulas_tabela(texto: str) -> tuple:
    """
    v2.28: Valida integridade das c√©lulas de tabela.

    Detecta:
    1. C√©lulas truncadas (texto cortado no meio de palavra)
    2. Headers incompletos (ex: 'Comcobra' em vez de 'Como a banca cobra')
    3. Linhas de tabela sem fechamento de pipe

    Returns:
        Tuple (is_valid, list of issues)
    """
    issues = []
    linhas = texto.split('\n')

    # Padr√µes conhecidos de truncamento
    TRUNCAMENTOS_CONHECIDOS = [
        (r'\bonto\b', 'truncamento de "o territ√≥rio" ou similar'),
        (r'Comcobra', 'header truncado: "Como a banca cobra"'),
        (r'urban√≠stamos', 'palavra cortada: "urban√≠stico. Vamos"'),
        (r'\bsitua\s+compet√™ncia', 'frase cortada'),
        (r'\bEls\s+sobre', 'in√≠cio de frase cortada'),
        (r'\bou\s+[a-z]{1,3}\s+[A-Z]', 'poss√≠vel corte no meio de frase'),
    ]

    for i, linha in enumerate(linhas):
        # Verificar padr√µes de truncamento
        for pattern, desc in TRUNCAMENTOS_CONHECIDOS:
            if re.search(pattern, linha):
                issues.append({
                    'tipo': 'truncamento',
                    'linha': i + 1,
                    'descricao': desc,
                    'texto': linha[:100] + '...' if len(linha) > 100 else linha
                })

        # Verificar c√©lulas de tabela
        if linha.strip().startswith('|'):
            # Linha de tabela deve terminar com |
            if not linha.strip().endswith('|'):
                issues.append({
                    'tipo': 'tabela_aberta',
                    'linha': i + 1,
                    'descricao': 'Linha de tabela n√£o fechada com |',
                    'texto': linha[-50:] if len(linha) > 50 else linha
                })

            # Verificar c√©lulas muito curtas (poss√≠vel truncamento)
            celulas = linha.split('|')[1:-1]  # Remove primeiro e √∫ltimo vazio
            for j, celula in enumerate(celulas):
                celula_limpa = celula.strip()
                # C√©lula com menos de 3 chars e n√£o √© separador pode ser truncamento
                if len(celula_limpa) < 3 and not re.match(r'^[-:]+$', celula_limpa) and celula_limpa != '‚Äî':
                    issues.append({
                        'tipo': 'celula_suspeita',
                        'linha': i + 1,
                        'descricao': f'C√©lula {j+1} muito curta: "{celula_limpa}"',
                        'texto': linha[:80]
                    })

    is_valid = len(issues) == 0

    if not is_valid:
        print(f"{Fore.RED}   ‚ö†Ô∏è  Encontrados {len(issues)} problemas de integridade:")
        for issue in issues[:5]:  # Mostrar no m√°ximo 5
            print(f"{Fore.YELLOW}      L{issue['linha']}: {issue['descricao']}")
        if len(issues) > 5:
            print(f"{Fore.YELLOW}      ... e mais {len(issues) - 5} problemas")

    return is_valid, issues


def chunk_texto_seguro(texto: str, max_chars: int = 25000, overlap_chars: int = 2000) -> list:
    """
    v2.28: Chunking inteligente que respeita limites naturais do texto.

    Prioridades de corte (em ordem):
    1. Antes de heading ## ou ### (novo bloco tem√°tico)
    2. Ap√≥s tabela completa (#### üéØ + tabela)
    3. Par√°grafo duplo (\\n\\n)
    4. Final de frase (. seguido de espa√ßo ou newline)
    5. Qualquer newline

    Nunca corta:
    - No meio de uma tabela
    - No meio de uma palavra
    - Imediatamente ap√≥s heading (deixa pelo menos 500 chars)

    Args:
        texto: Texto completo
        max_chars: Tamanho m√°ximo de cada chunk
        overlap_chars: Caracteres de overlap entre chunks

    Returns:
        Lista de chunks com integridade preservada
    """
    if len(texto) <= max_chars:
        return [texto]

    chunks = []
    inicio = 0

    # Pr√©-processar: identificar zonas "proibidas" para corte
    zonas_proibidas = []  # Lista de (inicio, fim) onde n√£o cortar

    # Encontrar todas as tabelas
    linhas = texto.split('\n')
    pos = 0
    em_tabela = False
    tabela_inicio = 0

    for i, linha in enumerate(linhas):
        if linha.strip().startswith('|') and not em_tabela:
            em_tabela = True
            tabela_inicio = pos
        elif not linha.strip().startswith('|') and em_tabela:
            em_tabela = False
            zonas_proibidas.append((tabela_inicio, pos))
        pos += len(linha) + 1  # +1 pelo \n
    if em_tabela:
        zonas_proibidas.append((tabela_inicio, pos))

    def esta_em_zona_proibida(posicao):
        for inicio_z, fim_z in zonas_proibidas:
            if inicio_z <= posicao <= fim_z:
                return True
        return False

    def encontrar_ponto_corte_seguro(texto_slice, pos_inicio_global):
        """Encontra o melhor ponto de corte dentro do slice."""

        # Zona de busca: √∫ltimos 30% do chunk
        zona_busca_inicio = int(len(texto_slice) * 0.7)
        zona_busca = texto_slice[zona_busca_inicio:]

        # Prioridade 1: Antes de heading ## ou ###
        headings = list(re.finditer(r'(?m)^#{2,3}\s+', zona_busca))
        if headings:
            pos = headings[-1].start()
            pos_global = pos_inicio_global + zona_busca_inicio + pos
            if not esta_em_zona_proibida(pos_global):
                return zona_busca_inicio + pos

        # Prioridade 2: Ap√≥s tabela de pegadinhas (üéØ)
        match = re.search(r'\n(?=####?\s*üéØ)', zona_busca)
        if match:
            # Encontrar fim da tabela ap√≥s o heading
            after_heading = zona_busca[match.end():]
            # Procurar fim da tabela (linha que n√£o come√ßa com |)
            lines_after = after_heading.split('\n')
            pos_apos_tabela = match.end()
            for j, line in enumerate(lines_after):
                if j > 2 and not line.strip().startswith('|'):  # Passou da tabela
                    pos_apos_tabela += sum(len(l) + 1 for l in lines_after[:j])
                    break
            pos_global = pos_inicio_global + zona_busca_inicio + pos_apos_tabela
            if not esta_em_zona_proibida(pos_global) and pos_apos_tabela < len(zona_busca):
                return zona_busca_inicio + pos_apos_tabela

        # Prioridade 3: Par√°grafo duplo
        pos = zona_busca.rfind('\n\n')
        if pos != -1:
            pos_global = pos_inicio_global + zona_busca_inicio + pos
            if not esta_em_zona_proibida(pos_global):
                return zona_busca_inicio + pos + 2  # +2 para incluir os \n\n

        # Prioridade 4: Final de frase
        finais_frase = list(re.finditer(r'[.!?][\s\n]+', zona_busca))
        if finais_frase:
            pos = finais_frase[-1].end()
            pos_global = pos_inicio_global + zona_busca_inicio + pos
            if not esta_em_zona_proibida(pos_global):
                return zona_busca_inicio + pos

        # Prioridade 5: Qualquer newline
        pos = zona_busca.rfind('\n')
        if pos != -1:
            pos_global = pos_inicio_global + zona_busca_inicio + pos
            if not esta_em_zona_proibida(pos_global):
                return zona_busca_inicio + pos + 1

        # Fallback: cortar no max_chars mesmo
        return len(texto_slice)

    print(f"{Fore.CYAN}   üî™ Iniciando chunking seguro (max: {max_chars} chars, overlap: {overlap_chars})...")

    while inicio < len(texto):
        fim_ideal = min(inicio + max_chars, len(texto))

        if fim_ideal >= len(texto):
            # √öltimo chunk
            chunks.append(texto[inicio:])
            break

        texto_slice = texto[inicio:fim_ideal]
        ponto_corte_relativo = encontrar_ponto_corte_seguro(texto_slice, inicio)
        fim_real = inicio + ponto_corte_relativo

        chunk = texto[inicio:fim_real].strip()
        chunks.append(chunk)

        print(f"{Fore.GREEN}   ‚úÇÔ∏è  Chunk {len(chunks)}: {inicio} ‚Üí {fim_real} ({len(chunk)} chars)")

        # Pr√≥ximo in√≠cio com overlap
        inicio = fim_real - overlap_chars if fim_real > overlap_chars else fim_real

    print(f"{Fore.GREEN}   ‚úÖ Criados {len(chunks)} chunks com integridade preservada")

    return chunks


SEGMENT_BOUNDARY_RE = re.compile(
    r'(?m)(?=^\s*(?:'
    r'\[\d{1,2}:\d{2}(?::\d{2})?\]\s+'
    r'|\*\*[^*]{1,40}\*\*:\s+'
    r'|(?:SPEAKER|FALANTE)\s*\d{1,3}\s*[:\-]'
    r'))'
)


def _segmentar_texto_para_mapeamento(texto: str) -> list[str]:
    """Segmenta o texto em blocos naturais (timestamps/speaker labels) para mapeamento."""
    if not texto:
        return []
    if not SEGMENT_BOUNDARY_RE.search(texto):
        return []
    partes = [p for p in re.split(SEGMENT_BOUNDARY_RE, texto) if p and p.strip()]
    return partes


def chunk_texto_por_segmentos(
    texto: str,
    *,
    max_chars: int = 25000,
    overlap_chars: int = 2000,
    min_segments: int = 3,
) -> Optional[list[str]]:
    """
    Chunking baseado em segmentos (timestamps/speaker labels), evitando cortes artificiais.
    Retorna None quando n√£o h√° segmentos suficientes.
    """
    segmentos = _segmentar_texto_para_mapeamento(texto)
    if not segmentos or len(segmentos) < min_segments:
        return None

    chunks: list[str] = []
    cur: list[str] = []
    cur_len = 0

    def _flush_current():
        if not cur:
            return
        chunks.append("\n\n".join(cur).strip())

    def _build_overlap(prev_segments: list[str]) -> list[str]:
        if not prev_segments or overlap_chars <= 0:
            return []
        overlap_list: list[str] = []
        acc = 0
        for seg in reversed(prev_segments):
            seg_len = len(seg) + (2 if overlap_list else 0)
            acc += seg_len
            overlap_list.insert(0, seg)
            if acc >= overlap_chars:
                break
        return overlap_list

    for seg in segmentos:
        seg = seg.strip()
        if not seg:
            continue
        seg_len = len(seg) + (2 if cur else 0)
        if cur and (cur_len + seg_len) > max_chars:
            _flush_current()
            cur = _build_overlap(cur)
            cur_len = sum(len(s) for s in cur) + max(0, len(cur) - 1) * 2
        if not cur:
            cur = [seg]
            cur_len = len(seg)
        else:
            cur.append(seg)
            cur_len += seg_len

    _flush_current()
    return [c for c in chunks if c]


def validar_integridade_pos_merge(texto: str, raise_on_error: bool = False) -> tuple:
    """
    v2.28: Valida√ß√£o completa de integridade ap√≥s merge de chunks.

    Verifica:
    1. Palavras cortadas no meio
    2. Headers incompletos
    3. Tabelas abertas (sem fechamento)
    4. Padr√µes conhecidos de truncamento

    Args:
        texto: Texto merged
        raise_on_error: Se True, levanta TruncamentoError em problemas cr√≠ticos

    Returns:
        Tuple (is_valid, issues, texto_corrigido)
    """
    issues = []
    texto_corrigido = texto

    # 1. Validar c√©lulas de tabela
    is_table_valid, table_issues = validar_celulas_tabela(texto)
    issues.extend(table_issues)

    # 2. Verificar headings duplicados
    if re.search(r'^#{1,6}\s*#{1,6}', texto, re.MULTILINE):
        texto_corrigido = corrigir_headings_duplicados(texto_corrigido)
        issues.append({
            'tipo': 'heading_duplicado',
            'descricao': 'Headings duplicados encontrados e corrigidos',
            'linha': 0
        })

    # 3. Detectar pares de tabelas incompletos
    pares = detectar_tabelas_em_par(texto)
    incompletos = [p for p in pares if not p['completo']]
    for p in incompletos:
        issues.append({
            'tipo': 'par_tabela_incompleto',
            'descricao': f"Par incompleto: {p['quadro_titulo'][:40]}",
            'linha': p['quadro_linha']
        })

    # 4. Padr√µes de texto truncado (mais gen√©ricos)
    pattern_truncado = r'\b([a-z√°√©√≠√≥√∫√†√¢√£√™√Æ√¥√ª√ß]{2,})\s{2,}([a-z√°√©√≠√≥√∫√†√¢√£√™√Æ√¥√ª√ß]{2,})\b'
    matches = list(re.finditer(pattern_truncado, texto, re.IGNORECASE))
    for match in matches[:5]:  # Limitar a 5
        # Verificar se parece truncamento (palavras soltas)
        antes = match.group(1)
        depois = match.group(2)
        if len(antes) < 6 and len(depois) < 6:
            issues.append({
                'tipo': 'possivel_truncamento',
                'descricao': f'Poss√≠vel corte: "{antes} {depois}"',
                'linha': texto[:match.start()].count('\n') + 1
            })

    is_valid = len([i for i in issues if i['tipo'] in ['truncamento', 'tabela_aberta']]) == 0

    if raise_on_error and not is_valid:
        criticos = [i for i in issues if i['tipo'] in ['truncamento', 'tabela_aberta']]
        raise TruncamentoError(f"Detectados {len(criticos)} problemas cr√≠ticos de truncamento")

    return is_valid, issues, texto_corrigido


def remover_marcadores_continua(texto: str) -> str:
    """
    Remove marcadores artificiais de continua√ß√£o inseridos pelo LLM.

    Exemplos removidos:
    - [continua], [continua√ß√£o], [continuacao]
    - (continua), (continua√ß√£o), (continuacao)
    """
    if not texto:
        return texto

    # Linha isolada com marcador
    out = re.sub(
        r"(?im)^[ \t]*(?:\[\s*(?:continua|continua√ß√£o|continuacao)\s*\]|\(\s*(?:continua|continua√ß√£o|continuacao)\s*\))[ \t]*\n",
        "",
        texto,
    )

    # Marcador inline (substitui por um espa√ßo)
    out = re.sub(
        r"(?i)\s*(?:\[\s*(?:continua|continua√ß√£o|continuacao)\s*\]|\(\s*(?:continua|continua√ß√£o|continuacao)\s*\))\s*",
        " ",
        out,
    )

    # Normalizar espa√ßos m√∫ltiplos gerados pela remo√ß√£o
    out = re.sub(r"[ \t]{2,}", " ", out)
    out = re.sub(r"\n{3,}", "\n\n", out)
    return out


def sanitizar_markdown_final(texto: str) -> str:
    """
    v2.28: Sanitiza√ß√£o final do markdown antes de salvar.

    Aplica todas as corre√ß√µes em sequ√™ncia:
    1. Corrige headings duplicados
    2. Padroniza separadores
    3. Remove linhas em branco excessivas
    4. Valida integridade (sem raise)

    Returns:
        Texto sanitizado
    """
    print(f"{Fore.CYAN}üßπ Sanitizando markdown final...")

    # 1. Headings duplicados
    texto = corrigir_headings_duplicados(texto)

    # 2. Separadores
    texto = padronizar_separadores(texto, estilo="remover")

    # 3. Linhas em branco excessivas (mais de 2 consecutivas ‚Üí 2)
    texto = re.sub(r'\n{4,}', '\n\n\n', texto)

    # 3.5 Remover vocativos/g√≠rias em forma de chamada ("Meu irm√£o,", "cara,", ...)
    texto = remover_vocativos_girias(texto)

    # 3.6 Normalizar refer√™ncias "Tema" frequentemente erradas por ASR (ex.: 234‚Üí1.234, 1933‚Üí1.033)
    texto = normalizar_temas_markdown(texto)

    # 3.7 Remover marcadores artificiais de continua√ß√£o (ex.: "[continua]")
    texto = remover_marcadores_continua(texto)

    # 4. Valida√ß√£o (sem raise, apenas log)
    is_valid, issues, texto = validar_integridade_pos_merge(texto, raise_on_error=False)

    if is_valid:
        print(f"{Fore.GREEN}   ‚úÖ Markdown validado sem problemas cr√≠ticos")
    else:
        print(f"{Fore.YELLOW}   ‚ö†Ô∏è  {len(issues)} issues encontradas (n√£o-cr√≠ticas mantidas)")

    return texto


def normalizar_temas_markdown(texto: str) -> str:
    """
    Normaliza varia√ß√µes comuns de "Tema" geradas por ASR/edi√ß√£o que criam refer√™ncias inexistentes.

    Regras (conservadoras):
    - S√≥ corrige quando o documento j√° cont√©m a forma can√¥nica.
    - Remove par√™nteses/apostos que preservam variantes erradas (ex.: "(ou 1933)").
    """
    if not texto:
        return texto

    out = texto

    def _has(pattern: str) -> bool:
        try:
            return re.search(pattern, out, flags=re.IGNORECASE) is not None
        except re.error:
            return False

    # 234 -> 1.234 (apenas se "Tema 1.234" j√° aparece no documento)
    if _has(r"\b[Tt]ema\s+1\.234\b"):
        out = re.sub(r"\b([Tt]ema)\s+234\b", r"\1 1.234", out)
        # Remove explica√ß√µes que mant√™m a variante errada (somente casos t√≠picos: "(ou 234)", "(tema 234)")
        out = re.sub(r"\s*\(\s*(?:ou\s+)?(?:tema\s+)?234\s*\)", "", out, flags=re.IGNORECASE)

    # 1933 / 1.933 -> 1.033 (apenas se "Tema 1.033" j√° aparece no documento)
    if _has(r"\b[Tt]ema\s+1\.033\b"):
        # Normalize numeric variants
        out = re.sub(r"\b([Tt]ema)\s+1933\b", r"\1 1.033", out)
        out = re.sub(r"\b([Tt]ema)\s+1\.933\b", r"\1 1.033", out)
        out = re.sub(r"\b([Tt]ema)\s+1\s*933\b", r"\1 1.033", out)
        # Remove parenthetical that keeps wrong aliases (somente casos t√≠picos: "(ou 1933)", "(tema 1933)")
        out = re.sub(r"\s*\(\s*(?:ou\s+)?(?:tema\s+)?(?:1933|1\.933)\s*\)", "", out, flags=re.IGNORECASE)
        # Normalize table-style combos: "Tema 1.033 / 1933" etc.
        out = re.sub(r"\b(Tema\s+1\.033)\s*/\s*(?:1933|1\.933)\b", r"\1", out, flags=re.IGNORECASE)
        out = re.sub(r"\b(Tema\s+1\.033)\s*\(\s*ou\s*(?:1933|1\.933)\s*\)", r"\1", out, flags=re.IGNORECASE)

    return out


# ==================== HELPERS PORTED FROM GPT SCRIPT ====================

def limpar_tags_xml(texto):
    texto = re.sub(r'</?[a-z_][\w\-]*>', '', texto, flags=re.IGNORECASE)
    texto = re.sub(r'<[a-z_][\w\-]*\s+[^>]+>', '', texto, flags=re.IGNORECASE)
    return texto


def remover_vocativos_girias(texto: str) -> str:
    """
    Remove vocativos/g√≠rias comuns que n√£o agregam conte√∫do e n√£o devem constar no formatado.
    Ex.: "Meu irm√£o,", "cara,", "mano!", "minha gente:" etc.

    Observa√ß√£o: aplica apenas em texto fora de code fences e com pontua√ß√£o t√≠pica de vocativo,
    para evitar apagar informa√ß√£o factual ("meu irm√£o" como parentesco) quando n√£o estiver em forma de vocativo.
    """
    if not texto:
        return texto

    vocativos = [
        r"meu\s+irm[a√£]o",
        r"mano",
        r"cara",
        r"minha\s+gente",
        r"galera",
        r"meu\s+velho",
    ]
    voc = "|".join(vocativos)
    # Start-of-line vocative: "Meu irm√£o, ..."
    re_start = re.compile(rf"^(\s*)(?:{voc})\s*[,!?:;\-‚Äì‚Äî]+\s*", flags=re.IGNORECASE)
    # Mid-line after sentence boundary: ". Meu irm√£o, ..."
    re_mid = re.compile(rf"([.!?:;])\s+(?:{voc})\s*[,!?:;\-‚Äì‚Äî]+\s*", flags=re.IGNORECASE)

    out_lines = []
    in_fence = False
    for line in texto.splitlines():
        stripped = line.strip()
        if stripped.startswith("```"):
            in_fence = not in_fence
            out_lines.append(line)
            continue
        if in_fence:
            out_lines.append(line)
            continue
        new_line = re_start.sub(r"\1", line)
        new_line = re_mid.sub(r"\1 ", new_line)
        out_lines.append(new_line)
    return "\n".join(out_lines)

# ==================== LOGGER SHIM ====================
class Logger:
    def info(self, msg):
        print(f"{Fore.CYAN}{msg}")
    def warning(self, msg):
        print(f"{Fore.YELLOW}{msg}")
    def error(self, msg):
        print(f"{Fore.RED}{msg}")

logger = Logger()

# ==================== FUNCIONALIDADES PORTADAS DO GEMINI SCRIPT ====================

# ==================== SMART STITCHING & FIDELIDADE (V2.10 PROTADAS) ====================

def remover_eco_do_contexto(resposta_api, contexto_enviado):
    """
    Remove o in√≠cio da resposta se for apenas um 'eco' do final do contexto.
    """
    if not contexto_enviado or not resposta_api:
        return resposta_api

    final_contexto = contexto_enviado.strip()[-300:]
    inicio_resposta = resposta_api.strip()[:300]

    matcher = difflib.SequenceMatcher(None, final_contexto, inicio_resposta)
    match = matcher.find_longest_match(0, len(final_contexto), 0, len(inicio_resposta))

    if match.size > 50:
        print(f"Scissors Eco detectado! Removendo {match.size} chars repetidos no in√≠cio.")
        return resposta_api.strip()[match.size:].strip()
    
    return resposta_api


def _extract_style_context(text: str, max_chars: int = 2500) -> str:
    """
    v2.27: Extrai um contexto de estilo maior, INCLUINDO tabelas recentes para continuidade.
    
    Melhorias v2.27:
    - Se houver tabela nas √∫ltimas 100 linhas, inclui a tabela COMPLETA no contexto
    - Permite contexto maior (at√© 1.5x max_chars) quando h√° tabela
    - Garante que o LLM veja a estrutura da tabela do chunk anterior
    """
    if not text:
        return ""

    lines = text.splitlines()
    
    # v2.27: Detectar se h√° tabela recente que deve ser inclu√≠da
    last_table_start = None
    last_table_end = None
    search_range = min(100, len(lines))
    
    for i in range(len(lines) - 1, max(0, len(lines) - search_range), -1):
        line = lines[i].strip()
        if line.startswith('|') and line.endswith('|') and '---' not in line:
            if last_table_end is None:
                last_table_end = i
            last_table_start = i
        elif last_table_start is not None and not line.startswith('|'):
            # Encontramos o in√≠cio da tabela (linha antes n√£o √© tabela)
            break
    
    # Se h√° tabela recente, incluir ela completa no contexto
    if last_table_start is not None and last_table_end is not None:
        # Incluir algumas linhas antes da tabela (t√≠tulo/contexto)
        table_context_start = max(0, last_table_start - 5)
        table_with_context = '\n'.join(lines[table_context_start:])
        
        # Permitir contexto maior se tiver tabela (at√© 1.5x)
        extended_max = int(max_chars * 1.5)
        if len(table_with_context) <= extended_max:
            return table_with_context
        else:
            # Tabela √© muito grande, pegar s√≥ as √∫ltimas linhas dela
            return table_with_context[-extended_max:]
    
    # Fallback: comportamento original (filtrar tabelas do contexto de estilo)
    filtered = []
    for ln in lines:
        s = ln.strip()
        # Remove linhas de tabela markdown e separadores
        if (s.startswith('|') and s.endswith('|')) or re.match(r'^\s*\|[\s:|-]+\|[\s:|-]*$', s):
            continue
        # Remove t√≠tulos de quadros/tabelas no contexto (para n√£o "puxar" s√≥ o fechamento)
        if re.match(r'^#{3,5}\s*[üìãüéØ].*', s):
            continue
        filtered.append(ln)

    filtered_text = "\n".join(filtered).strip()
    candidate = filtered_text[-max_chars:] if len(filtered_text) > max_chars else filtered_text
    # Fallback: se filtrou demais, usa o fim do texto original
    if len(candidate.split()) < 30:
        candidate = text[-max_chars:] if len(text) > max_chars else text
    return candidate

def titulos_sao_similares(t1, t2, threshold=0.90):
    """Verifica se dois t√≠tulos s√£o semanticamente iguais (fuzzy matching)."""
    def normalizar(t):
        return re.sub(r'[^a-z0-9 ]', '', t.lower())
    
    nt1 = normalizar(t1)
    nt2 = normalizar(t2)
    
    if not nt1 or not nt2: return False
    
    # PROTE√á√ÉO 1: Diferen√ßa de tamanho
    nt1_compact = nt1.replace(' ', '')
    nt2_compact = nt2.replace(' ', '')
    len_ratio = min(len(nt1_compact), len(nt2_compact)) / max(len(nt1_compact), len(nt2_compact))
    if len_ratio < 0.8: return False
    
    # PROTE√á√ÉO 2: Verifica√ß√£o por palavras exclusivas
    palavras1 = set(nt1.split())
    palavras2 = set(nt2.split())
    diferenca = palavras1.symmetric_difference(palavras2)
    if any(len(w) > 3 for w in diferenca): return False
        
    return difflib.SequenceMatcher(None, nt1_compact, nt2_compact).ratio() > threshold

def limpar_inicio_redundante(texto_novo, texto_acumulado):
    """Remove t√≠tulo no in√≠cio do novo chunk se similar ao do acumulado."""
    if not texto_acumulado.strip(): return texto_novo

    ultimas_linhas = texto_acumulado.strip().split('\n')[-30:]
    ultimo_titulo = None
    for linha in reversed(ultimas_linhas):
        if linha.strip().startswith('##'):
            ultimo_titulo = re.sub(r'^#+\s*(?:\d+(?:\.\d+)*\.?)?\s*', '', linha).strip()
            break
    
    if not ultimo_titulo: return texto_novo

    linhas_novas = texto_novo.strip().split('\n')
    for i, linha in enumerate(linhas_novas[:10]):
        if linha.strip().startswith('##'):
            novo_titulo = re.sub(r'^#+\s*(?:\d+(?:\.\d+)*\.?)?\s*', '', linha).strip()
            if titulos_sao_similares(ultimo_titulo, novo_titulo):
                print(f"Scissors T√≠tulo duplicado na jun√ß√£o: '{novo_titulo}' ‚âà '{ultimo_titulo}'")
                return '\n'.join(linhas_novas[i+1:])
    return texto_novo

def detectar_secoes_duplicadas(texto):
    """v2.11: Detecta se√ß√µes duplicadas por t√≠tulos (Fuzzy Matching) - Inclui H3 e normaliza '(Continua√ß√£o)'"""
    print("Magnifying glass tilt left Detectando se√ß√µes duplicadas (fuzzy)...")
    
    linhas = texto.split('\n')
    titulos_vistos = []
    secoes_duplicadas = []
    
    for i, linha in enumerate(linhas):
        linha_strip = linha.strip()
        # Detecta H2 (##) e H3 (###)
        if linha_strip.startswith('##'):
            # Remove numera√ß√£o e emojis
            titulo_normalizado = re.sub(r'^#{2,4}\s*\d+(?:\.\d+)*\.?\s*', '', linha_strip)
            titulo_normalizado = re.sub(r'[üìãüìäüóÇ]', '', titulo_normalizado).strip()
            # Remove "(Continua√ß√£o)" para compara√ß√£o
            titulo_para_comparar = re.sub(r'\s*\(Continua√ß√£o\)\s*$', '', titulo_normalizado, flags=re.IGNORECASE).strip()
            
            duplicado = False
            for t_visto, linha_visto in titulos_vistos:
                if titulos_sao_similares(titulo_para_comparar, t_visto):
                    print(f"Warning Duplicado (fuzzy): '{linha_strip[:50]}...' ‚âà '{t_visto[:50]}...'")
                    secoes_duplicadas.append({
                        'titulo': titulo_normalizado,
                        'primeira_linha': linha_visto,
                        'duplicada_linha': i
                    })
                    duplicado = True
                    break
            
            if not duplicado:
                titulos_vistos.append((titulo_para_comparar, i))
    
    if secoes_duplicadas:
        print(f"Cross mark {len(secoes_duplicadas)} se√ß√µes duplicadas detectadas!")
    else:
        print("Check mark button Nenhuma se√ß√£o duplicada detectada")
    
    return secoes_duplicadas

def remover_secoes_duplicadas(texto, mode="APOSTILA"):
    """v2.17: Remove se√ß√µes duplicadas com LIMIAR ADAPTATIVO por modo.
    
    Usa LIMIAR_SECOES diferenciado - mais cuidado pois professor pode repetir propositalmente.
    - FIDELIDADE: 0.70 (mais conservador)
    - APOSTILA: 0.60 (mais agressivo)
    """
    from difflib import SequenceMatcher
    
    # Limiares adaptativos por camada de deduplica√ß√£o
    # Se√ß√µes duplicadas: mais cuidado, professor pode repetir propositalmente
    LIMIAR_SECOES = 0.70 if mode == "FIDELIDADE" else 0.60
    
    secoes_dup = detectar_secoes_duplicadas(texto)
    if not secoes_dup: return texto
    
    print("Broom Removendo se√ß√µes duplicadas (Smart Dedupe v2.14)...")
    linhas = texto.split('\n')
    
    # Rastreia o √öLTIMO segmento adicionado para cada t√≠tulo (para evitar dilui√ß√£o na compara√ß√£o)
    ultimo_segmento_visto = {}  # titulo_normalizado -> √∫ltimo texto adicionado
    linhas_para_remover = set()
    linhas_para_adicionar_separador = set()
    
    for dup in secoes_dup:
        # --- 1. Extrair Conte√∫do Original ---
        idx_orig = dup['primeira_linha']
        header_orig = linhas[idx_orig].strip()
        match_orig = re.match(r'^(#+)', header_orig)
        nivel_orig = len(match_orig.group(1)) if match_orig else 2
        
        # Extrai conte√∫do da se√ß√£o original
        content_orig = []
        for i in range(idx_orig + 1, len(linhas)):
            line = linhas[i].strip()
            if line.startswith('#'):
                match_now = re.match(r'^(#+)', line)
                if match_now and len(match_now.group(1)) <= nivel_orig: break
            content_orig.append(linhas[i])
        text_orig = "\n".join(content_orig)
        
        titulo_key = re.sub(r'^#{2,4}\s*\d+(?:\.\d+)*\.?\s*', '', header_orig)
        titulo_key = re.sub(r'\s*\(Continua√ß√£o\)\s*$', '', titulo_key, flags=re.IGNORECASE).strip()
        
        # Inicializa o rastreamento se for a primeira vez
        if titulo_key not in ultimo_segmento_visto:
            ultimo_segmento_visto[titulo_key] = text_orig

        # --- 2. Extrair Conte√∫do Duplicado ---
        idx_dup = dup['duplicada_linha']
        header_dup = linhas[idx_dup].strip()
        match_dup = re.match(r'^(#+)', header_dup)
        nivel_dup = len(match_dup.group(1)) if match_dup else 2
        
        fim_dup_idx = len(linhas)
        content_dup = []
        for i in range(idx_dup + 1, len(linhas)):
            line = linhas[i].strip()
            if line.startswith('#'):
                match_now = re.match(r'^(#+)', line)
                if match_now and len(match_now.group(1)) <= nivel_dup:
                    fim_dup_idx = i
                    break
            content_dup.append(linhas[i])
        text_dup = "\n".join(content_dup)
        
        # --- 3. Comparar Conte√∫do (L√≥gica Janelada v2.16 MELHORADA) ---
        # Compara APENAS com o √∫ltimo segmento conhecido dessa se√ß√£o
        texto_referencia = ultimo_segmento_visto.get(titulo_key, text_orig)
        
        len_dup = len(text_dup.strip())
        len_ref = len(texto_referencia.strip())
        
        # v2.16: NOVIDADE - Verificar se o t√≠tulo √© 100% id√™ntico ap√≥s normaliza√ß√£o
        # Se for, for√ßa a mesclagem independente do conte√∫do (evita redund√¢ncia de temas)
        titulo_dup_key = re.sub(r'^#{2,4}\s*\d+(?:\.\d+)*\.?\s*', '', header_dup)
        titulo_dup_key = re.sub(r'\s*\(Continua√ß√£o\)\s*$', '', titulo_dup_key, flags=re.IGNORECASE).strip()
        
        titulos_identicos = (titulo_key.lower() == titulo_dup_key.lower())
        
        # L√≥gica de decis√£o baseada em tamanho
        if len_dup < 50:
            # Duplicado curto demais = lixo, deletar
            sim = 1.0
            print(f"   ‚ö†Ô∏è  Se√ß√£o duplicada muito curta ({len_dup} chars) - marcando para remo√ß√£o")
        elif titulos_identicos and len_dup > 100:
            # v2.16: T√≠tulos id√™nticos = for√ßa mesclagem, mant√©m conte√∫do novo sob o mesmo t√≥pico
            sim = 0.5  # Valor que for√ßa mesclagem (remove header, mant√©m conte√∫do)
            print(f"   üîÑ  T√≠tulos 100% id√™nticos - for√ßando mesclagem de conte√∫do")
        elif len_ref < 50 and len_dup >= 200:
            # Original curto, mas duplicado substancial = original estava incompleto
            # Manter o duplicado como novo conte√∫do
            sim = 0.0
            print(f"   ‚ÑπÔ∏è  Original curto ({len_ref}c), duplicado substancial ({len_dup}c) - mantendo novo conte√∫do")
        else:
            sim = SequenceMatcher(None, texto_referencia, text_dup).ratio()
            
        print(f"   Similaridade: {sim:.1%} | Linha {idx_dup} | '{titulo_key[:40]}...'")

        
        if sim > LIMIAR_SECOES:  # Usa limiar de se√ß√µes (0.70 Fidelidade / 0.60 Apostila)
            print(f"   üóëÔ∏è  Removendo SE√á√ÉO INTEIRA (Duplicata confirmada)")
            for i in range(idx_dup, fim_dup_idx):
                linhas_para_remover.add(i)
        else:
            print(f"   üîó Mesclando conte√∫do (Nova informa√ß√£o detectada)")
            linhas_para_remover.add(idx_dup)
            if idx_dup + 1 < len(linhas):
                linhas_para_adicionar_separador.add(idx_dup + 1)
            
            # ATUALIZA o √∫ltimo segmento visto para a pr√≥xima compara√ß√£o
            ultimo_segmento_visto[titulo_key] = text_dup

    # --- 4. Reconstru√ß√£o ---
    linhas_limpas = []
    for i, linha in enumerate(linhas):
        if i in linhas_para_remover:
            continue
        if i in linhas_para_adicionar_separador:
            linhas_limpas.append("") 
        linhas_limpas.append(linha)
        
    print(f"Check mark button {len(linhas_para_remover)} linhas removidas")
    return '\n'.join(linhas_limpas)


def remover_paragrafos_duplicados(texto: str, min_chars: int = 80) -> str:
    """
    v2.17: Remove par√°grafos duplicados dentro do documento.
    
    L√≥gica:
    - Divide o texto em blocos (separados por linhas em branco).
    - Normaliza cada bloco (lowercase, sem pontua√ß√£o extra).
    - Mant√©m apenas a primeira ocorr√™ncia de cada bloco normalizado.
    - Ignora blocos muito curtos (< min_chars) para n√£o afetar listas.
    - Preserva tabelas e headers intactos.
    
    Args:
        texto: Texto markdown completo
        min_chars: Tamanho m√≠nimo do par√°grafo para considerar na deduplica√ß√£o
    
    Returns:
        Texto com par√°grafos duplicados removidos
    """
    import unicodedata
    
    print("üîÑ Removendo par√°grafos duplicados (v2.17)...")
    
    # Dividir em blocos por linhas em branco duplas
    blocos = re.split(r'\n\s*\n', texto)
    
    vistos = set()
    blocos_limpos = []
    removidos = 0
    
    for bloco in blocos:
        bloco_stripped = bloco.strip()
        
        # Preservar headers, tabelas e blocos curtos sem verifica√ß√£o
        if (bloco_stripped.startswith('#') or 
            bloco_stripped.startswith('|') or 
            bloco_stripped.startswith('```') or
            bloco_stripped.startswith('> [!') or
            len(bloco_stripped) < min_chars):
            blocos_limpos.append(bloco)
            continue
        
        # Normalizar para compara√ß√£o
        # Remove pontua√ß√£o, m√∫ltiplos espa√ßos, lowercase
        normalizado = bloco_stripped.lower()
        normalizado = re.sub(r'[^\w\s]', '', normalizado)  # Remove pontua√ß√£o
        normalizado = re.sub(r'\s+', ' ', normalizado).strip()  # Normaliza espa√ßos
        # Remove acentos para compara√ß√£o mais robusta
        normalizado = unicodedata.normalize('NFKD', normalizado)
        normalizado = normalizado.encode('ASCII', 'ignore').decode('ASCII')
        
        # Hash do conte√∫do normalizado
        bloco_hash = hash(normalizado)
        
        if bloco_hash in vistos:
            removidos += 1
            continue  # Pula duplicata
        
        vistos.add(bloco_hash)
        blocos_limpos.append(bloco)
    
    if removidos > 0:
        print(f"   ‚úÖ {removidos} par√°grafos duplicados removidos")
    else:
        print(f"   ‚ÑπÔ∏è  Nenhum par√°grafo duplicado encontrado")
    
    return '\n\n'.join(blocos_limpos)


def remover_titulos_orfaos(texto: str, similaridade_minima: float = 0.85) -> str:
    """
    v2.17: Remove linhas que s√£o varia√ß√µes de t√≠tulos H2 j√° existentes.
    
    Detecta e remove:
    - Linhas bold que repetem um H2 (ex: **3. Execu√ß√µes...**)
    - H3 que s√£o c√≥pias de H2 (ex: ### Execu√ß√µes... quando j√° existe ## Execu√ß√µes...)
    - Numera√ß√£o inconsistente (ex: "3. Tema" como texto simples)
    
    Args:
        texto: Texto markdown
        similaridade_minima: Threshold para considerar como duplicata (0-1)
    
    Returns:
        Texto limpo
    """
    from difflib import SequenceMatcher
    
    print("üßπ Removendo t√≠tulos √≥rf√£os (v2.17)...")
    
    linhas = texto.split('\n')
    
    # 1. Extrair todos os t√≠tulos H2 existentes (normalizados)
    titulos_h2 = []
    for linha in linhas:
        if linha.strip().startswith('## '):
            # Extrair apenas o texto do t√≠tulo (sem ## e numera√ß√£o)
            titulo_limpo = re.sub(r'^##\s*\d+(?:\.\d+)*\.?\s*', '', linha.strip())
            titulo_limpo = titulo_limpo.strip().lower()
            if titulo_limpo:
                titulos_h2.append(titulo_limpo)
    
    if not titulos_h2:
        print("   ‚ÑπÔ∏è  Nenhum H2 encontrado para compara√ß√£o")
        return texto
    
    # 2. Identificar linhas √≥rf√£s para remo√ß√£o
    linhas_para_remover = set()
    
    for i, linha in enumerate(linhas):
        stripped = linha.strip()
        
        # Ignorar linhas vazias, headers reais, tabelas
        if not stripped or stripped.startswith('## ') or stripped.startswith('|'):
            continue
        
        # Detectar padr√µes de "t√≠tulo √≥rf√£o"
        texto_candidato = None
        
        # Padr√£o 1: Linha bold com numera√ß√£o (ex: **3. Execu√ß√µes...**)
        match_bold = re.match(r'^\*\*\d+\.?\s*(.+?)\*\*\s*$', stripped)
        if match_bold:
            texto_candidato = match_bold.group(1).strip().lower()
        
        # Padr√£o 2: Numera√ß√£o simples no in√≠cio (ex: "3. Execu√ß√µes Envolvendo...")
        elif re.match(r'^\d+\.\s+[A-Z]', stripped):
            texto_candidato = re.sub(r'^\d+\.\s*', '', stripped).strip().lower()
        
        # Padr√£o 3: H3 que pode ser duplicata de H2
        elif stripped.startswith('### '):
            texto_candidato = re.sub(r'^###\s*\d*\.?\s*', '', stripped).strip().lower()
        
        if texto_candidato:
            # Comparar com todos os H2
            for h2 in titulos_h2:
                sim = SequenceMatcher(None, texto_candidato, h2).ratio()
                if sim >= similaridade_minima:
                    linhas_para_remover.add(i)
                    break
    
    # 3. Reconstruir sem as linhas √≥rf√£s
    if linhas_para_remover:
        print(f"   ‚úÖ {len(linhas_para_remover)} t√≠tulos √≥rf√£os removidos")
        linhas_limpas = [l for i, l in enumerate(linhas) if i not in linhas_para_remover]
        return '\n'.join(linhas_limpas)
    
    print("   ‚ÑπÔ∏è  Nenhum t√≠tulo √≥rf√£o detectado")
    return texto


def _split_long_paragraphs_markdown(
    texto: str,
    *,
    max_paragraph_chars: int = 900,
    skip_timestamped: bool = False,
) -> tuple[str, int]:
    """
    Quebra par√°grafos muito longos em Markdown (apenas texto "plain"), preservando
    blocos estruturais como t√≠tulos, listas, tabelas, cita√ß√µes e code fences.

    Returns:
        tuple: (texto_novo, qtd_paragrafos_quebrados)
    """
    if not texto:
        return texto, 0

    try:
        max_paragraph_chars = int(max_paragraph_chars)
    except Exception:
        max_paragraph_chars = 900

    if max_paragraph_chars <= 0:
        return texto, 0

    sentence_boundary_re = re.compile(r'(?<=[.!?])\s+(?=[A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï√ú0-9‚Äú"(\[])')
    timestamp_re = re.compile(r"\[\d{1,2}:\d{2}(?::\d{2})?\]")
    abbrev_re = re.compile(
        r'(?:\b(?:art|arts|dr|dra|sr|sra|etc|cf|n|no|n¬∫|inc|par|fls|p|pp|ex)\.)$',
        re.IGNORECASE,
    )

    def _is_special_paragraph(paragraph_lines: list[str]) -> bool:
        for ln in paragraph_lines:
            s = ln.strip()
            if not s:
                continue
            if s.startswith(("```", "~~~")):
                return True
            if s.startswith("#"):
                return True
            if s.startswith(">"):
                return True
            if s.startswith("|") and s.endswith("|"):
                return True
            if re.match(r"^\s*(?:[-*+]|\\d+\\.)\\s+", ln):
                return True
        return False

    def _split_into_sentences(text: str) -> list[str]:
        parts = [p.strip() for p in sentence_boundary_re.split(text.strip()) if p.strip()]
        if len(parts) <= 1:
            return parts

        merged: list[str] = []
        i = 0
        while i < len(parts):
            cur = parts[i]
            if i + 1 < len(parts) and abbrev_re.search(cur.rstrip()):
                cur = f"{cur} {parts[i + 1]}"
                i += 2
                merged.append(cur)
                continue
            merged.append(cur)
            i += 1
        return merged

    def _group_sentences(sentences: list[str]) -> list[str]:
        # Par√°grafos din√¢micos: 2‚Äì4 frases, tamanho confort√°vel para leitura.
        target_min = max(220, min(320, max_paragraph_chars // 3))
        target_max = max(420, min(650, max_paragraph_chars - 200))

        paras: list[str] = []
        cur: list[str] = []
        cur_len = 0

        def flush():
            nonlocal cur, cur_len
            if cur:
                paras.append(" ".join(cur).strip())
            cur = []
            cur_len = 0

        for s in sentences:
            s = s.strip()
            if not s:
                continue
            add_len = len(s) + (1 if cur else 0)
            if not cur:
                cur = [s]
                cur_len = len(s)
                continue
            if cur_len + add_len <= target_max:
                cur.append(s)
                cur_len += add_len
                continue
            if cur_len < target_min:
                cur.append(s)
                cur_len += add_len
                flush()
                continue
            flush()
            cur = [s]
            cur_len = len(s)
        flush()

        return [p for p in paras if p]

    def _fallback_word_chunk(text: str) -> list[str]:
        words = [w for w in re.split(r"\s+", text.strip()) if w]
        if not words:
            return []
        target_max = max(420, min(650, max_paragraph_chars - 200))
        paras: list[str] = []
        cur: list[str] = []
        cur_len = 0
        for w in words:
            add_len = len(w) + (1 if cur else 0)
            if cur and cur_len + add_len > target_max:
                paras.append(" ".join(cur))
                cur = [w]
                cur_len = len(w)
                continue
            cur.append(w)
            cur_len += add_len
        if cur:
            paras.append(" ".join(cur))
        return paras

    lines = texto.split("\n")
    out_lines: list[str] = []
    in_fence = False
    paragraph_lines: list[str] = []
    changed_paragraphs = 0

    def flush_paragraph():
        nonlocal changed_paragraphs, paragraph_lines, out_lines
        if not paragraph_lines:
            return
        if in_fence or _is_special_paragraph(paragraph_lines):
            out_lines.extend(paragraph_lines)
            paragraph_lines = []
            return

        joined = " ".join([l.strip() for l in paragraph_lines]).strip()
        if skip_timestamped:
            if timestamp_re.search(joined):
                out_lines.extend(paragraph_lines)
                paragraph_lines = []
                return
            first = paragraph_lines[0].strip()
            if re.match(r"^\\*\\*[^*]{1,30}\\*\\*:\\s+", first):
                out_lines.extend(paragraph_lines)
                paragraph_lines = []
                return
        if len(joined) <= max_paragraph_chars:
            out_lines.extend(paragraph_lines)
            paragraph_lines = []
            return

        sentences = _split_into_sentences(joined)
        if len(sentences) <= 1:
            new_paras = _fallback_word_chunk(joined)
        else:
            new_paras = _group_sentences(sentences)

        if len(new_paras) <= 1:
            out_lines.extend(paragraph_lines)
            paragraph_lines = []
            return

        changed_paragraphs += 1
        for idx, p in enumerate(new_paras):
            if idx > 0:
                out_lines.append("")
            out_lines.append(p)
        paragraph_lines = []

    for ln in lines:
        stripped = ln.strip()
        if stripped.startswith(("```", "~~~")):
            flush_paragraph()
            out_lines.append(ln)
            in_fence = not in_fence
            continue

        if in_fence:
            out_lines.append(ln)
            continue

        if stripped == "":
            flush_paragraph()
            out_lines.append(ln)
            continue

        paragraph_lines.append(ln)

    flush_paragraph()

    return "\n".join(out_lines), changed_paragraphs


def aplicar_correcoes_automaticas(texto: str, *, mode: str | None = None) -> tuple:
    """
    v2.19: Aplica corre√ß√µes autom√°ticas baseadas em padr√µes comuns de erro.
    Portado de format_transcription_gemini.py.
    
    Corre√ß√µes aplicadas:
    1. Remove sauda√ß√µes duplicadas ("Ol√°, sejam bem-vindos...")
    2. Remove apresenta√ß√µes repetidas ("Eu sou o professor...")
    3. Padroniza nome do professor (varia√ß√µes ‚Üí nome can√¥nico)
    4. Corrige itens de lista malformados ("3. \\n Requisitos" ‚Üí "3. Requisitos")
    5. Remove linhas em branco excessivas
    
    Returns:
        tuple: (texto_corrigido, lista_de_correcoes)
    """
    from difflib import SequenceMatcher
    
    print(f"{Fore.CYAN}üîß Auto-Fix Pass (v2.19)...")
    
    correcoes = []
    texto_original = texto
    
    # 1. Remover sauda√ß√µes duplicadas (apenas mant√©m a primeira)
    saudacoes_pattern = r'(?:Ol√°|Oi),?\s*(?:sejam?\s+)?(?:bem[- ]?vindos?(?:\s+e\s+bem[- ]?vindas?)?)[.,!]?'
    matches = list(re.finditer(saudacoes_pattern, texto, re.IGNORECASE))
    if len(matches) > 1:
        for match in reversed(matches[1:]):
            start = texto.rfind('\n', 0, match.start()) + 1
            end = texto.find('\n', match.end())
            if end == -1: end = len(texto)
            linha = texto[start:end].strip()
            if len(linha) < 150:
                texto = texto[:start] + texto[end+1:]
                correcoes.append(f"Sauda√ß√£o duplicada removida")
    
    # 2. Remover apresenta√ß√µes repetidas do professor
    apresentacao_pattern = r'Eu sou o professor\s+\w+(?:\s+\w+)?'
    matches = list(re.finditer(apresentacao_pattern, texto, re.IGNORECASE))
    if len(matches) > 1:
        for match in reversed(matches[1:]):
            start = texto.rfind('\n', 0, match.start()) + 1
            end = texto.find('\n', match.end())
            if end == -1: end = len(texto)
            linha = texto[start:end].strip()
            if len(linha) < 200:
                texto = texto[:start] + texto[end+1:]
                correcoes.append(f"Apresenta√ß√£o duplicada removida")
    
    # 3. Padronizar nome do professor
    nome_match = re.search(r'professor\s+(\w+(?:\s+\w+)?)', texto, re.IGNORECASE)
    if nome_match:
        nome_canonico = nome_match.group(1)
        variacoes_pattern = rf'\bprofessor\s+(\w+(?:\s+\w+)?)\b'
        for m in re.finditer(variacoes_pattern, texto, re.IGNORECASE):
            nome_atual = m.group(1)
            if nome_atual.lower() != nome_canonico.lower():
                sim = SequenceMatcher(None, nome_canonico.lower(), nome_atual.lower()).ratio()
                if sim > 0.6 and sim < 1.0:
                    texto = texto.replace(f"professor {nome_atual}", f"professor {nome_canonico}")
                    texto = texto.replace(f"Professor {nome_atual}", f"Professor {nome_canonico}")
                    correcoes.append(f"Nome padronizado: '{nome_atual}' ‚Üí '{nome_canonico}'")
    
    # 4. Corrigir itens de lista vazios ou malformados
    texto_temp = re.sub(r'(\d+\.)\s*\n\s*((?:Requisitos|Preenchimento|Fundamento|Artigo|Lei))', r'\1 \2', texto)
    if texto_temp != texto:
        texto = texto_temp
        correcoes.append("Itens de lista malformados corrigidos")
    
    # 5. Remover linhas em branco excessivas
    texto_limpo = re.sub(r'\n{4,}', '\n\n\n', texto)
    if texto_limpo != texto:
        texto = texto_limpo
        correcoes.append("Linhas em branco excessivas removidas")

    # 6. Limpar placeholders de tabelas (evita vazamento de exemplos do prompt)
    #    Exemplos proibidos: "...", "Art. X", "Lei Y", "Art. X, Lei Y"
    def _is_table_separator_line(line: str) -> bool:
        return bool(re.match(r'^\s*\|[\s:|-]+\|[\s:|-]*$', line.strip()))

    linhas = texto.split('\n')
    substituicoes = 0
    for i, line in enumerate(linhas):
        l = line.strip()
        if not (l.startswith('|') and l.endswith('|')):
            continue
        if _is_table_separator_line(l):
            continue

        # S√≥ mexe se parecer placeholder
        if ('...' not in l and '‚Ä¶' not in l and 'Art. X' not in l and 'Lei Y' not in l):
            continue

        cells = [c.strip() for c in l.split('|')[1:-1]]
        new_cells = []
        changed = False
        for c in cells:
            c_clean = c.strip()
            if c_clean in {'...', '‚Ä¶'}:
                new_cells.append('‚Äî')
                changed = True
            elif re.search(r'\bArt\.\s*X\b', c_clean):
                new_cells.append('‚Äî')
                changed = True
            elif re.search(r'\bLei\s*Y\b', c_clean):
                new_cells.append('‚Äî')
                changed = True
            elif 'Art. X' in c_clean or 'Lei Y' in c_clean:
                # Cobrir combina√ß√µes como "Art. X, Lei Y"
                new_cells.append('‚Äî')
                changed = True
            else:
                new_cells.append(c)

        if changed:
            substituicoes += 1
            linhas[i] = '| ' + ' | '.join(new_cells) + ' |'

    if substituicoes > 0:
        texto = '\n'.join(linhas)
        correcoes.append(f"Placeholders de tabela substitu√≠dos por '‚Äî' ({substituicoes} linha(s))")

    # 7. Quebrar par√°grafos muito longos (APOSTILA/FIDELIDADE)
    mode_norm = (mode or "").strip().upper()
    if mode_norm in {"APOSTILA", "FIDELIDADE"}:
        import os

        env_key = "IUDEX_APOSTILA_MAX_PARAGRAPH_CHARS" if mode_norm == "APOSTILA" else "IUDEX_FIDELIDADE_MAX_PARAGRAPH_CHARS"
        default_max = "500" if mode_norm == "APOSTILA" else "1200"  # v2.41: APOSTILA 900‚Üí500 (mais granular, alinhado com format_transcription_gemini)
        try:
            max_chars = int(os.getenv(env_key, default_max))
        except Exception:
            max_chars = int(default_max)

        texto_split, changed = _split_long_paragraphs_markdown(
            texto,
            max_paragraph_chars=max_chars,
            skip_timestamped=(mode_norm == "FIDELIDADE"),
        )
        if changed > 0 and texto_split != texto:
            texto = texto_split
            correcoes.append(f"Par√°grafos longos quebrados ({mode_norm}: {changed} par√°grafo(s))")
    
    if correcoes:
        print(f"   ‚úÖ {len(correcoes)} corre√ß√µes aplicadas:")
        for c in correcoes[:3]:
            print(f"      - {c}")
        if len(correcoes) > 3:
            print(f"      ... e mais {len(correcoes) - 3}")
    else:
        print(f"   ‚ÑπÔ∏è  Nenhuma corre√ß√£o necess√°ria")
    
    return texto, correcoes


def normalize_headings(texto):
    """
    v1.0: Normaliza t√≠tulos semanticamente similares para uma vers√£o √∫nica.
    - Agrupa t√≠tulos por similaridade
    - Escolhe o t√≠tulo mais descritivo de cada grupo
    - Remove sufixos como "(Continua√ß√£o)"
    """
    from difflib import SequenceMatcher
    
    print("üî§ Normalizando t√≠tulos similares...")
    linhas = texto.split('\n')
    
    # 1. Extrair todos os t√≠tulos com info de n√≠vel e posi√ß√£o
    titulos = []
    for i, linha in enumerate(linhas):
        stripped = linha.strip()
        if stripped.startswith('##'):
            match = re.match(r'^(#+)\s*', stripped)
            nivel = len(match.group(1)) if match else 2
            # Extrai t√≠tulo limpo (sem # e sem numera√ß√£o)
            titulo_limpo = re.sub(r'^#{2,4}\s*\d+(?:\.\d+)*\.?\s*', '', stripped).strip()
            # Remove "(Continua√ß√£o)" para compara√ß√£o
            titulo_base = re.sub(r'\s*\(Continua√ß√£o\)\s*$', '', titulo_limpo, flags=re.IGNORECASE).strip()
            titulos.append({
                'linha': i,
                'nivel': nivel,
                'original': stripped,
                'limpo': titulo_limpo,
                'base': titulo_base
            })
    
    if not titulos:
        return texto
    
    # 2. Agrupar t√≠tulos similares (mesmo n√≠vel + similaridade > 0.85)
    grupos = []
    usados = set()
    
    for i, t1 in enumerate(titulos):
        if i in usados:
            continue
        grupo = [t1]
        usados.add(i)
        
        for j, t2 in enumerate(titulos):
            if j in usados or j <= i:
                continue
            if t1['nivel'] == t2['nivel']:
                sim = SequenceMatcher(None, t1['base'].lower(), t2['base'].lower()).ratio()
                if sim > 0.85:
                    grupo.append(t2)
                    usados.add(j)
        
        if len(grupo) > 1:
            grupos.append(grupo)
    
    if not grupos:
        # Nada para normalizar, mas ainda remove "(Continua√ß√£o)"
        texto_limpo = re.sub(r'\s*\(Continua√ß√£o\)\s*(?=\n|$)', '', texto, flags=re.IGNORECASE)
        return texto_limpo
    
    # 3. Para cada grupo, escolher o "melhor" t√≠tulo (mais curto entre os mais longos)
    #    L√≥gica: preferir t√≠tulos sem "(Continua√ß√£o)" e com descri√ß√£o completa
    substituicoes = {}
    for grupo in grupos:
        # Ordenar por: n√£o ter "(Continua√ß√£o)" primeiro, depois por comprimento (prefer m√©dio)
        candidatos = sorted(grupo, key=lambda x: (
            '(Continua√ß√£o)' in x['limpo'],  # False (0) vem antes de True (1)
            abs(len(x['limpo']) - 40)  # Preferir t√≠tulos de ~40 chars (nem muito curto nem muito longo)
        ))
        
        melhor = candidatos[0]['limpo']
        print(f"   üìù Grupo de {len(grupo)} t√≠tulos similares ‚Üí padronizando para: '{melhor[:50]}...'")
        
        for t in grupo:
            if t['limpo'] != melhor:
                substituicoes[t['linha']] = (t['nivel'], melhor)
    
    # 4. Aplicar substitui√ß√µes
    novas_linhas = []
    for i, linha in enumerate(linhas):
        if i in substituicoes:
            nivel, novo_titulo = substituicoes[i]
            # Preservar numera√ß√£o existente se houver
            match_num = re.match(r'^(#+\s*\d+(?:\.\d+)*\.?\s*)', linhas[i].strip())
            if match_num:
                prefixo = match_num.group(1)
                # Remove a numera√ß√£o do novo t√≠tulo se ele tiver uma
                novo_titulo = re.sub(r'^\d+(?:\.\d+)*\.?\s*', '', novo_titulo)
                novas_linhas.append(f"{prefixo}{novo_titulo}")
            else:
                novas_linhas.append(f"{'#' * nivel} {novo_titulo}")
        else:
            # Remove "(Continua√ß√£o)" de qualquer t√≠tulo restante
            if linha.strip().startswith('##'):
                linha = re.sub(r'\s*\(Continua√ß√£o\)\s*(?=\n|$)', '', linha, flags=re.IGNORECASE)
            novas_linhas.append(linha)
    
    print(f"   ‚úÖ {len(substituicoes)} t√≠tulos normalizados, '(Continua√ß√£o)' removidos")
    return '\n'.join(novas_linhas)

PROMPT_STRUCTURE_REVIEW = """Voc√™ √© um revisor especializado em estrutura de documentos jur√≠dicos educacionais.

## ESTRUTURA DE MAPEAMENTO INICIAL (Refer√™ncia - se dispon√≠vel):
{estrutura_mapeada}

## TAREFA
Revise a ESTRUTURA (headers/t√≠tulos) do documento abaixo, COMPARANDO com o mapeamento acima (se dispon√≠vel). Sua miss√£o √© harmonizar o mapeamento planejado com o CONTE√öDO REAL da aula.

## ‚úÖ O QUE VOC√ä DEVE FAZER:

### 1. COMPARAR E REFINAR T√çTULOS (CR√çTICO)
Verifique se los t√≠tulos refletem os t√≥picos do mapeamento. Se um t√≠tulo for gen√©rico, torne-o descritivo.
- ERRADO: "### Quest√£o" ou "### T√≥pico"
- CORRETO: "### Quest√£o 1: Responsabilidade Civil" (Descritivo conforme mapa/conte√∫do)
- **CR√çTICO:** Evite t√≠tulos id√™nticos em se√ß√µes "irm√£s". Diferencie-os pelo conte√∫do espec√≠fico de cada uma.

### 2. VALIDAR HIERARQUIA E PROMO√á√ÉO DE T√ìPICOS
Confirme se a estrutura segue l√≥gica consistente (##, ###, ####). 
- **PROMO√á√ÉO:** Se um sub-subt√≥pico (ex: 9.19.5) for extenso e tratar de um tema central (ex: Execu√ß√£o Fiscal), PROMOVA-O a um n√≠vel superior (ex: ### 9.20) para evitar fragmenta√ß√£o excessiva e respeitar o limite de n√≠veis.

### 3. RENUMERA√á√ÉO SEQUENCIAL OBRIGAT√ìRIA
Se voc√™ criar, deletar ou promover uma se√ß√£o, voc√™ DEVE renumerar TODAS as se√ß√µes subsequentes daquela mesma hierarquia para manter a sequ√™ncia l√≥gica (ex: se 9.20 foi criado, o antigo 9.20 vira 9.21, e assim por diante).

### 4. MESCLAR QUEST√ïES DUPLICADAS
Se duas se√ß√µes t√™m o mesmo n√∫mero de quest√£o na mesma √°rea, MESCLE-AS.
- ERRADO: "2.1. Quest√£o 1: TAC" + "2.2. Quest√£o 1: TAC" 
- CORRETO: "2.1. Quest√£o 1: TAC" (√önica, com todo o conte√∫do unificado)

### 5. PRIORIDADE DO CONTE√öDO REAL (DECIDIR ESTRUTURA)
Se houver conflito entre mapeamento e documento, escolha a estrutura que melhor reflete o CONTE√öDO REAL. O mapeamento √© apenas um guia.

### 6. LIMPEZA T√âCNICA (SINTAXE MARKDOWN)
Corrija problemas detalhados de formata√ß√£o:
- **Tabelas:** Alinhar colunas e adicionar separadores faltantes.
- **Listas:** Corrigir bullets ou numera√ß√£o mal formatada.
- **Espa√ßamento:** Padronizar linhas em branco entre se√ß√µes (m√≠nimo uma linha).
- **Headers Vazios:** Remover t√≠tulos sem conte√∫do abaixo.
- **Metadados:** Remover headers ou tags inline como "[TIPO: ...]", "**[TIPO: ...]**", ou "[BLOCO 0X]".

## ‚ùå O QUE VOC√ä N√ÉO DEVE FAZER:
1. **N√ÉO ALTERE O CONTE√öDO** dos par√°grafos - apenas os t√≠tulos e a organiza√ß√£o.
2. **NUNCA RESUMA** ou encurte o texto (mesmo tamanho de entrada/sa√≠da √© obrigat√≥rio).
3. **N√ÉO INVENTE** fatos jur√≠dicos.
4. **N√ÉO REMOVA** trechos t√©cnicos ou exemplos.

## REGRAS CR√çTICAS DE HIERARQUIA:
- Use **M√ÅXIMO 3** n√≠veis de hierarquia (##, ###, ####).
- Nunca use # (H1) para subt√≥picos (apenas para o t√≠tulo principal do documento).
- Preserve a ordem cronol√≥gica geral.
- **ANTI-FRAGMENTA√á√ÉO (CR√çTICO):** Se h√° 4+ se√ß√µes ## consecutivas que tratam de aspectos do MESMO tema, **REBAIXE-AS** para ### subt√≥picos de um ## tema-m√£e. Exemplo: "## Cita√ß√£o", "## Intima√ß√£o", "## Notifica√ß√£o" dentro de Atos de Comunica√ß√£o ‚Üí devem virar "## Atos de Comunica√ß√£o" com "### Cita√ß√£o", "### Intima√ß√£o", "### Notifica√ß√£o".
- **MARCOS LEGAIS como subt√≥picos:** S√∫mulas, Teses e Artigos explicados em profundidade devem ser ### (n√£o ##).

## DOCUMENTO PARA REVISAR:
{documento}

## üìù RELAT√ìRIO ESPERADO:
Ao final do documento, inclua um bloco de coment√°rio indicando:
- Quantos t√≠tulos foram refinados
- Promo√ß√µes de se√ß√µes e renumera√ß√µes realizadas
- Discrep√¢ncias com o mapeamento (se houver)

Formato:
<!-- RELAT√ìRIO: X t√≠tulos refinados | Y se√ß√µes promovidas/renumeradas | Discrep√¢ncias: [Nenhuma/Lista] -->

## RESPOSTA:
Retorne o documento COMPLETO E INTEGRAL (mesmo tamanho do original) com os t√≠tulos/headers corrigidos e o relat√≥rio no final. N√ÉO RESUMA."""

def renumerar_secoes(texto):
    """
    v1.0: Renumera√ß√£o Sequencial Determin√≠stica.
    
    Esta fun√ß√£o √© uma camada de seguran√ßa extra, aplicada AP√ìS o AI Review.
    Ela percorre todos os headers numerados e corrige qualquer duplica√ß√£o ou
    sequ√™ncia quebrada, garantindo que os n√∫meros sejam estritamente sequenciais
    dentro de cada n√≠vel de hierarquia.
    
    Exemplo de corre√ß√£o:
    - Input:  9.20, 9.21, 9.21, 9.35, 9.36  (duplica√ß√£o e pulo)
    - Output: 9.20, 9.21, 9.22, 9.23, 9.24  (sequencial)
    """
    logger.info("üî¢ Executando Renumera√ß√£o Sequencial Determin√≠stica...")
    
    linhas = texto.split('\n')
    novas_linhas = []
    
    # Contadores por n√≠vel de hierarquia (ex: {2: 9, 3: 44} -> ## 9.x, ### 9.44.x)
    # Estrutura: {header_level: {parent_prefix: next_number}}
    # Ex: para ### 9.20.1 -> level=3, parent_prefix="9.20", next_number=2 (para o pr√≥ximo .x)
    contadores = {}
    
    # Regex para detectar headers numerados: ### 9.20.1. T√≠tulo ou ## 5. T√≠tulo
    header_pattern = re.compile(r'^(#{1,4})\s+([\d.]+\.?)\s*(.*)$')
    
    for linha in linhas:
        match = header_pattern.match(linha)
        
        if match:
            hashes = match.group(1)       # "###"
            numero = match.group(2)       # "9.20.1." ou "9.20.1"
            titulo = match.group(3)       # "T√≠tulo..."
            
            nivel = len(hashes)
            numero_limpo = numero.rstrip('.')
            partes = numero_limpo.split('.')
            
            # Determina o prefixo pai e o sufixo atual
            if len(partes) == 1:
                # N√≠vel raiz: ## 1. ou ## 9.
                prefixo_pai = ""
                sufixo_atual = int(partes[0])
            else:
                # Subn√≠vel: ### 9.20. ou #### 9.20.1.
                prefixo_pai = '.'.join(partes[:-1])
                sufixo_atual = int(partes[-1])
            
            # Inicializa contador se n√£o existir
            chave = (nivel, prefixo_pai)
            if chave not in contadores:
                contadores[chave] = sufixo_atual  # Come√ßa do n√∫mero encontrado
            else:
                contadores[chave] += 1
            
            novo_sufixo = contadores[chave]
            
            # Reconstr√≥i o n√∫mero
            if prefixo_pai:
                novo_numero = f"{prefixo_pai}.{novo_sufixo}."
            else:
                novo_numero = f"{novo_sufixo}."
            
            # Reconstr√≥i a linha
            nova_linha = f"{hashes} {novo_numero} {titulo}"
            novas_linhas.append(nova_linha)
            
            # Log se houve mudan√ßa
            if numero_limpo != novo_numero.rstrip('.'):
                logger.info(f"   üîÑ {numero_limpo} ‚Üí {novo_numero.rstrip('.')}")
        else:
            novas_linhas.append(linha)
    
    logger.info("   ‚úÖ Renumera√ß√£o conclu√≠da.")
    return '\n'.join(novas_linhas)


def audit_heading_levels(texto: str, *, apply_fixes: bool = False) -> tuple[str, list[str]]:
    """
    v2.34: Auditoria determin√≠stica de hierarquia.

    Regras:
    - H2 com numera√ß√£o decimal (ex.: "## 18.2 ...") deve ser subt√≥pico (H3/H4).
    - H4 sem H3 anterior (desde o √∫ltimo H2) √© inconsistente.

    Returns:
        tuple: (texto_atualizado, issues)
    """
    if not texto:
        return texto, []

    lines = texto.split("\n")
    issues: list[str] = []
    new_lines: list[str] = []
    in_fence = False
    saw_h2 = False
    saw_h3_since_h2 = False

    heading_re = re.compile(r'^(#{2,4})\s+(.+)$')
    decimal_re = re.compile(r'^(\d+(?:\.\d+)+)\.?\s*(.+)$')

    for line in lines:
        stripped = line.strip()
        if stripped.startswith(("```", "~~~")):
            in_fence = not in_fence
            new_lines.append(line)
            continue

        if in_fence:
            new_lines.append(line)
            continue

        m = heading_re.match(stripped)
        if not m:
            new_lines.append(line)
            continue

        hashes = m.group(1)
        level = len(hashes)
        raw_title = m.group(2).strip()

        if level == 2:
            saw_h2 = True
            saw_h3_since_h2 = False
        elif level == 3:
            saw_h3_since_h2 = True

        dec = decimal_re.match(raw_title)
        if level == 2 and dec:
            num = dec.group(1)
            title = dec.group(2).strip()
            depth = num.count(".") + 1
            issues.append(f"Subt√≥pico numerado em H2: '{raw_title}'")
            if apply_fixes and saw_h2:
                target_level = 3 if depth == 2 else 4
                # Evitar H4 sem H3 no bloco atual.
                if target_level == 4 and not saw_h3_since_h2:
                    target_level = 3
                new_lines.append(f"{'#' * target_level} {num}. {title}".strip())
                if target_level == 3:
                    saw_h3_since_h2 = True
                continue

        if level == 4 and not saw_h3_since_h2:
            issues.append(f"H4 sem H3 anterior: '{raw_title}'")
            if apply_fixes:
                new_lines.append(f"### {raw_title}")
                saw_h3_since_h2 = True
                continue

        new_lines.append(line)

    return "\n".join(new_lines), issues


_TABLE_HEADING_RE = re.compile(
    r'^(#{3,5})\s*(?:[üìãüéØ]\s*)?(.*)$',
    re.IGNORECASE,
)

_HEADING_RE = re.compile(r'^(#{2,4})\s+(.+)$')
_HEADING_NUMBER_RE = re.compile(r'^(\d+(?:\.\d+)*)(?:\.)?\s*(.+)$')

_STOPWORDS_PT = {
    "para", "pela", "pelo", "como", "mais", "menos", "sobre", "entre", "depois", "antes", "quando",
    "onde", "outra", "outro", "outros", "outras", "seu", "sua", "seus", "suas", "que", "porque",
    "pois", "isso", "essa", "esse", "esta", "este", "estas", "estes", "nao", "n√£o", "sim", "com",
    "sem", "dos", "das", "nos", "nas", "por", "pro", "pra", "uma", "uns", "umas", "como", "pela",
    "pelo", "sobre", "sob", "entre", "na", "no", "em", "ao", "aos", "as", "os", "de", "da", "do",
    "das", "dos", "e", "a", "o", "um", "uma", "que", "ser", "sao", "s√£o",
}


def _keyword_set(texto: str) -> set[str]:
    tokens = re.findall(r"[A-Za-z√Ä-√ø0-9]+", (texto or "").lower())
    return {
        t for t in tokens
        if len(t) >= 4 and t not in _STOPWORDS_PT and not t.isdigit()
    }


def _keyword_similarity(a: str, b: str) -> float:
    sa = _keyword_set(a)
    sb = _keyword_set(b)
    if not sa or not sb:
        return 0.0
    return len(sa & sb) / max(1, len(sa | sb))


def _extract_headings(lines: list[str]) -> list[dict]:
    headings: list[dict] = []
    in_fence = False
    for idx, line in enumerate(lines):
        stripped = line.strip()
        if stripped.startswith(("```", "~~~")):
            in_fence = not in_fence
            continue
        if in_fence:
            continue
        m = _HEADING_RE.match(stripped)
        if not m:
            continue
        level = len(m.group(1))
        raw_title = m.group(2).strip()
        # Ignorar headings de tabelas/quadros para n√£o quebrar se√ß√µes.
        if level >= 3:
            lower_title = raw_title.lower()
            if any(tok in lower_title for tok in ("tabela", "quadro", "s√≠ntese", "sintese", "pegadinha", "banca", "üìã", "üéØ")):
                continue
        number = ""
        title = raw_title
        nm = _HEADING_NUMBER_RE.match(raw_title)
        if nm:
            number = nm.group(1)
            title = nm.group(2).strip()
        headings.append(
            {
                "line": idx,
                "level": level,
                "number": number,
                "title": title,
                "raw": raw_title,
            }
        )
    return headings


_CONVERSATIONAL_HEADING_PREFIXES = (
    "j√° ",
    "na prova",
    "para quem",
    "minha proposta",
    "bom dia",
    "gente ",
    "pessoal ",
    "vamos ",
    "ent√£o ",
    "logo ",
)

_TECHNICAL_HEADING_TOKENS = (
    "licita",
    "contrat",
    "lei",
    "decreto",
    "s√∫mula",
    "sumula",
    "tcu",
    "stj",
    "stf",
    "juris",
    "administra",
    "governan",
    "execu√ß√£o",
    "execucao",
    "fiscal",
    "responsabil",
    "constituci",
    "nulidade",
    "compet",
    "proced",
    "auditoria",
    "fidelidade",
)


def _normalize_heading_title(raw_title: str) -> str:
    title = (raw_title or "").strip()
    nm = _HEADING_NUMBER_RE.match(title)
    if nm:
        return nm.group(2).strip()
    return title


def _contains_technical_signal(title: str) -> bool:
    t = (title or "").lower()
    return any(tok in t for tok in _TECHNICAL_HEADING_TOKENS)


def _heading_quality_flags(title: str, level: int) -> list[str]:
    flags: list[str] = []
    normalized = re.sub(r"\s+", " ", (title or "").strip())
    lower = normalized.lower()
    words = re.findall(r"[A-Za-z√Ä-√ø0-9]+", normalized)

    if not normalized:
        flags.append("empty")
        return flags

    if len(normalized) > 110:
        flags.append("too_long_chars")
    if len(words) > 20:
        flags.append("too_long_words")
    if any(lower.startswith(pfx) for pfx in _CONVERSATIONAL_HEADING_PREFIXES):
        flags.append("conversational_prefix")
    if level == 2 and "na prova de" in lower:
        flags.append("exam_phrase_h2")
    if level in (2, 3) and not _contains_technical_signal(lower):
        flags.append("missing_technical_signal")

    return flags


def _extract_h2_h3_heading_entries(text: str) -> list[dict]:
    lines = (text or "").splitlines()
    entries: list[dict] = []
    for idx, line in enumerate(lines):
        m = _HEADING_RE.match(line.strip())
        if not m:
            continue
        level = len(m.group(1))
        if level not in (2, 3):
            continue
        raw_title = m.group(2).strip()
        entries.append(
            {
                "line_idx": idx,
                "level": level,
                "raw_title": raw_title,
                "title": _normalize_heading_title(raw_title),
            }
        )
    return entries


def enforce_fidelity_heading_guard(
    original_text: str,
    revised_text: str,
    *,
    freeze_h2_h3: bool = True,
) -> tuple[str, dict]:
    """
    Garante estabilidade de t√≠tulos no modo FIDELIDADE.

    - Compara H2/H3 entre texto original e revisado.
    - Opcionalmente congela H2/H3 (sempre restaura t√≠tulo original quando mudou).
    - Aplica rollback seletivo para t√≠tulos degradados (frase corrida/conversacional etc.).
    - Retorna texto corrigido + telemetria de drift.
    """
    orig_entries = _extract_h2_h3_heading_entries(original_text or "")
    rev_entries = _extract_h2_h3_heading_entries(revised_text or "")
    lines = (revised_text or "").splitlines()

    changed_count = 0
    restored_count = 0
    degraded_count = 0
    diffs: list[dict] = []

    for idx, (orig, rev) in enumerate(zip(orig_entries, rev_entries), start=1):
        original_title = (orig.get("raw_title") or "").strip()
        revised_title = (rev.get("raw_title") or "").strip()
        if not original_title or not revised_title:
            continue
        if original_title == revised_title:
            continue

        changed_count += 1
        flags = _heading_quality_flags(revised_title, int(rev.get("level") or 2))
        degraded = len(flags) > 0
        if degraded:
            degraded_count += 1

        should_restore = freeze_h2_h3 or degraded
        if should_restore:
            prefix = "#" * int(rev.get("level") or 2)
            target_line = int(rev.get("line_idx") or 0)
            if 0 <= target_line < len(lines):
                lines[target_line] = f"{prefix} {original_title}"
                restored_count += 1

        diffs.append(
            {
                "index": idx,
                "level": int(rev.get("level") or 2),
                "original": original_title,
                "revised": revised_title,
                "restored": bool(should_restore),
                "quality_flags": flags,
            }
        )

    telemetry = {
        "freeze_h2_h3": bool(freeze_h2_h3),
        "headers_changed_count": changed_count,
        "headers_restored_count": restored_count,
        "headers_degraded_count": degraded_count,
        "headers_diff": diffs,
    }
    return "\n".join(lines), telemetry


# ---------------------------------------------------------------------------
# Sanitiza√ß√£o de t√≠tulos na estrutura mapeada (v2.47)
# ---------------------------------------------------------------------------

_GREETING_TITLE_PREFIXES = (
    "bom dia", "boa tarde", "boa noite", "j√° ", "pessoal ",
    "gente ", "olha ", "obrigado", "obrigada",
)

_CANONICAL_LABEL_L1 = "Introdu√ß√£o e Contextualiza√ß√£o"
_CANONICAL_LABEL_SUB = "Abertura"

_MAX_MAPPED_TITLE_WORDS = 8
_MAX_MAPPED_TITLE_CHARS = 70


def _sanitize_mapped_structure(estrutura: str) -> str:
    """Valida e corrige t√≠tulos de estrutura que s√£o trechos literais de fala.

    Reutiliza ``_heading_quality_flags`` e ``_CONVERSATIONAL_HEADING_PREFIXES``
    para detectar t√≠tulos degradados no mapeamento.

    Regras (alinhadas com PROMPT_MAPEAMENTO regra 8):
    - T√≠tulos > 8 palavras ou > 70 chars ‚Üí r√≥tulo can√¥nico
    - Prefixos conversacionais (sauda√ß√µes, log√≠stica) ‚Üí r√≥tulo can√¥nico
    - Preserva √¢ncoras ``| ABRE: "..." | FECHA: "..."`` intactas
    """
    if not estrutura:
        return estrutura

    lines = estrutura.split('\n')
    fixed_lines: list[str] = []
    sanitized_count = 0

    for line in lines:
        stripped = line.strip()
        # Detecta linhas numeradas: "1. T√≠tulo", "   1.1. Subt√≠tulo"
        m = re.match(r'^(\s*\d+(?:\.\d+)*\.?\s+)(.*)', stripped)
        if not m:
            fixed_lines.append(line)
            continue

        prefix_num = m.group(1)
        rest = m.group(2).strip()

        # Separa √¢ncoras ABRE/FECHA preservando literalmente (incluindo aspas)
        anchor_part = ""
        title = rest
        anchor_idx = rest.find("| ABRE:")
        if anchor_idx >= 0:
            title = rest[:anchor_idx].strip()
            anchor_part = " " + rest[anchor_idx:]

        # Deriva n√≠vel: "1." ‚Üí level 2, "1.1." ‚Üí level 3
        parts_count = len([p for p in prefix_num.strip().rstrip('.').split('.') if p.strip().isdigit()])
        level = min(parts_count + 1, 4)  # 1. ‚Üí 2, 1.1. ‚Üí 3, 1.1.1. ‚Üí 4

        # Aplica heading quality flags existentes
        flags = _heading_quality_flags(title, level)

        needs_fix = False
        # Flags de qualidade indicam problema
        if "conversational_prefix" in flags or "too_long_chars" in flags or "too_long_words" in flags:
            needs_fix = True
        # Backup: checa limites alinhados com o prompt (8 palavras, 70 chars)
        words = re.findall(r'[A-Za-z√Ä-√ø0-9]+', title)
        if len(title) > _MAX_MAPPED_TITLE_CHARS or len(words) > _MAX_MAPPED_TITLE_WORDS:
            needs_fix = True

        if needs_fix:
            is_level1 = parts_count == 1
            title_lower = title.lower()

            if any(title_lower.startswith(pfx) for pfx in _GREETING_TITLE_PREFIXES):
                canonical = _CANONICAL_LABEL_L1 if is_level1 else _CANONICAL_LABEL_SUB
            elif is_level1:
                canonical = _CANONICAL_LABEL_L1
            else:
                canonical = _CANONICAL_LABEL_SUB

            sanitized_count += 1
            print(f"{Fore.YELLOW}‚ö†Ô∏è  T√≠tulo sanitizado: '{title[:60]}' ‚Üí '{canonical}'{Style.RESET_ALL}")
            fixed_lines.append(f"{prefix_num}{canonical}{anchor_part}")
        else:
            fixed_lines.append(line)

    if sanitized_count:
        print(f"{Fore.CYAN}üîß {sanitized_count} t√≠tulo(s) de estrutura sanitizado(s){Style.RESET_ALL}")

    return '\n'.join(fixed_lines)


def _extract_table_blocks(lines: list[str], start: int, end: int) -> list[dict]:
    blocks: list[dict] = []
    i = start
    while i < end:
        line = lines[i].strip()
        m = _TABLE_HEADING_RE.match(line)
        if m:
            heading_level = len(m.group(1))
            heading_text = m.group(2).strip()
            # Only treat as table header if it looks like table/quadros.
            if not any(tok in heading_text.lower() for tok in ("tabela", "quadro", "s√≠ntese", "sintese", "pegadinha", "banca")):
                i += 1
                continue
            block_start = i
            i += 1
            has_table_rows = False
            while i < end:
                nxt = lines[i].strip()
                if _HEADING_RE.match(nxt):
                    break
                if nxt.startswith("|"):
                    has_table_rows = True
                    i += 1
                    continue
                if has_table_rows and nxt == "":
                    i += 1
                    continue
                if has_table_rows and nxt and not nxt.startswith("|"):
                    break
                i += 1
            block_end = i
            block_text = "\n".join(lines[block_start:block_end]).strip()
            blocks.append(
                {
                    "start": block_start,
                    "end": block_end,
                    "heading_level": heading_level,
                    "heading_text": heading_text,
                    "text": block_text,
                }
            )
            continue
        i += 1
    return blocks


def reatribuir_tabelas_por_topico(
    texto: str,
    *,
    apply_fixes: bool = True,
    min_similarity: float = 0.08,
    margin: float = 0.08,
) -> tuple[str, list[str]]:
    """
    v2.34: Reatribui tabelas que parecem ter sido vinculadas ao t√≥pico errado.

    Heur√≠stica:
    - Avalia similaridade de palavras-chave entre tabela e t√≠tulo atual vs t√≠tulo pai.
    - Se a tabela est√° em subt√≥pico (numera√ß√£o decimal ou n√≠vel >=3) e o t√≠tulo pai √© mais similar,
      move a tabela para antes do heading do subt√≥pico.
    """
    if not texto:
        return texto, []

    lines = texto.split("\n")
    headings = _extract_headings(lines)
    if not headings:
        return texto, []

    # Construir intervalos de se√ß√£o por heading
    sections: list[dict] = []
    for idx, h in enumerate(headings):
        start = h["line"] + 1
        end = headings[idx + 1]["line"] if idx + 1 < len(headings) else len(lines)
        parent_idx = None
        for j in range(idx - 1, -1, -1):
            if headings[j]["level"] < h["level"]:
                parent_idx = j
                break
        sections.append(
            {
                "heading_index": idx,
                "start": start,
                "end": end,
                "parent_index": parent_idx,
            }
        )

    moves: list[dict] = []
    issues: list[str] = []

    for sec in sections:
        h = headings[sec["heading_index"]]
        parent_idx = sec["parent_index"]
        if parent_idx is None:
            continue
        parent = headings[parent_idx]
        # Considerar apenas subt√≥picos (decimais ou n√≠vel >=3)
        if "." not in (h.get("number") or "") and h.get("level", 2) < 3:
            continue
        table_blocks = _extract_table_blocks(lines, sec["start"], sec["end"])
        if not table_blocks:
            continue
        for block in table_blocks:
            def _context_slice(start_line: int, end_line: int, *, tail: bool = False, max_lines: int = 3) -> str:
                chunk = lines[start_line:end_line]
                nonempty = [ln.strip() for ln in chunk if ln.strip()]
                if not nonempty:
                    return ""
                if tail:
                    return " ".join(nonempty[-max_lines:])
                return " ".join(nonempty[:max_lines])

            # Contexto do subt√≥pico (linhas antes da tabela) e do pai (linhas imediatamente anteriores ao subt√≥pico).
            current_context = _context_slice(sec["start"], block["start"], tail=True)
            parent_context = _context_slice(
                sections[parent_idx]["start"],
                h["line"],
                tail=True,
            )
            current_score = _keyword_similarity(f"{h['title']} {current_context}", block["text"])
            parent_score = _keyword_similarity(f"{parent['title']} {parent_context}", block["text"])
            if parent_score >= min_similarity and (parent_score - current_score) >= margin:
                # Move a tabela para imediatamente antes do heading do subt√≥pico
                moves.append(
                    {
                        "start": block["start"],
                        "end": block["end"],
                        "insert_at": h["line"],
                        "from": h["title"],
                        "to": parent["title"],
                        "heading_text": block["heading_text"],
                        "scores": (current_score, parent_score),
                    }
                )

    if not moves or not apply_fixes:
        if moves:
            for m in moves:
                issues.append(
                    f"Tabela sugerida para mover ('{m['heading_text'][:40]}...'): '{m['from']}' ‚Üí '{m['to']}'"
                )
        return texto, issues

    # Aplicar movimentos de baixo para cima para preservar √≠ndices
    moves.sort(key=lambda m: m["start"], reverse=True)
    for m in moves:
        block_lines = lines[m["start"]:m["end"]]
        del lines[m["start"]:m["end"]]
        insert_at = m["insert_at"]
        if insert_at > m["start"]:
            insert_at = max(0, insert_at - (m["end"] - m["start"]))
        for offset, bl in enumerate(block_lines):
            lines.insert(insert_at + offset, bl)
        issues.append(
            f"Tabela reatribu√≠da: '{m['heading_text'][:40]}...' de '{m['from']}' ‚Üí '{m['to']}'"
        )

    return "\n".join(lines), issues


def coletar_candidatos_reatribuicao_tabelas(
    texto: str,
    *,
    min_similarity: float = 0.08,
    margin: float = 0.08,
    max_candidates: int = 5,
) -> list[dict]:
    """
    v2.34: Coleta casos amb√≠guos para reatribui√ß√£o de tabelas via IA.
    """
    if not texto:
        return []
    lines = texto.split("\n")
    headings = _extract_headings(lines)
    if not headings:
        return []

    sections: list[dict] = []
    for idx, h in enumerate(headings):
        start = h["line"] + 1
        end = headings[idx + 1]["line"] if idx + 1 < len(headings) else len(lines)
        parent_idx = None
        for j in range(idx - 1, -1, -1):
            if headings[j]["level"] < h["level"]:
                parent_idx = j
                break
        sections.append(
            {
                "heading_index": idx,
                "start": start,
                "end": end,
                "parent_index": parent_idx,
            }
        )

    def _context_slice(start_line: int, end_line: int, *, tail: bool = False, max_lines: int = 3) -> str:
        chunk = lines[start_line:end_line]
        nonempty = [ln.strip() for ln in chunk if ln.strip()]
        if not nonempty:
            return ""
        if tail:
            return " ".join(nonempty[-max_lines:])
        return " ".join(nonempty[:max_lines])

    candidates: list[dict] = []
    for sec in sections:
        h = headings[sec["heading_index"]]
        parent_idx = sec["parent_index"]
        if parent_idx is None:
            continue
        parent = headings[parent_idx]
        if "." not in (h.get("number") or "") and h.get("level", 2) < 3:
            continue
        table_blocks = _extract_table_blocks(lines, sec["start"], sec["end"])
        if not table_blocks:
            continue
        for block in table_blocks:
            current_context = _context_slice(sec["start"], block["start"], tail=True)
            parent_context = _context_slice(sections[parent_idx]["start"], h["line"], tail=True)
            current_score = _keyword_similarity(f"{h['title']} {current_context}", block["text"])
            parent_score = _keyword_similarity(f"{parent['title']} {parent_context}", block["text"])
            # Amb√≠guo: scores pr√≥ximos ou ambos baixos, mas h√° match m√≠nimo com um lado.
            if max(current_score, parent_score) < min_similarity:
                continue
            if abs(parent_score - current_score) < margin:
                candidates.append(
                    {
                        "start": block["start"],
                        "end": block["end"],
                        "insert_at": h["line"],
                        "current_title": h["title"],
                        "parent_title": parent["title"],
                        "current_context": current_context,
                        "parent_context": parent_context,
                        "table_text": block["text"],
                    }
                )
        if len(candidates) >= max_candidates:
            break

    return candidates[:max_candidates]

def deterministic_structure_fix(text):
    """
    v1.1: Reorganiza√ß√£o Estrutural Determin√≠stica (Regex).
    Adaptativo: Detecta se o documento usa H1 ou apenas H2 como n√≠vel principal.
    """
    print(f"{Fore.CYAN}üß© Executando Reorganiza√ß√£o Estrutural Determin√≠stica...")
    
    lines = text.split('\n')
    
    # Detec√ß√£o de Hierarquia
    has_h1 = any(re.match(r'^#\s+', line) for line in lines)
    header_level_regex = r'^#\s+' if has_h1 else r'^##\s+'
    print(f"   ‚ÑπÔ∏è  N√≠vel principal detectado: {'H1 (#)' if has_h1 else 'H2 (##)'}")

    # Estruturas de dados (Preserva ordem de inser√ß√£o)
    content_map = {
        "PREAMBULO": [],
        "DISCIPLINAS": {}, 
        "ENCERRAMENTO": []
    }
    
    current_area = "PREAMBULO"
    current_block = []
    disciplinas_order = [] 
    
    # Regex Adaptativo
    # Captura o texto do cabe√ßalho principal (seja H1 ou H2)
    # Exclui "Quest√£o" ou "Q." para n√£o quebrar simulados dentro de uma √°rea
    re_disciplina = re.compile(rf'{header_level_regex}(?!Quest√£o|Q\.)([^0-9\.]+.*)', re.IGNORECASE)
    re_encerramento = re.compile(rf'{header_level_regex}(?:ENCERRAMENTO|CONSIDERA√á√ïES|CONCLUS√ÉO)', re.IGNORECASE)
    
    def flush_block(area, block_lines):
        if not block_lines: return
        block_text = '\n'.join(block_lines)
        
        if area == "PREAMBULO":
            content_map["PREAMBULO"].append(block_text)
        elif area == "ENCERRAMENTO":
            content_map["ENCERRAMENTO"].append(block_text)
        else:
            if area not in content_map["DISCIPLINAS"]:
                content_map["DISCIPLINAS"][area] = []
                disciplinas_order.append(area)
            content_map["DISCIPLINAS"][area].append(block_text)

    for line in lines:
        # 1. Detectar mudan√ßa de disciplina macro
        match_disc = re_disciplina.match(line)
        if match_disc:
            flush_block(current_area, current_block)
            current_block = []
            
            raw_area = match_disc.group(1).strip().upper()
            
            # Normaliza√ß√£o de nome de √°rea
            if "DIREITO" not in raw_area and len(raw_area) < 50:
                 # Adiciona prefixo se parecer nome de mat√©ria jur√≠dica comum
                 if any(x in raw_area for x in ["CIVIL", "PENAL", "TRABALHO", "ADMINISTRATIVO", "CONSTITUCIONAL"]):
                     current_area = f"DIREITO {raw_area}"
                 else:
                     current_area = raw_area
            else:
                 current_area = raw_area
                 
            # IMPORTANTE: Se estamos operando em modo H2, essa linha √© um Header que queremos manter?
            # Se for modo H1, recriamos "# AREA". 
            # Se for modo H2, recriamos "# AREA" (upcast) ou mantemos "## AREA"?
            # Para padronizar Apostilas, vamos promover tudo a H1 na reconstru√ß√£o.
            continue 
            
        # 2. Detectar Encerramento
        if re_encerramento.match(line):
            flush_block(current_area, current_block)
            current_block = []
            current_area = "ENCERRAMENTO"
            continue

        current_block.append(line)
        
    flush_block(current_area, current_block)
    
    # Reconstru√ß√£o
    final_output = []
    
    # Preambulo
    if content_map["PREAMBULO"]:
        final_output.append("# ORIENTA√á√ïES GERAIS / INTRODU√á√ÉO")
        final_output.extend(content_map["PREAMBULO"])
        final_output.append("")

    # Disciplinas / T√≥picos Principais
    for area in disciplinas_order:
        area_clean = area.replace("#", "").strip()
        final_output.append(f"# {area_clean}")
        for block in content_map["DISCIPLINAS"][area]:
            final_output.append(block)
        final_output.append("")
        
    # Encerramento
    if content_map["ENCERRAMENTO"]:
        final_output.append("# CONSIDERA√á√ïES FINAIS")
        final_output.extend(content_map["ENCERRAMENTO"])
        
    num_identified = len(disciplinas_order)
    print(f"   ‚úÖ Reorganizado: {num_identified} se√ß√µes principais identificadas.")
    
    # Fallback: Se n√£o identificou nada (tudo preambulo), retorna original para n√£o estragar
    if num_identified == 0 and len(content_map["PREAMBULO"]) > 0:
        print("   ‚ö†Ô∏è Nenhuma estrutura detectada. Mantendo original.")
        return text
        
    return '\n'.join(final_output)

PROMPT_STRUCTURE_REVIEW_LITE = """Voc√™ √© um revisor editorial especializado em ESTRUTURA e FORMATA√á√ÉO de documentos educacionais. Voc√™ receber√°:
1. Uma **Estrutura de Mapeamento Inicial** (planejada antes da formata√ß√£o)
2. O **Documento Processado** (resultado da formata√ß√£o por chunks)

Sua tarefa √© analisar ambos e garantir que os t√≠tulos estejam **hierarquicamente corretos e alinhados com o conte√∫do real**, sem jamais alterar a ordem cronol√≥gica.

---

## üìã ESTRUTURA DE MAPEAMENTO INICIAL (Refer√™ncia):
{estrutura_mapeada}

---

## ‚úÖ O QUE VOC√ä DEVE FAZER:
1. **N√ÉO REESCREVER T√çTULOS EXISTENTES:** preserve o texto dos headings j√° presentes no documento. Ajuste apenas n√≠vel/hierarquia quando estritamente necess√°rio.
2. **Validar Hierarquia:** Confirme que a estrutura (##, ###, ####) segue uma l√≥gica consistente (ex: se√ß√µes > subse√ß√µes > detalhes).
3. **Decidir a Melhor Estrutura:** Se houver conflito entre mapeamento e documento, escolha a estrutura que melhor reflete o CONTE√öDO REAL do texto.
4. **Subt√≥picos √ìrf√£os:** Se detectar headers como "A.", "B.", "C." isolados como t√≥picos principais, converta-os em subn√≠veis do t√≥pico anterior (ex: ## para ###).
5. **Corrigir Sintaxe Markdown:** Tabelas, listas, espa√ßamento.
6. **Remover Vazios:** T√≠tulos sem conte√∫do abaixo.
7. **NUNCA alterar conte√∫do de par√°grafos** (somente forma/sintaxe).

## üî¥ REGRAS CR√çTICAS DE HIERARQUIA:
- Use **M√ÅXIMO 3** n√≠veis de hierarquia (##, ###, ####). Nunca use # (H1) para subt√≥picos.
- **ANTI-FRAGMENTA√á√ÉO (CR√çTICO):** Se h√° 4+ se√ß√µes ## consecutivas que tratam de aspectos do MESMO tema, **REBAIXE-AS** para ### subt√≥picos de um ## tema-m√£e. Exemplo: "## Cita√ß√£o", "## Intima√ß√£o", "## Notifica√ß√£o" dentro de Atos de Comunica√ß√£o ‚Üí devem virar "## Atos de Comunica√ß√£o" com "### Cita√ß√£o", "### Intima√ß√£o", "### Notifica√ß√£o".
- **MARCOS LEGAIS como subt√≥picos:** S√∫mulas, Teses de Repercuss√£o Geral e Artigos explicados em profundidade devem ser ### (n√£o ##).
- Preserve a ordem cronol√≥gica geral.

## üìå EXEMPLOS DE CORRE√á√ÉO:

**Subt√≥picos √ìrf√£os ‚Üí Hierarquia Correta:**
- ANTES:
  ```
  ## A. Requisitos do Dano
  ## B. Nexo Causal
  ```
- DEPOIS:
  ```
  ### A. Requisitos do Dano
  ### B. Nexo Causal
  ```

**Numera√ß√£o Duplicada ‚Üí Sequencial:**
- ANTES: `### 9.20`, `### 9.20`, `### 9.35`
- DEPOIS: `### 9.20`, `### 9.21`, `### 9.22`

## ‚ùå O QUE VOC√ä N√ÉO DEVE FAZER:
1. **N√ÉO MOVA** blocos de texto. A ordem deve permanecer 100% cronol√≥gica.
2. **N√ÉO MESCLE** se√ß√µes que apare√ßam em momentos diferentes da aula.
3. **N√ÉO RESUMA** nem altere o corpo dos par√°grafos.
4. **N√ÉO ADICIONE** conte√∫do novo.

## üìù RELAT√ìRIO ESPERADO:
Ao final do documento, inclua um bloco de coment√°rio (que ser√° removido) indicando:
- Quantos n√≠veis de heading foram ajustados (sem reescrever o texto dos t√≠tulos)
- Se a estrutura final segue o mapeamento ou foi adaptada
- Discrep√¢ncias encontradas (se houver)

Formato:
<!-- RELAT√ìRIO: X n√≠veis ajustados | Estrutura: [MAPEAMENTO/ADAPTADA] | Discrep√¢ncias: [Nenhuma/Lista] -->

---

## üìÑ DOCUMENTO PARA REVISAR:
{documento}

---

## RESPOSTA:
Retorne o documento COMPLETO com a formata√ß√£o corrigida e o relat√≥rio no final."""

async def ai_structure_review_lite(texto, client, model, estrutura_mapeada=None, metrics=None):
    """
    v2.3: Revis√£o LEVE de formata√ß√£o Markdown com VALIDA√á√ÉO CRUZADA.
    Compara o documento processado com a estrutura de mapeamento inicial.
    N√ÉO reescreve texto de t√≠tulos; valida hierarquia/sintaxe e reporta discrep√¢ncias.
    N√ÉO reorganiza nem mescla conte√∫do.

    Melhorias v2.3:
    - Melhor tratamento de rate limits
    - Integra√ß√£o com MetricsCollector
    - Op√ß√£o de contexto total (sem split/truncate) para m√°xima fidelidade
    """
    from difflib import SequenceMatcher
    import asyncio
    import time
    import json

    print(f"{Fore.MAGENTA}  üßπ Revis√£o Leve de Formata√ß√£o (IA - Modo Fidelidade v2.3)...{Style.RESET_ALL}")

    start_time = time.time()

    # v2.45: Modo totalidade de janela ativo por padr√£o absoluto
    # para preservar contexto m√°ximo na revis√£o leve.
    use_full_context = True
    split_threshold = int(os.getenv("IUDEX_SPLIT_REVIEW_THRESHOLD", "400000"))
    max_doc_chars = 800000

    # v2.3: Se documento muito grande, dividir em partes e processar em paralelo
    if not use_full_context and len(texto) > split_threshold:
        print(f"{Fore.CYAN}   üîÄ Documento grande ({len(texto)//1000}k chars), dividindo em partes paralelas...{Style.RESET_ALL}")

        # Dividir em partes de ~350k chars cada, com overlap de 10k para contexto
        part_size = 350000
        overlap = 10000
        parts = []
        idx = 0
        while idx < len(texto):
            end = min(idx + part_size, len(texto))
            # Tentar cortar em quebra de linha
            if end < len(texto):
                newline_pos = texto.rfind('\n', idx + part_size - 5000, end)
                if newline_pos > idx:
                    end = newline_pos + 1
            parts.append(texto[idx:end])
            idx = end - overlap if end < len(texto) else end

        print(f"   üì¶ Dividido em {len(parts)} partes para processamento paralelo")

        # Processar partes em paralelo com semaphore
        semaphore = asyncio.Semaphore(2)  # Max 2 paralelos para evitar rate limit

        async def process_part(part_idx: int, part_text: str) -> tuple:
            async with semaphore:
                try:
                    # Passar parte para processamento (recursivo, mas parte ser√° < threshold)
                    result = await ai_structure_review_lite(
                        part_text, client, model, estrutura_mapeada, metrics
                    )
                    return (part_idx, result, None)
                except Exception as e:
                    return (part_idx, part_text, e)  # Retorna original em caso de erro

        tasks = [process_part(i, p) for i, p in enumerate(parts)]
        results = await asyncio.gather(*tasks)

        # Ordenar e mesclar resultados
        results_sorted = sorted(results, key=lambda x: x[0])
        merged_parts = []
        for part_idx, result, error in results_sorted:
            if error:
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è Erro na parte {part_idx + 1}: {error}. Usando original.{Style.RESET_ALL}")
            merged_parts.append(result)

        # Remover overlaps duplicados na mesclagem
        final_text = merged_parts[0]
        for i in range(1, len(merged_parts)):
            # Encontrar ponto de sobreposi√ß√£o
            overlap_start = final_text[-overlap*2:] if len(final_text) > overlap*2 else final_text
            next_part = merged_parts[i]

            # Buscar melhor ponto de corte
            best_cut = 0
            for line in overlap_start.split('\n'):
                if line.strip() and line.strip() in next_part[:overlap*2]:
                    pos = next_part.find(line.strip())
                    if pos >= 0:
                        best_cut = pos + len(line.strip())
                        break

            final_text += next_part[best_cut:].lstrip()

        duration = time.time() - start_time
        print(f"{Fore.GREEN}   ‚úÖ Revis√£o paralela conclu√≠da ({len(parts)} partes, {duration:.1f}s).{Style.RESET_ALL}")
        return final_text

    # Processamento single-shot
    if not use_full_context and len(texto) > max_doc_chars:
        print(f"{Fore.YELLOW}   ‚ö†Ô∏è Documento muito longo ({len(texto)} chars), truncando para {max_doc_chars//1000}k...{Style.RESET_ALL}")
        texto_para_revisao = texto[:max_doc_chars] + "\n\n[... documento truncado para revis√£o ...]"
    else:
        texto_para_revisao = texto
    
    # Preparar estrutura mapeada (se dispon√≠vel)
    if estrutura_mapeada:
        estrutura_str = estrutura_mapeada if use_full_context else estrutura_mapeada[:50000]
        print(f"{Fore.CYAN}   üìã Usando estrutura de mapeamento inicial ({len(estrutura_mapeada)} chars) para valida√ß√£o cruzada.{Style.RESET_ALL}")
    else:
        estrutura_str = "[Estrutura de mapeamento n√£o dispon√≠vel - analisar documento para inferir estrutura ideal]"
        print(f"{Fore.YELLOW}   ‚ÑπÔ∏è  Sem mapeamento inicial, IA ir√° inferir estrutura ideal do pr√≥prio documento.{Style.RESET_ALL}")
    
    def call_gemini():
        return client.models.generate_content(
            model=model,
            contents=PROMPT_STRUCTURE_REVIEW_LITE.format(
                estrutura_mapeada=estrutura_str,
                documento=texto_para_revisao
            ),
            config=types.GenerateContentConfig(
                max_output_tokens=65536,  # M√°ximo permitido
                thinking_config=types.ThinkingConfig(
                    include_thoughts=False, 
                    thinking_level="HIGH"  # HIGH para an√°lise estrutural profunda
                ),
                safety_settings=[
                    types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                ]
            )
        )
    
    # Retry com backoff exponencial
    max_retries = 3
    response = None
    for tentativa in range(max_retries):
        try:
            response = await asyncio.to_thread(call_gemini)
            resultado = response.text.replace('```markdown', '').replace('```', '').strip()
            break
        except Exception as e:
            if tentativa < max_retries - 1:
                wait_time = 2 ** tentativa
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è Tentativa {tentativa + 1} falhou: {e}. Aguardando {wait_time}s...{Style.RESET_ALL}")
                await asyncio.sleep(wait_time)
            else:
                print(f"{Fore.RED}   ‚ö†Ô∏è Erro na revis√£o leve ap√≥s {max_retries} tentativas: {e}. Mantendo original.{Style.RESET_ALL}")
                return texto
    
    # M√©tricas
    duration = time.time() - start_time
    if metrics and response:
        try:
            usage = response.usage_metadata
            metrics.record_call(
                provider="gemini",
                prompt_tokens=getattr(usage, 'prompt_token_count', 0) or 0,
                completion_tokens=getattr(usage, 'candidates_token_count', 0) or 0,
                duration=duration,
                model=model,
                cached_tokens_in=getattr(usage, 'cached_content_token_count', 0) or 0,
            )
        except:
            pass  # Silently ignore metrics errors
    
    # Extrair relat√≥rio (suporta JSON e texto)
    relatorio_json_match = re.search(r'<!--\s*RELAT√ìRIO_JSON:\s*(\{.+?\})\s*-->', resultado, re.IGNORECASE | re.DOTALL)
    relatorio_match = re.search(r'<!--\s*RELAT√ìRIO:\s*(.+?)\s*-->', resultado, re.IGNORECASE)
    
    if relatorio_json_match:
        try:
            relatorio_data = json.loads(relatorio_json_match.group(1))
            print(f"{Fore.CYAN}   üìä Relat√≥rio (JSON): {relatorio_data}{Style.RESET_ALL}")
        except:
            print(f"{Fore.CYAN}   üìä Relat√≥rio: {relatorio_json_match.group(1)}{Style.RESET_ALL}")
        resultado = re.sub(r'<!--\s*RELAT√ìRIO_JSON:.+?-->\s*', '', resultado, flags=re.IGNORECASE | re.DOTALL).strip()
    elif relatorio_match:
        relatorio = relatorio_match.group(1)
        print(f"{Fore.CYAN}   üìä Relat√≥rio da IA: {relatorio}{Style.RESET_ALL}")
        resultado = re.sub(r'<!--\s*RELAT√ìRIO:.+?-->\s*', '', resultado, flags=re.IGNORECASE).strip()
    
    # Valida√ß√£o: resultado deve ter pelo menos 80% do tamanho (padronizado)
    if len(resultado) < len(texto) * 0.80:
        print(f"{Fore.YELLOW}   ‚ö†Ô∏è Revis√£o retornou texto muito curto ({len(resultado)} vs {len(texto)}). Mantendo original.{Style.RESET_ALL}")
        return texto
    
    # Verificar se a ordem dos headers foi preservada (usando similaridade)
    headers_original = re.findall(r'^(#{1,4})\s+(.+?)$', texto, re.MULTILINE)
    headers_revisado = re.findall(r'^(#{1,4})\s+(.+?)$', resultado, re.MULTILINE)
    
    if len(headers_original) > 5 and len(headers_revisado) > 5:
        # Usar similaridade ao inv√©s de igualdade exata
        similares = sum(1 for (_, h1), (_, h2) in zip(headers_original[:10], headers_revisado[:10]) 
                       if SequenceMatcher(None, h1.strip(), h2.strip()).ratio() > 0.6)
        if similares < 6:
            print(f"{Fore.YELLOW}   ‚ö†Ô∏è Ordem dos headers parece alterada ({similares}/10 similares). Mantendo original.{Style.RESET_ALL}")
            return texto
    
    # üìù Relat√≥rio de Altera√ß√µes nos T√≠tulos
    alteracoes = []
    for i, ((lvl_orig, h_orig), (lvl_rev, h_rev)) in enumerate(zip(headers_original[:50], headers_revisado[:50])):
        if h_orig.strip() != h_rev.strip():
            alteracoes.append(f"   ‚Ä¢ '{h_orig[:40]}...' ‚Üí '{h_rev[:40]}...'")
    
    if alteracoes:
        print(f"{Fore.CYAN}   üìù T√≠tulos Refinados ({len(alteracoes)}):{Style.RESET_ALL}")
        for alt in alteracoes[:10]:  # Mostrar no m√°ximo 10
            print(alt)
        if len(alteracoes) > 10:
            print(f"   ... e mais {len(alteracoes) - 10} altera√ß√µes.")
    else:
        print(f"{Fore.GREEN}   ‚ÑπÔ∏è  Nenhum t√≠tulo foi alterado (estrutura j√° estava OK).{Style.RESET_ALL}")
    
    print(f"{Fore.GREEN}   ‚úÖ Formata√ß√£o revisada (modo leve v2.2, {duration:.1f}s).{Style.RESET_ALL}")
    return resultado

async def ai_structure_review(texto, client, model, estrutura_mapeada=None, metrics=None):
    """
    v2.2: Revis√£o sem√¢ntica de estrutura usando IA com VALIDA√á√ÉO CRUZADA.
    Compara o documento com a estrutura de mapeamento inicial.
    Corrige: quest√µes duplicadas, subt√≥picos √≥rf√£os, fragmenta√ß√£o excessiva.
    
    Melhorias v2.2:
    - Contexto total por padr√£o na revis√£o sem√¢ntica (APOSTILA)
    - Valida√ß√£o de ordem dos headers
    - Integra√ß√£o com MetricsCollector
    - Suporte a relat√≥rio JSON
    """
    from google.genai import types
    from difflib import SequenceMatcher
    import asyncio
    import time
    import json
    
    print(f"{Fore.CYAN}üß† Revis√£o Sem√¢ntica de Estrutura (IA v2.2)...")
    
    start_time = time.time()
    
    # v2.45: Para APOSTILA, usar contexto completo por padr√£o (sem truncamento).
    # Mant√©m op√ß√£o de voltar ao legado via env em cen√°rios extremos.
    use_full_context = _env_truthy("IUDEX_APOSTILA_FULL_CONTEXT", default=True)
    max_doc_chars = 800000
    if not use_full_context and len(texto) > max_doc_chars:
        print(f"   ‚ö†Ô∏è Documento muito longo ({len(texto)} chars), truncando para {max_doc_chars//1000}k...")
        texto_para_revisao = texto[:max_doc_chars] + "\n\n[... documento truncado para revis√£o estrutural ...]"
    else:
        texto_para_revisao = texto

    # Preparar estrutura mapeada (se dispon√≠vel)
    if estrutura_mapeada:
        estrutura_str = estrutura_mapeada if use_full_context else estrutura_mapeada[:50000]
        print(f"{Fore.CYAN}   üìã Usando estrutura de mapeamento inicial ({len(estrutura_mapeada)} chars) para valida√ß√£o cruzada.{Style.RESET_ALL}")
    else:
        estrutura_str = "[Estrutura de mapeamento n√£o dispon√≠vel - analisar documento autonomamente]"
        print(f"{Fore.YELLOW}   ‚ÑπÔ∏è  Sem mapeamento inicial, IA revisar√° estrutura autonomamente.{Style.RESET_ALL}")
    
    def call_gemini():
        return client.models.generate_content(
            model=model,
            contents=PROMPT_STRUCTURE_REVIEW.format(
                estrutura_mapeada=estrutura_str,
                documento=texto_para_revisao
            ),
            config=types.GenerateContentConfig(
                max_output_tokens=65536,
                thinking_config=types.ThinkingConfig(
                    include_thoughts=False,
                    thinking_level="HIGH"
                ),
                safety_settings=[
                    types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                ]
            )
        )
    
    # Retry com backoff exponencial
    max_retries = 3
    response = None
    for tentativa in range(max_retries):
        try:
            response = await asyncio.to_thread(call_gemini)
            resultado = response.text.replace('```markdown', '').replace('```', '').strip()
            break
        except Exception as e:
            if tentativa < max_retries - 1:
                wait_time = 2 ** tentativa
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è Tentativa {tentativa + 1} falhou: {e}. Aguardando {wait_time}s...{Style.RESET_ALL}")
                await asyncio.sleep(wait_time)
            else:
                print(f"{Fore.RED}   ‚ö†Ô∏è Erro na revis√£o por IA ap√≥s {max_retries} tentativas: {e}. Mantendo estrutura original.{Style.RESET_ALL}")
                return texto
    
    # M√©tricas
    duration = time.time() - start_time
    if metrics and response:
        try:
            usage = response.usage_metadata
            metrics.record_call(
                provider="gemini",
                prompt_tokens=getattr(usage, 'prompt_token_count', 0) or 0,
                completion_tokens=getattr(usage, 'candidates_token_count', 0) or 0,
                duration=duration,
                model=model,
                cached_tokens_in=getattr(usage, 'cached_content_token_count', 0) or 0,
            )
        except:
            pass  # Silently ignore metrics errors
    
    # Valida√ß√£o: o resultado deve ter pelo menos 80% do tamanho original (padronizado)
    if len(resultado) < len(texto) * 0.80:
        print(f"   ‚ö†Ô∏è Revis√£o retornou texto muito curto ({len(resultado)} vs {len(texto)}). Mantendo original.")
        return texto
    
    # Extrair relat√≥rio (suporta JSON e texto)
    relatorio_json_match = re.search(r'<!--\s*RELAT√ìRIO_JSON:\s*(\{.+?\})\s*-->', resultado, re.IGNORECASE | re.DOTALL)
    relatorio_match = re.search(r'<!--\s*RELAT√ìRIO:\s*(.+?)\s*-->', resultado, re.IGNORECASE)
    
    if relatorio_json_match:
        try:
            relatorio_data = json.loads(relatorio_json_match.group(1))
            print(f"{Fore.CYAN}   üìä Relat√≥rio (JSON): {relatorio_data}{Style.RESET_ALL}")
        except:
            print(f"{Fore.CYAN}   üìä Relat√≥rio: {relatorio_json_match.group(1)}{Style.RESET_ALL}")
        resultado = re.sub(r'<!--\s*RELAT√ìRIO_JSON:.+?-->\s*', '', resultado, flags=re.IGNORECASE | re.DOTALL).strip()
    elif relatorio_match:
        relatorio = relatorio_match.group(1)
        print(f"{Fore.CYAN}   üìä Relat√≥rio da IA: {relatorio}{Style.RESET_ALL}")
        resultado = re.sub(r'<!--\s*RELAT√ìRIO:.+?-->\s*', '', resultado, flags=re.IGNORECASE).strip()
    
    # Valida√ß√£o de ordem dos headers (novo em v2.2)
    headers_original = re.findall(r'^(#{1,4})\s+(.+?)$', texto, re.MULTILINE)
    headers_revisado = re.findall(r'^(#{1,4})\s+(.+?)$', resultado, re.MULTILINE)
    
    if len(headers_original) > 5 and len(headers_revisado) > 5:
        # Usar similaridade para detectar reordena√ß√£o
        similares = sum(1 for (_, h1), (_, h2) in zip(headers_original[:10], headers_revisado[:10]) 
                       if SequenceMatcher(None, h1.strip(), h2.strip()).ratio() > 0.6)
        if similares < 6:
            print(f"{Fore.YELLOW}   ‚ö†Ô∏è Ordem dos headers parece alterada ({similares}/10 similares). Mantendo original.{Style.RESET_ALL}")
            return texto
    
    # Contar quantos headers foram alterados
    diff = abs(len(headers_original) - len(headers_revisado))
    
    print(f"{Fore.GREEN}   ‚úÖ Estrutura revisada: {len(headers_original)} ‚Üí {len(headers_revisado)} headers (Œî{diff}, {duration:.1f}s){Style.RESET_ALL}")
    return resultado

def normalizar_fingerprint(texto, tipo):
    """Normaliza texto para compara√ß√£o (ex: 'Lei 11.100' -> 'lei 11100')"""
    texto = texto.lower().strip()
    
    if tipo == 'leis':
        nums = re.findall(r'\d+', texto)
        if nums:
            num_full = ''.join(nums)
            if len(num_full) >= 4: return f"lei {num_full}"
            return None
            
    elif tipo == 'sumulas':
        nums = re.findall(r'\d+', texto)
        if nums: return f"s√∫mula {''.join(nums)}"
            
    elif tipo == 'artigos':
        nums = re.findall(r'\d+', texto)
        if nums: return f"artigo {''.join(nums)}"
            
    return re.sub(r'[^\w\s]', '', texto)

def extrair_fingerprints(texto):
    """Extrai 'fingerprints' √∫nicos e normalizados do texto"""
    fingerprints = {'leis': set(), 'sumulas': set(), 'artigos': set(), 'julgados': set()}
    
    lei_pattern = re.compile(r'\b(?:lei|l\.)\s*n?¬∫?\s*([\d\.]+)', re.IGNORECASE)
    sumula_pattern = re.compile(r'\bs√∫mula\s*(?:vinculante)?\s*n?¬∫?\s*(\d+)', re.IGNORECASE)
    
    for match in lei_pattern.finditer(texto):
        fp = normalizar_fingerprint(f"lei {match.group(1)}", 'leis')
        if fp: fingerprints['leis'].add(fp)
    
    for match in sumula_pattern.finditer(texto):
        fp = normalizar_fingerprint(f"s√∫mula {match.group(1)}", 'sumulas')
        if fp: fingerprints['sumulas'].add(fp)
        
    return fingerprints

def contar_ocorrencias_robust(fingerprints, texto):
    """Conta ocorr√™ncias com suporte a formata√ß√£o jur√≠dica formal"""
    contagens = {}
    texto_lower = texto.lower()
    
    for categoria, items in fingerprints.items():
        for item in items:
            key = f"{categoria}:{item}"
            if categoria == 'leis':
                num_bruto = item.split()[-1] 
                num = re.sub(r'[^\d]', '', num_bruto)
                num_regex = r"\.?".join(list(num))
                pattern = f"lei(?:\\s+|\\.|\\,|n¬∫|n\\.|n\\s|num\\.?)*{num_regex}\\b"
                matches = re.findall(pattern, texto_lower)
                contagens[key] = len(matches)
            elif categoria == 'sumulas':
                num = item.split()[-1]
                num_regex = r"\.?".join(list(num))
                pattern = f"s√∫mula(?:\\s+|\\.|\\,|vinculante|n¬∫|n\\.|n\\s)*{num_regex}\\b"
                matches = re.findall(pattern, texto_lower)
                contagens[key] = len(matches)
            else:
                contagens[key] = texto_lower.count(item)
    return contagens

def contar_ocorrencias_robust(fingerprints, texto):
    """Conta ocorrencias com suporte a formata√ß√£o jur√≠dica formal (Lei n¬∫ X)"""
    contagens = {}
    
    # CORRE√á√ÉO 1: N√£o remover pontua√ß√£o indiscriminadamente, apenas normalizar espa√ßos
    # Mantemos barras e pontos para evitar fus√£o de n√∫meros (11.101/2005)
    texto_lower = texto.lower()
    
    for categoria, items in fingerprints.items():
        for item in items:
            key = f"{categoria}:{item}"
            
            if categoria == 'leis':
                # item ex: "lei 4320" -> extrai "4320"
                # Remove pontua√ß√£o do item para garantir match limpo no n√∫mero
                num_bruto = item.split()[-1] 
                num = re.sub(r'[^\d]', '', num_bruto)
                
                # Permite pontos opcionais entre d√≠gitos (ex: 4.320 match com 4320)
                num_regex = r"\.?".join(list(num))
                
                # CORRE√á√ÉO 2: Regex flex√≠vel que aceita "n", "n¬∫", "no", "num" no meio
                # Aceita: "Lei 4320", "Lei n¬∫ 4.320", "Lei n. 4320"
                # O \W* permite pontos/barras entre Lei e o n√∫mero
                # Adicionado \b no final para evitar matches parciais (Lei 10 != Lei 100)
                pattern = f"lei(?:\\s+|\\.|\\,|n¬∫|n\\.|n\\s|num\\.?)*{num_regex}\\b"
                
                # Usamos findall no texto original (lower) para pegar varia√ß√µes com pontua√ß√£o
                matches = re.findall(pattern, texto_lower)
                contagens[key] = len(matches)
                
            elif categoria == 'sumulas':
                num = item.split()[-1]
                num_regex = r"\.?".join(list(num)) # S√∫mulas raramente tem ponto, mas por garantia
                
                # Mesma l√≥gica para s√∫mulas (S√∫mula Vinculante n¬∫ 10)
                pattern = f"s√∫mula(?:\\s+|\\.|\\,|vinculante|n¬∫|n\\.|n\\s)*{num_regex}\\b"
                matches = re.findall(pattern, texto_lower)
                contagens[key] = len(matches)
                
            else:
                # Fallback para outros tipos (busca literal simples)
                contagens[key] = texto_lower.count(item)
                
    return contagens

def verificar_cobertura(texto_original, texto_formatado, arquivo_saida=None):
    """Verifica omiss√µes e duplica√ß√µes artificiais entre original e formatado"""
    logger.info("üîç Verificando cobertura e duplica√ß√µes...")
    
    # Extrai fingerprints do original
    fp_original = extrair_fingerprints(texto_original)
    
    # Conta ocorr√™ncias em ambos
    contagem_original = contar_ocorrencias_robust(fp_original, texto_original)
    contagem_formatado = contar_ocorrencias_robust(fp_original, texto_formatado)
    
    omissoes = []
    duplicacoes = []
    
    for key, count_orig in contagem_original.items():
        count_fmt = contagem_formatado.get(key, 0)
        categoria, item = key.split(':', 1)
        
        # Omiss√£o: estava no original mas sumiu
        if count_orig > 0 and count_fmt == 0:
            omissoes.append({
                'categoria': categoria,
                'item': item,
                'original': count_orig,
                'formatado': count_fmt
            })
        
        # Duplica√ß√£o (agora considerada positiva em materiais did√°ticos)
        if count_fmt > count_orig:
            duplicacoes.append({
                'categoria': categoria,
                'item': item,
                'original': count_orig,
                'formatado': count_fmt,
                'extra': count_fmt - count_orig
            })
    
    # Gera relat√≥rio
    total_items = len([k for k, v in contagem_original.items() if v > 0])
    items_preservados = total_items - len(omissoes)
    cobertura = items_preservados / total_items * 100 if total_items > 0 else 100
    
    logger.info(f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    logger.info(f"üìä RELAT√ìRIO DE VERIFICA√á√ÉO")
    logger.info(f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    logger.info(f"‚úÖ Cobertura: {cobertura:.1f}% ({items_preservados}/{total_items} refer√™ncias)")
    
    relatorio_txt = []
    relatorio_txt.append(f"RELAT√ìRIO DE VERIFICA√á√ÉO DE FIDELIDADE")
    relatorio_txt.append(f"Cobertura: {cobertura:.1f}% ({items_preservados}/{total_items} refer√™ncias)")
    
    if omissoes:
        logger.warning(f"\n‚ùå POSS√çVEIS OMISS√ïES ({len(omissoes)}):")
        relatorio_txt.append(f"\n‚ùå POSS√çVEIS OMISS√ïES ({len(omissoes)}):")
        for o in omissoes[:15]: 
            msg = f"   - [{o['categoria']}] {o['item']}"
            logger.warning(msg)
            relatorio_txt.append(msg)
        if len(omissoes) > 15:
            logger.warning(f"   ... e mais {len(omissoes) - 15} omiss√µes")
            relatorio_txt.append(f"   ... e mais {len(omissoes) - 15} omiss√µes")
    else:
        logger.info("‚úÖ Nenhuma omiss√£o detectada")
        relatorio_txt.append("\n‚úÖ Nenhuma omiss√£o detectada")
    
    if duplicacoes:
        logger.info(f"\n‚ÑπÔ∏è CITA√á√ïES REFOR√áADAS (Tabelas/Resumos) ({len(duplicacoes)}):")
        relatorio_txt.append(f"\n‚ÑπÔ∏è CITA√á√ïES REFOR√áADAS ({len(duplicacoes)}):")
        for d in duplicacoes[:10]:
            msg = f"   - [{d['categoria']}] {d['item']}: {d['original']}x -> {d['formatado']}x"
            logger.info(msg)
            relatorio_txt.append(msg)
    
    logger.info(f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    
    # Salva relat√≥rio em arquivo se especificado
    if arquivo_saida:
        relatorio_path = arquivo_saida.replace('.md', '_verificacao.txt')
        with open(relatorio_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(relatorio_txt))
        logger.info(f"üìù Relat√≥rio salvo em: {relatorio_path}")

def corrigir_tabelas_prematuras(texto: str, min_chars_apos_tabela: int = 100, min_linhas_apos: int = 2) -> str:
    """
    v2.28: Detecta e corrige tabelas que aparecem antes do conte√∫do terminar.

    Problema: O LLM √†s vezes gera a tabela no meio do t√≥pico, antes de
    terminar de explicar todo o conte√∫do.

    Solu√ß√£o: Se houver texto substancial (>min_chars) AP√ìS uma tabela e ANTES
    do pr√≥ximo heading, move a tabela para depois desse texto.

    Args:
        texto: Markdown com poss√≠veis tabelas prematuras
        min_chars_apos_tabela: M√≠nimo de caracteres ap√≥s tabela para considerar prematura (default: 100)
        min_linhas_apos: M√≠nimo de linhas de conte√∫do ap√≥s tabela (default: 2)

    Returns:
        Texto com tabelas reposicionadas
    """
    linhas = texto.split('\n')
    resultado = []
    i = 0
    tabelas_corrigidas = 0

    def is_major_heading(line: str) -> bool:
        """Heading H1/H2/H3 que indica novo t√≥pico (e N√ÉO √© t√≠tulo de tabela)."""
        s = (line or "").strip()
        if not s.startswith('#'):
            return False
        level = 0
        for c in s:
            if c == '#':
                level += 1
            else:
                break
        if level <= 3:
            return not is_table_title(s)
        return False

    def is_table_line(line: str) -> bool:
        stripped = line.strip()
        return stripped.startswith('|') and '|' in stripped[1:]

    def is_table_title(line: str) -> bool:
        s = (line or "").strip()
        if not s:
            return False
        lower = s.lower()
        is_heading = bool(re.match(r'^#{3,6}\s+', s))
        starts_with_emoji = s.startswith('üìã') or s.startswith('üéØ')
        has_emoji = ('üìã' in s) or ('üéØ' in s)
        has_keyword = (
            'quadro-s√≠ntese' in lower
            or 'quadro s√≠ntese' in lower
            or 'pegadinha' in lower
            or 'como a banca' in lower
        )
        # Para evitar falsos positivos (texto corrido contendo "quadro"/"pegadinha"),
        # exigimos estrutura t√≠pica de "t√≠tulo": heading ou linha iniciando com emoji.
        if starts_with_emoji:
            return True
        if is_heading and (has_emoji or has_keyword):
            return True
        return False

    while i < len(linhas):
        linha = linhas[i]

        # Detectar t√≠tulo de tabela (üìã ou üéØ)
        if is_table_title(linha):
            group_title = linha
            tabelas_linhas = []

            # Capturar um grupo de tabelas consecutivas (ex.: üìã + üéØ)
            while i < len(linhas) and is_table_title(linhas[i]):
                titulo_tabela = linhas[i]
                tabelas_linhas.append(titulo_tabela)
                i += 1

                # Capturar a tabela (linhas com |) + linhas em branco imediatamente ao redor
                while i < len(linhas) and (is_table_line(linhas[i]) or not linhas[i].strip()):
                    tabelas_linhas.append(linhas[i])
                    i += 1

                # Se houver outra tabela logo em seguida (apenas com espa√ßos/linhas vazias entre),
                # capturamos como parte do mesmo grupo.
                k = i
                while k < len(linhas) and not linhas[k].strip():
                    k += 1
                if k < len(linhas) and is_table_title(linhas[k]):
                    # Preservar as linhas em branco entre as tabelas
                    tabelas_linhas.extend(linhas[i:k])
                    i = k
                    continue
                break

            # Verificar se h√° conte√∫do substancial AP√ìS o grupo de tabelas e ANTES do pr√≥ximo major heading
            j = i
            bloco_apos = []
            while j < len(linhas):
                if is_major_heading(linhas[j]):
                    break
                # Se surgir outra tabela n√£o-consecutiva, n√£o atravessar (evita reorder agressivo)
                if is_table_title(linhas[j]):
                    break
                bloco_apos.append(linhas[j])
                j += 1

            conteudo_apos = [l for l in bloco_apos if l.strip() and not is_table_line(l)]
            chars_apos = sum(len(l) for l in conteudo_apos)
            linhas_apos = len([l for l in conteudo_apos if l.strip()])

            # Se h√° texto substancial ap√≥s a tabela, √© uma tabela prematura
            if chars_apos >= min_chars_apos_tabela and linhas_apos >= min_linhas_apos:
                tabelas_corrigidas += 1
                print(
                    f"{Fore.YELLOW}   üîÑ Tabela prematura detectada: '{group_title[:50]}...' "
                    f"({chars_apos} chars, {linhas_apos} linhas de conte√∫do ap√≥s)"
                )

                # Conte√∫do primeiro
                resultado.extend(bloco_apos)
                # Depois o grupo de tabelas
                if resultado and resultado[-1].strip():
                    resultado.append('')
                resultado.extend(tabelas_linhas)
                if resultado and resultado[-1].strip():
                    resultado.append('')

                i = j  # Pular o texto j√° processado
                continue

            # Grupo no lugar certo, adicionar normalmente
            resultado.extend(tabelas_linhas)
            continue

        resultado.append(linha)
        i += 1

    if tabelas_corrigidas > 0:
        print(f"{Fore.GREEN}   ‚úÖ Corrigidas {tabelas_corrigidas} tabelas prematuras")

    return '\n'.join(resultado)


def mover_tabelas_para_fim_de_secao(texto):
    """
    v2.11: Reorganiza tabelas movendo-as para o final do BLOCO ATUAL (H2 ou H3).
    Corrige bug de tabelas sumindo ou ficando muito longe do contexto.
    """
    logger.info("üìä Reorganizando tabelas (Smart Layout)...")
    
    linhas = texto.split('\n')
    resultado = []
    tabelas_pendentes = [] 

    def _is_table_title_line(line: str) -> bool:
        s = (line or "").strip()
        if not s:
            return False
        is_heading = bool(re.match(r'^#{3,5}\s+', s))
        is_bold = s.startswith('**')
        if not (is_heading or is_bold):
            # Alguns modelos √†s vezes emitem o t√≠tulo sem markdown de heading.
            if s.startswith('üìã') or 'quadro-s√≠ntese' in s.lower() or 'quadro-sintese' in s.lower():
                return True
            return False

        lowered = s.lower()
        return any(
            x in lowered
            for x in [
                'tabela',
                'resumo',
                'quadro',
                's√≠ntese',
                'sintese',
                'esquema',
                'üìã',
                'prova',
                'banca',
                'pegadinha',
                'quest√£o',
                'questao',
                'quest√µes',
                'questoes',
            ]
        )

    def _pop_recent_table_title(result_lines: list, max_lookback_nonempty: int = 12):
        """
        Recupera um t√≠tulo de tabela recente (H3-H5 / bold / üìã) mesmo que
        haja algumas linhas de texto entre o t√≠tulo e a tabela.
        """
        nonempty_seen = 0
        for idx in range(len(result_lines) - 1, -1, -1):
            s = (result_lines[idx] or "").strip()
            if not s:
                continue
            if s.startswith('# ') or s.startswith('## '):
                break
            nonempty_seen += 1
            if nonempty_seen > max_lookback_nonempty:
                break
            if _is_table_title_line(result_lines[idx]):
                return result_lines.pop(idx)
        return None

    def _is_table_separator_line(line: str) -> bool:
        s = (line or "").strip()
        return bool(s) and s.startswith("|") and set(s.replace("|", "").strip()).issubset({"-", ":", " "})

    def _next_nonempty_index(lines: list, start_idx: int) -> int | None:
        j = start_idx
        while j < len(lines):
            if (lines[j] or "").strip():
                return j
            j += 1
        return None

    def _is_table_header_at(lines: list, idx: int) -> bool:
        """Heur√≠stica: linha com '|' seguida de uma linha separadora (pula vazias)."""
        if idx < 0 or idx >= len(lines):
            return False
        s = (lines[idx] or "").strip()
        if not s or "|" not in s:
            return False
        nxt = _next_nonempty_index(lines, idx + 1)
        if nxt is None:
            return False
        return _is_table_separator_line(lines[nxt])
    
    i = 0
    while i < len(linhas):
        linha = linhas[i]
        linha_strip = linha.strip()
        
        # 1. DETECTAR SE √â UM T√çTULO (apenas H1/H2/H3 delimitam "bloco")
        # Motivo: H4/H5 s√£o frequentemente usados como subt√≠tulos dentro do mesmo assunto.
        # Se flusharmos em qualquer '#', a tabela pode parar antes do assunto terminar.
        if re.match(r'^#{1,3}\s+', linha_strip):
            # Despeja tabelas antes de iniciar o novo t√≥pico
            if tabelas_pendentes:
                resultado.append('') # Espa√ßo antes
                for t_info in tabelas_pendentes:
                    if t_info['titulo']:
                        resultado.append(t_info['titulo'])
                    resultado.extend(t_info['linhas'])
                    resultado.append('') # Espa√ßo depois
                tabelas_pendentes = []
            
            resultado.append(linha)
            i += 1
            continue

        # 2. DETECTAR IN√çCIO DE TABELA
        eh_inicio_tabela = False
        if '|' in linha_strip:
            has_separator = False
            for lookahead in range(1, 3): 
                if i + lookahead < len(linhas):
                    prox = linhas[i + lookahead].strip()
                    if set(prox).issubset(set('|- :')): 
                         has_separator = True
                         break
            
            if has_separator or (linha_strip.startswith('|') and linha_strip.endswith('|')):
                eh_inicio_tabela = True

        if eh_inicio_tabela:
            # --- Captura da Tabela ---
            tabela_linhas = []
            titulo_tabela = None
            
            # v2.24+: Recupera t√≠tulo de tabela mesmo com "texto intruso" entre o t√≠tulo e a tabela.
            # Se o LLM inserir explica√ß√£o ap√≥s "üìã Quadro-s√≠ntese", este trecho remove o t√≠tulo
            # da posi√ß√£o original para reagrup√°-lo com a tabela no flush do bloco.
            if resultado:
                titulo_tabela = _pop_recent_table_title(resultado)

            # Captura as linhas da tabela
            j = i
            seen_separator = False
            while j < len(linhas):
                curr = linhas[j].strip()
                if not curr:
                    # Evita "colar" duas tabelas distintas separadas por linhas vazias.
                    # Linhas vazias dentro de tabela s√£o raras; aqui preferimos robustez.
                    # Se a pr√≥xima tabela come√ßar ap√≥s a quebra, ela ser√° capturada no loop externo.
                    nxt = _next_nonempty_index(linhas, j + 1)
                    if nxt is None:
                        break
                    if seen_separator and _is_table_header_at(linhas, nxt):
                        break
                    # Caso contr√°rio, apenas pula vazios (n√£o inclui no buffer da tabela).
                    j += 1
                    continue
                if '|' in curr:
                    # Se j√° vimos o separador, e encontrarmos um novo header+separador,
                    # tratamos como in√≠cio de OUTRA tabela (n√£o continua√ß√£o).
                    if seen_separator and _is_table_header_at(linhas, j):
                        break
                    tabela_linhas.append(linhas[j])
                    if _is_table_separator_line(linhas[j]):
                        seen_separator = True
                    j += 1
                else:
                    break # Texto normal

            if len(tabela_linhas) > 0:
                tabelas_pendentes.append({
                    'titulo': titulo_tabela,
                    'linhas': tabela_linhas
                })
                i = j # Pula as linhas processadas
                continue
            else:
                if titulo_tabela:
                    resultado.append(titulo_tabela)
        
        resultado.append(linha)
        i += 1
    
    # 3. FINAL DO DOCUMENTO
    if tabelas_pendentes:
        resultado.append('')
        for t_info in tabelas_pendentes:
            if t_info['titulo']:
                resultado.append(t_info['titulo'])
            resultado.extend(t_info['linhas'])
            resultado.append('')
            
    return '\n'.join(resultado)


def mesclar_tabelas_divididas(texto: str) -> str:
    """
    v2.27: Detecta tabelas que foram divididas entre chunks e as mescla.
    
    Padr√£o detectado:
    | Col1 | Col2 |
    |------|------|
    | A    | B    |
    
    [... linhas em branco ...]
    
    | Col1 | Col2 |      <-- Mesma estrutura = tabela continuada
    |------|------|
    | C    | D    |
    
    A fun√ß√£o identifica tabelas consecutivas com mesmo n√∫mero de colunas
    (separadas apenas por linhas em branco) e remove o header duplicado
    da segunda tabela para criar uma tabela unificada.
    """
    logger.info("üìä Mesclando tabelas divididas (v2.27)...")
    
    lines = texto.split('\n')
    result = []
    i = 0
    tables_merged = 0
    
    def is_table_line(line: str) -> bool:
        stripped = line.strip()
        return stripped.startswith('|') and stripped.endswith('|')
    
    def is_separator_line(line: str) -> bool:
        stripped = line.strip()
        return stripped.startswith('|') and set(stripped.replace('|', '').strip()).issubset({'-', ':', ' '})
    
    def count_columns(line: str) -> int:
        return line.count('|') - 1 if '|' in line else 0
    
    while i < len(lines):
        line = lines[i]
        result.append(line)
        
        # Detectar fim de tabela (linha atual √© tabela, pr√≥xima n√£o √©)
        if is_table_line(line) and not is_separator_line(line):
            # Verificar se a pr√≥xima linha n√£o √© tabela
            if i + 1 < len(lines) and not is_table_line(lines[i + 1]):
                last_table_cols = count_columns(line)
                
                # Encontrar o header da tabela atual (voltando at√© achar header + separator)
                last_table_header = None
                for back in range(len(result) - 1, -1, -1):
                    if is_separator_line(result[back]) and back > 0:
                        last_table_header = result[back - 1].strip()
                        break
                
                # Procurar pr√≥xima tabela (pulando linhas em branco)
                lookahead = 1
                while i + lookahead < len(lines):
                    next_line = lines[i + lookahead].strip()
                    if next_line == '':
                        lookahead += 1
                        continue
                    if next_line.startswith('#'):
                        # Novo t√≠tulo = tabelas s√£o de se√ß√µes diferentes, N√ÉO mesclar
                        break
                    if not is_table_line(lines[i + lookahead]):
                        # Qualquer linha n√£o-tabela (ex.: "üìã Quadro-s√≠ntese" em bold, explica√ß√£o, etc.)
                        # indica que n√£o √© continua√ß√£o direta da mesma tabela.
                        break
                    if is_table_line(lines[i + lookahead]):
                        next_table_cols = count_columns(lines[i + lookahead])
                        next_table_header = lines[i + lookahead].strip()
                        
                        # v2.27: Verificar se √© continua√ß√£o (mesmo n√∫mero de colunas E mesmo header)
                        headers_match = last_table_header == next_table_header if last_table_header else True
                        if last_table_cols == next_table_cols and last_table_cols >= 2 and headers_match:
                            # Verificar se a pr√≥xima linha √© separator (header + separator = pular ambos)
                            skip_count = 0
                            if i + lookahead + 1 < len(lines) and is_separator_line(lines[i + lookahead + 1]):
                                # Pular header duplicado e separator
                                skip_count = 2
                                tables_merged += 1
                                print(f"   üîó Mesclando tabela em linha {i + lookahead} ({next_table_cols} colunas)")
                            
                            if skip_count > 0:
                                # Remover linhas em branco que j√° foram adicionadas ao result
                                while result and result[-1].strip() == '':
                                    result.pop()
                                # Avan√ßar para depois do header+separator da segunda tabela
                                i += lookahead + skip_count - 1
                    break
        
        i += 1
    
    if tables_merged > 0:
        print(f"   ‚úÖ {tables_merged} tabela(s) mesclada(s)")
    else:
        print(f"   ‚ÑπÔ∏è  Nenhuma tabela dividida detectada")
    
    return '\n'.join(result)

def garantir_titulo_tabela_banca(texto: str) -> str:
    """
    Garante que a tabela "Como a banca cobra / pegadinhas" tenha t√≠tulo vis√≠vel.
    Evita que a tabela fique colada ao quadro-s√≠ntese sem o subt√≠tulo.
    """
    lines = texto.split('\n')
    output = []

    def _is_banca_header(line: str) -> bool:
        return (line or "").strip().lower().startswith('| como a banca cobra |')

    def _is_banca_title(line: str) -> bool:
        s = (line or "").strip().lower()
        return s.startswith('#') and ('üéØ' in s or 'banca cobra' in s or 'pegadinha' in s)

    def _is_heading(line: str) -> bool:
        return (line or "").strip().startswith('#')

    for line in lines:
        if _is_banca_header(line):
            has_title = False
            nonempty_seen = 0
            for back in range(len(output) - 1, -1, -1):
                s = (output[back] or "").strip()
                if not s:
                    continue
                nonempty_seen += 1
                if _is_banca_title(output[back]):
                    has_title = True
                    break
                if _is_heading(output[back]) or nonempty_seen >= 8:
                    break
            if not has_title:
                if output and output[-1].strip():
                    output.append('')
                output.append('#### üéØ Tabela ‚Äî Como a banca cobra / pegadinhas')
                output.append('')
        output.append(line)

    return '\n'.join(output)


def _similaridade_palavras(texto_a: str, texto_b: str) -> float:
    """
    v2.33: Calcula similaridade entre dois textos baseado em overlap de palavras.
    Retorna valor entre 0.0 (nenhuma similaridade) e 1.0 (id√™nticos).
    """
    if not texto_a or not texto_b:
        return 0.0

    # Normalizar: lowercase e remover pontua√ß√£o
    def normalizar(t):
        t = t.lower()
        t = re.sub(r'[^\w\s]', '', t)
        return set(w for w in t.split() if len(w) > 2)  # Ignorar palavras muito curtas

    palavras_a = normalizar(texto_a)
    palavras_b = normalizar(texto_b)

    if not palavras_a or not palavras_b:
        return 0.0

    intersecao = palavras_a & palavras_b
    uniao = palavras_a | palavras_b

    return len(intersecao) / len(uniao) if uniao else 0.0


def _buscar_ancora_no_texto(texto_lower: str, titulo: str, transcricao_completa: str) -> int:
    """
    v2.33: Busca inteligente de √¢ncora quando o modelo n√£o forneceu cita√ß√£o verbatim.

    Estrat√©gias:
    1. Buscar palavras-chave do t√≠tulo no texto
    2. Buscar frases de transi√ß√£o comuns pr√≥ximas √†s palavras-chave

    Returns:
        Posi√ß√£o no texto ou -1 se n√£o encontrar
    """
    # Extrair palavras significativas do t√≠tulo (ignorar palavras comuns)
    STOPWORDS = {'de', 'da', 'do', 'das', 'dos', 'em', 'na', 'no', 'nas', 'nos',
                 'para', 'por', 'com', 'sem', 'sobre', 'entre', 'at√©', 'como',
                 'uma', 'um', 'uns', 'umas', 'aos', '√†s', 'e', 'ou', 'que', 'se'}

    titulo_lower = titulo.lower()
    titulo_clean = re.sub(r'[^\w\s]', ' ', titulo_lower)
    palavras_titulo = [w for w in titulo_clean.split() if len(w) > 3 and w not in STOPWORDS]

    if not palavras_titulo:
        return -1

    # Estrat√©gia 1: Buscar sequ√™ncia de 2-3 palavras-chave consecutivas
    for n_palavras in [3, 2]:
        if len(palavras_titulo) >= n_palavras:
            busca = ' '.join(palavras_titulo[:n_palavras])
            pos = texto_lower.find(busca)
            if pos != -1:
                # Voltar at√© in√≠cio da frase/linha
                while pos > 0 and transcricao_completa[pos - 1] not in '.\n':
                    pos -= 1
                    if pos < len(transcricao_completa) - 200:  # Limite de 200 chars para tr√°s
                        break
                return pos

    # Estrat√©gia 2: Buscar frases de transi√ß√£o + primeira palavra-chave
    FRASES_TRANSICAO = [
        'vamos agora', 'passemos para', 'vamos falar', 'vamos tratar',
        'o pr√≥ximo tema', 'o pr√≥ximo ponto', 'agora vamos',
        'entrando no', 'entrando em', 'passando para', 'passando ao',
        'quanto ao', 'quanto √†', 'em rela√ß√£o ao', 'em rela√ß√£o √†',
        'no que tange', 'no que diz respeito', 'sobre o tema',
        'come√ßando por', 'iniciando com', 'primeiro tema',
        'vamos come√ßar', 'vamos iniciar'
    ]

    for frase in FRASES_TRANSICAO:
        pos_transicao = texto_lower.find(frase)
        if pos_transicao != -1:
            # Verificar se alguma palavra-chave do t√≠tulo est√° pr√≥xima (at√© 200 chars depois)
            zona = texto_lower[pos_transicao:pos_transicao + 200]
            for palavra in palavras_titulo[:2]:
                if palavra in zona:
                    # Voltar at√© in√≠cio da linha
                    while pos_transicao > 0 and transcricao_completa[pos_transicao - 1] not in '\n':
                        pos_transicao -= 1
                    return pos_transicao

    # Estrat√©gia 3: Buscar apenas a primeira palavra-chave significativa
    if palavras_titulo:
        palavra_principal = max(palavras_titulo[:3], key=len) if len(palavras_titulo) >= 3 else palavras_titulo[0]
        pos = texto_lower.find(palavra_principal)
        if pos != -1:
            # Voltar at√© in√≠cio da linha
            while pos > 0 and transcricao_completa[pos - 1] not in '\n':
                pos -= 1
            return pos

    return -1


def limpar_estrutura_para_review(mapping: str) -> str:
    """
    v2.25: Remove metadados de √¢ncora (ABRE/FECHA) do mapeamento para uso em ai_structure_review.

    Transforma:
        1. Introdu√ß√£o | ABRE: "frase" | FECHA: "frase"
    Em:
        1. Introdu√ß√£o
    """
    if not mapping:
        return mapping
    return re.sub(r'\s*\|\s*(?:ABRE|FECHA):\s*["\'][^"\']*["\']', '', mapping)
    
def filtrar_niveis_excessivos(estrutura: str, max_nivel: int = 3) -> str:
    """
    v2.41: Remove itens da estrutura mais profundos que max_nivel.
    Ex.: se max_nivel=3, remove 1.1.1.1.
    Portado de format_transcription_gemini.py.
    """
    if not estrutura:
        return estrutura
    linhas = estrutura.strip().split('\n')
    filtradas = []
    removidos = 0
    for linha in linhas:
        match = re.match(r'^(\d+(?:\.\d+)*)', linha.strip())
        if match:
            partes = [p for p in match.group(1).split('.') if p.isdigit()]
            if len(partes) <= max_nivel:
                filtradas.append(linha)
            else:
                removidos += 1
        else:
            filtradas.append(linha)
    if removidos:
        print(f"{Fore.CYAN}‚úÇÔ∏è  Filtrados {removidos} itens com n√≠vel > {max_nivel}{Style.RESET_ALL}")
    return '\n'.join(filtradas)


def _sample_evenly(items: list[str], limit: int) -> list[str]:
    """Seleciona itens distribu√≠dos ao longo da lista preservando in√≠cio e fim."""
    if limit <= 0 or len(items) <= limit:
        return list(items)
    if limit == 1:
        return [items[0]]

    step = (len(items) - 1) / (limit - 1)
    selected = []
    used = set()
    for i in range(limit):
        idx = int(round(i * step))
        idx = max(0, min(len(items) - 1, idx))
        if idx not in used:
            selected.append(items[idx])
            used.add(idx)

    # Se o arredondamento gerar menos itens √∫nicos, completa com faltantes na ordem.
    if len(selected) < limit:
        for item in items:
            if item not in selected:
                selected.append(item)
            if len(selected) >= limit:
                break
    return selected[:limit]


def _extract_outline_key(line: str) -> str | None:
    """
    Extrai chave num√©rica de outline (ex.: '1.2.3') de uma linha.
    Retorna None para linhas sem numera√ß√£o hier√°rquica detect√°vel.
    """
    if not line:
        return None
    m = re.match(r'^\s*(\d+(?:\.\d+)*)\.?\s+', line.strip())
    return m.group(1) if m else None


def _sample_with_parents(items: list[str], limit: int) -> list[str]:
    """
    Amostra distribu√≠da com fechamento pai-filho:
    se um item filho for selecionado, inclui seus pais quando presentes.
    """
    if limit <= 0 or len(items) <= limit:
        return list(items)

    key_to_idx: dict[str, int] = {}
    idx_to_key: dict[int, str] = {}
    for idx, line in enumerate(items):
        key = _extract_outline_key(line)
        if not key:
            continue
        # Mant√©m a primeira ocorr√™ncia para preservar ordem natural do outline.
        if key not in key_to_idx:
            key_to_idx[key] = idx
            idx_to_key[idx] = key

    def _closure(sampled_indexes: set[int]) -> set[int]:
        expanded = set(sampled_indexes)
        for idx in list(sampled_indexes):
            key = idx_to_key.get(idx)
            if not key:
                continue
            parts = key.split(".")
            for depth in range(len(parts) - 1, 0, -1):
                parent_key = ".".join(parts[:depth])
                parent_idx = key_to_idx.get(parent_key)
                if parent_idx is not None:
                    expanded.add(parent_idx)
        return expanded

    def _sample_indexes_evenly(indexes: list[int], sample_limit: int) -> list[int]:
        if sample_limit <= 0 or len(indexes) <= sample_limit:
            return list(indexes)
        if sample_limit == 1:
            return [indexes[0]]
        step = (len(indexes) - 1) / (sample_limit - 1)
        picked: list[int] = []
        used_pos = set()
        for i in range(sample_limit):
            pos = int(round(i * step))
            pos = max(0, min(len(indexes) - 1, pos))
            if pos not in used_pos:
                picked.append(indexes[pos])
                used_pos.add(pos)
        if len(picked) < sample_limit:
            for idx in indexes:
                if idx not in picked:
                    picked.append(idx)
                if len(picked) >= sample_limit:
                    break
        return picked[:sample_limit]

    # Ajusta a amostra-base at√© caber junto com ancestrais.
    base_limit = min(limit, len(items))
    selected_indexes: set[int] = set()
    all_indexes = list(range(len(items)))
    while base_limit > 0:
        base_indexes = set(_sample_indexes_evenly(all_indexes, base_limit))
        expanded = _closure(base_indexes)
        if len(expanded) <= limit:
            selected_indexes = expanded
            break
        base_limit -= 1

    if not selected_indexes:
        selected_indexes = {0}

    # Preenche vagas remanescentes distribuindo no restante sem quebrar o fechamento j√° constru√≠do.
    remaining_slots = limit - len(selected_indexes)
    if remaining_slots > 0:
        remaining = [i for i in range(len(items)) if i not in selected_indexes]
        if remaining:
            sampled_remaining = _sample_indexes_evenly(remaining, remaining_slots)
            for idx in sampled_remaining:
                selected_indexes.add(idx)
                if len(selected_indexes) >= limit:
                    break

    ordered_indexes = sorted(selected_indexes)[:limit]
    return [items[i] for i in ordered_indexes]


def simplificar_estrutura_se_necessario(
    estrutura: str,
    max_linhas: int = 120,
    max_nivel: int = 3,
) -> str:
    """
    v2.42: Se a estrutura tiver mais de max_linhas itens, preserva n√≠veis at√© max_nivel
    para evitar prompt bloat nos chunks.
    Portado de format_transcription_gemini.py.
    """
    if not estrutura:
        return estrutura
    max_nivel = max(1, min(3, int(max_nivel or 3)))
    linhas = [l for l in estrutura.strip().split('\n') if l.strip()]
    if len(linhas) <= max_linhas:
        return estrutura

    print(
        f"{Fore.CYAN}üìâ Estrutura longa ({len(linhas)} itens). "
        f"Simplificando para n√≠veis 1-{max_nivel}, m√°x {max_linhas}...{Style.RESET_ALL}"
    )
    nivel1 = []
    nivel2 = []
    nivel3 = []
    for l in linhas:
        s = l.strip()
        if re.match(r'^\d+\.\s', s):
            nivel1.append(l)
        elif re.match(r'^\d+\.\d+\.?\s', s):
            nivel2.append(l)
        elif re.match(r'^\d+\.\d+\.\d+\.?\s', s):
            nivel3.append(l)

    retained = set(nivel1 + nivel2)
    if max_nivel >= 3:
        retained.update(nivel3)

    if len(retained) < 5:
        return estrutura  # fallback

    nova = []
    vistos = set()
    for l in linhas:
        if l in vistos:
            continue
        if l in retained:
            nova.append(l)
            vistos.add(l)
    if len(nova) > max_linhas:
        # Evita vi√©s para o come√ßo e preserva coer√™ncia pai-filho.
        nova = _sample_with_parents(nova, max_linhas)

    print(
        f"{Fore.GREEN}‚úÖ Estrutura simplificada: {len(linhas)} ‚Üí {len(nova)} linhas "
        f"(n√≠veis 1-{max_nivel}).{Style.RESET_ALL}"
    )
    return '\n'.join(nova)


def dividir_sequencial(transcricao_completa, chars_por_parte=25000, estrutura_global=None):
    """
    v2.26: Divide documento em chunks SEQUENCIAIS com prefer√™ncia por √¢ncoras verbatim.
    
    Melhorias v2.26:
    - Log de cobertura de √¢ncoras (quantas foram encontradas)
    - Valida√ß√£o FECHA (verifica se chunk termina onde esperado)
    - Flag 'instituto_continua' quando um instituto √© partido
    
    Args:
        transcricao_completa: Texto bruto completo
        chars_por_parte: Tamanho alvo de cada chunk
        estrutura_global: String com a estrutura mapeada (opcional, com √¢ncoras ABRE/FECHA)
    
    Returns:
        Lista de dicts com 'inicio', 'fim', e metadados de continuidade
    """
    chunks = []
    tamanho_total = len(transcricao_completa)
    inicio = 0
    texto_lower = transcricao_completa.lower()
    
    # v2.26: Estrutura para rastrear √¢ncoras e cobertura
    ancoras_info = []  # Lista de dicts com 'titulo', 'abre_frase', 'fecha_frase', 'abre_pos', 'fecha_pos'
    ancoras_encontradas = 0
    ancoras_totais = 0
    ancoras_nao_encontradas = []
    
    # v2.25: Extrair √¢ncoras verbatim (ABRE/FECHA) da estrutura
    pontos_de_corte = []  # Lista de posi√ß√µes absolutas onde cortar
    
    if estrutura_global:
        # Regex para capturar: NUMERO. T√≠tulo | ABRE: "frase" | FECHA: "frase"
        anchor_pattern = re.compile(
            r'^\s*(\d+(?:\.\d+)*)\.\s*([^|]+)\|\s*ABRE:\s*["\']([^"\']+)["\']\s*\|\s*FECHA:\s*["\']([^"\']+)["\']',
            re.MULTILINE | re.IGNORECASE
        )
        
        for match in anchor_pattern.finditer(estrutura_global):
            numero = match.group(1)
            titulo = match.group(2).strip()
            frase_abre = match.group(3).strip().lower()
            frase_fecha = match.group(4).strip().lower()
            ancoras_totais += 1

            if len(frase_abre) < 10:
                continue  # √Çncora muito curta, pular

            # v2.33: Detectar √¢ncora "fake" (modelo usou t√≠tulo em vez de cita√ß√£o verbatim)
            similaridade = _similaridade_palavras(titulo, frase_abre)
            ancora_fake = similaridade > 0.6  # Mais de 60% de overlap = provavelmente fake

            if ancora_fake:
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è  √Çncora fake detectada (sim={similaridade:.0%}): '{frase_abre[:30]}...'")

            # Buscar a frase ABRE no texto
            pos_abre = texto_lower.find(frase_abre) if not ancora_fake else -1
            pos_fecha = None

            if pos_abre == -1 and not ancora_fake:
                # Tentar busca fuzzy com as primeiras 5 palavras
                palavras = frase_abre.split()[:5]
                frase_curta = ' '.join(palavras)
                pos_abre = texto_lower.find(frase_curta)
                if pos_abre != -1:
                    print(f"{Fore.YELLOW}   üìç √Çncora parcial: '{frase_curta}' @ {pos_abre}")

            if pos_abre == -1 and frase_abre and not ancora_fake:
                # Fallback: busca tolerante a quebras de linha (whitespace-insensitive)
                try:
                    pattern = r'\s+'.join(re.escape(w) for w in frase_abre.split())
                    m = re.search(pattern, transcricao_completa, flags=re.IGNORECASE)
                    if m:
                        pos_abre = m.start()
                        print(f"{Fore.YELLOW}   üìç √Çncora com whitespace-flex: '{frase_abre[:40]}...' @ {pos_abre}")
                except re.error:
                    pass

            # v2.33: Fallback inteligente para √¢ncoras fake ou n√£o encontradas
            if pos_abre == -1:
                pos_abre = _buscar_ancora_no_texto(texto_lower, titulo, transcricao_completa)
                if pos_abre != -1:
                    metodo = "busca por t√≠tulo" if ancora_fake else "fallback inteligente"
                    print(f"{Fore.CYAN}   üîç √Çncora via {metodo}: '{titulo[:30]}...' @ {pos_abre}")

            if pos_abre != -1:
                # Voltar at√© o in√≠cio da linha/par√°grafo
                while pos_abre > 0 and transcricao_completa[pos_abre - 1] not in '\n':
                    pos_abre -= 1
                pontos_de_corte.append(pos_abre)
                ancoras_encontradas += 1
                if not ancora_fake:
                    print(f"{Fore.GREEN}   üìç √Çncora ABRE: '{titulo[:30]}...' @ {pos_abre}")

                # v2.26: Buscar FECHA para valida√ß√£o
                if frase_fecha.lower() != 'fim':
                    pos_fecha = texto_lower.find(frase_fecha)
                    if pos_fecha != -1:
                        print(f"{Fore.CYAN}   üìç √Çncora FECHA: '{frase_fecha[:30]}...' @ {pos_fecha}")
            else:
                ancoras_nao_encontradas.append(f"{numero}. {titulo}")
                print(f"{Fore.RED}   ‚ùå √Çncora n√£o encontrada: '{titulo[:40]}...'")

            ancoras_info.append({
                'numero': numero,
                'titulo': titulo,
                'abre_frase': frase_abre,
                'fecha_frase': frase_fecha,
                'abre_pos': pos_abre if pos_abre != -1 else None,
                'fecha_pos': pos_fecha
            })
        
        # Ordenar e remover duplicatas
        pontos_de_corte = sorted(set(pontos_de_corte))
        
        # v2.26: Log de cobertura de √¢ncoras
        if ancoras_totais > 0:
            cobertura = (ancoras_encontradas / ancoras_totais) * 100
            cor = Fore.GREEN if cobertura >= 80 else (Fore.YELLOW if cobertura >= 50 else Fore.RED)
            print(f"{cor}   üìä Cobertura de √¢ncoras: {ancoras_encontradas}/{ancoras_totais} ({cobertura:.0f}%)")
            if ancoras_nao_encontradas:
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è  N√£o localizadas: {', '.join(ancoras_nao_encontradas[:5])}" + 
                      (f" (+{len(ancoras_nao_encontradas)-5} mais)" if len(ancoras_nao_encontradas) > 5 else ""))
    
    # Fallback: extrair √¢ncoras antigas (primeiras 3 palavras do t√≠tulo)
    ancoras_fallback = []
    if estrutura_global and not pontos_de_corte:
        for line in estrutura_global.split('\n'):
            line = line.strip()
            # Remover metadados de √¢ncora para extrair s√≥ o t√≠tulo
            line_clean = re.sub(r'\s*\|.*$', '', line)
            match = re.match(r'^\d+(?:\.\d+)*\.?\s+(.+)', line_clean)
            if match:
                titulo = match.group(1).strip()
                palavras = [w for w in titulo.split() if len(w) > 3][:3]
                if palavras:
                    ancoras_fallback.append(' '.join(palavras).lower())
    
    # v2.26: Construir mapa de intervalos de institutos (para detectar cortes no meio)
    intervalos_institutos = []
    for i, info in enumerate(ancoras_info):
        if info['abre_pos'] is not None:
            fim_instituto = tamanho_total  # Default: at√© o fim
            # O fim do instituto √© o in√≠cio do pr√≥ximo (ou fim do texto)
            for j in range(i + 1, len(ancoras_info)):
                if ancoras_info[j]['abre_pos'] is not None:
                    fim_instituto = ancoras_info[j]['abre_pos']
                    break
            intervalos_institutos.append({
                'titulo': info['titulo'],
                'inicio': info['abre_pos'],
                'fim': fim_instituto
            })
    
    while inicio < tamanho_total:
        fim_ideal = min(inicio + chars_por_parte, tamanho_total)
        fim = fim_ideal
        instituto_continua = False
        instituto_nome = None
        
        if fim < tamanho_total:
            bloco = transcricao_completa[inicio:fim]
            melhor_ponto = None
            
            # ESTRAT√âGIA 1: Usar pontos de corte de √¢ncoras verbatim
            if pontos_de_corte:
                # Encontrar o ponto de corte mais pr√≥ximo do fim_ideal (nos √∫ltimos 30%)
                limite_inferior = inicio + int(chars_por_parte * 0.7)
                for ponto in pontos_de_corte:
                    if limite_inferior <= ponto < fim_ideal:
                        melhor_ponto = ponto
                        print(f"{Fore.GREEN}   ‚úÇÔ∏è  Cortando em √¢ncora verbatim @ {ponto}")
                        break
            
            # ESTRAT√âGIA 2: Fallback para √¢ncoras antigas (primeiras 3 palavras)
            if melhor_ponto is None and ancoras_fallback:
                zona_busca = bloco[int(chars_por_parte * 0.7):]
                zona_offset = int(chars_por_parte * 0.7)
                
                for ancora in ancoras_fallback:
                    pos = zona_busca.lower().find(ancora)
                    if pos != -1:
                        ponto_corte = zona_offset + pos
                        while ponto_corte > 0 and bloco[ponto_corte] != '\n':
                            ponto_corte -= 1
                        if ponto_corte > chars_por_parte * 0.7:
                            melhor_ponto = inicio + ponto_corte
                            break
            
            # ESTRAT√âGIA 3: Fallback para fim de par√°grafo
            if melhor_ponto is None:
                ultimo_paragrafo = bloco.rfind('\n\n')
                if ultimo_paragrafo != -1 and ultimo_paragrafo > chars_por_parte * 0.8:
                    melhor_ponto = inicio + ultimo_paragrafo + 2
            
            if melhor_ponto:
                fim = melhor_ponto
            
            # v2.26: Verificar se estamos cortando no meio de um instituto
            for intervalo in intervalos_institutos:
                # Se o chunk come√ßa dentro de um instituto e termina antes do fim dele
                if intervalo['inicio'] <= inicio < intervalo['fim'] and fim < intervalo['fim']:
                    instituto_continua = True
                    instituto_nome = intervalo['titulo']
                    print(f"{Fore.YELLOW}   ‚ö†Ô∏è  Instituto '{instituto_nome[:30]}...' ser√° continuado no pr√≥ximo chunk")
                    break
        
        chunks.append({
            'inicio': inicio, 
            'fim': fim,
            'instituto_continua': instituto_continua,
            'instituto_nome': instituto_nome
        })
        inicio = fim
        
    return chunks

def dividir_por_blocos_markdown(
    texto: str,
    *,
    max_chars: int = 25000,
    block_prefix_pattern: Optional[str] = None,
    split_overlap_chars: int = 300,
) -> list:
    """
    Divide por blocos naturais em Markdown (v2.32).

    Detecta headings "## Bloco XX ‚Äî ..." (ou outros prefixos) e agrupa blocos inteiros at√© atingir `max_chars`.
    Se um bloco exceder `max_chars`, ele √© subdividido via `chunk_texto_seguro`.

    Retorna o mesmo formato de `dividir_sequencial`: lista de dicts {inicio, fim, ...}.
    """
    texto = texto or ""
    if not texto.strip():
        return []

    # Encontrar blocos por headings (prefixo configur√°vel)
    prefix = block_prefix_pattern or os.getenv("IUDEX_HEARING_BLOCK_PREFIX_REGEX", r"Bloco|Ato|Parte")
    try:
        block_regex = re.compile(rf'(?m)^##\s+(?:{prefix})\b', flags=re.IGNORECASE)
    except re.error:
        block_regex = re.compile(r'(?m)^##\s+Bloco\b', flags=re.IGNORECASE)
    matches = list(block_regex.finditer(texto))
    if len(matches) < 2:
        # Poucos blocos ‚Üí n√£o vale chunking por bloco
        return []

    block_ranges: list[tuple[int, int]] = []
    for idx, m in enumerate(matches):
        start = m.start()
        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(texto)
        block_ranges.append((start, end))

    chunks: list[dict] = []
    cur_start: Optional[int] = None
    cur_end: Optional[int] = None
    cur_len = 0
    cur_blocks: list[int] = []

    def _flush():
        nonlocal cur_start, cur_end, cur_len, cur_blocks
        if cur_start is None or cur_end is None:
            return
        chunks.append(
            {
                "inicio": cur_start,
                "fim": cur_end,
                "block_ids": cur_blocks[:],
                "instituto_continua": False,
                "instituto_nome": None,
            }
        )
        cur_start = None
        cur_end = None
        cur_len = 0
        cur_blocks = []

    for block_idx, (b_start, b_end) in enumerate(block_ranges):
        block_len = b_end - b_start
        if block_len > max_chars:
            # Se h√° chunk em andamento, fecha antes de subdividir bloco grande
            _flush()
            block_text = texto[b_start:b_end]
            overlap_chars = max(0, int(split_overlap_chars))
            parts = chunk_texto_seguro(block_text, max_chars=max_chars, overlap_chars=overlap_chars)
            pos = b_start
            for part in parts:
                part_len = len(part)
                end_pos = min(len(texto), pos + part_len)
                chunks.append(
                    {
                        "inicio": pos,
                        "fim": end_pos,
                        "block_ids": [block_idx],
                        "instituto_continua": False,
                        "instituto_nome": None,
                    }
                )
                pos = end_pos
            continue

        if cur_start is None:
            cur_start = b_start
            cur_end = b_end
            cur_len = block_len
            cur_blocks = [block_idx]
            continue

        if cur_len + block_len > max_chars and cur_blocks:
            _flush()
            cur_start = b_start
            cur_end = b_end
            cur_len = block_len
            cur_blocks = [block_idx]
            continue

        cur_end = b_end
        cur_len += block_len
        cur_blocks.append(block_idx)

    _flush()

    # Garantir contiguidade (sem gaps). Se houver gaps, fallback para dividir_sequencial.
    expected = 0
    for c in chunks:
        if c["inicio"] != expected:
            return []
        expected = c["fim"]
    if expected != len(texto):
        return []

    return chunks

def validar_chunks(chunks, texto_completo):
    """Valida se n√£o houve perda de texto entre chunks"""
    esperado = 0
    for c in chunks:
        if c['inicio'] != esperado:
             print(f"{Fore.RED}‚ö†Ô∏è GAP detectado! Chunk come√ßa em {c['inicio']} mas devia ser {esperado}")
        esperado = c['fim']
    
    if esperado != len(texto_completo):
        print(f"{Fore.RED}‚ö†Ô∏è Texto incompleto! Processado: {esperado}, Total: {len(texto_completo)}")
    else:
        print(f"{Fore.GREEN}‚úÖ Divis√£o de chunks validada (Bytes match).")

# Removed old detectar_secoes_duplicadas & remover_secoes_duplicadas since they are replaced by robust versions
# The new versions are placed in the helper section we just updated.

def remover_duplicacoes_literais(texto):
    """Remove par√°grafos individuais duplicados (v2.7 logic)"""
    from difflib import SequenceMatcher
    paragrafos = texto.split('\n\n')
    paragrafos_limpos = []
    dup_count = 0
    
    for i, para in enumerate(paragrafos):
        if i == 0:
            paragrafos_limpos.append(para)
            continue
        
        if len(para.strip()) < 80 or para.strip().startswith('#'):
            paragrafos_limpos.append(para)
            continue
        
        is_duplicate = False
        para_norm = ' '.join(para.split()).lower()
        
        # Check against last 3 paragraphs (v2.7 logic)
        for j in range(max(0, len(paragrafos_limpos) - 3), len(paragrafos_limpos)):
            para_ant = paragrafos_limpos[j]
            para_ant_norm = ' '.join(para_ant.split()).lower()
            
            ratio = SequenceMatcher(None, para_norm, para_ant_norm).ratio()
            
            if ratio > 0.95:
                is_duplicate = True
                dup_count += 1
                break
        
        if not is_duplicate:
            paragrafos_limpos.append(para)
    
    if dup_count > 0:
         print(f"‚ö†Ô∏è  {dup_count} par√°grafos duplicados removidos (Literal Dedup)")
    
    return '\n\n'.join(paragrafos_limpos)


def numerar_titulos(texto):
    """Adiciona numera√ß√£o sequencial aos t√≠tulos"""
    linhas = texto.split('\n')
    linhas_numeradas = []
    
    contador_h2 = 0
    contador_h3 = 0
    contador_h4 = 0
    
    titulo_pattern = re.compile(r'^(#{2,4})\s+(?:\d+(?:\.\d+)*\.?\s+)?(.+)$')
    
    for linha in linhas:
        match = titulo_pattern.match(linha)
        
        if match:
            nivel = len(match.group(1))
            texto_titulo = match.group(2).strip()
            
            # N√£o numera t√≠tulos de resumo/quadros
            if any(keyword in texto_titulo.lower() for keyword in ['resumo', 'quadro', 'esquema', 'üìã', 'üìä', 'üóÇÔ∏è']):
                linhas_numeradas.append(linha)
                continue
            
            if nivel == 2:
                contador_h2 += 1
                contador_h3 = 0
                contador_h4 = 0
                nova_linha = f"## {contador_h2}. {texto_titulo}"
            elif nivel == 3:
                contador_h3 += 1
                contador_h4 = 0
                nova_linha = f"### {contador_h2}.{contador_h3}. {texto_titulo}"
            elif nivel == 4:
                contador_h4 += 1
                nova_linha = f"#### {contador_h2}.{contador_h3}.{contador_h4}. {texto_titulo}"
            else:
                nova_linha = linha
            
            linhas_numeradas.append(nova_linha)
        else:
            linhas_numeradas.append(linha)
    
    return '\n'.join(linhas_numeradas)


# ==================== METRICS COLLECTOR (v2.10) ====================
class MetricsCollector:
    """Tracks API usage, timing, and cost estimation for optimization."""
    
    # Pre√ßos Gemini 3 Flash Preview (Dezembro 2025) - USD per 1M tokens
    PRICE_INPUT = 0.50   # $0.50 (Gemini 3 Flash Preview Input)
    PRICE_OUTPUT = 3.00  # $3.00 (Gemini 3 Flash Preview Output)
    
    # OpenAI GPT-5 Mini (Estimado)
    PRICE_OPENAI_INPUT = 0.15
    PRICE_OPENAI_OUTPUT = 0.60
    
    def __init__(self, provider="gemini"):
        self.provider = provider
        self.reset()
    
    def reset(self):
        self.api_calls = 0
        self.gemini_calls = 0
        self.openai_calls = 0
        self.total_prompt_tokens = 0
        self.total_completion_tokens = 0
        self.total_time_seconds = 0.0
        self.call_times = []
        self.cache_hits = 0
        self.adaptive_splits = 0
    
    def set_provider(self, provider: str):
        """Updates the provider for cost calculation."""
        self.provider = provider
    
    def record_call(
        self,
        provider: str,
        prompt_tokens: int,
        completion_tokens: int,
        duration: float,
        model: Optional[str] = None,
        cached_tokens_in: Optional[int] = None,
    ):
        """Records a single API call."""
        self.api_calls += 1
        if provider == "gemini":
            self.gemini_calls += 1
        elif provider == "openai":
            self.openai_calls += 1
        self.total_prompt_tokens += prompt_tokens
        self.total_completion_tokens += completion_tokens
        self.total_time_seconds += duration
        self.call_times.append(duration)
        _record_llm_usage(
            provider=provider,
            model=model,
            tokens_in=prompt_tokens,
            tokens_out=completion_tokens,
            cached_tokens_in=cached_tokens_in,
        )
    
    def record_cache_hit(self):
        self.cache_hits += 1
    
    def record_adaptive_split(self):
        self.adaptive_splits += 1
    
    def estimate_cost(self, provider="gemini") -> float:
        """Estimates total USD cost based on recorded tokens and provider pricing."""
        
        # Pricing Tables (USD per 1M tokens)
        PRICING = {
            "gemini": {
                "input": 0.075,       # Gemini 1.5 Pro
                "cached_input": 0.01875, # 25% of input
                "output": 0.30
            },
            "openai": {
                "input": 0.25,        # GPT-5 Mini
                "cached_input": 0.025, # 10x discount
                "output": 2.00
            }
        }
        
        # Fallback to gemini pricing if provider unknown
        prices = PRICING.get(provider, PRICING["gemini"])
        
        # Calculate cost
        # Note: cached_prompt_tokens tracking would be ideal, but for now we assume standard ratio or 0 if not tracked
        # Here we use total_prompt_tokens for standard input cost calculation
        
        # Separate cached vs uncached if available (future proofing)
        # For current implementation, we assume all prompts are uncached for conservative estimate
        # unless specific metric is added.
        
        cost = (
            (self.total_prompt_tokens * prices["input"] / 1_000_000) +
            (self.total_completion_tokens * prices["output"] / 1_000_000)
        )
        return cost
    
    def get_summary(self) -> str:
        """Returns a formatted summary string."""
        avg_time = (self.total_time_seconds / self.api_calls) if self.api_calls > 0 else 0
        cost = self.estimate_cost(self.provider)
        
        return f"""
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìä M√âTRICAS DE EXECU√á√ÉO
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
   üì° Total de Chamadas API: {self.api_calls}
      - Gemini: {self.gemini_calls}
      - OpenAI: {self.openai_calls}
      - Cache Hits: {self.cache_hits}
   ‚úÇÔ∏è Divis√µes Adaptativas: {self.adaptive_splits}
   üéØ Tokens Usados:
      - Prompt: {self.total_prompt_tokens:,}
      - Completion: {self.total_completion_tokens:,}
      - Total: {self.total_prompt_tokens + self.total_completion_tokens:,}
   ‚è±Ô∏è Tempo Total: {self.total_time_seconds:.1f}s (m√©dia: {avg_time:.2f}s/chamada)
   üí∞ Custo Estimado: ${cost:.4f} USD
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
"""

# Global metrics instance
metrics = MetricsCollector()

# ==================== CHECKPOINT SYSTEM ====================
def get_checkpoint_path(video_name, folder):
    return Path(folder) / f"{video_name}.checkpoint.json"

def save_checkpoint(video_name, folder, results, segments_info, current_idx):
    path = get_checkpoint_path(video_name, folder)
    data = {
        'video_name': video_name,
        'current_idx': current_idx,
        'total_segments': len(segments_info),
        'results': results,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_checkpoint(video_name, folder):
    path = get_checkpoint_path(video_name, folder)
    if path.exists():
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao carregar checkpoint: {e}")
    return None

def delete_checkpoint(video_name, folder):
    path = get_checkpoint_path(video_name, folder)
    if path.exists(): 
        path.unlink()
        print(f"üßπ Checkpoint removido: {path.name}")

def get_hil_output_path(video_name, folder, mode_suffix):
    return Path(folder) / f"{video_name}_{mode_suffix}_HIL.md"

def save_hil_output(formatted_text, video_name, folder, mode_suffix, reason=None):
    """Salva o texto formatado para revis√£o humana (HIL)."""
    path = get_hil_output_path(video_name, folder, mode_suffix)
    reason_note = f" | motivo: {reason}" if reason else ""
    header = f"<!-- HIL_CHECKPOINT{reason_note} | {time.strftime('%Y-%m-%d %H:%M:%S')} -->\n\n"
    try:
        with open(path, 'w', encoding='utf-8') as f:
            f.write(header)
            f.write(formatted_text or "")
        print(f"{Fore.YELLOW}‚è∏Ô∏è  HIL checkpoint salvo: {path.name}")
    except Exception as e:
        print(f"{Fore.YELLOW}‚ö†Ô∏è Falha ao salvar HIL checkpoint: {e}")

class VomoMLX:
    # GPT-5 Mini: 400k tokens input, 128k output
    MAX_CHUNK_SIZE = 100000  
    CHUNK_OVERLAP = 3000     # 5k overlap
    # Map structure settings (v2.31)
    MAP_MAX_SINGLE_CHARS = 350_000
    MAP_CHUNK_CHARS = 150_000
    MAP_CHUNK_OVERLAP_CHARS = 8_000
    MAP_MAX_LINES_PER_CHUNK = 60
    RAW_CONTEXT_OVERLAP_CHARS = 1200

    # ==================== PARALELIZA√á√ÉO (v2.40) ====================
    # N√∫mero de chunks processados em paralelo. Default=1 (sequencial).
    # Valores > 1 aceleram mas podem reduzir consist√™ncia de estilo entre chunks.
    # Recomendado: 2-3 para balan√ßo velocidade/qualidade.
    PARALLEL_CHUNKS = int(os.getenv("IUDEX_PARALLEL_CHUNKS", "1"))

    # ==================== CHUNKING DE √ÅUDIO LONGO (v2.32/v2.34) ====================
    # √Åudios maiores que este limite ser√£o divididos em partes para evitar
    # problemas de mem√≥ria/processamento do MLX-Whisper com arquivos muito longos
    AUDIO_MAX_DURATION_SECONDS = 2 * 60 * 60  # 2 horas
    AUDIO_CHUNK_OVERLAP_SECONDS = 45  # v2.34: 45s de overlap (era 30s) - mais seguro para frases longas
    # NOTA: Diariza√ß√£o em chunking pode resetar speaker IDs entre chunks.
    # Para diariza√ß√£o consistente em √°udios longos, use AssemblyAI ou diarize o √°udio inteiro separadamente.

    # ==================== IDIOMAS SUPORTADOS ====================
    # Expandido para incluir os principais idiomas suportados por Whisper e AssemblyAI
    SUPPORTED_LANGUAGES = {
        "auto": None,   # Whisper detecta automaticamente
        "pt": "pt",     # Portugu√™s
        "en": "en",     # Ingl√™s
        "es": "es",     # Espanhol
        "fr": "fr",     # Franc√™s
        "de": "de",     # Alem√£o
        "it": "it",     # Italiano
        "ja": "ja",     # Japon√™s
        "ko": "ko",     # Coreano
        "zh": "zh",     # Chin√™s
        "ru": "ru",     # Russo
        "ar": "ar",     # √Årabe
        "hi": "hi",     # Hindi
        "nl": "nl",     # Holand√™s
        "pl": "pl",     # Polon√™s
        "tr": "tr",     # Turco
        "sv": "sv",     # Sueco
        "da": "da",     # Dinamarqu√™s
        "fi": "fi",     # Finland√™s
        "no": "no",     # Noruegu√™s
        "uk": "uk",     # Ucraniano
    }

    # ==================== INITIAL PROMPTS POR MODO (v2.29) ====================
    # Contexto do Whisper ajustado ao tipo de √°udio para melhor reconhecimento de termos
    # Chave externa: (modo, idioma). Fallback: s√≥ modo (assume pt).
    INITIAL_PROMPTS = {
        "APOSTILA": "Esta √© uma transcri√ß√£o de aula jur√≠dica em portugu√™s brasileiro sobre direito administrativo, constitucional, civil, penal e processual.",
        "FIDELIDADE": "Esta √© uma transcri√ß√£o de aula jur√≠dica em portugu√™s brasileiro sobre direito administrativo, constitucional, civil, penal e processual.",
        "AUDIENCIA": "Esta √© uma transcri√ß√£o de audi√™ncia judicial em portugu√™s brasileiro. Termos forenses, procedimentos processuais e linguagem jur√≠dica formal.",
        "REUNIAO": "Esta √© uma transcri√ß√£o de reuni√£o profissional em portugu√™s brasileiro.",
        "DEPOIMENTO": "Esta √© uma transcri√ß√£o de depoimento judicial em portugu√™s brasileiro. Termos forenses e linguagem jur√≠dica formal.",
    }

    INITIAL_PROMPTS_I18N: dict[tuple[str, str], str] = {
        # Ingl√™s
        ("APOSTILA", "en"): "This is a transcription of a legal lecture in English about administrative, constitutional, civil, criminal and procedural law.",
        ("FIDELIDADE", "en"): "This is a transcription of a legal lecture in English about administrative, constitutional, civil, criminal and procedural law.",
        ("AUDIENCIA", "en"): "This is a transcription of a court hearing in English. Forensic terms, procedural law and formal legal language.",
        ("REUNIAO", "en"): "This is a transcription of a professional meeting in English.",
        ("DEPOIMENTO", "en"): "This is a transcription of a legal deposition in English. Forensic terms and formal legal language.",
        # Espanhol
        ("APOSTILA", "es"): "Esta es una transcripci√≥n de una clase jur√≠dica en espa√±ol sobre derecho administrativo, constitucional, civil, penal y procesal.",
        ("FIDELIDADE", "es"): "Esta es una transcripci√≥n de una clase jur√≠dica en espa√±ol sobre derecho administrativo, constitucional, civil, penal y procesal.",
        ("AUDIENCIA", "es"): "Esta es una transcripci√≥n de una audiencia judicial en espa√±ol. T√©rminos forenses, procedimientos procesales y lenguaje jur√≠dico formal.",
        ("REUNIAO", "es"): "Esta es una transcripci√≥n de una reuni√≥n profesional en espa√±ol.",
        ("DEPOIMENTO", "es"): "Esta es una transcripci√≥n de una declaraci√≥n judicial en espa√±ol. T√©rminos forenses y lenguaje jur√≠dico formal.",
        # Franc√™s
        ("APOSTILA", "fr"): "Ceci est une transcription d'un cours juridique en fran√ßais sur le droit administratif, constitutionnel, civil, p√©nal et proc√©dural.",
        ("FIDELIDADE", "fr"): "Ceci est une transcription d'un cours juridique en fran√ßais sur le droit administratif, constitutionnel, civil, p√©nal et proc√©dural.",
        ("AUDIENCIA", "fr"): "Ceci est une transcription d'une audience judiciaire en fran√ßais. Termes forensiques, proc√©dures judiciaires et langage juridique formel.",
        ("REUNIAO", "fr"): "Ceci est une transcription d'une r√©union professionnelle en fran√ßais.",
        ("DEPOIMENTO", "fr"): "Ceci est une transcription d'une d√©position judiciaire en fran√ßais. Termes forensiques et langage juridique formel.",
        # Alem√£o
        ("APOSTILA", "de"): "Dies ist eine Transkription einer juristischen Vorlesung auf Deutsch √ºber Verwaltungs-, Verfassungs-, Zivil-, Straf- und Verfahrensrecht.",
        ("FIDELIDADE", "de"): "Dies ist eine Transkription einer juristischen Vorlesung auf Deutsch √ºber Verwaltungs-, Verfassungs-, Zivil-, Straf- und Verfahrensrecht.",
        ("AUDIENCIA", "de"): "Dies ist eine Transkription einer Gerichtsverhandlung auf Deutsch. Forensische Begriffe, Verfahrensrecht und formale juristische Sprache.",
        ("REUNIAO", "de"): "Dies ist eine Transkription eines professionellen Meetings auf Deutsch.",
        ("DEPOIMENTO", "de"): "Dies ist eine Transkription einer gerichtlichen Aussage auf Deutsch. Forensische Begriffe und formale juristische Sprache.",
    }

    # ==================== MODULAR PROMPT COMPONENTS (v2.22) ====================
    # These components are composed by _build_system_prompt to allow partial customization.
    
    # --- APOSTILA MODE ---
    PROMPT_HEAD_APOSTILA = """# DIRETRIZES DE REDA√á√ÉO: MANUAL JUR√çDICO DID√ÅTICO (MODO APOSTILA)

## PAPEL
VOC√ä √â UM EXCELENT√çSSIMO REDATOR JUR√çDICO E DID√ÅTICO.
- **Tom:** doutrin√°rio, impessoal, estilo manual de Direito.
- **Pessoa:** 3¬™ pessoa ou constru√ß√µes impessoais ("O professor explica...", "A doutrina define...").
- **Estilo:** prosa densa, por√©m com par√°grafos curtos e did√°ticos.
- **Objetivo:** transformar a aula em texto de apostila/manual, sem alterar conte√∫do nem inventar informa√ß√µes.

## üö´ O QUE N√ÉO FAZER
1. **N√ÉO RESUMA**. O tamanho do texto de sa√≠da deve ser pr√≥ximo ao de entrada.
2. **N√ÉO OMITA** informa√ß√µes, exemplos, casos concretos ou explica√ß√µes.
3. **N√ÉO ALTERE** o significado ou a sequ√™ncia das ideias.
4. **N√ÉO CRIE PAR√ÅGRAFOS LONGOS**. M√°ximo 3-6 linhas visuais por par√°grafo.

## ‚ùå PRESERVE OBRIGATORIAMENTE
- **IDENTIFICA√á√ÉO DE FALANTES**: Se houver SPEAKER A/B/C ou similar, identifique o professor pelo contexto (quando ele se apresentar: "Eu sou o professor Jo√£o", "Meu nome √© Maria"). Substitua "SPEAKER X" pelo nome identificado. Se n√£o identificar, use "Professor" ou "Palestrante".
- **N√öMEROS EXATOS**: Artigos, Leis, S√∫mulas, Julgados, Temas de Repercuss√£o Geral, Recursos Repetitivos. **NUNCA OMITA N√öMEROS DE TEMAS OU S√öMULAS**.
- **JURISPRUD√äNCIA**: Se o texto citar "Tema 424", "RE 123", "ADI 555", **MANTENHA O N√öMERO**. N√£o generalize para "jurisprud√™ncia do STJ".
- **TODO o conte√∫do t√©cnico**: exemplos, explica√ß√µes, analogias, racioc√≠nios.
- **Refer√™ncias**: leis, artigos, jurisprud√™ncia (STF/STJ), autores, casos citados.
- **√änfases intencionais** e **Observa√ß√µes pedag√≥gicas**.

## üéØ PRESERVA√á√ÉO ESPECIAL: DICAS DE PROVA E EXAMINADORES (CR√çTICO)
Aulas presenciais frequentemente cont√™m informa√ß√µes valiosas sobre:
1. **Refer√™ncias a Examinadores**: Nomes de examinadores de concursos, suas prefer√™ncias, posicionamentos ou temas favoritos. **PRESERVE INTEGRALMENTE**.
2. **Dicas de Prova**: Orienta√ß√µes sobre o que costuma cair em provas, pegadinhas comuns, temas recorrentes.
3. **Estrat√©gias de Estudo**: Sugest√µes do professor sobre prioriza√ß√£o, macetes, formas de memoriza√ß√£o.
4. **Casos Pr√°ticos e Hist√≥rias Reais**: Exemplos de situa√ß√µes reais, casos julgados, hist√≥rias ilustrativas. **NUNCA RESUMA**.

> ‚ö†Ô∏è **ESSAS INFORMA√á√ïES S√ÉO O DIFERENCIAL DE UMA AULA AO VIVO.** Sua omiss√£o representa perda irrepar√°vel de valor did√°tico."""

    PROMPT_STYLE_APOSTILA = """## ‚úÖ DIRETRIZES DE ESTILO E FORMATA√á√ÉO VISUAL
1. **Corre√ß√£o Gramatical**: Ajuste a linguagem coloquial para o padr√£o culto.
2. **Limpeza**: Remova g√≠rias, vocativos e cacoetes ("n√©", "tipo assim", "ent√£o", "meu irm√£o", "cara", "mano", "galera") e v√≠cios de oralidade. Se houver parentesco factual (ex.: "Rodolfo (irm√£o do professor)"), mantenha a informa√ß√£o de forma formal.
3. **Coes√£o**: Use conectivos e pontua√ß√£o adequada para tornar o texto fluido.
4. **Legibilidade Visual** (OBRIGAT√ìRIO):
   - **PAR√ÅGRAFOS CURTOS**: m√°ximo **3-5 linhas visuais** por par√°grafo. **QUEBRE SEMPRE.**
   - **RECUOS COM MARCADORES**: Use `>` para cita√ß√µes, destaques ou observa√ß√µes importantes.
   - **NEGRITO MODERADO**: Destaque conceitos-chave com **negrito**, mas sem exagero.
   - **IT√ÅLICO**: Use para termos em latim, express√µes estrangeiras ou √™nfase leve.
5. **Formata√ß√£o Did√°tica** (use generosamente para legibilidade):
   - **Bullet points** (`-` ou `*`) para enumerar elementos, requisitos ou caracter√≠sticas.
   - **Listas numeradas** (`1.`, `2.`) para etapas, correntes doutrin√°rias ou exemplos ordenados.
   - **Marcadores relacionais** como `‚Üí` para consequ√™ncias l√≥gicas.
   - **Subse√ß√µes** (###, ####) para organizar subt√≥picos dentro de um mesmo tema.

## üé® FORMATA√á√ÉO VISUAL AVAN√áADA
Para garantir legibilidade superior:
1. **Ap√≥s cada conceito importante**, quebre o par√°grafo e inicie outro.
2. **Use listas** sempre que houver enumera√ß√£o de mais de 2 itens.
3. **Use cita√ß√µes recuadas** (`>`) para destacar teses jur√≠dicas, pontos pol√™micos, observa√ß√µes pr√°ticas e dicas de prova.
4. **Separe visualmente** diferentes aspectos de um mesmo tema com subse√ß√µes.
5. **Quest√µes e Exerc√≠cios**: Se o professor ditar uma quest√£o, exerc√≠cio ou caso hipot√©tico, **isole-o** em um bloco de cita√ß√£o:
   > **Quest√£o:** O prazo para agravo de peti√ß√£o √© de...
   - Separe claramente o enunciado da quest√£o da explica√ß√£o/gabarito subsequente.
6. **Destaques com Emojis** (use com modera√ß√£o para facilitar escaneamento visual):
   - üí° **Dica de Prova** ou **Observa√ß√£o Pedag√≥gica**: Quando o professor der uma dica espec√≠fica para provas ou concursos.
   - ‚ö†Ô∏è **Aten√ß√£o** ou **Cuidado**: Para alertas, pegadinhas ou pontos pol√™micos.
   - üìå **Ponto Importante**: Para conceitos-chave que merecem destaque especial.
   - Exemplo: `> üí° **Dica de Prova:** Esse tema caiu 3 vezes na PGM-Rio.`

## üíé PILAR 1: ESTILO (VOZ ATIVA E DIRETA)
> üö´ **PROIBIDO VOZ PASSIVA EXCESSIVA:** "Anunciou-se", "Informou-se".
> ‚úÖ **PREFIRA VOZ ATIVA:** "O professor explica...", "A doutrina define...", "O Art. 37 estabelece..."."""

    PROMPT_STRUCTURE_APOSTILA = """## üìù ESTRUTURA HIER√ÅRQUICA (CR√çTICO)

### REGRA DE OURO: T√ìPICOS-M√ÉE COM SUBT√ìPICOS
Organize o conte√∫do em **hierarquia pai‚Üífilho**. Se o professor aborda m√∫ltiplos aspectos de um mesmo tema, eles devem ser **subt√≥picos** (###) de um **t√≥pico-m√£e** (##), NUNCA t√≥picos ## separados.

### N√çVEIS DE HIERARQUIA (M√ÅXIMO 3):
| N√≠vel | Markdown | Uso | Exemplo |
|-------|----------|-----|---------|
| **Tema principal** | `##` | Mudan√ßa real de mat√©ria/assunto | `## 2. Execu√ß√£o Fiscal` |
| **Subtema** | `###` | Aspecto, instituto ou marco legal dentro do tema | `### 2.1. Procedimento da LEF (Lei 6.830/80)` |
| **Detalhamento** | `####` | Detalhe espec√≠fico, exemplo extenso ou ponto controverso | `#### 2.1.1. Cita√ß√£o por Hora Certa` |

### EXEMPLO DE HIERARQUIA CORRETA:
```
## 2. Execu√ß√£o Fiscal
### 2.1. Procedimento da LEF (Lei 6.830/80)
### 2.2. S√∫mula 314 do STJ ‚Äî Cita√ß√£o por Hora Certa
### 2.3. Tema 444 do STJ ‚Äî Redirecionamento ao S√≥cio
#### 2.3.1. Requisitos e Prazo
### 2.4. Exce√ß√£o de Pr√©-Executividade
## 3. Embargos √† Execu√ß√£o
### 3.1. Conceito e Natureza Jur√≠dica
### 3.2. Hip√≥teses de Cabimento
```

### ‚ùå ERRADO (tudo como ## sem hierarquia):
```
## 2. Execu√ß√£o Fiscal
## 3. Procedimento da LEF          ‚Üê ERRADO! Deveria ser ### 2.1
## 4. S√∫mula 314 do STJ            ‚Üê ERRADO! Deveria ser ### 2.2
## 5. Tema 444 do STJ              ‚Üê ERRADO! Deveria ser ### 2.3
## 6. Exce√ß√£o de Pr√©-Executividade  ‚Üê ERRADO! Deveria ser ### 2.4
```

### REGRAS ADICIONAIS:
- Mantenha a **sequ√™ncia cronol√≥gica** exata das falas.
- **N√ÉO crie subt√≥picos para frases soltas** ‚Äî use t√≠tulos APENAS para mudan√ßas reais de assunto.
- Se uma frase parece t√≠tulo mas n√£o inicia se√ß√£o, use **negrito** no texto, n√£o crie heading.
- **Marcos Legais** como subt√≥picos: S√∫mulas, Teses de Repercuss√£o Geral e Artigos de Lei explicados em profundidade devem virar ### subt√≥picos (ex: `### 2.3. S√∫mula 314 do STJ`).
- **Anti-fragmenta√ß√£o**: Se o professor trata 4+ aspectos de um tema, TODOS devem ser ### sob um ## tema-m√£e.
- Nunca use # (H1) para subt√≥picos ‚Äî apenas para o t√≠tulo principal do documento."""

    PROMPT_TABLE_APOSTILA = """## üìä QUADRO-S√çNTESE (OBRIGAT√ìRIO)
Ao final de CADA t√≥pico principal (## ou ###), fa√ßa um fechamento did√°tico com UM quadro-s√≠ntese.
SEMPRE que houver diferencia√ß√£o de conceitos, prazos, procedimentos, requisitos ou regras, o quadro √© OBRIGAT√ìRIO.

1) Adicione um subt√≠tulo de fechamento **adaptado ao caso concreto**:
- Comece sempre com `#### üìã` (obrigat√≥rio para organiza√ß√£o interna).
- Depois, use um r√≥tulo contextual espec√≠fico do tema (evite repetir sempre "Quadro-s√≠ntese").
- **Prefer√™ncia:** use o t√≠tulo original do t√≥pico como base e apenas complemente/especialize quando necess√°rio.
- Exemplo: `#### üìã Matriz comparativa ‚Äî Compet√™ncia tribut√°ria municipal`
- Exemplo: `#### üìã Requisitos essenciais ‚Äî Improbidade administrativa`

2) Em seguida, gere UMA tabela Markdown (sem placeholders):

| Item (conceito/tema) | Regra/defini√ß√£o (1 frase) | Elementos / requisitos / condi√ß√µes | Base legal / jurisprud√™ncia citada | Pegadinha / exemplo / como cai |
| :--- | :--- | :--- | :--- | :--- |

**REGRAS CR√çTICAS (n√£o negocie):**
1. **Sem placeholders:** PROIBIDO usar `"..."`, `"Art. X"`, `"Lei Y"`. Se algo n√£o aparecer no trecho, use `"‚Äî"`.
2. **Completude:** 1 linha por item mencionado no bloco (conte mentalmente e confira antes de finalizar).
3. **Concis√£o:** m√°ximo ~35‚Äì45 palavras por c√©lula; frases curtas e diretas.
4. **Compatibilidade:** PROIBIDO usar o caractere `|` dentro de c√©lulas (isso quebra a tabela). Evite quebras de linha dentro das c√©lulas.
5. **Sem c√≥digo:** PROIBIDO blocos de c√≥digo em c√©lulas.
6. **Posicionamento:** o quadro vem **APENAS AO FINAL** do bloco conclu√≠do (fechamento l√≥gico da se√ß√£o).
7. **Lastro obrigat√≥rio no texto:** cada linha da tabela deve corresponder a conte√∫do **j√° exposto antes** no texto explicativo do mesmo t√≥pico/bloco. **PROIBIDO antecipar** conceito, exce√ß√£o, fundamento legal ou dica ainda n√£o explicados.

## ‚ö†Ô∏è ORDEM OBRIGAT√ìRIA: CONTE√öDO PRIMEIRO, TABELA DEPOIS
**NUNCA** gere a tabela antes de terminar TODO o conte√∫do explicativo do t√≥pico.
A sequ√™ncia correta √© SEMPRE:
1. TODO o texto explicativo do t√≥pico (par√°grafos, exemplos, observa√ß√µes)
2. DEPOIS (e somente depois) o üìã Quadro-s√≠ntese
3. DEPOIS (se aplic√°vel) a üéØ Tabela de pegadinhas
4. DEPOIS o pr√≥ximo t√≥pico (## ou ###)

**ERRADO** (tabela no meio do conte√∫do):
```
## T√≥pico X
Explica√ß√£o inicial...
üìã Quadro-s√≠ntese    ‚Üê ERRADO!
| ... |
Mais explica√ß√£o...   ‚Üê Deveria estar ANTES da tabela!
```

**CORRETO**:
```
## T√≥pico X
Explica√ß√£o inicial...
Mais explica√ß√£o...   ‚Üê TODO conte√∫do primeiro
üìã Quadro-s√≠ntese    ‚Üê Tabela s√≥ no final
| ... |
```"""

    PROMPT_TABLE_APOSTILA += """

## üéØ TABELA 2 (QUANDO APLIC√ÅVEL): COMO A BANCA COBRA / PEGADINHAS
Se (e somente se) o bloco contiver **dicas de prova**, men√ß√µes a **banca**, **pegadinhas**, ‚Äúisso cai‚Äù, ‚Äúcuidado‚Äù, ‚Äútema recorrente‚Äù ou exemplos de como a quest√£o aparece:

1) Adicione um subt√≠tulo **adaptado ao caso concreto**:
- Comece sempre com `#### üéØ` (obrigat√≥rio para organiza√ß√£o interna).
- Depois, use um r√≥tulo contextual de prova/armadilha para o tema.
- **Prefer√™ncia:** use o t√≠tulo original do t√≥pico/bloco como base e apenas complemente para destacar cobran√ßa, risco ou pegadinha.
- Exemplo: `#### üéØ Armadilhas de prova ‚Äî Controle de constitucionalidade`
- Exemplo: `#### üéØ Como a banca explora o tema ‚Äî Imunidades tribut√°rias`

2) Gere UMA tabela Markdown:
| Como a banca cobra | Resposta correta (curta) | Erro comum / pegadinha |
| :--- | :--- | :--- |

**REGRAS:**
- Sem placeholders (`...`, `Art. X`, `Lei Y`) ‚Üí use `‚Äî` quando n√£o houver dado no trecho.
- 1 linha por pegadinha/dica/forma de cobran√ßa mencionada.
- Respostas objetivas (1‚Äì2 frases curtas por c√©lula).
- PROIBIDO usar `|` dentro de c√©lulas e evitar quebras de linha dentro das c√©lulas.
- **Somente com base no j√° exposto:** n√£o inclua na tabela de pegadinhas conte√∫do que n√£o tenha sido explicado antes no mesmo bloco.
- Se n√£o houver material de prova no bloco, **N√ÉO crie** esta Tabela 2."""

    # --- FIDELIDADE MODE ---
    PROMPT_HEAD_FIDELIDADE = """# DIRETRIZES DE FORMATA√á√ÉO E REVIS√ÉO (MODO FIDELIDADE)

## PAPEL
VOC√ä √â UM EXCELENT√çSSIMO REDATOR T√âCNICO E DID√ÅTICO.
- **Tom:** did√°tico, como o professor explicando em aula.
- **Pessoa:** MANTENHA a pessoa original da transcri√ß√£o (1¬™ pessoa se for assim na fala).
- **Estilo:** texto corrido, com par√°grafos curtos, sem "inventar" doutrina nova.
- **Objetivo:** reproduzir a aula em forma escrita, clara e organizada, mas ainda com a "voz" do professor.

# OBJETIVO
- Transformar a transcri√ß√£o em um texto claro, leg√≠vel e coeso, em Portugu√™s Padr√£o, MANTENDO A FIDELIDADE TOTAL ao conte√∫do original.
- **Tamanho:** a sa√≠da deve ficar **entre 95% e 115%** do tamanho do trecho de entrada (salvo remo√ß√£o de muletas e log√≠stica).

## üö´ O QUE N√ÉO FAZER
1. **N√ÉO RESUMA**. O tamanho do texto de sa√≠da deve ser pr√≥ximo ao de entrada.
2. **N√ÉO OMITA** informa√ß√µes, exemplos, casos concretos ou explica√ß√µes.
3. **N√ÉO ALTERE** o significado ou a sequ√™ncia das ideias e das falas do professor.
4. **N√ÉO CRIE MUITOS BULLET POINTS**. PREFIRA UM FORMATO DE MANUAL DID√ÅTICO, n√£o checklist.
5. **N√ÉO USE NEGRITOS EM EXCESSO**. Use apenas para conceitos-chave realmente importantes.

## ‚ùå PRESERVE OBRIGATORIAMENTE
- **N√öMEROS EXATOS**: Artigos, Leis, S√∫mulas, Julgados (REsp/Informativos). **NUNCA OMITA N√öMEROS DE LEIS OU S√öMULAS**.
- **TODO o conte√∫do t√©cnico**: exemplos, explica√ß√µes, analogias, racioc√≠nios.
- **Refer√™ncias**: leis, artigos, jurisprud√™ncia, autores, casos citados.
- **√änfases intencionais**: "isso √© MUITO importante" (mantenha o destaque).
- **Observa√ß√µes pedag√≥gicas**: "cuidado com isso!", "ponto pol√™mico".
- **Encerramento real da aula**: se houver despedida/aviso final/hor√°rio no fim do trecho, mantenha (pode organizar como uma se√ß√£o curta "Encerramento").

## üéØ PRESERVA√á√ÉO ESPECIAL: DICAS DE PROVA E EXAMINADORES (CR√çTICO)
Aulas presenciais frequentemente cont√™m informa√ß√µes valiosas sobre:
1. **Refer√™ncias a Examinadores**: Nomes de examinadores de concursos, suas prefer√™ncias, posicionamentos ou temas favoritos. **PRESERVE INTEGRALMENTE**.
2. **Dicas de Prova**: Orienta√ß√µes sobre o que costuma cair em provas, pegadinhas comuns, temas recorrentes.
3. **Estrat√©gias de Estudo**: Sugest√µes do professor sobre prioriza√ß√£o, macetes, formas de memoriza√ß√£o.
4. **Casos Pr√°ticos e Hist√≥rias Reais**: Exemplos de situa√ß√µes reais, casos julgados, hist√≥rias ilustrativas. **NUNCA RESUMA**.

> ‚ö†Ô∏è **ESSAS INFORMA√á√ïES S√ÉO O DIFERENCIAL DE UMA AULA AO VIVO.** Sua omiss√£o representa perda irrepar√°vel de valor did√°tico."""

    PROMPT_STYLE_FIDELIDADE = """## ‚úÖ DIRETRIZES DE ESTILO
1. **Corre√ß√£o Gramatical**: Corrija erros gramaticais, reg√™ncias, ortogr√°ficos e de pontua√ß√£o.
2. **Limpeza Profunda:**
   - **REMOVA** marcadores de oralidade: "n√©", "t√°?", "entende?", "veja bem", "tipo assim".
   - **REMOVA** g√≠rias e vocativos: "meu irm√£o", "cara", "mano", "galera", "minha gente" (n√£o agregam conte√∫do).
     - Se a express√£o for PARENTESCO factual ("meu irm√£o" = irm√£o do professor), reescreva de forma formal (ex.: "Rodolfo (irm√£o do professor)").
   - **REMOVA** intera√ß√µes diretas com a turma: "Isso mesmo", "A colega perguntou", "J√° est√£o me vendo?", "Est√£o ouvindo?".
   - **N√ÉO REMOVA** o encerramento do professor (ex.: agradecimentos, aviso de hor√°rio, "at√© a pr√≥xima", "boa prova") quando estiver no fim do trecho: preserve como um par√°grafo final ou uma se√ß√£o curta "Encerramento".
   - **REMOVA** redund√¢ncias: "subir para cima", "cria√ß√£o nova".
   - **TRANSFORME** perguntas ret√≥ricas em afirma√ß√µes quando poss√≠vel.
3. **Coes√£o**: Utilize conectivos para tornar o texto mais fluido. Aplique pontua√ß√£o adequada.
4. **Legibilidade**:
   - **USE TEXTO CORRIDO NA MEDIDA DO POSS√çVEL.**
   - **PAR√ÅGRAFOS CURTOS**: m√°ximo **3-5 linhas visuais** por par√°grafo.
   - **QUEBRE** blocos de texto maci√ßos em par√°grafos menores.
   - Seja did√°tico sem perder detalhes e conte√∫do.
5. **Linguagem**: Ajuste a linguagem coloquial para portugu√™s padr√£o, mantendo o significado original.
6. **Cita√ß√µes**: Use *it√°lico* para cita√ß√µes curtas e recuo em it√°lico para cita√ß√µes longas.
7. **Negrito**: Use **negrito** para destacar conceitos-chave (sem exagero).
8. **Formata√ß√£o Did√°tica** (use com modera√ß√£o, sem excesso):
   - **Bullet points** para enumerar elementos, requisitos ou caracter√≠sticas.
   - **Listas numeradas** para etapas, correntes ou exemplos.
   - **Marcadores relacionais** como "‚Üí" para consequ√™ncias l√≥gicas.
9. **Quest√µes e Exerc√≠cios**:
   - Se o professor ditar uma quest√£o, exerc√≠cio ou caso hipot√©tico, **ISOLE-O** em um bloco de cita√ß√£o:
   > **Quest√£o:** O prazo para agravo de peti√ß√£o √© de...
   - Separe claramente o enunciado da quest√£o da explica√ß√£o/gabarito subsequente.
10. **Destaques com Emojis** (use com modera√ß√£o para facilitar escaneamento visual):
   - üí° **Dica de Prova** ou **Observa√ß√£o Pedag√≥gica**
   - ‚ö†Ô∏è **Aten√ß√£o** ou **Cuidado** (pegadinhas, pontos pol√™micos)
   - üìå **Ponto Importante** (conceitos-chave)
   - Exemplo: `> üí° **Dica de Prova:** Esse tema caiu 3 vezes na PGM-Rio.`"""

    PROMPT_STRUCTURE_FIDELIDADE = """## üìù ESTRUTURA E T√çTULOS
- Mantenha a sequ√™ncia exata das falas.
- Use T√≠tulos Markdown (##, ###) para organizar os t√≥picos, se identific√°veis.
- **N√ÉO crie subt√≥picos para frases soltas.**
- Use t√≠tulos **APENAS** para mudan√ßas reais de assunto.
- Se uma frase parece um t√≠tulo mas n√£o inicia uma nova se√ß√£o, mantenha como texto normal e use **negrito** se necess√°rio."""

    PROMPT_TABLE_FIDELIDADE = """## üìä QUADRO-S√çNTESE (CAPTURA COMPLETA)
Ao final de cada **bloco tem√°tico relevante**, produza um quadro-s√≠ntese did√°tico.

1) Adicione um subt√≠tulo de fechamento **adaptado ao caso concreto**:
- Comece sempre com `#### üìã` (obrigat√≥rio para organiza√ß√£o interna).
- Depois, use um r√≥tulo contextual espec√≠fico do tema (evite repetir sempre "Quadro-s√≠ntese").
- **Prefer√™ncia:** use o t√≠tulo original do t√≥pico como base e apenas complemente/especialize quando necess√°rio.
- Exemplo: `#### üìã Matriz comparativa ‚Äî Compet√™ncia tribut√°ria municipal`
- Exemplo: `#### üìã Requisitos essenciais ‚Äî Improbidade administrativa`

2) Em seguida, gere UMA tabela Markdown (sem placeholders):

| Item (conceito/tema) | Defini√ß√£o/regra (1 frase) | Detalhes (requisitos, exce√ß√µes, prazos) | Base legal / jurisprud√™ncia citada | Dica de prova / ponto pol√™mico |
| :--- | :--- | :--- | :--- | :--- |

**O QUE DEVE SER CAPTURADO NA TABELA (OBRIGAT√ìRIO):**
1. **CONCEITOS/INSTITUTOS**: Todo termo t√©cnico definido pelo professor.
2. **FUNDAMENTOS LEGAIS**: Artigos, Leis, Decretos, S√∫mulas, Enunciados citados.
3. **JURISPRUD√äNCIA**: Julgados do STF, STJ, TCU, tribunais citados (ex: REsp, Ac√≥rd√£o).
4. **PONTOS POL√äMICOS**: Quest√µes controvertidas destacadas pelo professor.
5. **DICAS DE PROVA**: Alertas como "isso cai muito", "cuidado com isso", "a banca gosta de cobrar".
6. **DIVERG√äNCIAS**: Posi√ß√µes doutrin√°rias conflitantes (ex: "a doutrina majorit√°ria entende X, mas h√° quem defenda Y").
7. **OBSERVA√á√ïES PEDAG√ìGICAS**: Destaques do professor sobre import√¢ncia, frequ√™ncia em provas, etc.

**REGRAS CR√çTICAS (n√£o negocie):**
1. **Sem placeholders:** PROIBIDO usar `"..."`, `"Art. X"`, `"Lei Y"`. Se algo n√£o aparecer no trecho, use `"‚Äî"`.
2. **Completude:** Se o professor mencionou 5 itens no bloco, a tabela DEVE ter 5 linhas (ou mais).
3. **Concis√£o:** m√°ximo ~35‚Äì45 palavras por c√©lula.
4. **Compatibilidade:** PROIBIDO usar o caractere `|` dentro de c√©lulas. Evite quebras de linha dentro das c√©lulas.
5. **Sem c√≥digo:** PROIBIDO blocos de c√≥digo em c√©lulas.
6. **Posicionamento:** o quadro vem **APENAS AO FINAL** do bloco conclu√≠do, **NUNCA** no meio de explica√ß√£o.
7. **Lastro obrigat√≥rio no texto:** a tabela deve refletir somente itens **j√° tratados anteriormente** no texto do mesmo bloco tem√°tico. **N√ÉO introduza** informa√ß√£o nova na tabela."""

    PROMPT_TABLE_FIDELIDADE += """

## üéØ TABELA 2 (QUANDO APLIC√ÅVEL): COMO A BANCA COBRA / PEGADINHAS
Se (e somente se) o bloco contiver **dicas de prova**, men√ß√µes a **banca**, **pegadinhas**, ‚Äúisso cai‚Äù, ‚Äúcuidado‚Äù ou exemplos de como a quest√£o aparece:

1) Adicione um subt√≠tulo **adaptado ao caso concreto**:
- Comece sempre com `#### üéØ` (obrigat√≥rio para organiza√ß√£o interna).
- Depois, use um r√≥tulo contextual de prova/armadilha para o tema.
- **Prefer√™ncia:** use o t√≠tulo original do t√≥pico/bloco como base e apenas complemente para destacar cobran√ßa, risco ou pegadinha.
- Exemplo: `#### üéØ Armadilhas de prova ‚Äî Controle de constitucionalidade`
- Exemplo: `#### üéØ Como a banca explora o tema ‚Äî Imunidades tribut√°rias`

2) Gere UMA tabela Markdown:
| Como a banca cobra | Resposta correta (curta) | Erro comum / pegadinha |
| :--- | :--- | :--- |

**REGRAS:**
- Sem placeholders (`...`, `Art. X`, `Lei Y`) ‚Üí use `‚Äî` quando n√£o houver dado no trecho.
- 1 linha por pegadinha/dica/forma de cobran√ßa mencionada.
- Respostas objetivas (1‚Äì2 frases curtas por c√©lula).
- PROIBIDO usar `|` dentro de c√©lulas e evitar quebras de linha dentro das c√©lulas.
- **Somente com base no j√° exposto:** n√£o inclua na tabela de pegadinhas conte√∫do que n√£o tenha sido explicado antes no mesmo bloco.
- Se n√£o houver material de prova no bloco, **N√ÉO crie** esta Tabela 2."""

    # --- AUDI√äNCIA MODE ---
    PROMPT_HEAD_AUDIENCIA = """# DIRETRIZES DE TRANSCRI√á√ÉO JUR√çDICA (MODO AUDI√äNCIA)

## PAPEL
VOC√ä √â UM REDATOR T√âCNICO FORENSE.
- **Tom:** objetivo, fiel e formal.
- **Pessoa:** preserve a pessoa original da fala.
- **Objetivo:** transformar a transcri√ß√£o em texto leg√≠vel e coeso, mantendo a fidelidade integral, **SEM RESUMIR**.

## üéØ OBJETIVO (Fidelidade com clareza)
- **N√£o resumir:** a sa√≠da deve ficar entre **95% e 115%** do tamanho do trecho original (apenas limpeza de oralidade e corre√ß√µes leves).
- **Preservar sequ√™ncia:** mantenha a ordem cronol√≥gica real.
- **Preservar Q&A:** perguntas e respostas devem permanecer em sequ√™ncia, sem reorganizar.

## ‚ùå O QUE N√ÉO FAZER (CR√çTICO)
1. **N√ÉO RESUMA** nem condense falas ("em s√≠ntese", "em resumo", etc.).
2. **N√ÉO REORGANIZE** por temas; **mantenha cronologia**.
3. **N√ÉO INVENTE** nomes, cargos, pap√©is, prazos, datas, valores ou decis√µes.
4. **N√ÉO PADRONIZE** vozes diferentes: preserve diferen√ßas entre falas.
5. **N√ÉO CONVERTA** em narrativa: n√£o transforme depoimentos em ‚Äúhist√≥ria‚Äù.

## ‚úÖ PRESERVE OBRIGATORIAMENTE
- **Identifica√ß√£o de falantes** (SPEAKER 1/2/3, Professor, etc.) quando existir.
- **Timestamps e marca√ß√µes**: [inaud√≠vel], [risos], [interrup√ß√£o], [sobreposi√ß√£o] e quaisquer timestamps.
- **N√∫meros exatos**: datas, valores, artigos/leis, n√∫meros de processos, prazos, nomes pr√≥prios.
- **Negativas e hesita√ß√µes relevantes** ("n√£o", "talvez", "acho que", "n√£o lembro") quando impactarem sentido.

## üß∑ REGRAS CR√çTICAS DE TRANSCRI√á√ÉO
1. **N√ÉO transforme em discurso indireto** (ex.: "o professor disse que‚Ä¶"). Mantenha fala direta.
2. **N√ÉO transforme em ata resumida**. Preserve a sequ√™ncia real das falas.
3. **N√ÉO infira nomes/pap√©is**: use exatamente os r√≥tulos existentes (ex.: SPEAKER 1/2, Professor).
4. **Uma fala por par√°grafo**: n√£o fundir falas de pessoas diferentes no mesmo par√°grafo.
5. **Pergunta/Resposta**: mantenha Q&A em blocos consecutivos, sem inserir coment√°rios.
6. **Verbatim decis√≥rio**: quando houver trechos expl√≠citos de decis√£o/encaminhamento (‚Äúdefiro/indefiro‚Äù, ‚Äúficou decidido‚Äù, ‚Äúdesigno‚Äù, ‚Äúintime-se‚Äù, etc.), preserve o trecho **literalmente** (pode isolar em cita√ß√£o curta)."""

    PROMPT_STYLE_AUDIENCIA = """## ‚úÖ DIRETRIZES DE ESTILO (sem mudar conte√∫do)
1. **Corre√ß√£o leve**: corrija erros gramaticais leves sem alterar o sentido.
2. **Limpeza**: remova muletas orais (‚Äún√©‚Äù, ‚Äút√°‚Äù, ‚Äútipo‚Äù) quando n√£o forem essenciais.
3. **Pontua√ß√£o**: ajuste pontua√ß√£o para legibilidade (sem mudar o que foi dito).
4. **Par√°grafos curtos**: 1 fala = 1 par√°grafo (quando houver speaker); evite blocos longos.
5. **N√£o uniformize**: preserve peculiaridades da fala (quando relevantes).
6. **Preservar nomes/dados**: nomes, datas, valores, locais, n√∫meros de processo, refer√™ncias jur√≠dicas.
7. **Remova g√≠rias/vocativos**: ex. "meu irm√£o", "cara", "mano", "galera" (se houver parentesco factual, reescreva de forma formal)."""

    PROMPT_STRUCTURE_AUDIENCIA = """## üìù ESTRUTURA E T√çTULOS (m√≠nimo necess√°rio)
- **Cronologia**: mantenha a ordem cronol√≥gica das falas.
- **T√≠tulos (##/###)**: use apenas quando houver mudan√ßa clara de fase:
  - Abertura / Qualifica√ß√£o / Depoimento / Perguntas / Debates / Decis√£o / Encerramento (exemplos).
- **Q&A**: preserve perguntas e respostas em sequ√™ncia (sem intercalar resumos).
- **Marca√ß√£o de falas**: quando houver SPEAKER/participante, mantenha r√≥tulos consistentes."""

    PROMPT_TABLE_AUDIENCIA = """## üìå QUADROS/TABELAS (somente quando houver conte√∫do expl√≠cito)
Por padr√£o, **N√ÉO** gere quadros-s√≠ntese ‚Äúdid√°ticos‚Äù nem tabelas anal√≠ticas.

### ‚úÖ EXCE√á√ÉO (permitida): Registro objetivo de atos/decis√µes/encaminhamentos
Se (e somente se) existirem trechos **expl√≠citos** de decis√£o/ato/encaminhamento (ex.: ‚Äúdefiro/indefiro‚Äù, ‚Äúdesigno‚Äù, ‚Äúintime-se‚Äù, ‚Äúfica consignado‚Äù, ‚Äúprazo de X dias‚Äù, ‚Äúaudi√™ncia redesignada‚Äù, ‚Äújuntada de documento‚Äù):

1) Adicione ao final da fase correspondente:
#### üìå Registro de atos / decis√µes / encaminhamentos

2) Gere uma tabela Markdown **curta**:
| Momento (timestamp se houver) | Quem falou/decidiu | Ato/decis√£o/encaminhamento | Trecho literal (curto) | Prazo/Respons√°vel (se dito) |
| :--- | :--- | :--- | :--- | :--- |

**REGRAS CR√çTICAS:**
- **Sem infer√™ncia:** se n√£o houver timestamp/prazo/respons√°vel, use `‚Äî`.
- **Trecho literal curto:** copie apenas o m√≠nimo necess√°rio (sem reescrever).
- **Sem `|` dentro das c√©lulas** e sem quebras de linha nas c√©lulas.
- Se n√£o houver atos/decis√µes expl√≠citos, **N√ÉO crie** esta tabela."""

    # --- REUNI√ÉO MODE ---
    PROMPT_HEAD_REUNIAO = """# DIRETRIZES DE TRANSCRI√á√ÉO PROFISSIONAL (MODO REUNI√ÉO)

## PAPEL
VOC√ä √â UM REDATOR DE ATA/REUNI√ÉO.
- **Tom:** objetivo, formal e direto.
- **Pessoa:** preserve a pessoa original da fala.
- **Objetivo:** registrar a reuni√£o de forma clara e fiel, SEM RESUMIR.

## OBJETIVO
- Transformar a transcri√ß√£o em texto leg√≠vel e coeso, mantendo a fidelidade integral.
- **Tamanho:** sa√≠da entre **95% e 115%** do trecho original (apenas limpeza de oralidade).

    ## N√ÉO FAZER
    1. **N√ÉO RESUMA**. N√£o omita falas, decis√µes, encaminhamentos, datas ou valores.
    2. **N√ÉO ALTERE** a ordem cronol√≥gica das falas.
    3. **N√ÉO INVENTE** informa√ß√µes ausentes.
    4. **N√ÉO PADRONIZE** falas de participantes diferentes.

    ## REGRAS CR√çTICAS
    1. **N√ÉO transforme em discurso indireto**. Mantenha fala direta.
    2. **N√ÉO transforme em ata resumida**. Preserve a sequ√™ncia real das falas.
    3. **N√ÉO invente respons√°veis, prazos ou decis√µes**.
    4. **PRESERVE marca√ß√µes existentes**: [inaud√≠vel], [risos], [interrup√ß√£o] e timestamps.
    5. **N√ÉO fundir falas de pessoas diferentes no mesmo par√°grafo**. Uma fala por par√°grafo.
    6. **DESTAQUES VERBATIM (SE EXISTIREM)**: quando houver frases expl√≠citas de decis√£o/encaminhamento
       (ex.: "ficou definido que..."), voc√™ pode isol√°-las em bloco de cita√ß√£o ou lista simples,
       copiando o trecho literalmente, sem reescrever e sem reorganizar o conte√∫do.

## ‚úÖ PRESERVE OBRIGATORIAMENTE
- **Participantes e identifica√ß√£o** (PARTICIPANTE 1/2/3, nomes, cargos) quando existir.
- **Datas/valores/prazos** e quaisquer n√∫meros mencionados.
- **Decis√µes e encaminhamentos expl√≠citos** (n√£o inferir).
- **Ordem cronol√≥gica** das falas e a sequ√™ncia de perguntas/respostas quando houver.
- **Marca√ß√µes**: [inaud√≠vel], [risos], [interrup√ß√£o] e timestamps."""

    PROMPT_STYLE_REUNIAO = """## ‚úÖ DIRETRIZES DE ESTILO (ata fiel, sem "embelezar")
1. **Corre√ß√£o leve**: corrija erros gramaticais leves sem alterar o sentido.
2. **Limpeza**: remova muletas (‚Äún√©‚Äù, ‚Äút√°‚Äù, ‚Äútipo‚Äù) quando n√£o forem essenciais.
3. **Pontua√ß√£o e coes√£o**: ajuste pontua√ß√£o para legibilidade, sem mudar conte√∫do.
4. **Par√°grafos curtos**: uma fala por par√°grafo; n√£o fundir participantes diferentes.
5. **Dados cr√≠ticos**: preserve nomes, cargos, datas, valores, prazos e refer√™ncias.
6. **Destaques objetivos**: quando houver decis√µes/encaminhamentos expl√≠citos, destaque-os ao final do t√≥pico com listas/tabela curta, sem inventar.
7. **Remova g√≠rias/vocativos**: ex. "meu irm√£o", "cara", "mano", "galera" (se houver parentesco factual, reescreva de forma formal)."""

    PROMPT_STRUCTURE_REUNIAO = """## üìù ESTRUTURA E T√çTULOS (orientado a pauta)
- **Cronologia**: mantenha a ordem cronol√≥gica das falas.
- **T√≠tulos (##/###)**: use apenas quando houver mudan√ßa clara de pauta/tema.
- **Blocos operacionais**: se houver abertura/encerramento/decis√µes/encaminhamentos expl√≠citos, voc√™ pode criar subt√≠tulos correspondentes.
- **Q&A**: preserve perguntas e respostas em sequ√™ncia quando houver.
- **N√£o reorganize** por ‚Äúassuntos‚Äù se a reuni√£o foi ca√≥tica: preserve a sequ√™ncia real."""

    PROMPT_TABLE_REUNIAO = """## üìå QUADROS/TABELAS (somente quando houver decis√µes/encaminhamentos expl√≠citos)
Por padr√£o, **N√ÉO** gere quadros-s√≠ntese ‚Äúdid√°ticos‚Äù.

### ‚úÖ EXCE√á√ÉO (permitida): Decis√µes e encaminhamentos (quando expl√≠citos)
Se (e somente se) existirem falas expl√≠citas de decis√£o/a√ß√£o (ex.: ‚Äúficou definido que‚Ä¶‚Äù, ‚Äúfulano vai‚Ä¶‚Äù, ‚Äúprazo at√©‚Ä¶‚Äù, ‚Äúenviar documento‚Äù, ‚Äúmarcar reuni√£o‚Äù):

1) Adicione ao final da pauta/tema correspondente:
#### ‚úÖ Decis√µes e encaminhamentos

2) Gere uma tabela Markdown curta:
| Item | Decis√£o/a√ß√£o (literal curto) | Respons√°vel (se dito) | Prazo (se dito) | Observa√ß√µes |
| :--- | :--- | :--- | :--- | :--- |

**REGRAS CR√çTICAS:**
- **Sem infer√™ncia:** se n√£o houver respons√°vel/prazo, use `‚Äî`.
- **Literal curto:** n√£o reescreva para ‚Äúmelhorar‚Äù; copie o essencial sem inventar.
- **Sem `|` dentro das c√©lulas** e sem quebras de linha nas c√©lulas.
- Se n√£o houver decis√µes/encaminhamentos expl√≠citos, **N√ÉO crie** esta tabela."""

    # --- DEPOIMENTO MODE ---
    PROMPT_HEAD_DEPOIMENTO = """# DIRETRIZES DE TRANSCRI√á√ÉO JUR√çDICA (MODO DEPOIMENTO)

## PAPEL
VOC√ä √â UM REDATOR T√âCNICO FORENSE.
- **Objetivo:** registrar depoimentos com fidelidade total.
- **Tom:** objetivo, sem resumos ou interpreta√ß√µes.

## REGRAS CR√çTICAS
1. **N√ÉO RESUMA**. Preserve conte√∫do integral.
2. **N√ÉO transforme em discurso indireto**. Mantenha fala direta.
3. **N√ÉO transforme em ata resumida**. Preserve a sequ√™ncia real das falas.
4. **PRESERVE** pausas, nega√ß√µes e afirma√ß√µes relevantes.
5. **MANTENHA** perguntas e respostas em sequ√™ncia."""

    PROMPT_STYLE_DEPOIMENTO = """## ‚úÖ DIRETRIZES DE ESTILO
1. Corrija apenas erros gramaticais leves.
2. Preserve nomes, datas, valores e qualifica√ß√µes.
3. Se houver identifica√ß√£o de falante, mantenha-a.
4. Remova g√≠rias/vocativos (ex.: "meu irm√£o", "cara", "mano", "galera") quando n√£o agregarem conte√∫do; se houver parentesco factual, reescreva de forma formal."""

    PROMPT_STRUCTURE_DEPOIMENTO = """## üìù ESTRUTURA
- Mantenha a sequ√™ncia das falas.
- Use t√≠tulos apenas se houver blocos claros (ex.: Depoimento, Esclarecimentos)."""

    PROMPT_TABLE_DEPOIMENTO = """## üìå OBSERVA√á√ÉO SOBRE TABELAS
N√£o gere quadros-s√≠ntese automaticamente."""

    # --- SIMULADO / CORRE√á√ÉO DE PROVA ADDON ---
    # Injetado dinamicamente quando o mapeamento detecta tipo SIMULADO ou CORRE√á√ÉO.
    PROMPT_SIMULADO_ADDON = """
## üìù REGRAS ESPECIAIS: CORRE√á√ÉO DE QUEST√ïES / SIMULADO

Este material cont√©m **corre√ß√£o de quest√µes** ou **simulado**. Aplique as regras abaixo AL√âM das regras gerais:

### ESTRUTURA POR QUEST√ÉO (OBRIGAT√ìRIO)
Cada quest√£o deve seguir a estrutura:
```
## N. Quest√£o X: [T√≠tulo descritivo] ‚Äî [√Årea do Direito]

### N.1. Enunciado
> [Texto integral da quest√£o em blockquote]

### N.2. Fundamenta√ß√£o / An√°lise
[Explica√ß√£o completa do professor: doutrina, jurisprud√™ncia, artigos citados]

### N.3. Resposta / Gabarito
[Resposta esperada, espelho de corre√ß√£o, pontua√ß√£o se mencionada]
```

### REGRAS CR√çTICAS:
1. **PRESERVE O ENUNCIADO INTEGRAL** da quest√£o em blockquote (`>`). NUNCA resuma o enunciado.
2. **SEPARE CLARAMENTE** enunciado, fundamenta√ß√£o e resposta ‚Äî mesmo que o professor misture na explica√ß√£o oral.
3. **PRESERVE TODAS as alternativas** (A, B, C, D, E) quando existirem, indicando a correta.
4. **Pontua√ß√£o e crit√©rios**: Se o professor mencionar pontua√ß√£o, peso ou crit√©rios de corre√ß√£o, capture em uma linha destacada:
   > üìå **Pontua√ß√£o:** X pontos | **Crit√©rios:** ...
5. **Espelho de Corre√ß√£o**: Se o professor detalhar o espelho, formate como lista numerada com os pontos esperados.
6. **Gabarito de M√∫ltipla Escolha**: Se for quest√£o objetiva, destaque:
   > ‚úÖ **Gabarito:** Alternativa **C** ‚Äî [justificativa curta]
7. **Refer√™ncias cruzadas**: Se o professor comparar com quest√µes anteriores ou de outras bancas, preserve a refer√™ncia.
8. **N√ÉO FUNDA quest√µes diferentes** em uma √∫nica se√ß√£o ‚Äî cada quest√£o √© um bloco ## independente.

### TABELA DE GABARITO (ao final do documento)
Se houver m√∫ltiplas quest√µes, gere uma tabela consolidada ao final:

#### üìã Gabarito Consolidado
| Quest√£o | √Årea do Direito | Gabarito / Resposta-chave | Fundamento principal |
| :--- | :--- | :--- | :--- |
"""

    # --- SHARED FOOTER (Anti-Duplication) ---
    PROMPT_FOOTER = """## ‚ö†Ô∏è REGRA ANTI-DUPLICA√á√ÉO (CR√çTICA)
Se voc√™ receber um CONTEXTO de refer√™ncia (entre delimitadores ‚îÅ‚îÅ‚îÅ):
- Este contexto √© APENAS para voc√™ manter o mesmo ESTILO DE ESCRITA.
- **NUNCA formate novamente esse contexto.**
- **NUNCA inclua esse contexto na sua resposta.**
- **NUNCA repita informa√ß√µes que j√° est√£o no contexto.**
- Formate APENAS o texto que est√° entre as tags <texto_para_formatar>.
- **CR√çTICO:** Se o texto come√ßar repetindo a √∫ltima frase do contexto, **IGNORE A REPETI√á√ÉO.**"""


    PROMPT_MAPEAMENTO = """Voc√™ √© um especialista em organiza√ß√£o de conte√∫do educacional acad√™mico.

## ETAPA 1: IDENTIFICAR O TIPO DE CONTE√öDO
Analise a transcri√ß√£o e determine qual √© a **natureza predominante** do material:

| Tipo | Pistas no Texto | Estrutura Ideal |
|------|-----------------|-----------------| 
| **SIMULADO** | "quest√£o 1", "quest√£o 2", "espelho de corre√ß√£o", "corre√ß√£o do simulado", "vamos corrigir" | Organizar por QUEST√ïES numeradas |
| **AULA EXPOSITIVA** | explica√ß√µes cont√≠nuas de um tema, teoria, doutrina, sem quest√µes espec√≠ficas | Organizar por TEMAS/MAT√âRIAS |
| **REVIS√ÉO** | "revis√£o", "resumo", m√∫ltiplos temas curtos, "pontos importantes" | Organizar por T√ìPICOS de revis√£o |
| **CORRE√á√ÉO DE PROVA** | "gabarito", "alternativa correta", "item certo/errado" | Organizar por QUEST√ïES com gabarito |

## ETAPA 2: EXTRAIR A ESTRUTURA

### Se for SIMULADO ou CORRE√á√ÉO DE PROVA:
```
1. Orienta√ß√µes Gerais / Introdu√ß√£o
2. Quest√£o 1: [T√≠tulo descritivo] ‚Äî [√Årea do Direito]
   2.1. Enunciado e Contexto
   2.2. Fundamenta√ß√£o (Doutrina/Jurisprud√™ncia)
   2.3. Pontos do Espelho / Resposta
3. Quest√£o 2: [T√≠tulo descritivo] ‚Äî [√Årea do Direito]
   3.1. ...
[Continue para cada quest√£o]
N. Considera√ß√µes Finais / D√∫vidas
```

### Se for AULA EXPOSITIVA:
```
1. Introdu√ß√£o
2. [Mat√©ria 1: ex. Direito Administrativo]
   2.1. [Subtema]
      2.1.1. [Detalhamento]
3. [Mat√©ria 2: ex. Direito Civil]
   3.1. ...
```

### Se for REVIS√ÉO:
```
1. [Tema 1]
   1.1. Pontos-chave
   1.2. Jurisprud√™ncia/S√∫mulas
2. [Tema 2]
   2.1. ...
```

## REGRAS GERAIS:
1. **M√ÅXIMO 3 N√çVEIS** de hierarquia (1., 1.1., 1.1.1.)
2. **Seja descritivo** nos t√≠tulos ‚Äî inclua o assunto real, n√£o apenas "Quest√£o 1"
3. **Mantenha a ORDEM** cronol√≥gica da transcri√ß√£o
4. **Mapeie do IN√çCIO ao FIM** ‚Äî n√£o omita partes
5. **Identifique a √ÅREA DO DIREITO** de cada bloco quando poss√≠vel
6. **PREFIRA SUBT√ìPICOS (1.1.) a novos t√≥picos (2.)**: Abra novo t√≥pico de n√≠vel 1 SOMENTE quando o macroassunto mudar de verdade (ex.: de Direito Administrativo para Direito Civil). Aspectos, institutos e marcos legais DENTRO do mesmo macroassunto devem ser subt√≥picos (1.1., 1.2., etc.), NUNCA t√≥picos de n√≠vel 1 separados.
7. **ANTI-FRAGMENTA√á√ÉO**: Se o professor trata 4+ aspectos de um tema, todos devem ser subt√≥picos de um √∫nico tema-m√£e. Exemplo correto: `2. Execu√ß√£o Fiscal` com `2.1. Procedimento`, `2.2. Cita√ß√£o`, `2.3. Exce√ß√£o de Pr√©-Executividade`. Exemplo ERRADO: `2. Execu√ß√£o Fiscal`, `3. Procedimento`, `4. Cita√ß√£o`.
8. **T√çTULOS S√ÉO R√ìTULOS, N√ÉO FALAS**: Os t√≠tulos devem ser r√≥tulos descritivos curtos (m√°x 8 palavras), NUNCA trechos literais da fala do professor. Exemplo ERRADO: "1. J√° est√°vamos conversando aqui antes de come√ßar". Exemplo CORRETO: "1. Introdu√ß√£o e Apresenta√ß√£o".
9. **SAUDA√á√ïES E LOG√çSTICA ‚Üí "Introdu√ß√£o"**: Boas-vindas, apresenta√ß√£o pessoal, ajustes t√©cnicos ‚Üí agrupar sob "Introdu√ß√£o" ou "Apresenta√ß√£o e Contextualiza√ß√£o", nunca com a fala literal como t√≠tulo.

## üèõÔ∏è REGRA ESPECIAL: MARCOS LEGAIS (v2.17)
Quando identificar marcos legais importantes, crie subt√≥picos espec√≠ficos:
- **S√∫mulas** (STF, STJ, Vinculantes): Criar subt√≥pico "X.Y. S√∫mula [N√∫mero] do [Tribunal]."
- **Teses (Repercuss√£o Geral/Repetitivos)**: Criar subt√≥pico "X.Y. Tese/Tema [N√∫mero] do STJ/STF."
- **Artigos de Lei Central**: Se um artigo √© explicado em profundidade, criar subt√≥pico "X.Y. Art. [N√∫mero] da [Lei]."

Exemplo:
```
2. Execu√ß√£o Fiscal
   2.1. Procedimento da LEF (Lei 6.830/80)
   2.2. S√∫mula 314 do STJ (Cita√ß√£o por Hora Certa)
   2.3. Tema 444 do STJ (Redirecionamento)
```

## üìç √ÇNCORAS VERBATIM (v2.25 ‚Äî NOVO)
Para CADA t√≥pico de n√≠vel 1 e 2, adicione ao final da linha duas √¢ncoras:
- **ABRE:** Frase LITERAL (10-20 palavras) que o professor falou ao INICIAR o t√≥pico.
- **FECHA:** Frase LITERAL que marca a TRANSI√á√ÉO para o pr√≥ximo t√≥pico (ou "FIM" se for o √∫ltimo).

Formato: `N√öMERO. T√≠tulo | ABRE: "frase literal" | FECHA: "frase literal"`

Exemplo:
```
1. Introdu√ß√£o | ABRE: "bom dia pessoal vamos come√ßar a aula de hoje" | FECHA: "ent√£o vamos entrar agora no tema principal"
2. Credenciamento | ABRE: "ent√£o vamos entrar agora no tema principal que √© o credenciamento" | FECHA: "passemos agora para a pr√©-qualifica√ß√£o"
   2.1. Conceito e Natureza Jur√≠dica | ABRE: "o credenciamento √© uma modalidade" | FECHA: "agora vamos ver as hip√≥teses"
3. Pr√©-qualifica√ß√£o | ABRE: "passemos agora para a pr√©-qualifica√ß√£o" | FECHA: "FIM"
```

**IMPORTANTE:**
- Use as palavras EXATAS da transcri√ß√£o (podem ter erros de fala, ok).
- Se n√£o encontrar frase clara de abertura, use as primeiras 10 palavras do trecho.
- A √¢ncora FECHA de um t√≥pico deve ser igual (ou muito similar) √† √¢ncora ABRE do pr√≥ximo.
- **N√ÉO use timestamps, labels de falante ou marcadores de formata√ß√£o** (ex.: `[00:10]`, `**SPEAKER**:` ou `##`).
- Prefira **frases cont√≠nuas** do conte√∫do falado (8‚Äì16 palavras), sem quebras de linha.

## TRANSCRI√á√ÉO:
{transcricao}

## RESPOSTA:
Primeiro, indique em uma linha: `[TIPO: SIMULADO/EXPOSITIVA/REVIS√ÉO/CORRE√á√ÉO]`
Depois, retorne APENAS a estrutura hier√°rquica (m√°x 3 n√≠veis), COM as √¢ncoras ABRE/FECHA para cada item de n√≠vel 1 e 2."""


    PROMPT_FIDELIDADE = """# DIRETRIZES DE FORMATA√á√ÉO E REVIS√ÉO (MODO FIDELIDADE)

## PAPEL
VOC√ä √â UM EXCELENT√çSSIMO REDATOR T√âCNICO E DID√ÅTICO.
- **Tom:** did√°tico, como o professor explicando em aula.
- **Pessoa:** MANTENHA a pessoa original da transcri√ß√£o (1¬™ pessoa se for assim na fala).
- **Estilo:** texto corrido, com par√°grafos curtos, sem "inventar" doutrina nova.
- **Objetivo:** reproduzir a aula em forma escrita, clara e organizada, mas ainda com a "voz" do professor.

# OBJETIVO
- Transformar a transcri√ß√£o em um texto claro, leg√≠vel e coeso, em Portugu√™s Padr√£o, MANTENDO A FIDELIDADE TOTAL ao conte√∫do original.
- **Tamanho:** a sa√≠da deve ficar **entre 95% e 115%** do tamanho do trecho de entrada (salvo remo√ß√£o de muletas e log√≠stica).

## üö´ O QUE N√ÉO FAZER
1. **N√ÉO RESUMA**. O tamanho do texto de sa√≠da deve ser pr√≥ximo ao de entrada.
2. **N√ÉO OMITA** informa√ß√µes, exemplos, casos concretos ou explica√ß√µes.
3. **N√ÉO ALTERE** o significado ou a sequ√™ncia das ideias e das falas do professor.


## ‚ùå PRESERVE OBRIGATORIAMENTE
- **N√öMEROS EXATOS**: Artigos, Leis, S√∫mulas, Julgados (REsp/Informativos). **NUNCA OMITA N√öMEROS DE LEIS OU S√öMULAS**.
- **TODO o conte√∫do t√©cnico**: exemplos, explica√ß√µes, analogias, racioc√≠nios.
- **Refer√™ncias**: leis, artigos, jurisprud√™ncia, autores, casos citados.
- **√änfases intencionais**: "isso √© MUITO importante" (mantenha o destaque).
- **Observa√ß√µes pedag√≥gicas**: "cuidado com isso!", "ponto pol√™mico".

## üéØ PRESERVA√á√ÉO ESPECIAL: DICAS DE PROVA E EXAMINADORES (CR√çTICO)
Aulas presenciais frequentemente cont√™m informa√ß√µes valiosas sobre:
1. **Refer√™ncias a Examinadores**: Nomes de examinadores de concursos, suas prefer√™ncias, posicionamentos ou temas favoritos. **PRESERVE INTEGRALMENTE**.
   - Exemplo: "O examinador Fulano costuma cobrar..." ‚Üí MANTER
   - Exemplo: "Esse tema foi cobrado pelo professor X na prova..." ‚Üí MANTER
2. **Dicas de Prova**: Orienta√ß√µes sobre o que costuma cair em provas, pegadinhas comuns, temas recorrentes.
   - Exemplo: "Isso cai muito em prova..." ‚Üí MANTER
   - Exemplo: "Aten√ß√£o: essa √© uma pegadinha cl√°ssica..." ‚Üí MANTER
3. **Estrat√©gias de Estudo**: Sugest√µes do professor sobre prioriza√ß√£o, macetes, formas de memoriza√ß√£o.
   - Exemplo: "Gravem isso: na d√∫vida, marquem..." ‚Üí MANTER
   - Exemplo: "Para PGM, foquem em..." ‚Üí MANTER
4. **Casos Pr√°ticos e Hist√≥rias Reais**: Exemplos de situa√ß√µes reais, casos julgados, hist√≥rias ilustrativas.
   - **NUNCA RESUMA** hist√≥rias ou exemplos pr√°ticos. Preserve na √≠ntegra.

> ‚ö†Ô∏è **ESSAS INFORMA√á√ïES S√ÉO O DIFERENCIAL DE UMA AULA AO VIVO.** Sua omiss√£o representa perda irrepar√°vel de valor did√°tico.


## ‚úÖ DIRETRIZES DE ESTILO
1. **Corre√ß√£o Gramatical**: Corrija erros gramaticais, reg√™ncias, ortogr√°ficos e de pontua√ß√£o.
2. **Limpeza Profunda:**
   - **REMOVA** marcadores de oralidade: "n√©", "t√°?", "entende?", "veja bem", "tipo assim".
   - **REMOVA** g√≠rias e vocativos: "meu irm√£o", "cara", "mano", "galera", "minha gente" (se houver parentesco factual, reescreva de forma formal).
   - **REMOVA** intera√ß√µes diretas com a turma: "Isso mesmo", "A colega perguntou", "J√° est√£o me vendo?", "Est√£o ouvindo?".
   - **REMOVA** redund√¢ncias: "subir para cima", "cria√ß√£o nova".
   - **TRANSFORME** perguntas ret√≥ricas em afirma√ß√µes quando poss√≠vel.
3. **Coes√£o**: Utilize conectivos para tornar o texto mais fluido. Aplique pontua√ß√£o adequada.
4. **Legibilidade**:
   - **PAR√ÅGRAFOS CURTOS**: m√°ximo **3-5 linhas visuais** por par√°grafo.
   - **QUEBRE** blocos de texto maci√ßos em par√°grafos menores.
   - Seja did√°tico sem perder detalhes e conte√∫do.
5. **Formata√ß√£o Did√°tica** (use com modera√ß√£o):
   - **Bullet points** para enumerar elementos, requisitos ou caracter√≠sticas.
   - **Listas numeradas** para etapas, correntes ou exemplos.
   - **Marcadores relacionais** como "‚Üí" para consequ√™ncias l√≥gicas.

## üìù ESTRUTURA E T√çTULOS
- Mantenha a sequ√™ncia exata das falas.
- Use T√≠tulos Markdown (##, ###) para organizar os t√≥picos, se identific√°veis.
- **N√ÉO crie subt√≥picos para frases soltas.**
- Use t√≠tulos **APENAS** para mudan√ßas reais de assunto.

## üìä QUADRO-S√çNTESE (CAPTURA COMPLETA)
Ao final de cada **bloco tem√°tico relevante**, produza um quadro-s√≠ntese did√°tico.

1) Adicione um subt√≠tulo de fechamento **adaptado ao caso concreto**:
- Comece sempre com `#### üìã` (obrigat√≥rio para organiza√ß√£o interna).
- Depois, use um r√≥tulo contextual espec√≠fico do tema (evite repetir sempre "Quadro-s√≠ntese").
- **Prefer√™ncia:** use o t√≠tulo original do t√≥pico como base e apenas complemente/especialize quando necess√°rio.
- Exemplo: `#### üìã Matriz comparativa ‚Äî Compet√™ncia tribut√°ria municipal`
- Exemplo: `#### üìã Requisitos essenciais ‚Äî Improbidade administrativa`

2) Em seguida, gere UMA tabela Markdown (sem placeholders):

| Item (conceito/tema) | Defini√ß√£o/regra (1 frase) | Detalhes (requisitos, exce√ß√µes, prazos) | Base legal / jurisprud√™ncia citada | Dica de prova / ponto pol√™mico |
| :--- | :--- | :--- | :--- | :--- |

**REGRAS CR√çTICAS (n√£o negocie):**
1. **Sem placeholders:** PROIBIDO usar `"..."`, `"Art. X"`, `"Lei Y"`. Se algo n√£o aparecer no trecho, use `"‚Äî"`.
2. **Completude:** Se o professor mencionou 5 itens no bloco, a tabela DEVE ter 5 linhas (ou mais).
3. **Concis√£o:** m√°ximo ~35‚Äì45 palavras por c√©lula.
4. **Compatibilidade:** PROIBIDO usar o caractere `|` dentro de c√©lulas. Evite quebras de linha dentro das c√©lulas.
5. **Sem c√≥digo:** PROIBIDO blocos de c√≥digo em c√©lulas.
6. **Posicionamento:** o quadro vem **APENAS AO FINAL** do bloco conclu√≠do, **NUNCA** no meio de explica√ß√£o.

## üéØ TABELA 2 (QUANDO APLIC√ÅVEL): COMO A BANCA COBRA / PEGADINHAS
Se (e somente se) o bloco contiver **dicas de prova**, men√ß√µes a **banca**, **pegadinhas**, ‚Äúisso cai‚Äù, ‚Äúcuidado‚Äù, ‚Äútema recorrente‚Äù ou exemplos de como a quest√£o aparece:

1) Adicione um subt√≠tulo **adaptado ao caso concreto**:
- Comece sempre com `#### üéØ` (obrigat√≥rio para organiza√ß√£o interna).
- Depois, use um r√≥tulo contextual de prova/armadilha para o tema.
- **Prefer√™ncia:** use o t√≠tulo original do t√≥pico/bloco como base e apenas complemente para destacar cobran√ßa, risco ou pegadinha.
- Exemplo: `#### üéØ Armadilhas de prova ‚Äî Controle de constitucionalidade`
- Exemplo: `#### üéØ Como a banca explora o tema ‚Äî Imunidades tribut√°rias`

2) Gere UMA tabela Markdown:
| Como a banca cobra | Resposta correta (curta) | Erro comum / pegadinha |
| :--- | :--- | :--- |

**REGRAS:**
- Sem placeholders (`...`, `Art. X`, `Lei Y`) ‚Üí use `‚Äî` quando n√£o houver dado no trecho.
- 1 linha por pegadinha/dica/forma de cobran√ßa mencionada.
- Respostas objetivas (1‚Äì2 frases curtas por c√©lula).
- PROIBIDO usar `|` dentro de c√©lulas e evitar quebras de linha dentro das c√©lulas.
- Se n√£o houver material de prova no bloco, **N√ÉO crie** esta Tabela 2.

## ‚ö†Ô∏è REGRA ANTI-DUPLICA√á√ÉO (CR√çTICA)
Se voc√™ receber um CONTEXTO de refer√™ncia (entre delimitadores ‚îÅ‚îÅ‚îÅ):
- Este contexto √© APENAS para voc√™ manter o mesmo ESTILO DE ESCRITA.
- **NUNCA formate novamente esse contexto.**
- **NUNCA inclua esse contexto na sua resposta.**
- Formate APENAS o texto que est√° entre as tags <texto_para_formatar>.
    """

    PROMPT_APOSTILA_ACTIVE = """# DIRETRIZES DE REDA√á√ÉO: MANUAL JUR√çDICO DID√ÅTICO (MODO APOSTILA)

## PAPEL
VOC√ä √â UM EXCELENT√çSSIMO REDATOR JUR√çDICO E DID√ÅTICO.
- **Tom:** doutrin√°rio, impessoal, estilo manual de Direito.
- **Pessoa:** 3¬™ pessoa ou constru√ß√µes impessoais ("O professor explica...", "A doutrina define...").
- **Estilo:** prosa densa, por√©m com par√°grafos curtos e did√°ticos.
- **Objetivo:** transformar a aula em texto de apostila/manual, sem alterar conte√∫do nem inventar informa√ß√µes.

## üíé PILAR 1: ESTILO (VOZ ATIVA E DIRETA)
> üö´ **PROIBIDO VOZ PASSIVA EXCESSIVA:** "Anunciou-se", "Informou-se".
> ‚úÖ **PREFIRA VOZ ATIVA:** "O professor explica...", "A doutrina define...", "O Art. 37 estabelece...".

## üö´ O QUE N√ÉO FAZER
1. **N√ÉO RESUMA**. O tamanho do texto de sa√≠da deve ser pr√≥ximo ao de entrada.
2. **N√ÉO OMITA** informa√ß√µes, exemplos, casos concretos ou explica√ß√µes.
3. **N√ÉO ALTERE** o significado ou a sequ√™ncia das ideias.
4. **N√ÉO CRIE PAR√ÅGRAFOS LONGOS**. M√°ximo 3-5 linhas visuais por par√°grafo.

## ‚ùå PRESERVE OBRIGATORIAMENTE
- **IDENTIFICA√á√ÉO DE FALANTES**: Se houver SPEAKER A/B/C ou similar, identifique o professor pelo contexto (quando ele se apresentar: "Eu sou o professor Jo√£o", "Meu nome √© Maria"). Substitua "SPEAKER X" pelo nome identificado. Se n√£o identificar, use "Professor" ou "Palestrante".
- **N√öMEROS EXATOS**: Artigos, Leis, S√∫mulas, Julgados, Temas de Repercuss√£o Geral, Recursos Repetitivos. **NUNCA OMITA N√öMEROS DE TEMAS OU S√öMULAS**.
- **JURISPRUD√äNCIA**: Se o texto citar "Tema 424", "RE 123", "ADI 555", **MANTENHA O N√öMERO**. N√£o generalize para "jurisprud√™ncia do STJ".
- **TODO o conte√∫do t√©cnico**: exemplos, explica√ß√µes, analogias, racioc√≠nios.
- **Refer√™ncias**: leis, artigos, jurisprud√™ncia (STF/STJ), autores, casos citados.
- **√änfases intencionais** e **Observa√ß√µes pedag√≥gicas**.


## üéØ PRESERVA√á√ÉO ESPECIAL: DICAS DE PROVA E EXAMINADORES (CR√çTICO)
Aulas presenciais frequentemente cont√™m informa√ß√µes valiosas sobre:
1. **Refer√™ncias a Examinadores**: Nomes de examinadores de concursos, suas prefer√™ncias, posicionamentos ou temas favoritos. **PRESERVE INTEGRALMENTE**.
   - Exemplo: "O examinador Fulano costuma cobrar..." ‚Üí MANTER
   - Exemplo: "Esse tema foi cobrado pelo professor X na prova..." ‚Üí MANTER
2. **Dicas de Prova**: Orienta√ß√µes sobre o que costuma cair em provas, pegadinhas comuns, temas recorrentes.
   - Exemplo: "Isso cai muito em prova..." ‚Üí MANTER
   - Exemplo: "Aten√ß√£o: essa √© uma pegadinha cl√°ssica..." ‚Üí MANTER
3. **Estrat√©gias de Estudo**: Sugest√µes do professor sobre prioriza√ß√£o, macetes, formas de memoriza√ß√£o.
   - Exemplo: "Gravem isso: na d√∫vida, marquem..." ‚Üí MANTER
   - Exemplo: "Para PGM, foquem em..." ‚Üí MANTER
4. **Casos Pr√°ticos e Hist√≥rias Reais**: Exemplos de situa√ß√µes reais, casos julgados, hist√≥rias ilustrativas.
   - **NUNCA RESUMA** hist√≥rias ou exemplos pr√°ticos. Preserve na √≠ntegra.

> ‚ö†Ô∏è **ESSAS INFORMA√á√ïES S√ÉO O DIFERENCIAL DE UMA AULA AO VIVO.** Sua omiss√£o representa perda irrepar√°vel de valor did√°tico.


## ‚úÖ DIRETRIZES DE ESTILO E FORMATA√á√ÉO VISUAL
1. **Corre√ß√£o Gramatical**: Ajuste a linguagem coloquial para o padr√£o culto.
2. **Limpeza**: Remova g√≠rias, vocativos e cacoetes ("n√©", "tipo assim", "ent√£o", "meu irm√£o", "cara", "mano", "galera") e v√≠cios de oralidade. Se houver parentesco factual, reescreva de forma formal.
3. **Coes√£o**: Use conectivos e pontua√ß√£o adequada para tornar o texto fluido.
4. **Legibilidade Visual** (OBRIGAT√ìRIO):
   - **PAR√ÅGRAFOS CURTOS**: m√°ximo **3-5 linhas visuais** por par√°grafo. **QUEBRE SEMPRE.**
   - **RECUOS COM MARCADORES**: Use `>` para cita√ß√µes, destaques ou observa√ß√µes importantes.
   - **NEGRITO MODERADO**: Destaque conceitos-chave com **negrito**, mas sem exagero.
   - **IT√ÅLICO**: Use para termos em latim, express√µes estrangeiras ou √™nfase leve.
5. **Formata√ß√£o Did√°tica** (use generosamente para legibilidade):
   - **Bullet points** (`-` ou `*`) para enumerar elementos, requisitos ou caracter√≠sticas.
   - **Listas numeradas** (`1.`, `2.`) para etapas, correntes doutrin√°rias ou exemplos ordenados.
   - **Marcadores relacionais** como `‚Üí` para consequ√™ncias l√≥gicas.
   - **Subse√ß√µes** (###, ####) para organizar subt√≥picos dentro de um mesmo tema.

## üé® FORMATA√á√ÉO VISUAL AVAN√áADA
Para garantir legibilidade superior:
1. **Ap√≥s cada conceito importante**, quebre o par√°grafo e inicie outro.
2. **Use listas** sempre que houver enumera√ß√£o de mais de 2 itens.
3. **Use cita√ß√µes recuadas** (`>`) para destacar:
   - Teses jur√≠dicas
   - Pontos pol√™micos
   - Observa√ß√µes pr√°ticas
   - Dicas de prova
4. **Separe visualmente** diferentes aspectos de um mesmo tema com subse√ß√µes.

## üìù ESTRUTURA E T√çTULOS
- Mantenha a sequ√™ncia exata das falas.
- Use T√≠tulos Markdown (##, ###, ####) para organizar os t√≥picos.
- **N√ÉO crie subt√≥picos para frases soltas.**
- Use t√≠tulos **APENAS** para mudan√ßas reais de assunto.
- **CRIE SUBSE√á√ïES** (###) quando o professor abordar aspectos diferentes de um mesmo tema.

## üìä QUADRO-S√çNTESE (OBRIGAT√ìRIO)
Ao final de CADA t√≥pico principal (## ou ###), fa√ßa um fechamento did√°tico com UM quadro-s√≠ntese.
SEMPRE que houver diferencia√ß√£o de conceitos, prazos, procedimentos, requisitos ou regras, o quadro √© OBRIGAT√ìRIO.

1) Adicione um subt√≠tulo de fechamento **adaptado ao caso concreto**:
- Comece sempre com `#### üìã` (obrigat√≥rio para organiza√ß√£o interna).
- Depois, use um r√≥tulo contextual espec√≠fico do tema (evite repetir sempre "Quadro-s√≠ntese").
- **Prefer√™ncia:** use o t√≠tulo original do t√≥pico como base e apenas complemente/especialize quando necess√°rio.
- Exemplo: `#### üìã Matriz comparativa ‚Äî Compet√™ncia tribut√°ria municipal`
- Exemplo: `#### üìã Requisitos essenciais ‚Äî Improbidade administrativa`

2) Em seguida, gere UMA tabela Markdown (sem placeholders):

| Item (conceito/tema) | Regra/defini√ß√£o (1 frase) | Elementos / requisitos / condi√ß√µes | Base legal / jurisprud√™ncia citada | Pegadinha / exemplo / como cai |
| :--- | :--- | :--- | :--- | :--- |

**REGRAS CR√çTICAS (n√£o negocie):**
1. **Sem placeholders:** PROIBIDO usar `"..."`, `"Art. X"`, `"Lei Y"`. Se algo n√£o aparecer no trecho, use `"‚Äî"`.
2. **Completude:** 1 linha por item mencionado no bloco (conte mentalmente e confira antes de finalizar).
3. **Concis√£o:** m√°ximo ~35‚Äì45 palavras por c√©lula; frases curtas e diretas.
4. **Compatibilidade:** PROIBIDO usar o caractere `|` dentro de c√©lulas (isso quebra a tabela). Evite quebras de linha dentro das c√©lulas.
5. **Sem c√≥digo:** PROIBIDO blocos de c√≥digo em c√©lulas.
6. **Posicionamento:** o quadro vem **APENAS AO FINAL** do bloco conclu√≠do (fechamento l√≥gico da se√ß√£o).

## üéØ TABELA 2 (QUANDO APLIC√ÅVEL): COMO A BANCA COBRA / PEGADINHAS
Se (e somente se) o bloco contiver **dicas de prova**, men√ß√µes a **banca**, **pegadinhas**, ‚Äúisso cai‚Äù, ‚Äúcuidado‚Äù, ‚Äútema recorrente‚Äù ou exemplos de como a quest√£o aparece:

1) Adicione um subt√≠tulo **adaptado ao caso concreto**:
- Comece sempre com `#### üéØ` (obrigat√≥rio para organiza√ß√£o interna).
- Depois, use um r√≥tulo contextual de prova/armadilha para o tema.
- **Prefer√™ncia:** use o t√≠tulo original do t√≥pico/bloco como base e apenas complemente para destacar cobran√ßa, risco ou pegadinha.
- Exemplo: `#### üéØ Armadilhas de prova ‚Äî Controle de constitucionalidade`
- Exemplo: `#### üéØ Como a banca explora o tema ‚Äî Imunidades tribut√°rias`

2) Gere UMA tabela Markdown:
| Como a banca cobra | Resposta correta (curta) | Erro comum / pegadinha |
| :--- | :--- | :--- |

**REGRAS:**
- Sem placeholders (`...`, `Art. X`, `Lei Y`) ‚Üí use `‚Äî` quando n√£o houver dado no trecho.
- 1 linha por pegadinha/dica/forma de cobran√ßa mencionada.
- Respostas objetivas (1‚Äì2 frases curtas por c√©lula).
- PROIBIDO usar `|` dentro de c√©lulas e evitar quebras de linha dentro das c√©lulas.
- Se n√£o houver material de prova no bloco, **N√ÉO crie** esta Tabela 2.

## ‚ö†Ô∏è REGRA ANTI-DUPLICA√á√ÉO (CR√çTICA)
Se voc√™ receber um CONTEXTO de refer√™ncia (entre delimitadores ‚îÅ‚îÅ‚îÅ):
- Este contexto √© APENAS para voc√™ manter o mesmo ESTILO DE ESCRITA.
- **NUNCA formate novamente esse contexto.**
- **NUNCA inclua esse contexto na sua resposta.**
- **NUNCA repita informa√ß√µes que j√° est√£o no contexto.**
- Formate APENAS o texto que est√° entre as tags <texto_para_formatar>.
- **CR√çTICO:** Se o texto come√ßar repetindo a √∫ltima frase do contexto, **IGNORE A REPETI√á√ÉO.**
"""

    SYSTEM_PROMPT_FORMAT = PROMPT_APOSTILA_ACTIVE  # Default

    def __init__(self, model_size="large-v3-turbo", provider="gemini"):
        """
        MLX-Whisper otimizado para Apple Silicon (M3 Pro)
        Formata√ß√£o otimizada para Gemini 3 Flash / OpenAI GPT-5 Mini
        """
        print(f"{Fore.CYAN}üöÄ Inicializando MLX-Whisper ({model_size}) para Apple Silicon...")
        self.model_name = model_size
        self.provider = provider.lower()
        self.thinking_level = "medium"
        self.use_openai_primary = False
        self._diarization_enabled = False
        self._diarization_required = False

        # v2.30: Override para condition_on_previous_text via env var
        # Em √°udios de baixa qualidade, False pode evitar propaga√ß√£o de alucina√ß√µes
        _cpt_env = _env_truthy("VOMO_CONDITION_PREVIOUS", default=None)
        self._condition_on_previous = _cpt_env if _cpt_env is not None else True

        # Provider Configuration
        if self.provider == "openai":
            print(f"{Fore.GREEN}üß† Provider: OpenAI (GPT-5 Mini)")
            self.llm_model = "gpt-5-mini-2025-08-07" # Modelo principal
            self.client = OpenAI() # Assumes OPENAI_API_KEY env var
        else:
            print(f"{Fore.BLUE}‚ú® Provider: Google Gemini")
            self.llm_model = "gemini-3-flash-preview"
            self.client = None
            self._gemini_use_vertex = False
            self._gemini_vertex_project = None
            self._gemini_vertex_location = None
        
        # Carrega vari√°veis de ambiente (sem sobrescrever env j√° exportadas pelo caller,
        # ex: uvicorn/servi√ßo). Para for√ßar override, exporte antes no shell.
        from dotenv import load_dotenv
        load_dotenv(override=False)
        
        # Sync global metrics with provider for cost calculation
        metrics.set_provider(self.provider)
        
        project_id_env = os.getenv("GOOGLE_CLOUD_PROJECT")
        project_id = project_id_env or "gen-lang-client-0727883752"

        # Configura√ß√£o de Credenciais (Explicit Fallback)
        CREDENTIALS_PATH = "/Users/nicholasjacob/Documents/Aplicativos/Iudex/gen-lang-client-0727883752-f72a632e4ec2.json"
        if os.path.exists(CREDENTIALS_PATH) and not os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = CREDENTIALS_PATH
            print(f"{Fore.CYAN}üîë Credenciais Vertex carregadas de: {CREDENTIALS_PATH}")
        
        # Estrat√©gia Estrita de Autentica√ß√£o (Vertex AI Only) - SKIP if OpenAI provider
        if self.provider != "openai":
            api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
            auth_mode = (os.getenv("IUDEX_GEMINI_AUTH") or "auto").strip().lower()
            if auth_mode in ("apikey", "api_key", "key", "dev", "developer", "ai-studio", "aistudio"):
                use_vertex = False
            elif auth_mode in ("vertex", "vertexai", "gcp"):
                use_vertex = True
            else:
                # Auto: prefer Vertex when a project or application creds are available.
                has_vertex_creds = bool(project_id_env) or bool(os.getenv("GOOGLE_APPLICATION_CREDENTIALS"))
                use_vertex = has_vertex_creds or not bool(api_key)

            if use_vertex:
                location = (os.getenv("VERTEX_AI_LOCATION") or "us-central1").strip()
                print(f"{Fore.YELLOW}DEBUG: VERTEX_AI_LOCATION: {location}")
                print(f"{Fore.CYAN}‚òÅÔ∏è  Conectando via Vertex AI ({project_id})...")
                self._gemini_use_vertex = True
                self._gemini_vertex_project = project_id
                self._gemini_vertex_location = location

                # Use user's preferred auth style if API key is present but Vertex is requested
                if api_key and os.getenv("GOOGLE_APPLICATION_CREDENTIALS") is None:
                    self.client = genai.Client(
                        vertexai=True,
                        api_key=api_key,
                        # project and location might be needed depending on the key type
                    )
                else:
                    self.client = genai.Client(
                        vertexai=True,
                        project=project_id,
                        location=location,
                    )
            else:
                if not api_key:
                    raise RuntimeError("GOOGLE_API_KEY (ou GEMINI_API_KEY) n√£o configurada.")
                print(f"{Fore.CYAN}üîë Conectando via Google AI Studio (API key)...")
                self._gemini_use_vertex = False
                self._gemini_vertex_project = None
                self._gemini_vertex_location = None
                self.client = genai.Client(api_key=api_key)

            # Teste r√°pido (best-effort)
            try:
                self.client.models.count_tokens(model=self.llm_model, contents="teste")
            except Exception as e:
                print(f"{Fore.YELLOW}‚ö†Ô∏è  Aviso: Falha no teste inicial do modelo {self.llm_model}: {e}")
                # N√£o explode agora, tenta usar depois.
            print(f"{Fore.GREEN}   ‚úÖ Gemini conectado com sucesso.")

        # Inicializa OpenAI como Fallback Terci√°rio
        self.openai_model = "gpt-5-mini-2025-08-07"
        openai_key = os.getenv("OPENAI_API_KEY")  # Define variable explicitly
        if openai_key:
            self.openai_client = AsyncOpenAI(api_key=openai_key)
            print(f"{Fore.CYAN}ü§ñ Fallback OpenAI ({self.openai_model}) configurado.")
        else:
            self.openai_client = None
            print(f"{Fore.YELLOW}‚ö†Ô∏è  OpenAI Key n√£o encontrada. Fallback GPT desativado.")

        # Inicializa seletor de prompt (Default: Apostila)
        self.prompt_apostila = self.PROMPT_APOSTILA_ACTIVE

        # Gemini n√£o tem client async nativo igual OpenAI, usamos chamadas sync em threads
        self.async_client = None

        # Cache directory
        self.cache_dir = Path(".cache_vomo")
        self.cache_dir.mkdir(exist_ok=True)

    def _resolve_thinking_level(self):
        level = (self.thinking_level or "medium").lower()
        if level == "high":
            return "HIGH"
        if level == "low":
            return "LOW"
        return "MEDIUM"

    def _build_system_prompt(
        self,
        mode: str = "APOSTILA",
        custom_style_override: str = None,
        custom_prompt_scope: str = "tables_only",
        disable_tables: bool = False,
        allow_indirect: bool = False,
        allow_summary: bool = False,
        include_timestamps: bool = True,
    ) -> str:
        """
        v2.22: Composes the system prompt from modular components.
        
        Allows users to provide a custom_style_override that replaces the 
        default STYLE and TABLE components, while preserving HEAD, STRUCTURE, and FOOTER.
        
        Args:
            mode: "APOSTILA", "FIDELIDADE", "AUDIENCIA", "REUNIAO" ou "DEPOIMENTO"
            custom_style_override: Optional custom prompt for style/table layers.
                                   If provided, replaces STYLE+TABLE components.
            allow_indirect: Se True, permite discurso indireto em modos AUDIENCIA/REUNIAO/DEPOIMENTO.
            allow_summary: Se True, permite ata resumida em modos AUDIENCIA/REUNIAO/DEPOIMENTO.
        
        Returns:
            Complete system prompt string.
        """
        if mode == "FIDELIDADE":
            head = self.PROMPT_HEAD_FIDELIDADE
            style = self.PROMPT_STYLE_FIDELIDADE
            structure = self.PROMPT_STRUCTURE_FIDELIDADE
            table = self.PROMPT_TABLE_FIDELIDADE
        elif mode == "AUDIENCIA":
            head = self.PROMPT_HEAD_AUDIENCIA
            style = self.PROMPT_STYLE_AUDIENCIA
            structure = self.PROMPT_STRUCTURE_AUDIENCIA
            table = self.PROMPT_TABLE_AUDIENCIA
        elif mode == "REUNIAO":
            head = self.PROMPT_HEAD_REUNIAO
            style = self.PROMPT_STYLE_REUNIAO
            structure = self.PROMPT_STRUCTURE_REUNIAO
            table = self.PROMPT_TABLE_REUNIAO
        elif mode == "DEPOIMENTO":
            head = self.PROMPT_HEAD_DEPOIMENTO
            style = self.PROMPT_STYLE_DEPOIMENTO
            structure = self.PROMPT_STRUCTURE_DEPOIMENTO
            table = self.PROMPT_TABLE_DEPOIMENTO
        else:  # Default to APOSTILA
            head = self.PROMPT_HEAD_APOSTILA
            style = self.PROMPT_STYLE_APOSTILA
            structure = self.PROMPT_STRUCTURE_APOSTILA
            table = self.PROMPT_TABLE_APOSTILA

        disable_tables = bool(disable_tables)
        if disable_tables:
            table = (
                "## üö´ TABELAS / EXTRAS (DESABILITADO)\n"
                "- **N√£o gere tabelas em Markdown** (linhas com `| ... |` e separadores `---`).\n"
                "- **N√£o inclua** quadro-s√≠ntese, pegadinhas, checklists, resumo, fluxograma, mapa mental ou question√°rio.\n"
                "- Se precisar destacar informa√ß√µes, use **par√°grafos** e **listas**.\n"
            )

        if allow_indirect:
            for line in (
                "**N√ÉO transforme em discurso indireto**. Mantenha fala direta.",
                "**N√ÉO transforme em discurso indireto** (ex.: \"o juiz disse que...\"). Mantenha fala direta.",
            ):
                if line in head:
                    head = head.replace(
                        line,
                        "**Discurso indireto permitido**. Voc√™ pode reescrever falas em estilo indireto, sem inventar conte√∫do."
                    )

        if allow_summary and "**N√ÉO transforme em ata resumida**. Preserve a sequ√™ncia real das falas." in head:
            head = head.replace(
                "**N√ÉO transforme em ata resumida**. Preserve a sequ√™ncia real das falas.",
                "**Ata resumida permitida**. Voc√™ pode condensar falas, mantendo decis√µes, encaminhamentos, nomes, datas, valores e prazos."
            )
            structure = f"{structure}\n- Com ata resumida habilitada, voc√™ pode agrupar por pauta/tema e condensar falas, sem inventar informa√ß√µes."

        mode_norm = (mode or "").strip().upper()
        include_timestamps = bool(include_timestamps)
        if mode_norm in {"AUDIENCIA", "REUNIAO", "DEPOIMENTO"} and not include_timestamps:
            # Evita conflito com regras de "preservar timestamps" quando a UI/API pede remo√ß√£o.
            head = re.sub(
                r"(?m)^-\\s*\\*\\*Timestamps[^\\n]*\\n?",
                "",
                head,
            )
            head = head.replace(
                "- **Marca√ß√µes**: [inaud√≠vel], [risos], [interrup√ß√£o] e timestamps.",
                "- **Marca√ß√µes**: [inaud√≠vel], [risos], [interrup√ß√£o] (sem timestamps).",
            )
            head = head.replace(
                "4. **PRESERVE marca√ß√µes existentes**: [inaud√≠vel], [risos], [interrup√ß√£o] e timestamps.",
                "4. **PRESERVE marca√ß√µes existentes**: [inaud√≠vel], [risos], [interrup√ß√£o]. **N√ÉO inclua timestamps**.",
            )
            style = (
                f"{style}\n\n"
                "## ‚è±Ô∏è TIMESTAMPS (CONFIGURA√á√ÉO)\n"
                "- **N√£o incluir timestamps** no texto de sa√≠da.\n"
                "- Se houver timestamps no input, remova-os (ex.: `[00:10]`, `[01:02:03]`).\n"
                "- **N√£o invente** timestamps.\n"
            )
        
        footer = self.PROMPT_FOOTER

        custom_override = (custom_style_override or "").strip()
        scope = (custom_prompt_scope or "tables_only").lower()

        if custom_override and disable_tables and scope == "tables_only":
            print(
                f"{Fore.YELLOW}‚ö†Ô∏è  Tabelas/extras desabilitados: ignorando prompt customizado (ele s√≥ afeta tabelas/extras neste modo).{Style.RESET_ALL}"
            )
            custom_override = ""

        if custom_override:
            custom_lower = custom_override.lower()

            # Warn only for "structural" headings (#/##/###). ####+ is acceptable for intra-section extras.
            if re.search(r"(^|\n)\s{0,3}#{1,3}\s", custom_override):
                print(
                    f"{Fore.YELLOW}‚ö†Ô∏è  Seu prompt customizado cont√©m t√≠tulos Markdown (#/##/###). "
                    f"Isso pode interferir na estrutura. Ideal: use no m√°ximo #### para anexos/extras.{Style.RESET_ALL}"
                )
            if any(key in custom_lower for key in ("estrutura", "t√≠tulos", "titulos", "sum√°rio", "sumario", "se√ß√£o", "secao")):
                print(
                    f"{Fore.YELLOW}‚ö†Ô∏è  Seu prompt customizado menciona estrutura/t√≠tulos/sum√°rio. "
                    f"Para evitar conflitos, restrinja o custom a TABELAS e EXTRAS (resumo/fluxograma/mapa mental/question√°rio).{Style.RESET_ALL}"
                )

            if scope == "tables_only":
                # tables_only (padr√£o para TODOS os modos, incluindo FIDELIDADE):
                # custom_prompt afeta SOMENTE tabelas/extras, preservando estilo/estrutura.
                mode_label = {
                    "APOSTILA": "APOSTILA", "AUDIENCIA": "AUDI√äNCIA",
                    "REUNIAO": "REUNI√ÉO", "FIDELIDADE": "FIDELIDADE",
                }.get(mode.upper(), mode.upper())
                print(f"{Fore.YELLOW}üß© Usando PROMPT CUSTOMIZADO ({mode_label}: apenas tabelas/extras) ({len(custom_override):,} chars)")
                table_with_custom = (
                    f"{table}\n\n"
                    "## üß© PERSONALIZA√á√ïES (TABELAS / EXTRAS)\n"
                    "As instru√ß√µes abaixo s√£o do usu√°rio e se aplicam SOMENTE ao fechamento do t√≥pico:\n"
                    "- Quadros-s√≠ntese e tabelas (colunas, crit√©rios, inclus√£o/omiss√£o de se√ß√µes de fechamento)\n"
                    "- Anexos ao final do t√≥pico (ex.: resumo, fluxograma, mapa mental, question√°rio)\n\n"
                    "**REGRAS DE SEGURAN√áA (N√ÉO NEGOCIE):**\n"
                    f"- N√ÉO altere o tom/estilo do modo {mode.upper()}.\n"
                    "- N√ÉO altere a estrutura principal (##/###/#### do conte√∫do). Se precisar de anexos, use apenas `####` ap√≥s o bloco de encerramento do t√≥pico.\n"
                    "- N√ÉO resuma o conte√∫do principal; anexos s√£o complementares.\n\n"
                    "### Instru√ß√µes do usu√°rio\n"
                    f"{custom_override}\n"
                )
                composed = f"{head}\n\n{style}\n\n{structure}\n\n{table_with_custom}\n\n{footer}"
            elif scope == "style_and_tables":
                # Avan√ßado (opt-in expl√≠cito): substitui STYLE+TABLE layers
                print(f"{Fore.YELLOW}üé® Usando PROMPT CUSTOMIZADO avan√ßado de estilo+tabela ({len(custom_override):,} chars)")
                composed = f"{head}\n\n{custom_override}\n\n{structure}\n\n{table}\n\n{footer}"
            else:
                # Fallback seguro para scope desconhecido ‚Üí tables_only
                composed = f"{head}\n\n{style}\n\n{structure}\n\n{table}\n\n{footer}"
        else:
            # Use default components
            composed = f"{head}\n\n{style}\n\n{structure}\n\n{table}\n\n{footer}"

        # Instru√ß√£o de idioma de sa√≠da (padr√£o: mesmo idioma do √°udio de entrada)
        output_lang = getattr(self, "_output_language", None)
        input_lang = getattr(self, "_current_language", "pt") or "pt"
        effective_lang = output_lang or input_lang

        lang_names = {
            "en": "English",
            "es": "espa√±ol",
            "fr": "fran√ßais",
            "de": "Deutsch",
            "pt": "portugu√™s",
        }

        if effective_lang == "auto":
            # Auto-detect: instruir LLM a manter o idioma do texto de entrada
            composed += (
                "\n\n## IDIOMA DE SA√çDA\n"
                "- O idioma do √°udio foi detectado automaticamente.\n"
                "- Identifique o idioma do texto de entrada e escreva TODA a sa√≠da nesse MESMO idioma.\n"
                "- T√≠tulos, tabelas, legendas e conte√∫do devem estar no idioma original.\n"
                "- N√ÉO traduza para portugu√™s se o √°udio n√£o for em portugu√™s.\n"
            )
        elif effective_lang and effective_lang != "pt":
            lang_name = lang_names.get(effective_lang, effective_lang)
            composed += (
                f"\n\n## IDIOMA DE SA√çDA\n"
                f"- O √°udio de entrada est√° em **{lang_name}**.\n"
                f"- Toda a formata√ß√£o, t√≠tulos, tabelas e conte√∫do DEVEM ser escritos em **{lang_name}**.\n"
                f"- N√ÉO traduza para portugu√™s. Mantenha o idioma original do √°udio.\n"
            )

        return composed

    def create_context_cache(self, transcription, global_structure=None):
        """
        v2.2: Wrapper method that calls criar_cache_contexto with class attributes.
        
        Args:
            transcription: Full transcription text
            global_structure: Mapped structure (optional)
        
        Returns:
            Cache object or None
        """
        return criar_cache_contexto(
            client=self.client,
            transcricao_completa=transcription,
            system_prompt=self.prompt_apostila,  # Uses current mode's prompt
            estrutura_global=global_structure,
            model_name=self.llm_model
        )

    def optimize_audio(self, file_path):
        """Extrai √°udio otimizado (16kHz mono)"""
        print(f"{Fore.YELLOW}‚ö° Verificando √°udio...")

        mp3_path = Path(file_path).with_suffix('.mp3')
        if mp3_path.exists():
            print(f"   üìÇ Usando MP3 existente: {mp3_path.name}")
            return str(mp3_path)

        # Cache baseado em nome do arquivo + tamanho (independente do job ID)
        file_name = Path(file_path).name
        try:
            file_size = os.path.getsize(file_path)
        except OSError:
            file_size = 0
        cache_key = f"{file_name}_{file_size}"
        file_hash = hashlib.md5(cache_key.encode()).hexdigest()[:8]
        output_path = f"temp_{file_hash}.wav"

        if os.path.exists(output_path):
            print(f"   ‚ôªÔ∏è Cache encontrado: {output_path} (mesmo arquivo: {file_name})")
            return output_path
        
        print(f"   üîÑ Extraindo √°udio...")
        enable_loudnorm = str(os.environ.get("IUDEX_AUDIO_LOUDNORM", "1")).strip().lower() not in {"0", "false", "no", "off"}
        ffmpeg_cmd = [
            "ffmpeg",
            "-i",
            file_path,
            "-vn",  # Sem v√≠deo
            "-sn",  # Sem legendas
            "-dn",  # Sem data streams
            "-map",
            "0:a:0?",
        ]
        if enable_loudnorm:
            ffmpeg_cmd += ["-af", "loudnorm=I=-16:TP=-1.5:LRA=11"]
        ffmpeg_cmd += [
            "-ac",
            "1",  # Mono
            "-ar",
            "16000",  # 16kHz para Whisper
            "-acodec",
            "pcm_s16le",
            output_path,
            "-y",
            "-hide_banner",
            "-loglevel",
            "error",
        ]
        subprocess.run(ffmpeg_cmd, check=True, capture_output=True)
        
        return output_path

    def _resolve_whisper_language(self) -> Optional[str]:
        """Resolve o c√≥digo de idioma para o Whisper. None = auto-detect."""
        lang = getattr(self, "_current_language", "pt") or "pt"
        return self.SUPPORTED_LANGUAGES.get(lang, lang if lang != "auto" else None)

    def _detect_speech_segments_silero(self, audio_path: str) -> list[dict]:
        """
        Detecta segmentos de fala usando Silero VAD (mais preciso que RMS).

        Returns:
            Lista de dicts {'start': float, 'end': float} com segmentos de fala em segundos.
        """
        try:
            from silero_vad import load_silero_vad, read_audio, get_speech_timestamps

            model = load_silero_vad()
            wav = read_audio(audio_path)

            # Obter timestamps de fala (em segundos)
            speech_timestamps = get_speech_timestamps(
                wav,
                model,
                return_seconds=True,
                min_speech_duration_ms=500,   # Ignora sons < 0.5s
                min_silence_duration_ms=500,  # Une pausas < 0.5s
            )

            return speech_timestamps

        except Exception as e:
            print(f"{Fore.YELLOW}   ‚ö†Ô∏è Silero VAD falhou: {e}{Style.RESET_ALL}")
            return []

    def _detect_speech_start_silero(self, audio_path: str) -> float:
        """
        Detecta onde a fala come√ßa usando Silero VAD.

        Returns:
            Offset em segundos onde a fala come√ßa (0 se n√£o detectar sil√™ncio inicial)
        """
        segments = self._detect_speech_segments_silero(audio_path)

        if segments and len(segments) > 0:
            first_speech = segments[0].get('start', 0)
            if first_speech > 0:
                print(f"{Fore.YELLOW}   üîá Silero VAD: Detectado sil√™ncio inicial de {first_speech:.0f}s{Style.RESET_ALL}")
            return first_speech

        return 0.0

    def _detect_speech_start_rms(self, audio_path: str, chunk_seconds: float = 30.0, threshold_db: float = -40.0) -> float:
        """
        Fallback: Detecta onde a fala come√ßa usando RMS (energia do sinal).
        Usado se Silero VAD n√£o estiver dispon√≠vel.
        """
        import subprocess
        import re

        try:
            probe_cmd = [
                "ffprobe", "-v", "error", "-show_entries", "format=duration",
                "-of", "default=noprint_wrappers=1:nokey=1", audio_path
            ]
            duration_result = subprocess.run(probe_cmd, capture_output=True, text=True)
            total_duration = float(duration_result.stdout.strip())

            speech_start = 0.0
            max_chunks_to_check = min(20, int(total_duration / chunk_seconds))

            for i in range(max_chunks_to_check):
                offset = i * chunk_seconds
                cmd = [
                    "ffmpeg", "-ss", str(offset), "-t", str(chunk_seconds),
                    "-i", audio_path, "-af", "volumedetect", "-f", "null", "-"
                ]
                result = subprocess.run(cmd, capture_output=True, text=True, stderr=subprocess.STDOUT)
                output = result.stdout + result.stderr if result.stderr else result.stdout

                match = re.search(r"mean_volume:\s*([-\d.]+)\s*dB", output)
                if match:
                    mean_vol = float(match.group(1))
                    if mean_vol > threshold_db:
                        if i > 0:
                            fine_start = max(0, (i - 1) * chunk_seconds)
                            fine_chunk = 5.0
                            for j in range(int(chunk_seconds / fine_chunk) + 2):
                                fine_offset = fine_start + j * fine_chunk
                                if fine_offset >= offset + chunk_seconds:
                                    break
                                cmd_fine = [
                                    "ffmpeg", "-ss", str(fine_offset), "-t", str(fine_chunk),
                                    "-i", audio_path, "-af", "volumedetect", "-f", "null", "-"
                                ]
                                result_fine = subprocess.run(cmd_fine, capture_output=True, text=True, stderr=subprocess.STDOUT)
                                output_fine = result_fine.stdout + (result_fine.stderr or "")
                                match_fine = re.search(r"mean_volume:\s*([-\d.]+)\s*dB", output_fine)
                                if match_fine and float(match_fine.group(1)) > threshold_db:
                                    speech_start = fine_offset
                                    break
                            else:
                                speech_start = offset
                        break

            if speech_start > 0:
                print(f"{Fore.YELLOW}   üîá RMS VAD: Detectado sil√™ncio inicial de {speech_start:.0f}s{Style.RESET_ALL}")

            return speech_start

        except Exception as e:
            print(f"{Fore.YELLOW}   ‚ö†Ô∏è RMS VAD falhou: {e}{Style.RESET_ALL}")
            return 0.0

    # ==================== CHUNKING DE √ÅUDIO LONGO (v2.32) ====================

    def _get_audio_duration(self, audio_path: str) -> float:
        """
        Obt√©m a dura√ß√£o do √°udio em segundos usando ffprobe.

        v2.34: Melhorada robustez com valida√ß√£o e fallback por tamanho.
        """
        duration = 0.0

        # M√©todo 1: ffprobe
        try:
            cmd = [
                "ffprobe", "-v", "error", "-show_entries", "format=duration",
                "-of", "default=noprint_wrappers=1:nokey=1", audio_path
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            if result.returncode == 0 and result.stdout.strip():
                duration = float(result.stdout.strip())
                if duration > 0:
                    return duration
        except subprocess.TimeoutExpired:
            print(f"{Fore.YELLOW}   ‚ö†Ô∏è ffprobe timeout para {audio_path}{Style.RESET_ALL}")
        except (ValueError, Exception) as e:
            print(f"{Fore.YELLOW}   ‚ö†Ô∏è Erro ffprobe: {e}{Style.RESET_ALL}")

        # M√©todo 2: wave module (apenas para WAV)
        if audio_path.lower().endswith('.wav'):
            try:
                import wave
                with wave.open(audio_path, 'rb') as wav:
                    frames = wav.getnframes()
                    rate = wav.getframerate()
                    if rate > 0:
                        duration = float(frames) / float(rate)
                        if duration > 0:
                            print(f"{Fore.CYAN}   üìè Dura√ß√£o via wave: {duration/3600:.2f}h{Style.RESET_ALL}")
                            return duration
            except Exception:
                pass

        # M√©todo 3: Estimativa por tamanho do arquivo (fallback)
        # MP3 ~128kbps = ~960KB/min, WAV 16kHz mono = ~1.92MB/min
        try:
            file_size_mb = os.path.getsize(audio_path) / (1024 * 1024)
            if audio_path.lower().endswith('.wav'):
                # WAV 16kHz mono 16-bit = ~1.92 MB/min
                estimated_minutes = file_size_mb / 1.92
            else:
                # MP3 ~128kbps = ~0.96 MB/min (conservador)
                estimated_minutes = file_size_mb / 0.96

            estimated_seconds = estimated_minutes * 60
            if estimated_seconds > self.AUDIO_MAX_DURATION_SECONDS:
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è Dura√ß√£o estimada por tamanho: {estimated_seconds/3600:.1f}h (arquivo: {file_size_mb:.0f}MB){Style.RESET_ALL}")
                return estimated_seconds
        except Exception:
            pass

        return duration

    def _split_audio_into_chunks(self, audio_path: str, chunk_duration: float, overlap: float = 30.0) -> list:
        """
        Divide √°udio longo em chunks tempor√°rios.

        v2.32: Evita problemas de mem√≥ria do MLX-Whisper com arquivos muito longos.

        Args:
            audio_path: Caminho do arquivo de √°udio
            chunk_duration: Dura√ß√£o de cada chunk em segundos
            overlap: Overlap entre chunks em segundos (para continuidade)

        Returns:
            Lista de dicts: [{'path': str, 'start': float, 'end': float, 'is_temp': bool}]
        """
        import tempfile

        total_duration = self._get_audio_duration(audio_path)
        if total_duration <= 0:
            return [{'path': audio_path, 'start': 0, 'end': 0, 'is_temp': False}]

        # Se √°udio √© menor que o limite, retorna sem dividir
        if total_duration <= chunk_duration:
            return [{'path': audio_path, 'start': 0, 'end': total_duration, 'is_temp': False}]

        chunks = []
        current_start = 0.0
        chunk_index = 0

        print(f"{Fore.CYAN}   üî™ Dividindo √°udio longo ({total_duration/3600:.1f}h) em chunks de {chunk_duration/3600:.1f}h...{Style.RESET_ALL}")

        while current_start < total_duration:
            chunk_end = min(current_start + chunk_duration, total_duration)
            actual_duration = chunk_end - current_start

            # Criar arquivo tempor√°rio para o chunk
            base_name = Path(audio_path).stem
            temp_dir = tempfile.gettempdir()
            chunk_path = os.path.join(temp_dir, f"{base_name}_chunk{chunk_index}.wav")

            # Extrair chunk com ffmpeg
            cmd = [
                "ffmpeg", "-y",
                "-ss", str(current_start),
                "-i", audio_path,
                "-t", str(actual_duration),
                "-ar", "16000",
                "-ac", "1",
                "-acodec", "pcm_s16le",
                chunk_path,
                "-hide_banner",
                "-loglevel", "error"
            ]

            try:
                subprocess.run(cmd, check=True, capture_output=True)
                chunks.append({
                    'path': chunk_path,
                    'start': current_start,
                    'end': chunk_end,
                    'is_temp': True
                })
                print(f"{Fore.GREEN}      ‚úÇÔ∏è Chunk {chunk_index + 1}: {self._format_timestamp(current_start)} ‚Üí {self._format_timestamp(chunk_end)}{Style.RESET_ALL}")
            except subprocess.CalledProcessError as e:
                print(f"{Fore.RED}      ‚ùå Erro ao criar chunk {chunk_index}: {e}{Style.RESET_ALL}")

            # Pr√≥ximo chunk com overlap
            current_start = chunk_end - overlap
            if current_start >= total_duration - overlap:
                break
            chunk_index += 1

        print(f"{Fore.GREEN}   ‚úÖ Criados {len(chunks)} chunks de √°udio{Style.RESET_ALL}")
        return chunks

    def _cleanup_audio_chunks(self, chunks: list):
        """Remove arquivos tempor√°rios de chunks."""
        for chunk in chunks:
            if chunk.get('is_temp') and os.path.exists(chunk['path']):
                try:
                    os.unlink(chunk['path'])
                except Exception:
                    pass

    def _merge_chunk_segments(self, all_segments: list, overlap_seconds: float = 30.0) -> list:
        """
        Mescla segmentos de m√∫ltiplos chunks, removendo duplicatas do overlap.

        v2.32: Usa fingerprinting de texto para detectar e remover duplicatas.
        v2.34: Melhorado algoritmo de detec√ß√£o de duplicatas com m√∫ltiplas estrat√©gias.
        """
        if not all_segments:
            return []

        merged = []
        last_end_time = 0.0
        recent_texts = []  # Buffer das √∫ltimas N frases para detectar duplicatas

        def normalize_for_compare(text: str) -> str:
            """Normaliza texto para compara√ß√£o (lowercase, sem pontua√ß√£o extra)."""
            import re
            text = (text or '').strip().lower()
            text = re.sub(r'[^\w\s]', '', text)  # Remove pontua√ß√£o
            text = re.sub(r'\s+', ' ', text)  # Normaliza espa√ßos
            return text

        def is_duplicate(new_text: str, recent: list) -> bool:
            """Verifica se texto √© duplicata de algum texto recente."""
            new_norm = normalize_for_compare(new_text)
            if not new_norm or len(new_norm) < 10:
                return False

            for old_text in recent:
                old_norm = normalize_for_compare(old_text)
                if not old_norm:
                    continue

                # Estrat√©gia 1: Texto exatamente igual
                if new_norm == old_norm:
                    return True

                # Estrat√©gia 2: Um cont√©m o outro (substring)
                if len(new_norm) > 20 and len(old_norm) > 20:
                    if new_norm in old_norm or old_norm in new_norm:
                        return True

                # Estrat√©gia 3: Similaridade alta (Jaccard de palavras)
                new_words = set(new_norm.split())
                old_words = set(old_norm.split())
                if new_words and old_words:
                    intersection = len(new_words & old_words)
                    union = len(new_words | old_words)
                    if union > 0 and intersection / union > 0.8:  # 80% similaridade
                        return True

                # Estrat√©gia 4: In√≠cio igual (primeiras N palavras)
                new_start = ' '.join(new_norm.split()[:8])
                old_start = ' '.join(old_norm.split()[:8])
                if len(new_start) > 20 and new_start == old_start:
                    return True

            return False

        for chunk_idx, segments in enumerate(all_segments):
            chunk_start_time = segments[0].get('start', 0) if segments else 0

            for seg in segments:
                seg_start = seg.get('start', 0)
                seg_text = (seg.get('text') or '').strip()

                if not seg_text:
                    continue

                # Para segmentos no per√≠odo de overlap, verificar duplicatas mais rigorosamente
                in_overlap_zone = seg_start < last_end_time + overlap_seconds * 0.5

                if in_overlap_zone and chunk_idx > 0:
                    # Verificar se √© duplicata
                    if is_duplicate(seg_text, recent_texts):
                        continue

                merged.append(seg)
                last_end_time = max(last_end_time, seg.get('end', seg_start))

                # Manter buffer das √∫ltimas 10 frases para compara√ß√£o
                recent_texts.append(seg_text)
                if len(recent_texts) > 10:
                    recent_texts.pop(0)

        print(f"{Fore.CYAN}   üîó Merge: {sum(len(s) for s in all_segments)} ‚Üí {len(merged)} segmentos (removidas duplicatas do overlap){Style.RESET_ALL}")
        return merged

    def _transcribe_chunked(self, audio_path: str, *, beam_size: Optional[int] = None, cache_file: str = None, initial_prompt: str = "") -> str:
        """
        Transcreve √°udio longo dividindo em chunks de 3h cada.

        v2.32: Evita degrada√ß√£o do MLX-Whisper com arquivos muito longos.
        O Whisper pode gerar apenas pontua√ß√£o quando processa √°udios > 3-4h de uma vez.
        """
        print(f"{Fore.CYAN}   üé¨ Iniciando transcri√ß√£o em chunks (m√°x {self.AUDIO_MAX_DURATION_SECONDS/3600:.0f}h cada)...{Style.RESET_ALL}")
        start_time = time.time()

        # Dividir √°udio em chunks
        chunks = self._split_audio_into_chunks(
            audio_path,
            chunk_duration=self.AUDIO_MAX_DURATION_SECONDS,
            overlap=self.AUDIO_CHUNK_OVERLAP_SECONDS
        )

        if not chunks:
            print(f"{Fore.RED}   ‚ùå Falha ao dividir √°udio em chunks{Style.RESET_ALL}")
            return ""

        all_segments = []
        whisper_lang = self._resolve_whisper_language()
        no_speech_thresh = float(os.getenv("VOMO_NO_SPEECH_THRESHOLD", "0.8"))

        mlx_kwargs = dict(
            path_or_hf_repo=f"mlx-community/whisper-{self.model_name}",
            **({"language": whisper_lang} if whisper_lang else {}),
            temperature=0.0,
            initial_prompt=(initial_prompt or None),
            word_timestamps=True,
            fp16=True,
            no_speech_threshold=no_speech_thresh,
            logprob_threshold=-0.5,
            compression_ratio_threshold=2.2,
            condition_on_previous_text=self._condition_on_previous,
            suppress_tokens=[-1],
            verbose=False,
        )
        if beam_size and beam_size > 1:
            mlx_kwargs["beam_size"] = int(beam_size)

        try:
            for i, chunk in enumerate(chunks):
                chunk_path = chunk['path']
                chunk_start = chunk['start']
                chunk_end = chunk['end']

                print(f"{Fore.CYAN}   üìù Transcrevendo chunk {i+1}/{len(chunks)} ({self._format_timestamp(chunk_start)} ‚Üí {self._format_timestamp(chunk_end)})...{Style.RESET_ALL}")

                try:
                    result = self._transcribe_with_vad(chunk_path, mlx_kwargs, skip_silence=True)
                except TypeError:
                    mlx_kwargs_copy = dict(mlx_kwargs)
                    mlx_kwargs_copy.pop("beam_size", None)
                    mlx_kwargs_copy.pop("best_of", None)
                    result = self._transcribe_with_vad(chunk_path, mlx_kwargs_copy, skip_silence=True)

                segments = result.get("segments", [])

                # Ajustar timestamps para o offset do chunk (segmentos E words)
                for seg in segments:
                    seg['start'] = seg.get('start', 0) + chunk_start
                    seg['end'] = seg.get('end', 0) + chunk_start
                    # v2.33: Ajustar tamb√©m timestamps das words individuais
                    if 'words' in seg and seg['words']:
                        for word in seg['words']:
                            if 'start' in word:
                                word['start'] = word['start'] + chunk_start
                            if 'end' in word:
                                word['end'] = word['end'] + chunk_start

                all_segments.append(segments)
                print(f"{Fore.GREEN}      ‚úÖ Chunk {i+1}: {len(segments)} segmentos{Style.RESET_ALL}")

        finally:
            # Limpar arquivos tempor√°rios
            self._cleanup_audio_chunks(chunks)

        # Mesclar segmentos de todos os chunks
        merged_segments = self._merge_chunk_segments(all_segments, overlap_seconds=self.AUDIO_CHUNK_OVERLAP_SECONDS)

        # Filtrar segmentos
        segments, filter_stats = self._filter_asr_segments(merged_segments)
        if filter_stats.get("dropped"):
            reasons = ", ".join(
                f"{k}={v}" for k, v in sorted((filter_stats.get("reason_counts") or {}).items())
            )
            print(f"{Fore.YELLOW}   üßπ ASR: removidos {filter_stats['dropped']} segmento(s) suspeitos ({reasons}){Style.RESET_ALL}")

        # Formatar resultado
        lines = []
        current_block = []
        last_timestamp = None

        for segment in segments:
            start = segment['start']
            text = segment['text'].strip()

            if not text:
                continue

            text = self._normalize_raw_text(text)

            if self._should_add_timestamp(start, last_timestamp, interval_seconds=self._get_timestamp_interval_for_mode()):
                if current_block:
                    lines.append(" ".join(current_block))
                    current_block = []

                ts = self._format_timestamp(start)
                current_block.append(f"[{ts}] {text}")
                last_timestamp = start
            else:
                current_block.append(text)

        if current_block:
            lines.append(" ".join(current_block))

        transcript_result = "\n\n".join(lines).strip()
        transcript_result = self._strip_leaked_initial_prompt(transcript_result, initial_prompt)

        elapsed = time.time() - start_time
        print(f"{Fore.GREEN}   ‚úÖ Transcri√ß√£o chunked conclu√≠da em {elapsed:.1f}s ({len(chunks)} chunks, {len(segments)} segmentos){Style.RESET_ALL}")

        # Salvar cache
        if cache_file:
            try:
                with open(cache_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        'transcript': transcript_result,
                        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                        'chunks': len(chunks)
                    }, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"Erro ao salvar cache: {e}")

        return transcript_result

    def _transcribe_with_segments_chunked(self, audio_path: str, *, beam_size: Optional[int] = None) -> dict:
        """
        v2.33: Transcreve √°udio longo em chunks com suporte a segmentos e diariza√ß√£o.

        Divide o √°udio em chunks menores, transcreve cada um, ajusta timestamps,
        e opcionalmente aplica diariza√ß√£o por chunk para evitar estouro de mem√≥ria.
        """
        import gc

        print(f"{Fore.CYAN}   üé¨ Iniciando transcri√ß√£o chunked com segmentos (m√°x {self.AUDIO_MAX_DURATION_SECONDS/3600:.0f}h cada)...{Style.RESET_ALL}")
        start_time = time.time()

        # Dividir √°udio em chunks
        chunks = self._split_audio_into_chunks(
            audio_path,
            chunk_duration=self.AUDIO_MAX_DURATION_SECONDS,
            overlap=self.AUDIO_CHUNK_OVERLAP_SECONDS
        )

        if not chunks:
            print(f"{Fore.RED}   ‚ùå Falha ao dividir √°udio em chunks{Style.RESET_ALL}")
            return {"text": "", "segments": [], "words": [], "diarization": []}

        # Preparar kwargs do Whisper
        initial_prompt = self._get_whisper_initial_prompt_for_asr(high_accuracy=bool(beam_size and beam_size > 1)) or ""
        whisper_lang = self._resolve_whisper_language()
        mlx_kwargs = dict(
            path_or_hf_repo=f"mlx-community/whisper-{self.model_name}",
            **({"language": whisper_lang} if whisper_lang else {}),
            temperature=0.0,
            initial_prompt=(initial_prompt or None),
            word_timestamps=True,
            fp16=True,
            no_speech_threshold=float(os.getenv("VOMO_NO_SPEECH_THRESHOLD", "0.8")),
            logprob_threshold=-1.0,
            compression_ratio_threshold=float(os.getenv("VOMO_COMPRESSION_THRESHOLD", "2.2")),
            condition_on_previous_text=self._condition_on_previous,
            suppress_tokens=[-1],
            verbose=False,
        )
        if beam_size and beam_size > 1:
            mlx_kwargs["beam_size"] = int(beam_size)
            mlx_kwargs["best_of"] = int(beam_size)

        all_segments = []
        all_diarization = []
        token = self._get_hf_token() if self._diarization_enabled else None
        diarization_pipeline = None

        # Inicializar pipeline de diariza√ß√£o uma vez (se habilitado)
        if self._diarization_enabled and Pipeline and token:
            try:
                self._ensure_diarization_available_or_raise()
                diarization_pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-community-1",
                    token=token
                )
                device = "mps" if torch.backends.mps.is_available() else "cpu"
                diarization_pipeline.to(torch.device(device))
                print(f"{Fore.GREEN}   ‚úÖ Pipeline de diariza√ß√£o inicializado ({device}){Style.RESET_ALL}")
            except Exception as e:
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è Erro ao inicializar diariza√ß√£o: {e}{Style.RESET_ALL}")
                diarization_pipeline = None

        try:
            for i, chunk in enumerate(chunks):
                chunk_path = chunk['path']
                chunk_start = chunk['start']
                chunk_end = chunk['end']

                print(f"{Fore.CYAN}   üìù Transcrevendo chunk {i+1}/{len(chunks)} ({self._format_timestamp(chunk_start)} ‚Üí {self._format_timestamp(chunk_end)})...{Style.RESET_ALL}")

                # Transcrever chunk
                try:
                    result = self._transcribe_with_vad(chunk_path, mlx_kwargs, skip_silence=True)
                except TypeError:
                    mlx_kwargs_copy = dict(mlx_kwargs)
                    mlx_kwargs_copy.pop("beam_size", None)
                    mlx_kwargs_copy.pop("best_of", None)
                    result = self._transcribe_with_vad(chunk_path, mlx_kwargs_copy, skip_silence=True)

                segments = result.get("segments", [])

                # Ajustar timestamps (segmentos e words)
                for seg in segments:
                    seg['start'] = seg.get('start', 0) + chunk_start
                    seg['end'] = seg.get('end', 0) + chunk_start
                    if 'words' in seg and seg['words']:
                        for word in seg['words']:
                            if 'start' in word:
                                word['start'] = word['start'] + chunk_start
                            if 'end' in word:
                                word['end'] = word['end'] + chunk_start

                # Diariza√ß√£o por chunk (se habilitado)
                if diarization_pipeline:
                    try:
                        diarization = diarization_pipeline(chunk_path)
                        for turn, _, speaker in diarization.itertracks(yield_label=True):
                            speaker_id = speaker.split('_')[-1]
                            all_diarization.append({
                                "start": float(turn.start) + chunk_start,
                                "end": float(turn.end) + chunk_start,
                                "speaker_label": f"SPEAKER {int(speaker_id) + 1}"
                            })
                        # Criar segmentos tempor√°rios com timestamps originais do chunk para atribui√ß√£o
                        temp_segments = []
                        for seg in segments:
                            temp_seg = dict(seg)
                            temp_seg['start'] = seg['start'] - chunk_start
                            temp_seg['end'] = seg['end'] - chunk_start
                            temp_segments.append(temp_seg)
                        # Atribuir labels de diariza√ß√£o
                        labeled_temp = self._assign_diarization_labels(temp_segments, diarization)
                        # Copiar labels de volta para segments
                        for seg, labeled in zip(segments, labeled_temp):
                            seg['speaker_label'] = labeled.get('speaker_label', 'SPEAKER 1')
                    except Exception as e:
                        print(f"{Fore.YELLOW}      ‚ö†Ô∏è Diariza√ß√£o chunk {i+1} falhou: {e}{Style.RESET_ALL}")
                        for seg in segments:
                            seg['speaker_label'] = "SPEAKER 1"
                else:
                    # Sem diariza√ß√£o - atribuir SPEAKER 1
                    for seg in segments:
                        seg['speaker_label'] = "SPEAKER 1"

                all_segments.append(segments)
                print(f"{Fore.GREEN}      ‚úÖ Chunk {i+1}: {len(segments)} segmentos{Style.RESET_ALL}")

                # Liberar mem√≥ria entre chunks
                gc.collect()
                if torch and torch.backends.mps.is_available():
                    try:
                        torch.mps.empty_cache()
                    except Exception:
                        pass

        finally:
            # Limpar arquivos tempor√°rios
            self._cleanup_audio_chunks(chunks)

        # Mesclar segmentos de todos os chunks
        merged_segments = self._merge_chunk_segments(all_segments, overlap_seconds=self.AUDIO_CHUNK_OVERLAP_SECONDS)

        # Filtrar segmentos suspeitos
        filtered_segments, filter_stats = self._filter_asr_segments(merged_segments)
        if filter_stats.get("dropped"):
            reasons = ", ".join(
                f"{k}={v}" for k, v in sorted((filter_stats.get("reason_counts") or {}).items())
            )
            print(f"{Fore.YELLOW}   üßπ ASR: removidos {filter_stats['dropped']} segmento(s) suspeitos ({reasons}){Style.RESET_ALL}")

        # Garantir que todos os segmentos t√™m speaker_label
        for seg in filtered_segments:
            if 'speaker_label' not in seg:
                seg['speaker_label'] = "SPEAKER 1"
            if 'words' not in seg:
                seg['words'] = []

        # Extrair lista flat de words
        all_words = []
        for seg in filtered_segments:
            seg_words = seg.get("words", [])
            speaker = seg.get("speaker_label", "")
            for w in seg_words:
                all_words.append({
                    "word": w.get("word", w.get("text", "")),
                    "start": w.get("start", 0),
                    "end": w.get("end", 0),
                    "speaker": speaker,
                })

        # Gerar texto formatado
        transcript_text = self._segments_to_text(filtered_segments)
        transcript_text = self._strip_leaked_initial_prompt(transcript_text, initial_prompt)

        elapsed = time.time() - start_time
        print(f"{Fore.GREEN}   ‚úÖ Transcri√ß√£o chunked conclu√≠da em {elapsed:.1f}s ({len(chunks)} chunks, {len(filtered_segments)} segmentos){Style.RESET_ALL}")

        return {
            "text": transcript_text,
            "segments": filtered_segments,
            "words": all_words,
            "diarization": all_diarization
        }

    def _transcribe_with_vad(self, audio_path: str, mlx_kwargs: dict, skip_silence: bool = True) -> dict:
        """
        Transcreve √°udio com detec√ß√£o de atividade de voz (VAD).

        Pipeline:
        1. Silero VAD detecta onde h√° fala (mais preciso)
        2. Fallback para RMS se Silero falhar
        3. Se sil√™ncio inicial > 30s, pula para economizar processamento
        """
        import tempfile

        speech_start = 0.0
        temp_audio = None

        if skip_silence and _env_truthy("VOMO_VAD_SKIP_SILENCE", default=True):
            # Tentar Silero VAD primeiro (mais preciso)
            try:
                speech_start = self._detect_speech_start_silero(audio_path)
            except Exception as e:
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è Silero VAD indispon√≠vel, usando RMS: {e}{Style.RESET_ALL}")
                speech_start = self._detect_speech_start_rms(audio_path)

            # Se h√° mais de 30s de sil√™ncio inicial, criar arquivo sem o sil√™ncio
            if speech_start > 30:
                print(f"{Fore.CYAN}   ‚úÇÔ∏è Pulando {speech_start:.0f}s de sil√™ncio inicial...{Style.RESET_ALL}")

                # Criar arquivo tempor√°rio sem o sil√™ncio
                temp_fd, temp_audio = tempfile.mkstemp(suffix=".wav")
                os.close(temp_fd)

                cmd = [
                    "ffmpeg", "-y", "-ss", str(speech_start), "-i", audio_path,
                    "-ar", "16000", "-ac", "1", "-acodec", "pcm_s16le",
                    temp_audio, "-hide_banner", "-loglevel", "error"
                ]
                subprocess.run(cmd, check=True, capture_output=True)
                audio_path = temp_audio

        try:
            result = mlx_whisper.transcribe(audio_path, **mlx_kwargs)

            # Ajustar timestamps se pulamos sil√™ncio (segmentos E words)
            if speech_start > 0 and result.get("segments"):
                for seg in result["segments"]:
                    seg["start"] = seg.get("start", 0) + speech_start
                    seg["end"] = seg.get("end", 0) + speech_start
                    # v2.34: Ajustar tamb√©m timestamps das words individuais
                    if seg.get("words"):
                        for word in seg["words"]:
                            if "start" in word:
                                word["start"] = word["start"] + speech_start
                            if "end" in word:
                                word["end"] = word["end"] + speech_start
                if "duration" in result:
                    result["duration"] = result["duration"] + speech_start

            return result

        finally:
            # Limpar arquivo tempor√°rio
            if temp_audio and os.path.exists(temp_audio):
                try:
                    os.unlink(temp_audio)
                except Exception:
                    pass

    def transcribe(self, audio_path, *, beam_size: Optional[int] = None):
        """
        MLX-Whisper OTIMIZADO com GPU acelerado + Diariza√ß√£o

        Otimiza√ß√µes v2.0:
        - VAD filtering (pula sil√™ncio)
        - Batched inference (m√∫ltiplos chunks GPU)
        - condition_on_previous_text (contexto melhorado)
        - Hallucination suppression (evita texto inventado)
        """
        print(f"{Fore.GREEN}üéôÔ∏è  Iniciando transcri√ß√£o OTIMIZADA (MLX GPU)...")
        start_time = time.time()
        
        # Cache de transcri√ß√£o (separa diariza√ß√£o ON/OFF + hash de par√¢metros)
        # Importante: incluir par√¢metros que mudam o output (ex.: initial_prompt).
        initial_prompt = self._get_whisper_initial_prompt_for_asr(high_accuracy=bool(beam_size and beam_size > 1)) or ""
        prompt_hash = hashlib.sha256(initial_prompt.encode()).hexdigest()[:8] if initial_prompt else "noprompt"
        clean_enabled = _env_truthy("VOMO_FILTER_ASR_HALLUCINATIONS", default=True)
        params_str = f"{self.model_name}_{self._diarization_enabled}_{self._condition_on_previous}_{prompt_hash}_clean{int(bool(clean_enabled))}"
        if beam_size and beam_size > 1:
            params_str += f"_beam{int(beam_size)}"
        params_hash = hashlib.sha256(params_str.encode()).hexdigest()[:8]
        cache_tag = "DIARIZATION" if self._diarization_enabled else "ASR"
        cache_file = audio_path.replace('.wav', f'_{cache_tag}_{params_hash}.json').replace('.mp3', f'_{cache_tag}_{params_hash}.json')
        
        if os.path.exists(cache_file):
            try:
                print(f"{Fore.CYAN}   üìÇ Cache encontrado, carregando...")
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                return cache_data['transcript']
            except Exception:
                pass

        if not mlx_whisper:
            raise ImportError("mlx_whisper n√£o instalado.")

        # ==================== CHUNKING DE √ÅUDIO LONGO (v2.32) ====================
        audio_duration = self._get_audio_duration(audio_path)
        max_duration = self.AUDIO_MAX_DURATION_SECONDS

        # v2.34: Log detalhado para debug
        print(f"{Fore.CYAN}   üìè Dura√ß√£o detectada: {audio_duration/3600:.2f}h (limite: {max_duration/3600:.0f}h){Style.RESET_ALL}")

        if audio_duration > max_duration:
            print(f"{Fore.YELLOW}   ‚ö†Ô∏è √Åudio longo detectado ({audio_duration/3600:.1f}h > {max_duration/3600:.0f}h) - ATIVANDO CHUNKING{Style.RESET_ALL}")
            return self._transcribe_chunked(audio_path, beam_size=beam_size, cache_file=cache_file, initial_prompt=initial_prompt)
        elif audio_duration == 0:
            print(f"{Fore.RED}   ‚ùå AVISO: Dura√ß√£o n√£o detectada! Chunking desabilitado. Arquivo: {audio_path}{Style.RESET_ALL}")

        # ==================== PAR√ÇMETROS OTIMIZADOS ====================
        print("   üîç Transcrevendo com par√¢metros otimizados...")
        
        whisper_lang = self._resolve_whisper_language()

        # v2.31: Threshold mais alto para filtrar sil√™ncio/ru√≠do melhor
        no_speech_thresh = float(os.getenv("VOMO_NO_SPEECH_THRESHOLD", "0.8"))

        mlx_kwargs = dict(
            path_or_hf_repo=f"mlx-community/whisper-{self.model_name}",
            **({"language": whisper_lang} if whisper_lang else {}),
            # === PRECIS√ÉO ===
            temperature=0.0,  # Mais determin√≠stico (desativa sampling estoc√°stico)
            # === CONTEXTO E GLOSS√ÅRIO (v2.29: contextual por modo) ===
            initial_prompt=(initial_prompt or None),
            # === TIMESTAMPS ===
            word_timestamps=True,
            # === PERFORMANCE ===
            fp16=True,  # Usa float16 (mais r√°pido na GPU)
            # === QUALIDADE (Hallucination Suppression) - v2.31: thresholds ajustados ===
            no_speech_threshold=no_speech_thresh,  # v2.31: 0.8 (era 0.6) - mais agressivo em sil√™ncio
            logprob_threshold=-0.5,  # v2.31: -0.5 (era -1.0) - rejeita tokens menos confiantes
            compression_ratio_threshold=2.2,  # v2.31: 2.2 (era 2.4) - detecta repeti√ß√£o mais cedo
            # === CONTEXTO ===
            condition_on_previous_text=self._condition_on_previous,  # v2.30: configur√°vel via VOMO_CONDITION_PREVIOUS
            # === SUPRESS√ÉO DE TOKENS PROBLEM√ÅTICOS ===
            suppress_tokens=[-1],  # Suprime token de padding
            verbose=False,
        )
        if beam_size and beam_size > 1:
            mlx_kwargs["beam_size"] = int(beam_size)
            # `best_of` s√≥ √© aceito em algumas implementa√ß√µes; aplicamos best-effort.
            mlx_kwargs["best_of"] = int(beam_size)

        # v2.31: Usar VAD para pular sil√™ncio inicial extenso
        try:
            result = self._transcribe_with_vad(audio_path, mlx_kwargs, skip_silence=True)
        except TypeError:
            # Compatibilidade: algumas vers√µes aceitam `beam_size` mas n√£o `best_of` (ou vice-versa).
            if "best_of" in mlx_kwargs:
                mlx_kwargs.pop("best_of", None)
                try:
                    result = self._transcribe_with_vad(audio_path, mlx_kwargs, skip_silence=True)
                except TypeError:
                    mlx_kwargs.pop("beam_size", None)
                    result = self._transcribe_with_vad(audio_path, mlx_kwargs, skip_silence=True)
            else:
                mlx_kwargs.pop("beam_size", None)
                result = self._transcribe_with_vad(audio_path, mlx_kwargs, skip_silence=True)

        segments, filter_stats = self._filter_asr_segments(result.get("segments", []))
        if filter_stats.get("dropped"):
            reasons = ", ".join(
                f"{k}={v}" for k, v in sorted((filter_stats.get("reason_counts") or {}).items())
            )
            print(f"{Fore.YELLOW}   üßπ ASR: removidos {filter_stats['dropped']} segmento(s) suspeitos ({reasons})")
        
        elapsed = time.time() - start_time
        audio_duration = result.get('duration', 0) if isinstance(result, dict) else 0
        rtf = elapsed / audio_duration if audio_duration > 0 else 0
        
        print(f"{Fore.GREEN}   ‚úÖ Transcri√ß√£o conclu√≠da em {elapsed:.1f}s (RTF: {rtf:.2f}x)")
        
        transcript_result = None
        
        # Diariza√ß√£o (condicional por pol√≠tica)
        if self._diarization_enabled:
            self._ensure_diarization_available_or_raise()
        token = self._get_hf_token()
        if self._diarization_enabled and Pipeline and "torch" in globals() and token:
            try:
                print("   üó£Ô∏è  Iniciando Diariza√ß√£o (Pyannote)...")
                pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-community-1",
                    token=token
                )
                
                device = "mps" if torch.backends.mps.is_available() else "cpu"
                pipeline.to(torch.device(device))
                diarization = pipeline(audio_path)
                
                transcript_result = self._align_diarization(segments, diarization)
                print(f"{Fore.GREEN}‚úÖ Diariza√ß√£o conclu√≠da")
            
            except Exception as e:
                print(f"{Fore.YELLOW}‚ö†Ô∏è Erro na Diariza√ß√£o: {e}")
        
        if transcript_result is None:
            # Fallback sem diariza√ß√£o - v2.28: pr√©-formata√ß√£o condensada
            lines = []
            current_block = []
            last_timestamp = None
            
            for segment in segments:
                start = segment['start']
                text = segment['text'].strip()
                
                if not text:
                    continue
                
                # Normaliza√ß√£o leve de ru√≠do
                text = self._normalize_raw_text(text)
                
                # Timestamp a cada 60 segundos
                if self._should_add_timestamp(start, last_timestamp, interval_seconds=self._get_timestamp_interval_for_mode()):
                    # Flush previous block
                    if current_block:
                        lines.append(" ".join(current_block))
                        current_block = []
                    
                    ts = self._format_timestamp(start)
                    current_block.append(f"[{ts}] {text}")
                    last_timestamp = start
                else:
                    current_block.append(text)
            
            # Flush final block
            if current_block:
                lines.append(" ".join(current_block))
            
            transcript_result = "\n\n".join(lines).strip()

        transcript_result = self._strip_leaked_initial_prompt(transcript_result, initial_prompt)
        if _env_truthy("VOMO_ASR_NORMALIZE_TEMAS", default=True):
            try:
                transcript_result, stats = self._normalize_asr_temas_consistency(transcript_result)
                if stats.get("changed", 0) > 0:
                    print(
                        f"{Fore.YELLOW}   üß© ASR: normalizados {stats['changed']} tema(s) inconsistentes "
                        f"(234‚Üí1234: {stats.get('fixed_3_to_4', 0)}, varia√ß√µes: {stats.get('fixed_variants', 0)}){Style.RESET_ALL}"
                    )
            except Exception:
                pass
        
        # Salvar cache
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'transcript': transcript_result,
                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Erro ao salvar cache: {e}")
        
        return transcript_result

    def transcribe_beam_search(self, audio_path):
        """
        Transcri√ß√£o de ALTA PRECIS√ÉO usando faster-whisper com Beam Search.
        
        Usa beam_size=5 para explorar m√∫ltiplos caminhos de frase,
        resultando em transcri√ß√µes mais precisas para termos jur√≠dicos complexos.
        
        Ativar via: --high-accuracy
        """
        beam_size = self._get_asr_beam_size()
        if not FASTER_WHISPER_AVAILABLE:
            print(f"{Fore.YELLOW}‚ö†Ô∏è faster-whisper n√£o instalado. Tentando Beam Search via MLX ({beam_size})...")
            return self.transcribe(audio_path, beam_size=beam_size)
        
        print(f"{Fore.MAGENTA}üéØ Transcri√ß√£o ALTA PRECIS√ÉO (Beam Search)...")
        start_time = time.time()
        
        # Cache de transcri√ß√£o (com hash de par√¢metros para invalida√ß√£o)
        beam_model_size = "large-v3-turbo"
        initial_prompt = self._get_whisper_initial_prompt_for_asr(high_accuracy=True) or ""
        prompt_hash = hashlib.sha256(initial_prompt.encode()).hexdigest()[:8] if initial_prompt else "noprompt"
        cache_params = f"{beam_model_size}_{self._condition_on_previous}_{prompt_hash}_beam{beam_size}"
        beam_hash = hashlib.sha256(cache_params.encode()).hexdigest()[:8]
        cache_file = audio_path.replace('.wav', f'_BEAM_SEARCH_{beam_hash}.json').replace('.mp3', f'_BEAM_SEARCH_{beam_hash}.json')
        
        if os.path.exists(cache_file):
            try:
                print(f"{Fore.CYAN}   üìÇ Cache Beam Search encontrado, carregando...")
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                return cache_data['transcript']
            except Exception:
                pass
        
        # Modelo faster-whisper (CPU/GPU via ctranslate2)
        model_size = "large-v3-turbo"  # Compat√≠vel com o modelo MLX
        print(f"   üì¶ Carregando modelo faster-whisper ({model_size})...")
        
        model = WhisperModel(model_size, device="cpu", compute_type="int8")
        
        whisper_lang = self._resolve_whisper_language()
        segments, info = model.transcribe(
            audio_path,
            language=whisper_lang,
            beam_size=beam_size,           # Explora m√∫ltiplos caminhos de frase
            best_of=beam_size,             # Escolhe o melhor de N candidatos
            patience=1.0,          # Prefer√™ncia por transcri√ß√µes completas
            length_penalty=1.0,    # Evita cortes abruptos
            temperature=0.0,       # Determin√≠stico
            condition_on_previous_text=self._condition_on_previous,
            no_speech_threshold=float(os.getenv("VOMO_NO_SPEECH_THRESHOLD", "0.8")),
            compression_ratio_threshold=float(os.getenv("VOMO_COMPRESSION_THRESHOLD", "2.2")),
            initial_prompt=(initial_prompt or None),
            word_timestamps=True,
        )

        # Formatar output - v2.28: pr√©-formata√ß√£o com line breaks
        lines = []
        last_timestamp = None
        clean_enabled = _env_truthy("VOMO_FILTER_ASR_HALLUCINATIONS", default=True)
        last_key = None
        repeat_run = 0
        
        for segment in segments:
            start = segment.start
            text = segment.text.strip()
            
            if not text:
                continue
            
            # Normaliza√ß√£o leve de ru√≠do
            text = self._normalize_raw_text(text)
            if clean_enabled:
                if self._asr_is_noise_only(text) or self._asr_looks_like_hallucination(text):
                    continue
                key = self._asr_repeat_key(text)
                if key and key == last_key and len(key) <= 80:
                    repeat_run += 1
                    if repeat_run >= 2:
                        continue
                else:
                    last_key = key
                    repeat_run = 0
            
            # Timestamp a cada 30 segundos
            if self._should_add_timestamp(start, last_timestamp, interval_seconds=self._get_timestamp_interval_for_mode()):
                ts = self._format_timestamp(start)
                lines.append(f"[{ts}] {text}")
                last_timestamp = start
            else:
                lines.append(text)
        
        transcript_result = "\n".join(lines).strip()
        transcript_result = self._strip_leaked_initial_prompt(transcript_result, initial_prompt)
        if _env_truthy("VOMO_ASR_NORMALIZE_TEMAS", default=True):
            try:
                transcript_result, stats = self._normalize_asr_temas_consistency(transcript_result)
                if stats.get("changed", 0) > 0:
                    print(
                        f"{Fore.YELLOW}   üß© ASR: normalizados {stats['changed']} tema(s) inconsistentes "
                        f"(234‚Üí1234: {stats.get('fixed_3_to_4', 0)}, varia√ß√µes: {stats.get('fixed_variants', 0)}){Style.RESET_ALL}"
                    )
            except Exception:
                pass
        
        elapsed = time.time() - start_time
        audio_duration = info.duration if info else 0
        rtf = elapsed / audio_duration if audio_duration > 0 else 0
        
        print(f"{Fore.GREEN}   ‚úÖ Transcri√ß√£o Beam Search conclu√≠da em {elapsed:.1f}s (RTF: {rtf:.2f}x)")
        
        # Salvar cache
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'transcript': transcript_result,
                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                    'backend': 'faster-whisper',
                    'beam_size': beam_size
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Erro ao salvar cache: {e}")
        
        return transcript_result
    
    def transcribe_with_segments(self, audio_path, *, beam_size: Optional[int] = None):
        """
        Transcreve e retorna segmentos com timestamps e speaker_label quando diariza√ß√£o estiver dispon√≠vel.

        v2.33: Suporte a chunking para √°udios longos (> AUDIO_MAX_DURATION_SECONDS).
        """
        if not mlx_whisper:
            raise ImportError("mlx_whisper n√£o instalado.")

        # v2.33/v2.34: Verificar se √°udio √© longo e precisa de chunking
        audio_duration = self._get_audio_duration(audio_path)
        max_duration = self.AUDIO_MAX_DURATION_SECONDS

        # v2.34: Log detalhado
        print(f"{Fore.CYAN}   üìè Dura√ß√£o (segments): {audio_duration/3600:.2f}h (limite: {max_duration/3600:.0f}h){Style.RESET_ALL}")

        if audio_duration > max_duration:
            print(f"{Fore.YELLOW}   ‚ö†Ô∏è √Åudio longo detectado ({audio_duration/3600:.1f}h) - ATIVANDO CHUNKING{Style.RESET_ALL}")
            return self._transcribe_with_segments_chunked(audio_path, beam_size=beam_size)
        elif audio_duration == 0:
            print(f"{Fore.RED}   ‚ùå AVISO: Dura√ß√£o n√£o detectada! Arquivo: {audio_path}{Style.RESET_ALL}")

        initial_prompt = self._get_whisper_initial_prompt_for_asr(high_accuracy=bool(beam_size and beam_size > 1)) or ""
        whisper_lang = self._resolve_whisper_language()
        mlx_kwargs = dict(
            path_or_hf_repo=f"mlx-community/whisper-{self.model_name}",
            **({"language": whisper_lang} if whisper_lang else {}),
            temperature=0.0,
            initial_prompt=(initial_prompt or None),
            word_timestamps=True,
            fp16=True,
            no_speech_threshold=float(os.getenv("VOMO_NO_SPEECH_THRESHOLD", "0.8")),
            logprob_threshold=-1.0,
            compression_ratio_threshold=float(os.getenv("VOMO_COMPRESSION_THRESHOLD", "2.2")),
            condition_on_previous_text=self._condition_on_previous,
            suppress_tokens=[-1],
            verbose=False,
        )
        if beam_size and beam_size > 1:
            mlx_kwargs["beam_size"] = int(beam_size)
            mlx_kwargs["best_of"] = int(beam_size)
        try:
            # v2.31: usar VAD tamb√©m no fluxo com segmentos
            result = self._transcribe_with_vad(audio_path, mlx_kwargs, skip_silence=True)
        except TypeError:
            if "best_of" in mlx_kwargs:
                mlx_kwargs.pop("best_of", None)
                try:
                    result = self._transcribe_with_vad(audio_path, mlx_kwargs, skip_silence=True)
                except TypeError:
                    mlx_kwargs.pop("beam_size", None)
                    result = self._transcribe_with_vad(audio_path, mlx_kwargs, skip_silence=True)
            else:
                mlx_kwargs.pop("beam_size", None)
                result = self._transcribe_with_vad(audio_path, mlx_kwargs, skip_silence=True)

        diarization_segments = []
        diarization = None
        labeled_segments = None
        if self._diarization_enabled:
            self._ensure_diarization_available_or_raise()
        token = self._get_hf_token()
        if self._diarization_enabled and Pipeline and "torch" in globals() and token:
            try:
                pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-community-1",
                    token=token
                )
                device = "mps" if torch.backends.mps.is_available() else "cpu"
                pipeline.to(torch.device(device))
                diarization = pipeline(audio_path)
                for turn, _, speaker in diarization.itertracks(yield_label=True):
                    speaker_id = speaker.split('_')[-1]
                    diarization_segments.append({
                        "start": float(turn.start),
                        "end": float(turn.end),
                        "speaker_label": f"SPEAKER {int(speaker_id) + 1}"
                    })
            except Exception as e:
                print(f"{Fore.YELLOW}‚ö†Ô∏è Erro na diariza√ß√£o (segments): {e}")

        if diarization:
            asr_segments, filter_stats = self._filter_asr_segments(result.get("segments", []))
            if filter_stats.get("dropped"):
                reasons = ", ".join(
                    f"{k}={v}" for k, v in sorted((filter_stats.get("reason_counts") or {}).items())
                )
                print(f"{Fore.YELLOW}   üßπ ASR: removidos {filter_stats['dropped']} segmento(s) suspeitos ({reasons})")
            labeled_segments = self._assign_diarization_labels(asr_segments, diarization)
        else:
            asr_segments, filter_stats = self._filter_asr_segments(result.get("segments", []))
            if filter_stats.get("dropped"):
                reasons = ", ".join(
                    f"{k}={v}" for k, v in sorted((filter_stats.get("reason_counts") or {}).items())
                )
                print(f"{Fore.YELLOW}   üßπ ASR: removidos {filter_stats['dropped']} segmento(s) suspeitos ({reasons})")
            labeled_segments = [
                {
                    "start": seg["start"],
                    "end": seg["end"],
                    "text": seg["text"],
                    "speaker_label": "SPEAKER 1",
                    "words": seg.get("words", []),  # Preservar words do Whisper
                }
                for seg in asr_segments
            ]

        # Extrair lista flat de todas as words com timestamps
        all_words = []
        for seg in labeled_segments:
            seg_words = seg.get("words", [])
            speaker = seg.get("speaker_label", "")
            for w in seg_words:
                all_words.append({
                    "word": w.get("word", w.get("text", "")),
                    "start": w.get("start", 0),
                    "end": w.get("end", 0),
                    "speaker": speaker,
                })

        transcript_text = self._segments_to_text(labeled_segments)
        transcript_text = self._strip_leaked_initial_prompt(transcript_text, initial_prompt)
        return {
            "text": transcript_text,
            "segments": labeled_segments,
            "words": all_words,  # Lista flat de words para o player
            "diarization": diarization_segments
        }

    def transcribe_beam_with_segments(self, audio_path):
        """
        Transcri√ß√£o Beam Search com retorno de segmentos.

        v2.33/v2.34: Suporte a chunking para √°udios longos.
        """
        beam_size = self._get_asr_beam_size()

        # v2.33/v2.34: Verificar se √°udio √© longo - delegar para vers√£o com chunking
        audio_duration = self._get_audio_duration(audio_path)
        max_duration = self.AUDIO_MAX_DURATION_SECONDS

        # v2.34: Log detalhado
        print(f"{Fore.CYAN}   üìè Dura√ß√£o (beam+segments): {audio_duration/3600:.2f}h (limite: {max_duration/3600:.0f}h){Style.RESET_ALL}")

        if audio_duration > max_duration:
            print(f"{Fore.YELLOW}   ‚ö†Ô∏è √Åudio longo detectado ({audio_duration/3600:.1f}h) - ATIVANDO CHUNKING{Style.RESET_ALL}")
            return self._transcribe_with_segments_chunked(audio_path, beam_size=beam_size)
        elif audio_duration == 0:
            print(f"{Fore.RED}   ‚ùå AVISO: Dura√ß√£o n√£o detectada! Arquivo: {audio_path}{Style.RESET_ALL}")

        if not FASTER_WHISPER_AVAILABLE:
            print(f"{Fore.YELLOW}‚ö†Ô∏è faster-whisper n√£o instalado. Tentando Beam Search via MLX ({beam_size})...")
            return self.transcribe_with_segments(audio_path, beam_size=beam_size)

        model_size = "large-v3-turbo"
        model = WhisperModel(model_size, device="cpu", compute_type="int8")
        initial_prompt = self._get_whisper_initial_prompt_for_asr(high_accuracy=True) or ""
        whisper_lang = self._resolve_whisper_language()
        segments, info = model.transcribe(
            audio_path,
            language=whisper_lang,
            beam_size=beam_size,
            best_of=beam_size,
            patience=1.0,
            length_penalty=1.0,
            temperature=0.0,
            condition_on_previous_text=self._condition_on_previous,
            no_speech_threshold=float(os.getenv("VOMO_NO_SPEECH_THRESHOLD", "0.8")),
            compression_ratio_threshold=float(os.getenv("VOMO_COMPRESSION_THRESHOLD", "2.2")),
            initial_prompt=(initial_prompt or None),
            word_timestamps=True,
        )

        asr_segments = [
            {
                "start": float(seg.start),
                "end": float(seg.end),
                "text": seg.text,
                "words": [
                    {"word": w.word, "start": float(w.start), "end": float(w.end)}
                    for w in (seg.words or [])
                ] if hasattr(seg, 'words') and seg.words else [],
            }
            for seg in segments
        ]

        diarization_segments = []
        diarization = None
        if self._diarization_enabled:
            self._ensure_diarization_available_or_raise()
        token = self._get_hf_token()
        if self._diarization_enabled and Pipeline and "torch" in globals() and token:
            try:
                pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-community-1",
                    token=token
                )
                device = "mps" if torch.backends.mps.is_available() else "cpu"
                pipeline.to(torch.device(device))
                diarization = pipeline(audio_path)
                for turn, _, speaker in diarization.itertracks(yield_label=True):
                    speaker_id = speaker.split('_')[-1]
                    diarization_segments.append({
                        "start": float(turn.start),
                        "end": float(turn.end),
                        "speaker_label": f"SPEAKER {int(speaker_id) + 1}"
                    })
            except Exception as e:
                print(f"{Fore.YELLOW}‚ö†Ô∏è Erro na diariza√ß√£o (beam segments): {e}")

        if diarization:
            labeled_segments = self._assign_diarization_labels(asr_segments, diarization)
        else:
            labeled_segments = [
                {
                    "start": seg["start"],
                    "end": seg["end"],
                    "text": seg["text"],
                    "speaker_label": "SPEAKER 1",
                    "words": seg.get("words", []),  # Preservar words
                }
                for seg in asr_segments
            ]

        # Extrair lista flat de todas as words com timestamps
        all_words = []
        for seg in labeled_segments:
            seg_words = seg.get("words", [])
            speaker = seg.get("speaker_label", "")
            for w in seg_words:
                all_words.append({
                    "word": w.get("word", ""),
                    "start": w.get("start", 0),
                    "end": w.get("end", 0),
                    "speaker": speaker,
                })

        transcript_text = self._segments_to_text(labeled_segments)
        transcript_text = self._strip_leaked_initial_prompt(transcript_text, initial_prompt)
        return {
            "text": transcript_text,
            "segments": labeled_segments,
            "words": all_words,  # Lista flat de words para o player
            "diarization": diarization_segments
        }

    def _segments_to_text(self, segments, timestamp_interval_seconds=None):
        """
        Converte segmentos em texto pr√©-formatado para melhor chunking.

        v2.29: Pr√©-formata√ß√£o com agrupamento por intervalo de tempo:
        - APOSTILA/FIDELIDADE: agrupa segmentos no mesmo intervalo de 60s em par√°grafos
        - AUDIENCIA/REUNIAO: 1 segmento = 1 linha (por utterance)
        - Mudan√ßa de speaker ‚Üí quebra dupla + header
        - Normaliza√ß√µes leves de ru√≠do
        """
        if not segments:
            return ""

        lines = []
        current_speaker = None
        last_timestamp = None

        # v2.29: Determinar intervalo de agrupamento baseado no modo
        mode = getattr(self, "_current_mode", "APOSTILA").upper()
        group_by_interval = mode in ("APOSTILA", "FIDELIDADE")
        interval = timestamp_interval_seconds if timestamp_interval_seconds is not None else self._get_timestamp_interval_for_mode()

        # Buffer para acumular texto no mesmo intervalo de timestamp
        current_paragraph = []
        paragraph_timestamp = None

        for segment in segments:
            speaker_label = segment.get("speaker_label", "")
            text = (segment.get("text") or "").strip()
            start = segment.get("start") or 0

            # Pular segmentos vazios
            if not text:
                continue

            # Normaliza√ß√£o leve de ru√≠do
            text = self._normalize_raw_text(text)

            # Mudan√ßa de speaker ‚Üí flush buffer e quebra dupla + header
            if speaker_label and speaker_label != current_speaker:
                # Flush buffer atual antes de mudar de speaker
                if current_paragraph:
                    para_text = " ".join(current_paragraph)
                    ts_str = f"[{self._format_timestamp(paragraph_timestamp)}] " if paragraph_timestamp is not None else ""
                    lines.append(f"{ts_str}{para_text}")
                    current_paragraph = []
                    paragraph_timestamp = None

                if lines:  # N√£o adiciona linha em branco se for o primeiro
                    lines.append("")  # Linha em branco para separar
                lines.append(f"{speaker_label}")
                current_speaker = speaker_label
                last_timestamp = None  # Reset timestamp para novo speaker

            # v2.29: Agrupamento por intervalo para APOSTILA/FIDELIDADE
            if group_by_interval and interval > 0:
                # Verificar se deve iniciar novo par√°grafo
                should_new_paragraph = self._should_add_timestamp(start, last_timestamp, interval_seconds=interval)

                if should_new_paragraph:
                    # Flush buffer atual
                    if current_paragraph:
                        para_text = " ".join(current_paragraph)
                        ts_str = f"[{self._format_timestamp(paragraph_timestamp)}] " if paragraph_timestamp is not None else ""
                        lines.append(f"{ts_str}{para_text}")
                        current_paragraph = []

                    # Iniciar novo par√°grafo
                    paragraph_timestamp = start
                    last_timestamp = start

                current_paragraph.append(text)
            else:
                # Modo por utterance (AUDIENCIA, REUNIAO, etc.) - 1 segmento = 1 linha
                if self._should_add_timestamp(start, last_timestamp, interval_seconds=timestamp_interval_seconds):
                    timestamp_str = f"[{self._format_timestamp(start)}] "
                    last_timestamp = start
                else:
                    timestamp_str = ""

                lines.append(f"{timestamp_str}{text}")

        # v2.29: Flush buffer final para modo agrupado
        if current_paragraph:
            para_text = " ".join(current_paragraph)
            ts_str = f"[{self._format_timestamp(paragraph_timestamp)}] " if paragraph_timestamp is not None else ""
            lines.append(f"{ts_str}{para_text}")

        transcript = "\n".join(lines).strip()
        if _env_truthy("VOMO_ASR_NORMALIZE_TEMAS", default=True):
            try:
                transcript, stats = self._normalize_asr_temas_consistency(transcript)
                if stats.get("changed", 0) > 0:
                    print(
                        f"{Fore.YELLOW}   üß© ASR: normalizados {stats['changed']} tema(s) inconsistentes "
                        f"(234‚Üí1234: {stats.get('fixed_3_to_4', 0)}, varia√ß√µes: {stats.get('fixed_variants', 0)}){Style.RESET_ALL}"
                    )
            except Exception:
                pass

        return transcript

    def _normalize_asr_temas_consistency(self, text: str):
        """
        Corrige inconsist√™ncias comuns de ASR em refer√™ncias do tipo "Tema N":
        - Perda do d√≠gito inicial em temas de 4 d√≠gitos (ex.: 234 vs 1234), quando a forma can√¥nica j√° aparece no texto.
        - Varia√ß√µes 4-d√≠gitos com mesmo sufixo (ex.: 1033 vs 1933), quando uma forma √© dominante.

        Regra conservadora: s√≥ corrige quando h√° evid√™ncia interna (forma can√¥nica presente).
        """
        import re

        if not text:
            return text, {"changed": 0, "fixed_3_to_4": 0, "fixed_variants": 0}

        # Capture occurrences like: "Tema 1.234", "tema 1234", "tema n¬∞ 234"
        pattern = re.compile(r"\b[Tt]ema\b\s*(?:n[¬∫¬∞]?\s*)?(\d{1,4})(?:\.(\d{3}))?\b")
        matches = list(pattern.finditer(text))
        if not matches:
            return text, {"changed": 0, "fixed_3_to_4": 0, "fixed_variants": 0}

        def _digits_from_match(m) -> str:
            g1 = m.group(1) or ""
            g2 = m.group(2) or ""
            digits = re.sub(r"\D+", "", f"{g1}{g2}")
            if digits and 2 <= len(digits) <= 4:
                return digits
            return ""

        themes: list[str] = []
        for m in matches:
            d = _digits_from_match(m)
            if d:
                themes.append(d)

        if not themes:
            return text, {"changed": 0, "fixed_3_to_4": 0, "fixed_variants": 0}

        counts: dict[str, int] = {}
        for d in themes:
            counts[d] = counts.get(d, 0) + 1

        fixed_3_to_4 = 0
        fixed_variants = 0
        out = text

        def _digits_to_optional_thousands_regex(digits: str) -> str:
            digits = re.sub(r"\D+", "", digits or "")
            if not digits:
                return ""
            if len(digits) <= 3:
                return re.escape(digits)
            if len(digits) > 6:
                return re.escape(digits)
            prefix = re.escape(digits[:-3])
            suffix = re.escape(digits[-3:])
            return rf"{prefix}\.?{suffix}"

        # Optional explicit overrides for known ASR confusions (highest priority).
        # Format: VOMO_ASR_TEMA_OVERRIDES="1933=1033,234=1234"
        overrides_raw = (os.getenv("VOMO_ASR_TEMA_OVERRIDES") or "").strip()
        if overrides_raw:
            pairs = []
            for part in overrides_raw.split(","):
                if "=" not in part:
                    continue
                src, dst = part.split("=", 1)
                src = re.sub(r"\D+", "", src.strip())
                dst = re.sub(r"\D+", "", dst.strip())
                if src and dst and 2 <= len(src) <= 6 and 2 <= len(dst) <= 6:
                    pairs.append((src, dst))
            for src, dst in pairs:
                src_re = _digits_to_optional_thousands_regex(src)
                if not src_re:
                    continue
                before = out
                out = re.sub(
                    rf"\b([Tt]ema)\s*(?:n[¬∫¬∞]?\s*)?{src_re}\b",
                    rf"\1 {dst}",
                    out,
                )
                if out != before:
                    fixed_variants += 1

        # Rule A: 3-digit -> 4-digit when the 4-digit variant (prefixed with '1') exists in the same transcript.
        for d in list(counts.keys()):
            if len(d) != 3:
                continue
            target = f"1{d}"
            if target not in counts:
                continue
            before = out
            out = re.sub(
                rf"\b([Tt]ema)\s*(?:n[¬∫¬∞]?\s*)?{re.escape(d)}\b",
                rf"\1 {target}",
                out,
            )
            if out != before:
                fixed_3_to_4 += 1

        # Re-scan after replacements
        matches = list(pattern.finditer(out))
        counts = {}
        for m in matches:
            d = _digits_from_match(m)
            if d:
                counts[d] = counts.get(d, 0) + 1

        # Rule B: unify 4-digit variants that share the same last 3 digits, when one is clearly dominant.
        by_suffix: dict[str, list[str]] = {}
        for d in counts:
            if len(d) == 4:
                by_suffix.setdefault(d[-3:], []).append(d)
        for suffix, variants in by_suffix.items():
            if len(variants) <= 1:
                continue
            ones = [v for v in variants if v.startswith("1")]
            if ones:
                preferred = max(ones, key=lambda v: counts.get(v, 0))
            else:
                preferred = max(variants, key=lambda v: counts.get(v, 0))

            pref_count = counts.get(preferred, 0)
            for v in variants:
                if v == preferred:
                    continue
                v_count = counts.get(v, 0)
                # Only normalize if the preferred is at least 2x more common or the variant is rare (<=1 hit).
                if pref_count >= (2 * v_count) or v_count <= 1:
                    before = out
                    out = re.sub(
                        rf"\b([Tt]ema)\s*(?:n[¬∫¬∞]?\s*)?{re.escape(v)}\b",
                        rf"\1 {preferred}",
                        out,
                    )
                    if out != before:
                        fixed_variants += 1

        changed = fixed_3_to_4 + fixed_variants if out != text else 0
        return out, {"changed": changed, "fixed_3_to_4": fixed_3_to_4, "fixed_variants": fixed_variants}
    
    def _normalize_raw_text(self, text):
        """
        Normaliza√ß√µes leves e seguras no texto raw (sem reescrever frases).
        
        v2.28: Limpeza determin√≠stica:
        - Remove whitespace excessivo
        - Converte ru√≠dos em tokens padr√£o
        - Preserva todo conte√∫do sem√¢ntico
        """
        import re
        
        # Remove whitespace excessivo
        text = re.sub(r'\s+', ' ', text)
        
        # Normaliza tra√ßos e retic√™ncias
        text = re.sub(r'‚Äî|‚Äì', '-', text)  # Em-dash/en-dash ‚Üí h√≠fen
        text = re.sub(r'‚Ä¶', '...', text)  # Ellipsis ‚Üí tr√™s pontos
        
        # Tokens padr√£o para ru√≠dos (se detectados)
        noise_patterns = [
            (r'\[inaud√≠vel\]|\(inaud√≠vel\)|\[inaudible\]', '[inaud√≠vel]'),
            (r'\[risos\]|\(risos\)|\[laughter\]', '[risos]'),
            (r'\[pausa\]|\(pausa\)|\[pause\]', '[pausa]'),
            (r'\[m√∫sica\]|\(m√∫sica\)|\[music\]', '[m√∫sica]'),
            (r'\[aplausos\]|\(aplausos\)|\[applause\]', '[aplausos]'),
        ]
        for pattern, replacement in noise_patterns:
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        return text.strip()

    def _asr_repeat_key(self, text: str) -> str:
        """Normaliza√ß√£o agressiva apenas para detec√ß√£o de repeti√ß√£o (n√£o para sa√≠da)."""
        import re

        if not text:
            return ""
        key = re.sub(r"\s+", " ", str(text)).strip().lower()
        key = re.sub(r"[^\w√Ä-√ñ√ò-√∂√∏-√ø0-9 ]+", "", key)
        key = re.sub(r"\s+", " ", key).strip()
        return key

    def _asr_is_noise_only(self, text: str) -> bool:
        """True quando o segmento √© s√≥ marcador de ru√≠do (ex.: [m√∫sica])."""
        import re

        if not text:
            return True
        normalized = re.sub(r"\s+", " ", str(text)).strip()
        if not normalized:
            return True
        # Se n√£o houver nenhum caractere alfanum√©rico, trata como ru√≠do (ex.: ".", "..." ou "?!")
        if not re.search(r"[\w√Ä-√ñ√ò-√∂√∏-√ø0-9]", normalized):
            return True
        # Depois de _normalize_raw_text, os ru√≠dos ficam padronizados como [xxx]
        noise = r"(?:inaud√≠vel|risos|pausa|m√∫sica|aplausos)"
        return bool(re.fullmatch(rf"(?:\[(?:{noise})\]\s*)+", normalized, flags=re.IGNORECASE))

    def _get_asr_beam_size(self) -> int:
        """
        Beam size padr√£o para o modo "Alta Precis√£o" (Beam Search).

        - UI: on/off (alta precis√£o).
        - Ajuste avan√ßado via env:
          - VOMO_ASR_BEAM_SIZE (ou VOMO_BEAM_SIZE) -> inteiro
        """
        raw = (os.getenv("VOMO_ASR_BEAM_SIZE") or os.getenv("VOMO_BEAM_SIZE") or "").strip()
        try:
            value = int(raw) if raw else 5
        except Exception:
            value = 5
        # Beam search real come√ßa em 2; limite superior conservador para evitar explos√£o de custo.
        if value < 2:
            value = 2
        if value > 10:
            value = 10
        return value

    def _get_whisper_initial_prompt_for_asr(self, *, high_accuracy: bool) -> Optional[str]:
        """
        Decide o `initial_prompt` do Whisper para a etapa de ASR.

        Notas importantes:
        - Prompt pode melhorar vocabul√°rio, mas pode "vazar" como texto transcrito.
        - Por padr√£o, evitamos usar prompt no modo normal (r√°pido).
        - Em alta precis√£o (Beam Search), usamos o prompt por modo como padr√£o (best-effort),
          pois o usu√°rio j√° optou por mais qualidade e o risco √© mitigado por stripping.

        Controles:
        - VOMO_WHISPER_INITIAL_PROMPT="..." (sempre tem prioridade)
        - VOMO_WHISPER_USE_MODE_PROMPT=1 (for√ßa uso do prompt por modo em TODOS os modos)
        - Se VOMO_WHISPER_USE_MODE_PROMPT n√£o estiver definido:
          - high_accuracy=True -> usa prompt por modo
          - high_accuracy=False -> n√£o usa prompt (default)
        """
        explicit = (os.getenv("VOMO_WHISPER_INITIAL_PROMPT") or "").strip()
        if explicit:
            return explicit

        # Se o usu√°rio n√£o definiu explicitamente, decidimos pelo modo:
        # - Beam (high_accuracy): ON
        # - Normal: OFF
        use_mode_prompt = _env_truthy("VOMO_WHISPER_USE_MODE_PROMPT", default=None)
        if use_mode_prompt is None:
            use_mode_prompt = bool(high_accuracy)

        if not use_mode_prompt:
            return None

        mode_key = getattr(self, "_current_mode", "FIDELIDADE")
        if isinstance(mode_key, str):
            mode_key = mode_key.strip().upper() or "FIDELIDADE"
        else:
            mode_key = "FIDELIDADE"

        lang = getattr(self, "_current_language", "pt") or "pt"
        # Tenta prompt i18n primeiro, depois fallback para pt
        if lang != "pt" and lang != "auto":
            i18n_prompt = self.INITIAL_PROMPTS_I18N.get((mode_key, lang))
            if i18n_prompt:
                return i18n_prompt
        return self.INITIAL_PROMPTS.get(mode_key, self.INITIAL_PROMPTS["FIDELIDADE"])

    def _get_whisper_initial_prompt(self) -> Optional[str]:
        """
        Compat: mant√©m a API antiga (sem saber se √© high_accuracy).
        Por padr√£o, segue o comportamento do modo normal (n√£o-beam).
        """
        return self._get_whisper_initial_prompt_for_asr(high_accuracy=False)

    def _strip_leaked_initial_prompt(self, text: str, initial_prompt: str) -> str:
        """
        Best-effort: remove `initial_prompt` caso ele tenha vazado como primeira linha da transcri√ß√£o.

        Estrat√©gia:
        - Tokeniza primeira linha e o prompt, compara sobreposi√ß√£o.
        - Remove SOMENTE quando h√° alta similaridade (muito conservador).
        """
        import re

        if not text:
            return text
        prompt = (initial_prompt or "").strip()
        if not prompt:
            return text

        first_line, rest = (text.split("\n", 1) + [""])[:2]

        def _tokens(value: str) -> list[str]:
            return re.findall(r"[\w√Ä-√ñ√ò-√∂√∏-√ø0-9]+", (value or "").lower())

        prompt_tokens = _tokens(prompt)
        if len(prompt_tokens) < 8:
            return text
        line_tokens = _tokens(first_line)
        if not line_tokens:
            return text

        prompt_set = set(prompt_tokens)
        overlap = sum(1 for t in prompt_tokens if t in set(line_tokens))

        # Regras conservadoras: muita sobreposi√ß√£o e tamanho similar.
        overlap_ratio = overlap / max(1, len(prompt_tokens))
        size_ok = len(line_tokens) <= (len(prompt_tokens) + 6)
        if overlap_ratio >= 0.9 and size_ok:
            return (rest or "").lstrip()

        return text

    def _asr_has_repeated_ngram_run(self, tokens, *, max_ngram: int = 6) -> bool:
        """Detecta repeti√ß√µes consecutivas extremas (alucina√ß√£o t√≠pica do Whisper)."""
        token_count = len(tokens)
        if token_count < 12:
            return False

        max_ngram = min(max_ngram, token_count // 2)
        for n in range(1, max_ngram + 1):
            min_repeats = 8 if n == 1 else 4  # muito conservador para evitar falsos positivos
            if token_count < n * min_repeats:
                continue

            # Procura por runs consecutivos em qualquer offset.
            for i in range(0, token_count - n * min_repeats + 1):
                phrase = tokens[i : i + n]
                repeats = 1
                while i + (repeats + 1) * n <= token_count and tokens[i + repeats * n : i + (repeats + 1) * n] == phrase:
                    repeats += 1
                if repeats >= min_repeats:
                    return True

        return False

    def _asr_looks_like_hallucination(self, text: str) -> bool:
        """
        Heur√≠sticas conservadoras para filtrar segmentos obviamente quebrados:
        - Repeti√ß√£o extrema (palavra/frase em loop)
        - Sequ√™ncias num√©ricas repetidas (ex.: "50 50 50 ...")
        """
        import re

        if not text:
            return True

        normalized = re.sub(r"\s+", " ", str(text)).strip()
        if not normalized:
            return True

        # Tokens "palavra-like" (mant√©m n√∫meros; remove pontua√ß√£o)
        tokens = re.findall(r"[\w√Ä-√ñ√ò-√∂√∏-√ø0-9]+", normalized.lower())
        if len(tokens) < 12:
            return False

        # 1) Repeti√ß√£o consecutiva extrema de n-gramas curtos
        if self._asr_has_repeated_ngram_run(tokens):
            return True

        # 2) Baixa diversidade lexical em sequ√™ncia longa (ex.: mesmo slogan repetido)
        if len(tokens) >= 25:
            unique_ratio = len(set(tokens)) / max(1, len(tokens))
            if unique_ratio < 0.25:
                return True

        # 3) Segmento quase s√≥ n√∫meros e com pouca variedade (ex.: token IDs ou contagem)
        numeric_tokens = [t for t in tokens if t.isdigit()]
        if numeric_tokens and len(numeric_tokens) / len(tokens) > 0.85:
            if len(numeric_tokens) >= 12 and len(set(numeric_tokens)) <= 3:
                return True
            if len(numeric_tokens) >= 30 and len(set(numeric_tokens)) <= 8:
                return True

        return False

    def _filter_asr_segments(self, segments):
        """
        Remove segmentos claramente in√∫teis (ru√≠dos/loops) antes de formatar ou diarizar.
        Mant√©m timestamps originais.
        """
        if not segments:
            return [], {"dropped": 0, "reason_counts": {}}

        clean_enabled = _env_truthy("VOMO_FILTER_ASR_HALLUCINATIONS", default=True)
        if not clean_enabled:
            return segments, {"dropped": 0, "reason_counts": {}}

        dropped = 0
        reason_counts = {}
        cleaned = []

        last_key = None
        repeat_run = 0

        for seg in segments:
            raw_text = (seg.get("text") or "").strip()
            if not raw_text:
                dropped += 1
                reason_counts["empty"] = reason_counts.get("empty", 0) + 1
                continue

            text = self._normalize_raw_text(raw_text)
            if self._asr_is_noise_only(text):
                dropped += 1
                reason_counts["noise_only"] = reason_counts.get("noise_only", 0) + 1
                continue

            key = self._asr_repeat_key(text)
            if key and key == last_key and len(key) <= 80:
                repeat_run += 1
                # Permite no m√°ximo 2 repeti√ß√µes consecutivas de segmentos curtos id√™nticos.
                if repeat_run >= 2:
                    dropped += 1
                    reason_counts["repeat_loop"] = reason_counts.get("repeat_loop", 0) + 1
                    continue
            else:
                last_key = key
                repeat_run = 0

            if self._asr_looks_like_hallucination(text):
                dropped += 1
                reason_counts["hallucination"] = reason_counts.get("hallucination", 0) + 1
                continue

            new_seg = dict(seg)
            new_seg["text"] = text
            cleaned.append(new_seg)

        return cleaned, {"dropped": dropped, "reason_counts": reason_counts}

    def _assign_diarization_labels(self, segments, diarization_output):
        try:
            from intervaltree import IntervalTree
        except ImportError:
            return self._assign_diarization_labels_fallback(segments, diarization_output)

        tree = IntervalTree()
        for turn, _, speaker in diarization_output.itertracks(yield_label=True):
            tree[turn.start:turn.end] = speaker

        labeled_segments = []
        for segment in segments:
            start, end = segment['start'], segment['end']
            overlaps = tree[start:end]
            if overlaps:
                best_overlap = max(
                    overlaps,
                    key=lambda interval: min(end, interval.end) - max(start, interval.begin)
                )
                speaker_id = best_overlap.data.split('_')[-1]
                best_speaker = f"SPEAKER {int(speaker_id) + 1}"
            else:
                best_speaker = "SPEAKER 0"
            labeled_segments.append({
                "start": float(start),
                "end": float(end),
                "text": segment.get("text", ""),
                "speaker_label": best_speaker,
            })
        return labeled_segments

    def _assign_diarization_labels_fallback(self, segments, diarization_output):
        diarization_segments = [(t.start, t.end, s) for t, _, s in diarization_output.itertracks(yield_label=True)]
        labeled_segments = []
        for segment in segments:
            start, end = segment['start'], segment['end']
            best_speaker = "SPEAKER 0"
            max_overlap = 0
            for d_start, d_end, d_speaker in diarization_segments:
                overlap = max(0, min(end, d_end) - max(start, d_start))
                if overlap > max_overlap:
                    max_overlap = overlap
                    best_speaker = f"SPEAKER {int(d_speaker.split('_')[-1]) + 1}"
            labeled_segments.append({
                "start": float(start),
                "end": float(end),
                "text": segment.get("text", ""),
                "speaker_label": best_speaker,
            })
        return labeled_segments

    def _format_timestamp(self, seconds):
        m, s = divmod(seconds, 60)
        h, m = divmod(m, 60)
        return f"{int(h):02d}:{int(m):02d}:{int(s):02d}" if h > 0 else f"{int(m):02d}:{int(s):02d}"
    
    def _get_timestamp_interval_for_mode(self) -> int:
        """
        Retorna o intervalo de timestamps baseado no modo atual.

        - APOSTILA/FIDELIDADE: 60s (aulas longas, menos interrup√ß√µes)
        - AUDIENCIA/REUNIAO/LEGENDA: 0 (por utterance, cada segmento tem timestamp)

        Returns:
            int: Intervalo em segundos (0 = por utterance)
        """
        mode = getattr(self, "_current_mode", "APOSTILA").upper()
        if mode in ("APOSTILA", "FIDELIDADE"):
            return 60
        # AUDIENCIA, REUNIAO, LEGENDA, DEPOIMENTO ‚Üí por utterance
        return 0

    def _should_add_timestamp(self, current_seconds, last_timestamp_seconds, interval_minutes=None, interval_seconds=None):
        """
        Determina se um timestamp deve ser adicionado baseado no intervalo configurado.

        Args:
            current_seconds: Tempo atual em segundos
            last_timestamp_seconds: √öltimo timestamp inserido (None se for o primeiro)
            interval_minutes: Intervalo em minutos entre timestamps (padr√£o: 20 se nenhum especificado)
            interval_seconds: Intervalo em segundos (tem preced√™ncia sobre interval_minutes)
                              Se None e interval_minutes tamb√©m None, usa intervalo baseado no modo.

        Returns:
            bool: True se deve adicionar timestamp
        """
        if last_timestamp_seconds is None:
            return True  # Sempre adiciona o primeiro timestamp

        # interval_seconds tem preced√™ncia
        if interval_seconds is not None:
            target_interval = interval_seconds
        elif interval_minutes is not None:
            target_interval = interval_minutes * 60
        else:
            # Usar intervalo baseado no modo atual
            target_interval = self._get_timestamp_interval_for_mode()
            if target_interval == 0:
                return True  # Por utterance: sempre adiciona

        return (current_seconds - last_timestamp_seconds) >= target_interval

    def _align_diarization(self, segments, diarization_output):
        """
        Alinha segmentos com diariza√ß√£o (Vers√£o Otimizada com IntervalTree)
        
        v2.28: Sa√≠da pr√©-formatada com line breaks e timestamps frequentes
        """
        try:
            from intervaltree import IntervalTree
        except ImportError:
            print("‚ö†Ô∏è intervaltree n√£o instalado, usando fallback O(n)")
            return self._align_diarization_fallback(segments, diarization_output)
        
        lines = []
        current_speaker = None
        current_block = []
        last_timestamp = None
        
        # Pr√©-computar spatial index
        tree = IntervalTree()
        for turn, _, speaker in diarization_output.itertracks(yield_label=True):
            tree[turn.start:turn.end] = speaker
        
        for segment in segments:
            start, end = segment['start'], segment['end']
            text = segment.get('text', '').strip()
            
            if not text:
                continue
            
            # Normaliza√ß√£o leve
            text = self._normalize_raw_text(text)
            
            # Busca O(log n) para speaker
            overlaps = tree[start:end]
            
            if overlaps:
                best_overlap = max(
                    overlaps,
                    key=lambda interval: min(end, interval.end) - max(start, interval.begin)
                )
                speaker_id = best_overlap.data.split('_')[-1]
                best_speaker = f"SPEAKER {int(speaker_id) + 1}"
            else:
                best_speaker = "SPEAKER 1"
            
            # Mudan√ßa de speaker
            if best_speaker != current_speaker:
                # Flush previous block
                if current_block:
                    lines.append(" ".join(current_block))
                    current_block = []
                
                if lines:
                    lines.append("")  # Linha em branco extra
                
                lines.append(f"{best_speaker}")
                current_speaker = best_speaker
                last_timestamp = None  # Reset timestamp logic for new speaker? Or keep continuous? 
                # Keeping continuous is usually better for "every 60s" regardless of speaker, 
                # but resetting ensures first line of speaker has timestamp if we want.
                # User asked "timestamps a cada 60 segundos".
                # Let's keep logic simple: Check timestamp interval.
                # If we reset last_timestamp, we force a timestamp at speaker start.
                last_timestamp = None 
            
            # Timestamp a cada 60 segundos
            if self._should_add_timestamp(start, last_timestamp, interval_seconds=self._get_timestamp_interval_for_mode()):
                if current_block:
                    lines.append(" ".join(current_block))
                    current_block = []
                
                ts = self._format_timestamp(start)
                current_block.append(f"[{ts}] {text}")
                last_timestamp = start
            else:
                current_block.append(text)
        
        # Flush final block
        if current_block:
            lines.append(" ".join(current_block))
        
        return "\n\n".join(lines).strip()
    
    def _align_diarization_fallback(self, segments, diarization_output):
        """
        Fallback O(n) caso intervaltree n√£o esteja dispon√≠vel.
        
        v2.28: Sa√≠da pr√©-formatada condensada com line breaks e timestamps frequentes
        """
        lines = []
        current_speaker = None
        current_block = []
        last_timestamp = None
        diarization_segments = [(t.start, t.end, s) for t, _, s in diarization_output.itertracks(yield_label=True)]

        for segment in segments:
            start, end = segment['start'], segment['end']
            text = segment.get('text', '').strip()
            
            if not text:
                continue
            
            # Normaliza√ß√£o leve
            text = self._normalize_raw_text(text)
            
            best_speaker = "SPEAKER 1"
            max_overlap = 0
            
            # Busca O(n)
            for d_start, d_end, d_speaker in diarization_segments:
                overlap = max(0, min(end, d_end) - max(start, d_start))
                if overlap > max_overlap:
                    max_overlap = overlap
                    speaker_id = d_speaker.split('_')[-1]
                    best_speaker = f"SPEAKER {int(speaker_id) + 1}"
            
            # Mudan√ßa de speaker
            if best_speaker != current_speaker:
                # Flush previous block
                if current_block:
                    lines.append(" ".join(current_block))
                    current_block = []
                
                if lines:
                    lines.append("")  # Linha em branco extra
                
                lines.append(f"{best_speaker}")
                current_speaker = best_speaker
                last_timestamp = None
            
            # Timestamp a cada 60 segundos
            if self._should_add_timestamp(start, last_timestamp, interval_seconds=self._get_timestamp_interval_for_mode()):
                if current_block:
                    lines.append(" ".join(current_block))
                    current_block = []
                    
                ts = self._format_timestamp(start)
                current_block.append(f"[{ts}] {text}")
                last_timestamp = start
            else:
                current_block.append(text)
        
        # Flush final block
        if current_block:
            lines.append(" ".join(current_block))

        return "\n\n".join(lines).strip()

    def _segment_raw_transcription(self, raw_text):
        lines = raw_text.split('\n')
        speaker_pattern = re.compile(r'^SPEAKER \d+$')
        segments = []
        current_speaker = None
        current_content = []
        
        for line in lines:
            line = line.strip()
            if speaker_pattern.match(line):
                if current_speaker:
                    segments.append({'speaker': current_speaker, 'content': "\n".join(current_content)})
                current_speaker = line
                current_content = []
            else:
                if current_speaker: current_content.append(line)
        
        if current_speaker:
            segments.append({'speaker': current_speaker, 'content': "\n".join(current_content)})
            
        # Merge simples
        final_segments = []
        if segments:
            current = segments[0]
            for next_seg in segments[1:]:
                if next_seg['speaker'] == current['speaker']:
                    current['content'] += "\n" + next_seg['content']
                else:
                    final_segments.append(current)
                    current = next_seg
            final_segments.append(current)
            
        return final_segments

    def _smart_chunk_with_overlap(self, text, max_size=None, overlap=None):
        """
        v2.28: Chunking inteligente com overlap adaptativo para tabelas.

        Melhorias:
        - Overlap 30% maior quando chunk cont√©m tabela
        - Nunca corta no meio de tabela
        - Prioriza corte ap√≥s pares de tabelas (üìã + üéØ)
        """
        max_size = max_size or self.MAX_CHUNK_SIZE
        base_overlap = overlap or self.CHUNK_OVERLAP
        if len(text) <= max_size: return [text]

        chunks = []
        start = 0

        def _is_table_line(line: str) -> bool:
            stripped = line.strip()
            return bool(stripped) and stripped.startswith('|') and '|' in stripped

        def _prev_next_nonempty_lines_around(pos: int, window: int = 5000) -> tuple:
            s_start = max(0, pos - window)
            s_end = min(len(text), pos + window)
            s = text[s_start:s_end]
            p = pos - s_start
            before_lines = s[:p].splitlines()
            after_lines = s[p:].splitlines()
            prev_line = next((l for l in reversed(before_lines) if l.strip()), "")
            next_line = next((l for l in after_lines if l.strip()), "")
            return prev_line, next_line

        def _pos_inside_table_line(pos: int) -> bool:
            if pos <= 0 or pos >= len(text):
                return False
            # `end` (slice stop) √© seguro se estiver logo ap√≥s um '\n'
            if text[pos - 1] == '\n':
                return False
            line_start = text.rfind('\n', 0, pos) + 1
            line_end = text.find('\n', pos)
            if line_end == -1:
                line_end = len(text)
            return _is_table_line(text[line_start:line_end])

        def _table_block_bounds_around(pos: int, window: int = 15000) -> Optional[tuple]:
            s_start = max(0, pos - window)
            s_end = min(len(text), pos + window)
            s = text[s_start:s_end]
            p = pos - s_start

            lines = s.splitlines(keepends=True)
            if not lines:
                return None

            # Encontrar o √≠ndice da linha que cont√©m `p` (ou a anterior se `p` cair no separador)
            cumulative = 0
            idx = 0
            for idx, ln in enumerate(lines):
                nxt = cumulative + len(ln)
                if p < nxt:
                    break
                cumulative = nxt
            else:
                idx = len(lines) - 1

            if not _is_table_line(lines[idx]) and idx > 0 and _is_table_line(lines[idx - 1]):
                idx -= 1
            if not _is_table_line(lines[idx]):
                return None

            start_idx = idx
            while start_idx > 0 and _is_table_line(lines[start_idx - 1]):
                start_idx -= 1
            end_idx = idx
            while end_idx + 1 < len(lines) and _is_table_line(lines[end_idx + 1]):
                end_idx += 1

            start_off = sum(len(ln) for ln in lines[:start_idx])
            end_off = sum(len(ln) for ln in lines[:end_idx + 1])
            return (s_start + start_off, s_start + end_off)

        while start < len(text):
            end = start + max_size
            chunk_text = text[start:end] if end <= len(text) else text[start:]

            # v2.28: Detectar se chunk cont√©m tabela para overlap maior
            contem_tabela = '|' in chunk_text and re.search(r'^\s*\|', chunk_text, re.MULTILINE)
            current_overlap = int(base_overlap * 1.3) if contem_tabela else base_overlap

            if end < len(text):
                # Zona de busca para ponto de corte
                search_start = max(0, end - 3000)
                search_zone = text[search_start:end]
                best_break = -1

                # v2.28: Prioridade 1 - Ap√≥s tabela de pegadinhas completa
                match_pegadinha = re.search(r'\n(?=####?\s*üéØ.*\n)', search_zone)
                if match_pegadinha:
                    # Encontrar fim da tabela ap√≥s o heading
                    pos_heading = match_pegadinha.end()
                    remaining = search_zone[pos_heading:]
                    lines = remaining.split('\n')
                    pos_after_table = pos_heading
                    in_table = False
                    for i, line in enumerate(lines):
                        if line.strip().startswith('|'):
                            in_table = True
                        elif in_table and not line.strip().startswith('|'):
                            # Fim da tabela
                            pos_after_table = pos_heading + sum(len(l)+1 for l in lines[:i])
                            break
                    if pos_after_table > pos_heading:
                        best_break = pos_after_table

                # v2.28: Prioridade 2 - Antes de novo heading ## (bloco tem√°tico)
                if best_break == -1:
                    match_heading = list(re.finditer(r'\n(?=##\s+\d)', search_zone))
                    if match_heading:
                        best_break = match_heading[-1].end()  # √öltimo heading encontrado

                # v2.28: Prioridade 3 - Par√°grafo duplo (evitando meio de tabela)
                if best_break == -1:
                    # Verificar se estamos no meio de uma tabela
                    last_newlines = list(re.finditer(r'\n\n', search_zone))
                    for match in reversed(last_newlines):
                        pos = match.start()
                        # Verificar se pr√≥xima linha n√£o √© tabela
                        next_char_pos = match.end()
                        if next_char_pos < len(search_zone):
                            next_line_start = search_zone[next_char_pos:next_char_pos+50]
                            if not next_line_start.strip().startswith('|'):
                                best_break = pos + 2
                                break

                # Fallback: qualquer \n\n
                if best_break == -1:
                    last_break = search_zone.rfind('\n\n')
                    if last_break != -1:
                        best_break = last_break

                if best_break != -1:
                    end = search_start + best_break

                # v2.28: Nunca cortar no meio de uma tabela (separa√ß√£o por linha)
                prev_line, next_line = _prev_next_nonempty_lines_around(end)
                if (_is_table_line(prev_line) and _is_table_line(next_line)) or _pos_inside_table_line(end):
                    bounds = _table_block_bounds_around(end)
                    if bounds:
                        table_start, table_end = bounds
                        min_chunk_size = max(800, int(max_size * 0.2))
                        # Preferir cortar ANTES da tabela; se ficar pequeno demais, cortar AP√ìS a tabela
                        candidate = table_start
                        if candidate <= start + min_chunk_size and table_end > start + min_chunk_size:
                            candidate = table_end
                        if candidate > start:
                            end = candidate

            # Garantir progresso (evitar loop infinito se `end` voltar demais)
            if end <= start:
                end = min(start + max_size, len(text))
                if end <= start:
                    break

            chunks.append(text[start:end].strip())
            if end >= len(text): break
            next_start = end - current_overlap
            if next_start <= start:
                next_start = end
            start = next_start

        return chunks

    def _merge_chunks_deduplicated(self, chunks):
        if not chunks: return ""
        if len(chunks) == 1: return chunks[0]
        
        merged = chunks[0]
        for i in range(1, len(chunks)):
            current = chunks[i]
            tail = merged[-2000:]
            head = current[:2000]
            matcher = difflib.SequenceMatcher(None, tail, head)
            match = matcher.find_longest_match(0, len(tail), 0, len(head))
            
            if match.size > 200:
                merged += "\n\n" + current[match.b + match.size:]
            else:
                merged += "\n\n" + current
        return merged

    def _get_chunk_cache(self, chunk_text, prompt_hash):
        content_hash = hashlib.sha256(f"{chunk_text}{prompt_hash}".encode()).hexdigest()
        cache_path = self.cache_dir / f"{content_hash}.json"
        if cache_path.exists():
            try:
                with open(cache_path, 'r') as f:
                    return json.load(f)['result']
            except:
                return None
        return None

    def _save_chunk_cache(self, chunk_text, prompt_hash, result):
        content_hash = hashlib.sha256(f"{chunk_text}{prompt_hash}".encode()).hexdigest()
        cache_path = self.cache_dir / f"{content_hash}.json"
        try:
            with open(cache_path, 'w') as f:
                json.dump({'result': result}, f)
        except:
            pass

    def _detect_open_table_state(self, text: str) -> dict:
        """
        v2.27: Detecta se o texto termina com uma tabela/quadro aberto mas n√£o conclu√≠do.
        
        Casos detectados:
        1. T√≠tulo de tabela principal (#### üìã [r√≥tulo contextual]) sem tabela depois
        2. Tabela iniciada mas incompleta (menos linhas que o esperado)
        
        Returns:
            dict com 'needs_table_continuation' e 'context_hint' se aberto, {} se fechado
        """
        if not text or len(text) < 100:
            return {}
        
        lines = text.strip().splitlines()
        last_50 = lines[-50:] if len(lines) > 50 else lines
        
        # Caso 1: T√≠tulo de tabela principal (üìã) sem tabela
        for i, line in enumerate(last_50):
            if re.match(r'^#{3,5}\s*üìã', line):
                # H√° um t√≠tulo de quadro, verifica se tabela foi gerada depois
                remaining = last_50[i+1:]
                has_table = any('|' in l and l.strip().startswith('|') for l in remaining)
                if not has_table:
                    section_title = line.strip()
                    return {
                        "needs_table_continuation": True,
                        "open_section_title": section_title,
                        "context_hint": f"\n\n‚ö†Ô∏è **CONTINUA√á√ÉO OBRIGAT√ìRIA**: O chunk anterior terminou com o t√≠tulo '{section_title}' mas SEM a tabela correspondente. Voc√™ DEVE gerar a tabela Markdown desse bloco ANTES de qualquer novo conte√∫do."
                    }
        
        # Caso 2: √öltima linha √© tabela (pode precisar continua√ß√£o se poucos dados)
        # Apenas logamos, n√£o adicionamos instru√ß√£o expl√≠cita neste caso
        
        # Caso 3: C√©lula amputada (corte no meio da frase dentro da tabela)
        # Ex: "| Item 1 | O princ√≠pio da legalidade define que" (sem pipe final)
        if lines:
            last_line = lines[-1].strip()
            if last_line.startswith('|') and not last_line.endswith('|'):
                return {
                    "needs_table_continuation": True,
                    "open_section_title": "Tabela cortada no meio",
                    "context_hint": f"\n\n‚ö†Ô∏è CONTINUE a tabela de onde parou: '{last_line[-60:]}...' ‚Üí complete a frase, feche com '|', continue normalmente."
                }
        
        return {}

    async def _format_chunk_async(self, chunk_text, idx, prompt=None, total=1, context="", depth=0, **kwargs):
        """Wrapper de compatibilidade para process_chunk_async que aceita prompt (ignorado)"""
        return await self.process_chunk_async(
            chunk_text,
            idx=idx,
            total=total,
            previous_context=context,
            depth=depth,
            **kwargs
        )

    @retry(
        retry=retry_if_exception_type(Exception), 
        stop=stop_after_attempt(3), 
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def process_chunk_async(
        self,
        chunk_text,
        idx=0,
        total=1,
        previous_context="",
        depth=0,
        global_structure=None,
        overlap_text="",
        cached_content=None,
        max_output_tokens_override=None,
        disable_cache=False,
        table_retry=False,
        trunc_retry=False,
    ):
        """Processa um chunk de forma ass√≠ncrona com retry, cache e CHUNKING ADAPTATIVO (v2.10 Ported)"""
        
        # Calculate prompt hash for local caching (apenas se n√£o usar context cache)
        prompt_content = f"{chunk_text}_{previous_context}_{overlap_text}_{global_structure}"
        prompt_hash = hashlib.sha256(prompt_content.encode()).hexdigest()
        
        # Check local cache (only if depth 0 to avoid fragment caching issues)
        # Se estiver usando Context Caching, ignoramos o cache local para garantir uso do contexto global
        if not cached_content and not disable_cache:
            cached = self._get_chunk_cache(chunk_text, prompt_hash) if depth == 0 else None
            if cached:
                metrics.record_cache_hit()
                return cached
        
        # Constr√≥i o contexto e prompt
        contexto_estilo = f"√öltimos par√°grafos formatados:\n{previous_context}" if previous_context else ""
        contexto_raw = f"OVERLAP RAW (somente contexto, N√ÉO INCLUIR na resposta):\n{overlap_text}" if overlap_text else ""
        if not contexto_estilo and not contexto_raw:
            contexto_estilo = "Inicio do documento."
        
        # SE USAR CONTEXT CACHING: System Prompt j√° est√° no cache
        # SE N√ÉO USAR: System Prompt precisa ir no contents
        
        # Se n√£o tiver cache, montamos o system prompt completo
        system_prompt = self.prompt_apostila
            
        # Adiciona estrutura global se dispon√≠vel e N√ÉO estiver no cache (se estiver no cache, j√° foi inclu√≠da na cria√ß√£o)
        if global_structure and not cached_content:
            system_prompt += f"\n\n## ESTRUTURA GLOBAL (GUIA):\n{global_structure}"
            
        # Adiciona contexto anterior (estilo + overlap RAW)
        secao_contexto = ""
        if previous_context or overlap_text:
            blocks = []
            if contexto_estilo:
                blocks.append(contexto_estilo)
            if contexto_raw:
                blocks.append(contexto_raw)
            contexto_bloco = "\n\n".join(blocks).strip()
            secao_contexto = f"""
üîí CONTEXTO ANTERIOR (SOMENTE REFER√äNCIA DE ESTILO)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

{contexto_bloco}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚ö†Ô∏è ATEN√á√ÉO: O bloco acima J√Å FOI FORMATADO anteriormente.
- N√ÉO formate novamente esse conte√∫do
- N√ÉO inclua esse conte√∫do na sua resposta
- Use APENAS como refer√™ncia de estilo de escrita e continuidade
- Se houver OVERLAP RAW, use apenas para continuidade; n√£o copie nem reformate
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìù NOVO TEXTO PARA FORMATAR (comece aqui):
"""

        user_content = f"""{secao_contexto}
<texto_para_formatar>
{chunk_text}
</texto_para_formatar>

        **INSTRU√á√ïES FINAIS**:
        - Esta √© a parte {idx} de {total if total else '?'} (Profundidade {depth})
        - Formate APENAS o texto entre <texto_para_formatar>
        - Se houver contexto acima, N√ÉO o reprocesse
        - Retorne APENAS o Markdown formatado do NOVO texto
        - N√ÉO insira marcadores artificiais de continua√ß√£o (ex.: `[continua]`, `[continua√ß√£o]`, `(continua)`)
        """

        def _has_incomplete_table(text: str) -> bool:
            if not text:
                return False
            lines = [ln.rstrip() for ln in text.splitlines() if ln.strip()]
            if not lines:
                return False
            if "|" not in lines[-1]:
                return False
            end = len(lines) - 1
            start = end
            while start >= 0 and "|" in lines[start]:
                start -= 1
            start += 1
            block = lines[start:end + 1]
            if len(block) < 2:
                return False
            header = block[0]
            separator = block[1]
            if not re.search(r'-{3,}', separator):
                return False
            data_rows = block[2:]
            if not data_rows:
                return True
            header_pipes = header.count("|")
            last_pipes = data_rows[-1].count("|")
            return header_pipes >= 2 and last_pipes < header_pipes

        def _has_missing_table(text: str) -> bool:
            """v2.41: Detecta se o output tem t√≠tulos de quadro-s√≠ntese (üìã) sem tabela correspondente."""
            if not text:
                return False
            lines = text.splitlines()
            i = 0
            while i < len(lines):
                line = lines[i].strip()
                # Detecta t√≠tulo de quadro-s√≠ntese
                if re.match(r'^#{3,5}\s*üìã', line):
                    # Procura tabela nas pr√≥ximas 5 linhas n√£o-vazias
                    found_table = False
                    for j in range(i + 1, min(i + 8, len(lines))):
                        next_line = lines[j].strip()
                        if next_line.startswith('|') and '|' in next_line[1:]:
                            found_table = True
                            break
                        if next_line.startswith('#'):
                            break  # Novo heading sem tabela
                    if not found_table:
                        return True
                i += 1
            return False

        async def _retry_incomplete_table(result_text: str):
            incomplete_table = _has_incomplete_table(result_text or "")
            missing_table = _has_missing_table(result_text or "")
            needs_retry = incomplete_table or missing_table
            if needs_retry and not table_retry and depth < 2 and len(chunk_text) > 4000:
                reason = "incompleta" if incomplete_table else "ausente (t√≠tulo üìã sem tabela)"
                retry_tokens = max_output_tokens_override or 32000
                print(f"{Fore.MAGENTA}‚úÇÔ∏è Tabela {reason} no Chunk {idx}. Reprocessando com mais tokens...")
                retry_result = await self.process_chunk_async(
                    chunk_text,
                    idx=idx,
                    total=total,
                    previous_context=previous_context,
                    depth=depth,
                    global_structure=global_structure,
                    overlap_text=overlap_text,
                    cached_content=cached_content,
                    max_output_tokens_override=retry_tokens,
                    disable_cache=True,
                    table_retry=True
                )
                if retry_result and not _has_incomplete_table(retry_result):
                    if depth == 0 and not disable_cache:
                        self._save_chunk_cache(chunk_text, prompt_hash, retry_result)
                    return retry_result
                print(f"{Fore.MAGENTA}‚úÇÔ∏è Reprocessamento n√£o resolveu. Dividindo chunk...")
                metrics.record_adaptive_split()
                return await self._split_and_retry_async(
                    chunk_text,
                    idx,
                    system_prompt,
                    total,
                    contexto_estilo,
                    depth,
                    max_output_tokens_override=retry_tokens,
                    disable_cache=True
                )
            return None

        def _near_token_limit(completion_tokens: int, max_tokens: int) -> bool:
            try:
                completion_tokens = int(completion_tokens or 0)
                max_tokens = int(max_tokens or 0)
            except Exception:
                return False
            if completion_tokens <= 0 or max_tokens <= 0:
                return False
            return completion_tokens >= int(max_tokens * 0.92)

        def _looks_hard_truncated(text: str) -> bool:
            """
            Heur√≠stica conservadora para truncamento "duro" (normalmente por limite de tokens):
            - termina no meio de palavra
            - termina com bracket aberto ou fechamento "sobrando"
            - cont√©m marcador de continua√ß√£o no final
            """
            s = (text or "").strip()
            if not s:
                return True

            tail = s[-300:]
            if re.search(r"(?i)(?:\\[\\s*(?:continua|continua√ß√£o|continuacao)\\s*\\]|\\(\\s*(?:continua|continua√ß√£o|continuacao)\\s*\\))\\s*$", tail):
                return True

            last = s[-1]
            if last in "[({":
                return True

            # Fechamento "sobrando" no tail (ex.: termina com ']' sem haver '[' suficiente)
            tail2 = s[-2000:]
            if last == "]" and tail2.count("[") < tail2.count("]"):
                return True
            if last == ")" and tail2.count("(") < tail2.count(")"):
                return True
            if last == "}" and tail2.count("{") < tail2.count("}"):
                return True

            # Meio de palavra no final (sem pontua√ß√£o final t√≠pica)
            if last.isalnum():
                if not re.search(r"[.!?‚Ä¶][\"‚Äù‚Äô')\\]]?\\s*$", s):
                    return True

            return False
        
        try:
            # Configura√ß√£o de Seguran√ßa (Block None) e Par√¢metros
            max_output_tokens = max_output_tokens_override or 32000  # v2.41: Aumentado de 16k para 32k (alinhado com format_transcription_gemini)
            safety_config = [
                types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
            ]

            if self.use_openai_primary and self.openai_client:
                try:
                    start_time_oai = time.time()
                    openai_kwargs = {}
                    if max_output_tokens_override:
                        openai_kwargs["max_completion_tokens"] = max_output_tokens
                    response = await self.openai_client.chat.completions.create(
                        model=self.openai_model,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_content}
                        ],
                        timeout=180,
                        **openai_kwargs
                    )
                    duration_oai = time.time() - start_time_oai
                    result = response.choices[0].message.content

                    oai_prompt = getattr(response.usage, 'prompt_tokens', 0) if hasattr(response, 'usage') else 0
                    oai_compl = getattr(response.usage, 'completion_tokens', 0) if hasattr(response, 'usage') else 0
                    finish_reason = None
                    try:
                        finish_reason = response.choices[0].finish_reason
                    except Exception:
                        finish_reason = None
                    openai_truncated = (finish_reason == "length")
                    cached_tokens = 0
                    if hasattr(response, 'usage'):
                        details = getattr(response.usage, 'prompt_tokens_details', None)
                        cached_tokens = getattr(details, 'cached_tokens', 0) or 0 if details else 0
                    metrics.record_call(
                        "openai",
                        oai_prompt,
                        oai_compl,
                        duration_oai,
                        model=self.openai_model,
                        cached_tokens_in=cached_tokens,
                    )

                    if contexto_estilo and result:
                        result = remover_eco_do_contexto(result, contexto_estilo)

                    retry_result = await _retry_incomplete_table(result)
                    if retry_result is not None:
                        return retry_result

                    if result:
                        try:
                            result = remover_marcadores_continua(result)
                        except Exception:
                            pass

                    # Se aparenta truncamento por limite, split para preservar conte√∫do
                    openai_near_limit = _near_token_limit(oai_compl, max_output_tokens_override) if max_output_tokens_override else False
                    if (
                        not trunc_retry
                        and depth < 2
                        and len(chunk_text) > 4000
                        and _looks_hard_truncated(result or "")
                        and (openai_truncated or openai_near_limit)
                    ):
                        print(
                            f"{Fore.MAGENTA}‚úÇÔ∏è Sa√≠da aparenta truncada por limite (OpenAI, Chunk {idx}). "
                            "Dividindo chunk..."
                        )
                        metrics.record_adaptive_split()
                        return await self._split_and_retry_async(
                            chunk_text,
                            idx,
                            system_prompt,
                            total,
                            contexto_estilo,
                            depth,
                            max_output_tokens_override=max_output_tokens_override,
                            disable_cache=True,
                        )

                    if depth == 0:
                        self._save_chunk_cache(chunk_text, prompt_hash, result)
                    return result
                except Exception as e_openai:
                    print(f"{Fore.YELLOW}‚ö†Ô∏è Falha no OpenAI prim√°rio (Chunk {idx}): {e_openai}. Tentando Gemini...")

            def call_gemini():
                nonlocal cached_content
                gen_config = types.GenerateContentConfig(
                    max_output_tokens=max_output_tokens,
                    temperature=0.1,
                    top_p=0.9,
                    top_k=40,
                    safety_settings=safety_config,
                    thinking_config=types.ThinkingConfig(
                        include_thoughts=False,
                        thinking_level=self._resolve_thinking_level(),
                    ),
                )

                # SE USAR CACHE: Passar cached_content e APENAS user_content
                if cached_content:
                    gen_config.cached_content = cached_content.name
                    contents = user_content
                else:
                    # Sem cache: System + User
                    contents = f"{system_prompt}\n\n{user_content}"

                max_retries = int(os.getenv("IUDEX_GEMINI_RETRY_ATTEMPTS", "4"))
                base_sleep = float(os.getenv("IUDEX_GEMINI_RETRY_BASE_SECONDS", "6"))
                attempt = 0
                tried_global = False

                while True:
                    try:
                        return self.client.models.generate_content(
                            model=self.llm_model,
                            contents=contents,
                            config=gen_config,
                        )
                    except Exception as e:
                        msg = str(e)
                        is_model_not_found = (
                            ("404" in msg or "NOT_FOUND" in msg)
                            and "Publisher Model" in msg
                            and "was not found" in msg
                        )
                        if (
                            is_model_not_found
                            and not tried_global
                            and getattr(self, "_gemini_use_vertex", False)
                            and (getattr(self, "_gemini_vertex_location", None) or "").lower() not in ("", "global")
                            and os.getenv("IUDEX_VERTEX_FALLBACK_GLOBAL_ON_NOT_FOUND", "true").lower() in ("1", "true", "yes")
                        ):
                            tried_global = True
                            prev_loc = getattr(self, "_gemini_vertex_location", None)
                            print(
                                f"{Fore.YELLOW}‚ö†Ô∏è  Modelo '{self.llm_model}' indispon√≠vel em '{prev_loc}'. "
                                f"Tentando Vertex AI em 'global'..."
                            )
                            cached_content = None
                            try:
                                gen_config.cached_content = None
                            except Exception:
                                pass
                            project = (
                                getattr(self, "_gemini_vertex_project", None)
                                or os.getenv("GOOGLE_CLOUD_PROJECT")
                            )
                            self.client = genai.Client(vertexai=True, project=project, location="global")
                            self._gemini_vertex_location = "global"
                            contents = f"{system_prompt}\n\n{user_content}"
                            continue

                        is_rate_limit = (
                            "429" in msg
                            or "RESOURCE_EXHAUSTED" in msg
                            or "rate limit" in msg.lower()
                        )
                        if not is_rate_limit:
                            raise
                        attempt += 1
                        if attempt > max_retries:
                            raise
                        sleep_for = min(base_sleep * (2 ** (attempt - 1)), 60.0)
                        sleep_for += random.uniform(0.2, 1.5)
                        print(
                            f"{Fore.YELLOW}‚è≥ Rate limit Gemini (Chunk {idx}). "
                            f"Aguardando {sleep_for:.1f}s (tentativa {attempt}/{max_retries})..."
                        )
                        time.sleep(sleep_for)

            # Executa chamada s√≠ncrona em thread separada com timeout
            start_time = time.time()
            timeout_seconds = int(os.getenv("IUDEX_GEMINI_TIMEOUT_SECONDS", "120"))
            try:
                response = await asyncio.wait_for(asyncio.to_thread(call_gemini), timeout=timeout_seconds)
            except asyncio.TimeoutError as e:
                print(f"{Fore.YELLOW}‚è±Ô∏è Timeout no Gemini (Chunk {idx}) ap√≥s {timeout_seconds}s")
                if depth < 2 and len(chunk_text) > 4000:
                    print(f"{Fore.MAGENTA}‚úÇÔ∏è Timeout detectado. Tentando ADAPTIVE CHUNKING...")
                    return await self._split_and_retry_async(
                        chunk_text,
                        idx,
                        system_prompt,
                        total,
                        contexto_estilo,
                        depth,
                        max_output_tokens_override=max_output_tokens_override,
                        disable_cache=disable_cache
                    )
                raise e
            duration = time.time() - start_time
            
            # Extract token counts from response metadata
            prompt_tokens = 0
            completion_tokens = 0
            try:
                if hasattr(response, 'usage_metadata'):
                    usage = response.usage_metadata
                    prompt_tokens = getattr(usage, 'prompt_token_count', 0) or 0
                    completion_tokens = getattr(usage, 'candidates_token_count', 0) or 0
            except: pass
            
            cached_tokens = 0
            if hasattr(response, 'usage_metadata'):
                usage = response.usage_metadata
                cached_tokens = getattr(usage, 'cached_content_token_count', 0) or 0
            metrics.record_call(
                "gemini",
                prompt_tokens,
                completion_tokens,
                duration,
                model=self.llm_model,
                cached_tokens_in=cached_tokens,
            )
            
            try:
                result = response.text
            except ValueError:
                result = "" # Fallback se bloqueado ou vazio
                
            # === SMART STITCHING (v2.10) ===
            if contexto_estilo and result:
                result = remover_eco_do_contexto(result, contexto_estilo)
            retry_result = await _retry_incomplete_table(result)
            if retry_result is not None:
                return retry_result

            if result:
                try:
                    result = remover_marcadores_continua(result)
                except Exception:
                    pass

            # === ADAPTIVE CHUNCHING CHECK (v2.10) ===
            # v2.24: ratio por PALAVRAS (mais robusto que chars) e ignorando metadados do transcript
            # Motivo: chunks brutos podem conter "SPEAKER X" e timestamps [HH:MM], que s√£o removidos na formata√ß√£o
            # e derrubam artificialmente o ratio len(result)/len(chunk_text), ativando chunking adaptativo indevidamente.
            def _strip_transcript_metadata_for_ratio(text: str) -> str:
                if not text:
                    return ""
                out_lines = []
                for ln in text.splitlines():
                    s = ln.strip()
                    # Remove headers de diariza√ß√£o
                    if re.match(r'^SPEAKER\s+\d+\s*$', s):
                        continue
                    # Remove timestamps no in√≠cio da linha
                    s = re.sub(r'^\[\d{1,2}:\d{2}(?::\d{2})?\]\s*', '', s)
                    out_lines.append(s)
                return "\n".join(out_lines)

            def _count_words(text: str) -> int:
                if not text:
                    return 0
                return len(re.findall(r'\b\w+\b', text, flags=re.UNICODE))

            mode_now = getattr(self, "_current_mode", "APOSTILA")
            in_words = _count_words(_strip_transcript_metadata_for_ratio(chunk_text))
            out_words = _count_words(result)
            ratio = (out_words / in_words) if in_words > 0 else 1.0
            # Limiares: mais tolerante em APOSTILA (limpeza de oralidade), mais estrito em FIDELIDADE
            threshold = 0.55 if str(mode_now).upper() != "FIDELIDADE" else 0.70
            is_compressed = ratio < threshold
            
            if (is_compressed or not result) and depth < 2 and len(chunk_text) > 4000:
                reason = "compress√£o excessiva" if is_compressed else "resposta vazia"
                if is_compressed:
                    print(f"\n{Fore.MAGENTA}‚úÇÔ∏è ATIVANDO CHUNKING ADAPTATIVO para Chunk {idx} (Motivo: {reason} | Ratio(palavras): {ratio:.2f} | in={in_words} out={out_words} | limiar={threshold:.2f})")
                else:
                    print(f"\n{Fore.MAGENTA}‚úÇÔ∏è ATIVANDO CHUNKING ADAPTATIVO para Chunk {idx} (Motivo: {reason})")
                metrics.record_adaptive_split()
                return await self._split_and_retry_async(
                    chunk_text,
                    idx,
                    system_prompt,
                    total,
                    contexto_estilo,
                    depth,
                    max_output_tokens_override=max_output_tokens_override,
                    disable_cache=disable_cache
                )

            # Truncamento por limite de tokens: split para evitar perda (especialmente no final do doc)
            if (
                not trunc_retry
                and depth < 2
                and len(chunk_text) > 4000
                and _near_token_limit(completion_tokens, max_output_tokens)
                and _looks_hard_truncated(result or "")
            ):
                print(
                    f"{Fore.MAGENTA}‚úÇÔ∏è Sa√≠da aparenta truncada por limite (Gemini, Chunk {idx}). "
                    "Dividindo chunk..."
                )
                metrics.record_adaptive_split()
                return await self._split_and_retry_async(
                    chunk_text,
                    idx,
                    system_prompt,
                    total,
                    contexto_estilo,
                    depth,
                    max_output_tokens_override=max_output_tokens_override,
                    disable_cache=True,
                )

            if depth == 0 and not disable_cache:
                self._save_chunk_cache(chunk_text, prompt_hash, result)
            return result

        except Exception as e:
            # Log full exception details
            import traceback
            print(f"{Fore.RED}‚ùå Detalhes do erro no Chunk {idx}:")
            traceback.print_exc()
            
            # Fallback Logic can trigger adaptive chunking too if it's a token limit error
            if "429" in str(e) or "RESOURCE_EXHAUSTED" in str(e):
                if depth < 2:
                    print(f"{Fore.MAGENTA}‚úÇÔ∏è Erro de limite detectado. Tentando ADAPTIVE CHUNKING...")
                    return await self._split_and_retry_async(
                        chunk_text,
                        idx,
                        system_prompt,
                        total,
                        contexto_estilo,
                        depth,
                        max_output_tokens_override=max_output_tokens_override,
                        disable_cache=disable_cache
                    )
            
            print(f"{Fore.YELLOW}‚ö†Ô∏è  Falha no Gemini (Chunk {idx}): {e}")
            
            if self.openai_client:
                print(f"{Fore.CYAN}ü§ñ Tentando fallback para OpenAI ({self.openai_model})...")
                try:
                    start_time_oai = time.time()
                    openai_kwargs = {}
                    if max_output_tokens_override:
                        openai_kwargs["max_completion_tokens"] = max_output_tokens_override
                    response = await self.openai_client.chat.completions.create(
                        model=self.openai_model,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_content}
                        ],
                        timeout=180,
                        **openai_kwargs
                    )
                    duration_oai = time.time() - start_time_oai
                    result = response.choices[0].message.content
                    
                    # Record OpenAI metrics
                    oai_prompt = getattr(response.usage, 'prompt_tokens', 0) if hasattr(response, 'usage') else 0
                    oai_compl = getattr(response.usage, 'completion_tokens', 0) if hasattr(response, 'usage') else 0
                    finish_reason = None
                    try:
                        finish_reason = response.choices[0].finish_reason
                    except Exception:
                        finish_reason = None
                    openai_truncated = (finish_reason == "length")
                    cached_tokens = 0
                    if hasattr(response, 'usage'):
                        details = getattr(response.usage, 'prompt_tokens_details', None)
                        cached_tokens = getattr(details, 'cached_tokens', 0) or 0 if details else 0
                    metrics.record_call(
                        "openai",
                        oai_prompt,
                        oai_compl,
                        duration_oai,
                        model=self.openai_model,
                        cached_tokens_in=cached_tokens,
                    )
                    
                    # Apply cleanup to OpenAI result too
                    if contexto_estilo and result:
                        result = remover_eco_do_contexto(result, contexto_estilo)
                    if result:
                        try:
                            result = remover_marcadores_continua(result)
                        except Exception:
                            pass

                    openai_near_limit = _near_token_limit(oai_compl, max_output_tokens_override) if max_output_tokens_override else False
                    if (
                        not trunc_retry
                        and depth < 2
                        and len(chunk_text) > 4000
                        and _looks_hard_truncated(result or "")
                        and (openai_truncated or openai_near_limit)
                    ):
                        print(
                            f"{Fore.MAGENTA}‚úÇÔ∏è Sa√≠da aparenta truncada por limite (OpenAI fallback, Chunk {idx}). "
                            "Dividindo chunk..."
                        )
                        metrics.record_adaptive_split()
                        return await self._split_and_retry_async(
                            chunk_text,
                            idx,
                            system_prompt,
                            total,
                            contexto_estilo,
                            depth,
                            max_output_tokens_override=max_output_tokens_override,
                            disable_cache=True,
                        )
                        
                    if depth == 0 and not disable_cache:
                        self._save_chunk_cache(chunk_text, prompt_hash, result)
                    return result
                except Exception as e_openai:
                    print(f"{Fore.RED}‚ùå Falha tamb√©m no OpenAI: {e_openai}")
                    raise e_openai
            else:
                print(f"{Fore.RED}‚ùå Falha no chunk {idx} e sem fallback OpenAI configurado.")
                raise e # Let tenacity handle retry

    async def _split_and_retry_async(
        self,
        text,
        idx,
        prompt,
        total,
        context,
        depth,
        max_output_tokens_override=None,
        disable_cache=False
    ):
        """Divides chunk in half and processes recursively"""
        mid = len(text) // 2
        # Try to find paragraph break near middle
        margin = int(len(text) * 0.2)
        start_search = max(0, mid - margin)
        end_search = min(len(text), mid + margin)
        search_zone = text[start_search:end_search]
        
        split_pos = -1
        last_para = search_zone.rfind('\n\n')
        if last_para != -1:
            split_pos = start_search + last_para + 2
        else:
            split_pos = mid # Fallback hard split
            
        part_a = text[:split_pos]
        part_b = text[split_pos:]
        
        print(f"   -> Dividindo (Sequencial): Parte A ({len(part_a)}c) + Parte B ({len(part_b)}c)")
        
        # ===================================================================
        # SEQUENTIAL EXECUTION (v2.10 Improvement)
        # Process A first, then use A's tail as context for B.
        # Trades speed (~2x slower) for style continuity and coherence.
        # ===================================================================
        
        res_a = await self._format_chunk_async(
            part_a,
            f"{idx}.A",
            prompt,
            total,
            context,
            depth + 1,
            max_output_tokens_override=max_output_tokens_override,
            disable_cache=disable_cache
        )
        
        # Use the tail of A's result as context for B
        context_for_b = res_a[-2000:] if len(res_a) > 2000 else res_a
        
        res_b = await self._format_chunk_async(
            part_b,
            f"{idx}.B",
            prompt,
            total,
            context_for_b,
            depth + 1,
            max_output_tokens_override=max_output_tokens_override,
            disable_cache=disable_cache
        )
        
        return f"{res_a}\n\n{res_b}"

    async def _identify_speaker_async(self, content, professors_info, speaker_label):
        """Identifica speaker com cache e heur√≠stica"""
        # Cache simples em mem√≥ria
        if not hasattr(self, 'speaker_cache'): self.speaker_cache = {}
        if speaker_label in self.speaker_cache: return self.speaker_cache[speaker_label]
        
        prompt = f"""
        Analise o in√≠cio do texto abaixo e a lista de professores extra√≠da da introdu√ß√£o.
        Identifique quem √© o prov√°vel professor falando e qual a disciplina.
        
        Falante (Label): {speaker_label if speaker_label else "Desconhecido"}
        
        Lista de Professores (Contexto):
        {professors_info}
        
        Texto (In√≠cio):
        {content[:5000]}...
        
        Retorne APENAS um JSON:
        {{
            "nome": "Nome do Professor",
            "disciplina": "Disciplina"
        }}
        """
        
        try:
            def call_gemini():
                return self.client.models.generate_content(
                    model=self.llm_model,
                    contents=f"Voc√™ √© um assistente que identifica palestrantes.\n\n{prompt}",
                    config=types.GenerateContentConfig(
                        temperature=0.1,
                        max_output_tokens=20000,
                        thinking_config=types.ThinkingConfig(
                            include_thoughts=False,
                            thinking_level="LOW"
                        )
                    )
                )

            response = await asyncio.to_thread(call_gemini)
            _record_genai_usage(response, model=self.llm_model)
            content_json = response.text
            self.speaker_cache[speaker_label] = content_json
            return content_json
        except Exception as e:
            return '{"nome": "Professor", "disciplina": "Disciplina"}'

    def _extract_professors_context(self, full_text):
        """Extrai lista de professores (Deep Scan)"""
        print(f"   üïµÔ∏è  Extraindo contexto de professores (Scan Completo)...")
        
        intro_context = full_text[:5000]
        keywords = ["meu nome √©", "sou o professor", "sou a professora", "aqui √© o professor"]
        
        found_contexts = []
        lower_text = full_text.lower()
        
        for keyword in keywords:
            start_idx = 0
            while True:
                idx = lower_text.find(keyword, start_idx)
                if idx == -1: break
                start_ctx = max(0, idx - 500)
                end_ctx = min(len(full_text), idx + 500)
                found_contexts.append(full_text[start_ctx:end_ctx])
                start_idx = idx + len(keyword)
        
        combined_context = intro_context + "\n\n... [TRECHOS] ...\n\n" + "\n\n".join(found_contexts)
        if len(combined_context) > 50000: combined_context = combined_context[:50000]

        try:
            response = self.client.models.generate_content(
                model=self.llm_model,
                contents=f"Extraia professores JSON\n\n{combined_context}",
                config=types.GenerateContentConfig(
                    temperature=0.1,
                    thinking_config=types.ThinkingConfig(
                        include_thoughts=False,
                        thinking_level="LOW"
                    )
                )
            )
            _record_genai_usage(response, model=self.llm_model)
            return response.text
        except:
            return "{'professores': []}"

    async def _generate_header_async(self, formatted_content, professor_context_json):
        try:
            try:
                prof_ctx = json.loads(professor_context_json)
                prof = prof_ctx.get("nome", "Professor")
                disc = prof_ctx.get("disciplina", "Disciplina")
            except:
                prof = "Professor"
                disc = "Disciplina"
            
            prompt = f"""
            Gere APENAS o t√≠tulo Markdown para esta se√ß√£o.
            Professor: {prof}
            Disciplina: {disc}
            
            Conte√∫do:
            {formatted_content[:1000]}...
            
            FORMATO DE SA√çDA:
            # Prof. {prof} - {disc}
            """
            
            try:
                def call_gemini():
                    return self.client.models.generate_content(
                        model=self.llm_model,
                        contents=prompt,
                        config=types.GenerateContentConfig(
                            max_output_tokens=100,
                            thinking_config=types.ThinkingConfig(
                                include_thoughts=False,
                                thinking_level="LOW"
                            )
                        )
                    )
                response = await asyncio.to_thread(call_gemini)
                _record_genai_usage(response, model=self.llm_model)
                return response.text.strip()
            except Exception as e:
                print(f"{Fore.YELLOW}‚ö†Ô∏è  Falha no header (Gemini): {e}")
                
                if self.openai_client:
                     print(f"{Fore.CYAN}ü§ñ Fallback: Header com OpenAI...")
                     try:
                        response = await self.openai_client.chat.completions.create(
                            model=self.openai_model,
                            messages=[{"role": "user", "content": prompt}],
                            max_tokens=100
                        )
                        _record_openai_usage(response, model=self.openai_model)
                        return response.choices[0].message.content.strip()
                     except Exception as e_openai:
                         print(f"{Fore.RED}‚ùå Falha tamb√©m no OpenAI: {e_openai}")
                
                # Fallback final se tudo falhar
                return "## T√≥pico (Recuperado)"
        except: return f"# {prof} - {disc}\n"

    async def _process_segment_parallel(self, segment, professors_info, idx, system_prompt):
        speaker = segment['speaker']
        content = segment['content']
        
        print(f"\n{Fore.YELLOW}‚ñ∂ Segmento {idx+1} ({speaker})...")
        chunks = self._smart_chunk_with_overlap(content, max_size=8000, overlap=1500)
        print(f"   {len(chunks)} chunks de ~8k chars (com 1.5k overlap)")
        
        context_task = self._identify_speaker_async(content[:5000], professors_info, speaker)
        
        chunk_tasks = [
            self._format_chunk_async(chunk, j, system_prompt)
            for j, chunk in enumerate(chunks)
        ]
        
        prof_context, *formatted_parts = await asyncio.gather(context_task, *chunk_tasks)
        
        full_content = remover_overlap_duplicado(formatted_parts)
        # Fallback para limpeza fina
        full_content = remover_paragrafos_identicos_consecutivos(full_content)
        header = await self._generate_header_async(full_content[:10000], prof_context)
        
        return f"{header}\n\n{full_content}\n\n---\n\n"

    def _renumber_topics(self, text):
        """Renumera t√≥picos sequencialmente"""
        lines = text.split('\n')
        new_lines = []
        main_counter = 0
        sub_counter = 0
        
        for line in lines:
            match_main = re.match(r'^##\s+\d+\.?\s+(.+)', line)
            if match_main:
                main_counter += 1
                sub_counter = 0
                new_lines.append(f"## {main_counter}. {match_main.group(1)}")
                continue
            
            match_sub = re.match(r'^###\s+(?:\d+(?:\.\d+)?\.?)?\s*(.+)', line)
            if match_sub:
                sub_counter += 1
                new_lines.append(f"### {main_counter}.{sub_counter} {match_sub.group(1)}")
                continue
                
            new_lines.append(line)
        return "\n".join(new_lines)

    def _fix_omissions(self, raw_transcript, formatted_text, omissions_report):
        """Tenta corrigir as omiss√µes detectadas usando abordagem targeted chunk-by-chunk"""
        print(f"{Fore.CYAN}üîß Tentando corrigir omiss√µes automaticamente...")
        
        extraction_prompt = """# TAREFA: EXTRAIR CONTE√öDO OMITIDO
Voc√™ receber√°:
1. RELAT√ìRIO DE OMISS√ïES: Lista do que est√° faltando
2. TRANSCRI√á√ÉO BRUTA: Onde encontrar o conte√∫do

## SUA MISS√ÉO:
Localize na transcri√ß√£o bruta os trechos exatos que correspondem √†s omiss√µes listadas.
Para cada omiss√£o, extraia o trecho relevante da transcri√ß√£o.

## RELAT√ìRIO DE OMISS√ïES:
{report}

Retorne APENAS os trechos extra√≠dos, sem coment√°rios adicionais."""

        try:
            # v2.28: Expandir limite para cobrir transcri√ß√µes longas (Gemini suporta ~1M tokens)
            max_transcript_chars = 500_000
            transcript_excerpt = raw_transcript[:max_transcript_chars]
            if len(raw_transcript) > max_transcript_chars:
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è Transcri√ß√£o truncada para {max_transcript_chars:,} chars (total: {len(raw_transcript):,})")
            extract_response = self.client.models.generate_content(
                model=self.llm_model,
                contents=f"{extraction_prompt.format(report=omissions_report)}\n\nTRANSCRI√á√ÉO BRUTA:\n{transcript_excerpt}",
                config=types.GenerateContentConfig(
                    thinking_config=types.ThinkingConfig(
                        include_thoughts=False,
                        thinking_level="HIGH"
                    )
                )
            )
            _record_genai_usage(extract_response, model=self.llm_model)
            
            # Gemini client returns .text, not .choices[0].message.content (OpenAI style)
            missing_content = extract_response.text
            print(f"{Fore.CYAN}   üìù Conte√∫do omitido extra√≠do ({len(missing_content)} caracteres)")
            
            chunks = self._smart_chunk_with_overlap(formatted_text, max_size=self.MAX_CHUNK_SIZE)
            fixed_chunks = []
            
            insertion_prompt = """# TAREFA: INSERIR CONTE√öDO FALTANTE
Voc√™ receber√°:
1. Um TRECHO da apostila formatada
2. CONTE√öDO OMITIDO que precisa ser adicionado

## SUA MISS√ÉO:
Se o trecho da apostila √© onde o conte√∫do omitido deveria estar, insira-o naturalmente.
Se n√£o for o local apropriado, retorne o trecho INALTERADO.

## REGRAS:
- Mantenha toda formata√ß√£o Markdown
- Integre o conte√∫do omitido de forma fluida
- Use tom did√°tico e formal (3¬™ pessoa)
- N√ÉO remova nada existente

## CONTE√öDO OMITIDO A INSERIR:
{missing}

Retorne o trecho (modificado ou inalterado)."""

            for i, chunk in enumerate(chunks):
                print(f"{Fore.CYAN}   üîß Processando chunk {i+1}/{len(chunks)}...")
                try:
                    fix_response = self.client.models.generate_content(
                        model=self.llm_model,
                        contents=f"{insertion_prompt.format(missing=missing_content)}\n\n{chunk}",
                        config=types.GenerateContentConfig(
                            thinking_config=types.ThinkingConfig(
                                include_thoughts=False,
                                thinking_level="HIGH"
                            )
                        )
                    )
                    _record_genai_usage(fix_response, model=self.llm_model)
                    fixed_chunks.append(fix_response.text)
                except Exception as chunk_error:
                    print(f"{Fore.YELLOW}   ‚ö†Ô∏è  Erro no chunk {i+1}, mantendo original: {chunk_error}")
                    fixed_chunks.append(chunk)
            
            fixed_text = "\n\n".join(fixed_chunks)
            print(f"{Fore.GREEN}   ‚úÖ Texto corrigido gerado ({len(fixed_chunks)} chunks processados)")

            # v2.30: Re-valida√ß√£o heur√≠stica leve p√≥s-corre√ß√£o (sem custo LLM extra)
            reval_ok, reval_issues = self._validate_preservation_heuristics(raw_transcript, fixed_text)
            if not reval_ok:
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è Re-valida√ß√£o p√≥s-fix detectou {len(reval_issues)} problemas:")
                for ri in reval_issues[:3]:
                    print(f"      - {ri}")
            else:
                print(f"{Fore.GREEN}   ‚úÖ Re-valida√ß√£o p√≥s-fix aprovada")

            return fixed_text, reval_ok

        except Exception as e:
            print(f"{Fore.RED}   ‚ùå Falha ao corrigir omiss√µes: {e}")
            return formatted_text, False

    def _validate_preservation_heuristics(self, original_text, formatted_text):
        """Valida√ß√£o Heur√≠stica com Toler√¢ncia Adaptativa"""
        print(f"\n{Fore.CYAN}üîç Valida√ß√£o Heur√≠stica de Preserva√ß√£o (Adaptativa)...")
        issues = []
        
        # 1. Refer√™ncias legais e jurisprudenciais (v2.30: padr√£o expandido)
        _LEGAL_REF_PATTERN = (
            r'(?:Lei\s+(?:Complementar\s+|Ordin√°ria\s+)?|LC\s+|Decreto(?:-Lei)?\s+|DL\s+|MP\s+|Medida\s+Provis√≥ria\s+)'
            r'n?¬∫?\s*[\d\.]+(?:/\d+)?'
            r'|Art\.?\s*\d+[¬∞¬∫]?'
            r'|S√∫mula(?:\s+Vinculante)?\s+\d+'
            r'|(?:REsp|RE|HC|MS|ADI|ADPF|ADC|RCL|Rcl|AgRg|AREsp)\s*n?¬∫?\s*[\d\.\/\-]+'
            r'|Tema\s+(?:de\s+)?(?:Repercuss√£o\s+Geral\s+)?\d+'
            r'|Informativo\s+\d+'
        )
        original_laws = set(re.findall(_LEGAL_REF_PATTERN, original_text, re.IGNORECASE))
        formatted_laws = set(re.findall(_LEGAL_REF_PATTERN, formatted_text, re.IGNORECASE))
        missing_laws = original_laws - formatted_laws
        if missing_laws:
            # Logar as refer√™ncias espec√≠ficas para facilitar revis√£o
            samples = list(missing_laws)[:5]
            issues.append(f"‚ùå {len(missing_laws)} refer√™ncias legais/jurisprudenciais omitidas: {', '.join(samples)}")
        else:
            print(f"{Fore.GREEN}   ‚úÖ Refer√™ncias legais preservadas ({len(original_laws)} encontradas)")
        
        # 2. Comprimento Adaptativo (L√≥gica do script Gemini)
        palavras_input = len(original_text.split())
        palavras_output = len(formatted_text.split())
        
        if palavras_input == 0: ratio = 0
        else: ratio = palavras_output / palavras_input
        
        # Heur√≠stica de Oralidade
        marcadores_oralidade = ['n√©', 'ent√£o', 'tipo', 'a√≠', 'pessoal', 'galera', 't√°', 'olha', 'gente', 'veja', 'bom']
        input_lower = original_text.lower()
        count_oralidade = sum(input_lower.count(m) for m in marcadores_oralidade)
        densidade_oralidade = count_oralidade / palavras_input if palavras_input > 0 else 0
        
        # Define toler√¢ncia baseada na densidade
        if densidade_oralidade > 0.025:  # Muito coloquial (>2.5%)
            tolerancia = 0.45  # Aceita reduzir at√© 45%
            tipo = "Muito Coloquial"
        elif densidade_oralidade > 0.015:  # M√©dio
            tolerancia = 0.38
            tipo = "Coloquial"
        elif densidade_oralidade > 0.008:  # Pouca
            tolerancia = 0.30
            tipo = "Pouca Oralidade"
        else:  # T√©cnico
            tolerancia = 0.22
            tipo = "T√©cnico/Denso"
            
        limite_minimo = 1.0 - tolerancia
        
        print(f"   üìä An√°lise: {tipo} (Densidade: {densidade_oralidade:.2%})")
        print(f"      Ratio atual: {ratio:.1%} (M√≠nimo aceit√°vel: {limite_minimo:.1%})")
        
        if ratio < limite_minimo:
            issues.append(f"‚ö†Ô∏è Texto formatado muito curto ({ratio:.1%}). Esperado no m√≠nimo {limite_minimo:.1%}")
        else:
            print(f"{Fore.GREEN}   ‚úÖ Comprimento aprovado")
        
        if issues:
            print(f"{Fore.RED}‚îÅ‚îÅ‚îÅ PROBLEMAS ‚îÅ‚îÅ‚îÅ")
            for i in issues: print(f"   {i}")
            return False, issues
        return True, []

    def validate_completeness_full(self, raw_transcript, formatted_text, video_name, global_structure=None):
        """
        v2.16: Valida√ß√£o LLM Full-Context - Envia documento INTEIRO para an√°lise.
        Aproveita a janela de contexto do Gemini 3 Flash (2M tokens).
        """
        print(f"{Fore.YELLOW}üîç Valida√ß√£o LLM Full-Context (v2.16)...")
        
        # Calcular tamanho aproximado em tokens (estimativa: 4 chars = 1 token)
        total_chars = len(raw_transcript) + len(formatted_text)
        estimated_tokens = total_chars // 4
        print(f"   üìä Tamanho estimado: {estimated_tokens:,} tokens")
        
        # Safety check: Se exceder 1.5M tokens, usar fallback de amostragem
        if estimated_tokens > 1_500_000:
            print(f"{Fore.YELLOW}   ‚ö†Ô∏è Documento muito grande ({estimated_tokens:,} tokens). Usando valida√ß√£o por amostragem.")
            return self._validate_by_sampling(raw_transcript, formatted_text, video_name)
        
        validation_prompt = """# TAREFA DE VALIDA√á√ÉO DE FIDELIDADE (Full-Context)

Voc√™ √© um auditor de qualidade para transcri√ß√µes jur√≠dicas formatadas.

## SEU OBJETIVO
Compare o TEXTO ORIGINAL (transcri√ß√£o bruta) com o TEXTO FORMATADO (apostila) e identifique:

1. **OMISS√ïES GRAVES**: Conceitos jur√≠dicos, leis, s√∫mulas, artigos ou exemplos importantes que estavam no original mas foram omitidos no formatado.
2. **DISTOR√á√ïES**: Informa√ß√µes que foram alteradas de forma que mude o sentido jur√≠dico.
3. **ESTRUTURA**: Verifique se os t√≥picos e subt√≥picos est√£o organizados de forma l√≥gica e se n√£o h√° duplica√ß√µes.

## REGRAS
- N√ÉO considere como omiss√£o: hesita√ß√µes, "n√©", "ent√£o", dados repetitivos, conversas paralelas.
- CONSIDERE como omiss√£o: qualquer lei, s√∫mula, artigo, jurisprud√™ncia, exemplo pr√°tico ou dica de prova.
- Preste aten√ß√£o especial em: n√∫meros de leis, prazos, percentuais, valores monet√°rios.
- N√ÉO fa√ßa an√°lise jur√≠dica externa nem verifique a veracidade de leis.
- Sua sa√≠da deve refletir apenas diverg√™ncias entre o texto bruto e o formatado.

## FORMATO DE RESPOSTA (JSON)
{
    "aprovado": true/false,
    "nota_fidelidade": 0-10,
    "omissoes_graves": ["descri√ß√£o clara do item omitido"],
    "distorcoes": ["descri√ß√£o clara da distor√ß√£o"],
    "problemas_estrutura": ["t√≠tulos duplicados ou hierarquia quebrada"],
    "observacoes": "coment√°rio geral sobre a qualidade"
}

Retorne APENAS o JSON, sem markdown."""

        structure_context = ""
        if global_structure:
            structure_context = f"\n\n## ESTRUTURA ESPERADA (Mapeamento Inicial):\n{global_structure[:5000]}"
        
        try:
            response = self.client.models.generate_content(
                model=self.llm_model,
                contents=f"""{validation_prompt}{structure_context}

## TEXTO ORIGINAL (Transcri√ß√£o Bruta):
{raw_transcript}

## TEXTO FORMATADO (Apostila):
{formatted_text}
""",
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    max_output_tokens=8000,
                    thinking_config=types.ThinkingConfig(
                        include_thoughts=False,
                        thinking_level="HIGH"
                    )
                )
            )
            _record_genai_usage(response, model=self.llm_model)
            
            # === ROBUST JSON PARSING (v2.25) ===
            result = None
            raw_text = response.text.strip()
            
            # Attempt 1: Direct JSON parse
            try:
                result = json.loads(raw_text)
            except json.JSONDecodeError:
                pass
            
            # Attempt 2: Extract JSON from markdown code block
            if result is None:
                import re
                json_match = re.search(r'```(?:json)?\s*([\s\S]*?)```', raw_text)
                if json_match:
                    try:
                        result = json.loads(json_match.group(1).strip())
                    except json.JSONDecodeError:
                        pass
            
            # Attempt 3: Find JSON object in text
            if result is None:
                brace_start = raw_text.find('{')
                brace_end = raw_text.rfind('}')
                if brace_start != -1 and brace_end > brace_start:
                    try:
                        result = json.loads(raw_text[brace_start:brace_end+1])
                    except json.JSONDecodeError:
                        pass
            
            # Attempt 4: Try ast.literal_eval as last resort
            if result is None:
                import ast
                try:
                    brace_start = raw_text.find('{')
                    brace_end = raw_text.rfind('}')
                    if brace_start != -1 and brace_end > brace_start:
                        result = ast.literal_eval(raw_text[brace_start:brace_end+1])
                except (ValueError, SyntaxError):
                    pass
            
            # Attempt 5: Retry with stricter prompt
            if result is None:
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è JSON malformado, tentando retry com prompt estrito...")
                retry_response = self.client.models.generate_content(
                    model=self.llm_model,
                    contents=f"""O seguinte texto deveria ser um JSON v√°lido mas est√° malformado. 
Corrija-o e retorne APENAS o JSON v√°lido, sem explica√ß√µes:

{raw_text[:3000]}

O JSON deve ter exatamente esta estrutura:
{{"aprovado": true/false, "nota_fidelidade": 0-10, "omissoes_graves": [], "distorcoes": [], "problemas_estrutura": [], "observacoes": ""}}""",
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json",
                        max_output_tokens=2000
                    )
                )
                _record_genai_usage(retry_response, model=self.llm_model)
                try:
                    result = json.loads(retry_response.text.strip())
                except json.JSONDecodeError:
                    print(f"{Fore.RED}   ‚ùå Retry tamb√©m falhou, usando valores padr√£o.")
                    result = {"aprovado": False, "nota_fidelidade": 0, "erro_validacao": True, "requires_manual_review": True, "observacoes": "Parsing JSON falhou ap√≥s 5 tentativas. Revis√£o manual necess√°ria."}
            
            if isinstance(result, list):
                if len(result) > 0 and isinstance(result[0], dict):
                    result = result[0]
                else:
                    result = {}

            # Processar resultado
            aprovado = result.get('aprovado', True)
            nota = result.get('nota_fidelidade', 10)
            omissoes = result.get('omissoes_graves', [])
            distorcoes = result.get('distorcoes', [])
            problemas_estrutura = result.get('problemas_estrutura', [])
            observacoes = result.get('observacoes', '')
            
            # Log do resultado
            if aprovado:
                print(f"{Fore.GREEN}   ‚úÖ Valida√ß√£o Full-Context APROVADA (Nota: {nota}/10)")
            else:
                print(f"{Fore.RED}   ‚ùå Valida√ß√£o Full-Context REPROVADA (Nota: {nota}/10)")
                if omissoes:
                    print(f"{Fore.RED}   üìå Omiss√µes: {len(omissoes)}")
                    for o in omissoes[:3]:
                        print(f"      - {o[:100]}...")
                if distorcoes:
                    print(f"{Fore.RED}   ‚ö†Ô∏è Distor√ß√µes: {len(distorcoes)}")
                if problemas_estrutura:
                    print(f"{Fore.YELLOW}   üèóÔ∏è Problemas de Estrutura: {len(problemas_estrutura)}")
            
            # Retornar relat√≥rio completo
            return {
                'aprovado': aprovado,
                'nota': nota,
                'omissoes': omissoes,
                'distorcoes': distorcoes,
                'problemas_estrutura': problemas_estrutura,
                'observacoes': observacoes
            }
            
        except Exception as e:
            print(f"{Fore.RED}   ‚ùå Erro na valida√ß√£o Full-Context: {e}")
            print(f"{Fore.RED}   ‚ö†Ô∏è ATEN√á√ÉO: Documento requer revis√£o manual (valida√ß√£o falhou).")
            return {
                'aprovado': False,
                'nota': 0,
                'erro_validacao': True,
                'requires_manual_review': True,
                'omissoes': [],
                'distorcoes': [],
                'problemas_estrutura': [],
                'observacoes': f'ATEN√á√ÉO: Valida√ß√£o falhou ({str(e)}). Revis√£o manual recomendada.'
            }

    def validate_fidelity_primary(
        self,
        raw_transcript,
        formatted_text,
        video_name,
        modo="APOSTILA",
        include_sources=False,
    ):
        """
        Auditoria de fidelidade prim√°ria (preventiva) com fallback para full-context.
        Retorna um relat√≥rio compat√≠vel com o formato antigo (_fidelidade.json).
        """
        if FIDELITY_AUDIT_AVAILABLE and FIDELITY_AUDIT_ENABLED:
            result = auditar_fidelidade_preventiva(
                self.client,
                raw_transcript,
                formatted_text,
                video_name,
                output_path=None,
                modo=modo,
                include_sources=include_sources,
            )
            compat = (result or {}).get("compat_fidelidade")
            if isinstance(compat, dict) and compat:
                return compat
            return result or {}

        fallback = self.validate_completeness_full(raw_transcript, formatted_text, video_name, None)
        if isinstance(fallback, dict):
            fallback["source"] = "validate_completeness_full"
        return fallback

    def _validate_by_sampling(self, raw_transcript, formatted_text, video_name):
        """Fallback: Valida√ß√£o por amostragem para documentos muito grandes (>1.5M tokens).

        Processa 3 janelas (IN√çCIO, MEIO, FIM) de 80k chars cada via LLM e agrega os resultados.
        """
        print(f"{Fore.CYAN}   Usando valida√ß√£o por amostragem (3 janelas)...")

        window_size = 80000
        mid_raw = len(raw_transcript) // 2
        mid_fmt = len(formatted_text) // 2
        half_window = window_size // 2

        windows = [
            ("IN√çCIO", raw_transcript[:window_size], formatted_text[:window_size]),
            ("MEIO", raw_transcript[max(0, mid_raw - half_window):mid_raw + half_window],
                      formatted_text[max(0, mid_fmt - half_window):mid_fmt + half_window]),
            ("FIM", raw_transcript[-window_size:], formatted_text[-window_size:]),
        ]

        validation_prompt = """# TAREFA DE VALIDA√á√ÉO DE FIDELIDADE (Amostragem)

Voc√™ √© um auditor de qualidade para transcri√ß√µes jur√≠dicas formatadas.

## SEU OBJETIVO
Compare o TEXTO ORIGINAL (transcri√ß√£o bruta) com o TEXTO FORMATADO (apostila) e identifique:

1. **OMISS√ïES GRAVES**: Conceitos jur√≠dicos, leis, s√∫mulas, artigos ou exemplos importantes que estavam no original mas foram omitidos no formatado.
2. **DISTOR√á√ïES**: Informa√ß√µes que foram alteradas de forma que mude o sentido jur√≠dico.
3. **ESTRUTURA**: Verifique se os t√≥picos e subt√≥picos est√£o organizados de forma l√≥gica e se n√£o h√° duplica√ß√µes.

## REGRAS
- N√ÉO considere como omiss√£o: hesita√ß√µes, "n√©", "ent√£o", dados repetitivos, conversas paralelas.
- CONSIDERE como omiss√£o: qualquer lei, s√∫mula, artigo, jurisprud√™ncia, exemplo pr√°tico ou dica de prova.
- Preste aten√ß√£o especial em: n√∫meros de leis, prazos, percentuais, valores monet√°rios.
- N√ÉO fa√ßa an√°lise jur√≠dica externa nem verifique a veracidade de leis.
- Sua sa√≠da deve refletir apenas diverg√™ncias entre o texto bruto e o formatado.

## FORMATO DE RESPOSTA (JSON)
{
    "aprovado": true/false,
    "nota_fidelidade": 0-10,
    "omissoes_graves": ["descri√ß√£o clara do item omitido"],
    "distorcoes": ["descri√ß√£o clara da distor√ß√£o"],
    "problemas_estrutura": ["t√≠tulos duplicados ou hierarquia quebrada"],
    "observacoes": "coment√°rio geral sobre a qualidade"
}

Retorne APENAS o JSON, sem markdown."""

        all_results = []
        for label, raw_window, fmt_window in windows:
            print(f"{Fore.CYAN}   üìä Validando janela {label}...")
            try:
                response = self.client.models.generate_content(
                    model=self.llm_model,
                    contents=f"""{validation_prompt}

## JANELA: {label}

## TEXTO ORIGINAL (Transcri√ß√£o Bruta):
{raw_window}

## TEXTO FORMATADO (Apostila):
{fmt_window}
""",
                    config=types.GenerateContentConfig(
                        response_mime_type="application/json",
                        max_output_tokens=4000,
                        thinking_config=types.ThinkingConfig(
                            include_thoughts=False,
                            thinking_level="MEDIUM"
                        )
                    )
                )
                _record_genai_usage(response, model=self.llm_model)

                raw_text = response.text.strip()
                result = None
                try:
                    result = json.loads(raw_text)
                except json.JSONDecodeError:
                    brace_start = raw_text.find('{')
                    brace_end = raw_text.rfind('}')
                    if brace_start != -1 and brace_end > brace_start:
                        try:
                            result = json.loads(raw_text[brace_start:brace_end + 1])
                        except json.JSONDecodeError:
                            pass

                if isinstance(result, dict):
                    all_results.append(result)
                    nota_w = result.get('nota_fidelidade', 10)
                    print(f"{Fore.GREEN}      ‚úÖ {label}: nota {nota_w}/10")
                else:
                    print(f"{Fore.YELLOW}      ‚ö†Ô∏è {label}: resposta inv√°lida, ignorando janela")

            except Exception as e:
                print(f"{Fore.RED}      ‚ùå {label}: erro ‚Äî {e}")

        if not all_results:
            print(f"{Fore.RED}   ‚ùå Nenhuma janela validada. Revis√£o manual necess√°ria.")
            return {
                'aprovado': False, 'nota': 0,
                'omissoes': [], 'distorcoes': [], 'problemas_estrutura': [],
                'observacoes': 'Valida√ß√£o por amostragem falhou em todas as janelas. Revis√£o manual obrigat√≥ria.',
                'erro_validacao': True, 'requires_manual_review': True,
            }

        # Agregar resultados: aprovado s√≥ se TODAS as janelas aprovaram
        aprovado = all(r.get('aprovado', True) for r in all_results)
        notas = [r.get('nota_fidelidade', 10) for r in all_results]
        nota_media = sum(notas) / len(notas)
        omissoes = []
        distorcoes = []
        problemas = []
        for r in all_results:
            omissoes.extend(r.get('omissoes_graves', []) or [])
            distorcoes.extend(r.get('distorcoes', []) or [])
            problemas.extend(r.get('problemas_estrutura', []) or [])

        obs_parts = [r.get('observacoes', '') for r in all_results if r.get('observacoes')]
        observacoes = f"Valida√ß√£o por amostragem ({len(all_results)}/3 janelas). " + " | ".join(obs_parts)

        print(f"{Fore.GREEN if aprovado else Fore.RED}   {'‚úÖ' if aprovado else '‚ùå'} Resultado agregado: nota {nota_media:.1f}/10 ({len(all_results)} janelas)")

        return {
            'aprovado': aprovado,
            'nota': round(nota_media, 1),
            'omissoes': omissoes,
            'distorcoes': distorcoes,
            'problemas_estrutura': problemas,
            'observacoes': observacoes,
        }

    async def auto_fix_structure(self, formatted_text: str, problemas: list, global_structure: str = None) -> str:
        """
        v2.17: Corretor IA Ativo - Corrige automaticamente problemas estruturais.
        
        Recebe o texto formatado e a lista de problemas detectados pelo auditor,
        e envia ao LLM para corre√ß√£o autom√°tica.
        
        Args:
            formatted_text: Texto markdown com problemas
            problemas: Lista de strings descrevendo os problemas estruturais
            global_structure: Estrutura esperada (opcional)
        
        Returns:
            Texto corrigido
        """
        print(f"{Fore.CYAN}üîß Corretor IA Ativo (v2.17)...")
        print(f"   üìã Problemas a corrigir: {len(problemas)}")
        for p in problemas[:3]:
            print(f"      - {p[:80]}...")
        
        fix_prompt = """# TAREFA DE CORRE√á√ÉO ESTRUTURAL

Voc√™ √© um editor de documentos jur√≠dicos formatados em Markdown.

## SEU OBJETIVO
Corrija os problemas estruturais listados abaixo NO TEXTO FORNECIDO.

## PROBLEMAS A CORRIGIR:
{problemas}

## REGRAS DE CORRE√á√ÉO:
1. **T√≠tulos Duplicados**: Se um t√≠tulo H2 aparece duas vezes, REMOVA a segunda ocorr√™ncia e mescle o conte√∫do sob o primeiro.
2. **Hierarquia Quebrada**: Se um H3 est√° fora de seu H2 pai correto, MOVA-O para debaixo do H2 apropriado.
3. **Par√°grafos Repetidos**: Se o mesmo par√°grafo aparece duas vezes, REMOVA a duplicata.
4. **Numera√ß√£o Inconsistente**: Se a numera√ß√£o est√° errada (ex: 1, 2, 3, 3, 4), RENUMERE sequencialmente.

## IMPORTANTE:
- N√ÉO altere o conte√∫do textual, apenas a estrutura.
- N√ÉO adicione novos conte√∫dos.
- N√ÉO remova informa√ß√µes jur√≠dicas (leis, s√∫mulas, artigos).
- Mantenha todas as tabelas intactas.
- Preserve a formata√ß√£o Markdown (negrito, it√°lico, listas).

## FORMATO DE RESPOSTA:
Retorne APENAS o texto Markdown corrigido, sem explica√ß√µes adicionais."""

        structure_hint = ""
        if global_structure:
            structure_hint = f"\n\n## ESTRUTURA ESPERADA:\n{global_structure[:3000]}"
        
        try:
            response = self.client.models.generate_content(
                model=self.llm_model,
                contents=f"""{fix_prompt.format(problemas=chr(10).join(f'- {p}' for p in problemas))}{structure_hint}

## TEXTO A CORRIGIR:
{formatted_text}
""",
                config=types.GenerateContentConfig(
                    max_output_tokens=65000,  # Documento pode ser grande
                    thinking_config=types.ThinkingConfig(
                        include_thoughts=False,
                        thinking_level="HIGH"
                    )
                )
            )
            _record_genai_usage(response, model=self.llm_model)
            
            fixed_text = response.text.strip()
            
            # Valida√ß√£o b√°sica: o texto corrigido n√£o deve ser muito menor
            if len(fixed_text) < len(formatted_text) * 0.7:
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è Texto corrigido muito curto ({len(fixed_text)} vs {len(formatted_text)}). Mantendo original.")
                return formatted_text
            
            # Remover poss√≠veis wrappers de c√≥digo markdown que o LLM pode adicionar
            if fixed_text.startswith('```markdown'):
                fixed_text = fixed_text[len('```markdown'):].strip()
            if fixed_text.startswith('```'):
                fixed_text = fixed_text[3:].strip()
            if fixed_text.endswith('```'):
                fixed_text = fixed_text[:-3].strip()
            
            print(f"{Fore.GREEN}   ‚úÖ Corre√ß√£o autom√°tica aplicada com sucesso")
            return fixed_text
            
        except Exception as e:
            print(f"{Fore.RED}   ‚ùå Erro no Corretor IA: {e}")
            print(f"{Fore.YELLOW}   ‚ÑπÔ∏è Mantendo texto original")
            return formatted_text


    def _generate_audit_report(self, video_name, heuristic_issues, llm_issues):
        with open(f"audit_{video_name}.md", 'w') as f:
            f.write(f"# Auditoria: {video_name}\n")
            f.write(f"Heur√≠stica: {heuristic_issues}\n")
            f.write(f"LLM: {llm_issues}\n")

    async def map_structure(self, full_text):
        """Creates a global structure skeleton to guide the formatting."""
        _map_t0 = time.time()
        print(f"{Fore.CYAN}üó∫Ô∏è  Mapeando estrutura global do documento... [start={time.strftime('%H:%M:%S')}]")
        
        full_text = full_text or ""
        max_single = int(os.getenv("IUDEX_MAP_MAX_SINGLE_CHARS", self.MAP_MAX_SINGLE_CHARS))
        map_chunk_chars = int(os.getenv("IUDEX_MAP_CHUNK_CHARS", self.MAP_CHUNK_CHARS))
        map_chunk_overlap = int(os.getenv("IUDEX_MAP_CHUNK_OVERLAP_CHARS", self.MAP_CHUNK_OVERLAP_CHARS))

        # Preferir mapear o documento inteiro em uma chamada quando estiver dentro de um limite seguro.
        # Para transcri√ß√µes muito longas, usar chunking + merge determin√≠stico para cobrir IN√çCIO‚ÜíFIM.
        if len(full_text) <= max_single:
            input_samples = [full_text]
        else:
            print(f"{Fore.YELLOW}   ‚ÑπÔ∏è  Transcri√ß√£o longa ({len(full_text):,} chars). Mapeando em chunks...")
            map_chunking_mode = os.getenv("IUDEX_MAP_CHUNKING_MODE", "auto").strip().lower()
            input_samples = None
            if map_chunking_mode != "safe":
                input_samples = chunk_texto_por_segmentos(
                    full_text,
                    max_chars=map_chunk_chars,
                    overlap_chars=map_chunk_overlap,
                )
                if input_samples:
                    print(f"{Fore.CYAN}   üß© Chunking por segmentos (map) ativado: {len(input_samples)}")
            if not input_samples:
                input_samples = chunk_texto_seguro(
                    full_text,
                    max_chars=map_chunk_chars,
                    overlap_chars=map_chunk_overlap,
                )
                print(f"{Fore.CYAN}   üì¶ Chunks para mapeamento: {len(input_samples)}")
        fallback_sample = input_samples[0] if input_samples else full_text[:200000]

        def _extract_map_lines(text: str) -> list[dict]:
            items = []
            if not text:
                return items
            for raw_line in (text or "").splitlines():
                line = raw_line.strip()
                if not line or "| ABRE:" not in line:
                    continue
                m = re.match(
                    r'^\s*(\d+(?:\.\d+)*)\.\s*([^|]+?)\s*\|\s*ABRE:\s*"([^"]+)"\s*\|\s*FECHA:\s*"([^"]+)"\s*$',
                    raw_line,
                    flags=re.IGNORECASE,
                )
                if not m:
                    continue
                number = m.group(1).strip()
                title = m.group(2).strip()
                abre = m.group(3).strip()
                fecha = m.group(4).strip()
                depth = number.count(".") + 1
                items.append(
                    {
                        "depth": depth,
                        "title": title,
                        "abre": abre,
                        "fecha": fecha,
                    }
                )
            return items

        def _normalize_key(value: str) -> str:
            if not value:
                return ""
            value = value.lower()
            value = re.sub(r"[^\w\s]", " ", value, flags=re.UNICODE)
            value = re.sub(r"\s+", " ", value).strip()
            return value

        def _merge_structure_maps(maps: list[str]) -> Optional[str]:
            if not maps:
                return None
            all_items: list[dict] = []
            for part in maps:
                all_items.extend(_extract_map_lines(part or ""))
            if not all_items:
                return None

            # Dedup por √¢ncora ABRE (mais est√°vel) e fallback por t√≠tulo.
            seen: set[str] = set()
            ordered: list[dict] = []
            for item in all_items:
                key = _normalize_key(item.get("abre") or "") or _normalize_key(item.get("title") or "")
                if not key:
                    continue
                if key in seen:
                    continue
                seen.add(key)
                ordered.append(item)

            # Renumera√ß√£o determin√≠stica (m√°x 3 n√≠veis) para manter sequ√™ncia limpa.
            h1 = 0
            h2 = 0
            h3 = 0
            out_lines: list[str] = []
            for item in ordered:
                depth = int(item.get("depth") or 1)
                depth = max(1, min(3, depth))
                if depth == 1:
                    h1 += 1
                    h2 = 0
                    h3 = 0
                    number = f"{h1}."
                elif depth == 2:
                    if h1 <= 0:
                        h1 = 1
                    h2 += 1
                    h3 = 0
                    number = f"{h1}.{h2}."
                else:
                    if h1 <= 0:
                        h1 = 1
                    if h2 <= 0:
                        h2 = 1
                    h3 += 1
                    number = f"{h1}.{h2}.{h3}."

                indent = "   " * (depth - 1)
                title = (item.get("title") or "").strip()
                abre = (item.get("abre") or "").strip()
                fecha = (item.get("fecha") or "").strip()
                if not title or not abre or not fecha:
                    continue
                out_lines.append(f'{indent}{number} {title} | ABRE: "{abre}" | FECHA: "{fecha}"')

            return "\n".join(out_lines).strip() or None

        async def _map_one(sample: str, *, part_idx: int, total_parts: int) -> Optional[str]:
            if not sample:
                return None
            # Mant√©m prompt original; evita inserir marcadores no texto para n√£o contaminar √¢ncoras verbatim.
            prompt = self.PROMPT_MAPEAMENTO.format(transcricao=sample)
            try:
                if self.provider == "openai":
                    response = await self.client.chat.completions.create(
                        model=self.llm_model,
                        messages=[{"role": "system", "content": prompt}],
                        max_completion_tokens=16384,
                    )
                    _record_openai_usage(response, model=self.llm_model)
                    content = response.choices[0].message.content.replace('```markdown', '').replace('```', '')
                    print(f"{Fore.GREEN}   ‚úÖ Estrutura mapeada (OpenAI) [{part_idx}/{total_parts}]")
                    return content

                def call_gemini():
                    return self.client.models.generate_content(
                        model=self.llm_model,
                        contents=prompt,
                        config=types.GenerateContentConfig(
                            max_output_tokens=10000,
                            thinking_config=types.ThinkingConfig(
                                include_thoughts=False,
                                thinking_level="HIGH"
                            )
                        )
                    )

                response = await asyncio.to_thread(call_gemini)
                _record_genai_usage(response, model=self.llm_model)
                content = response.text.replace('```markdown', '').replace('```', '')
                print(f"{Fore.GREEN}   ‚úÖ Estrutura mapeada (Vertex AI) [{part_idx}/{total_parts}]")
                return content
            except Exception as e:
                print(f"{Fore.YELLOW}‚ö†Ô∏è  Falha no mapeamento (parte {part_idx}/{total_parts}) via {self.provider}: {e}")
                if self.openai_client:
                    print(f"{Fore.CYAN}ü§ñ Fallback: Mapeando com OpenAI ({self.openai_model})...")
                    try:
                        response = await self.openai_client.chat.completions.create(
                            model=self.openai_model,
                            messages=[{"role": "system", "content": prompt}],
                            max_completion_tokens=10000
                        )
                        _record_openai_usage(response, model=self.openai_model)
                        content = response.choices[0].message.content.replace('```markdown', '').replace('```', '')
                        print(f"{Fore.GREEN}   ‚úÖ Estrutura mapeada (OpenAI Fallback) [{part_idx}/{total_parts}]")
                        return content
                    except Exception as e_openai:
                        print(f"{Fore.RED}‚ùå Falha tamb√©m no OpenAI fallback: {e_openai}")
                        return None
                return None

        try:
            parts = []
            total_parts = len(input_samples)
            for idx, sample in enumerate(input_samples, start=1):
                mapped = await _map_one(sample, part_idx=idx, total_parts=total_parts)
                if mapped:
                    parts.append(mapped)

            if not parts:
                _elapsed = time.time() - _map_t0
                print(f"{Fore.YELLOW}üó∫Ô∏è  map_structure finalizado SEM resultado em {_elapsed:.1f}s")
                return None
            if len(parts) == 1:
                _elapsed = time.time() - _map_t0
                print(f"{Fore.GREEN}üó∫Ô∏è  map_structure OK em {_elapsed:.1f}s (1 parte)")
                return _sanitize_mapped_structure(parts[0])

            merged = _merge_structure_maps(parts)
            if merged:
                _elapsed = time.time() - _map_t0
                print(f"{Fore.CYAN}   üß© Estrutura global consolidada (chunks merged) em {_elapsed:.1f}s.")
                return _sanitize_mapped_structure(merged)
            # Fallback: concatena (melhor do que perder estrutura)
            _elapsed = time.time() - _map_t0
            print(f"{Fore.CYAN}üó∫Ô∏è  map_structure OK (concat fallback) em {_elapsed:.1f}s")
            return _sanitize_mapped_structure("\n\n".join(parts).strip())
        except Exception as e:
            _elapsed = time.time() - _map_t0
            print(f"{Fore.YELLOW}‚ö†Ô∏è  Falha no mapeamento via {self.provider} ap√≥s {_elapsed:.1f}s: {e}")

            # Fallback Universal
            if self.openai_client:
                print(f"{Fore.CYAN}ü§ñ Fallback: Mapeando com OpenAI ({self.openai_model})...")
                try:
                    response = await self.openai_client.chat.completions.create(
                        model=self.openai_model,
                        messages=[
                            {"role": "system", "content": self.PROMPT_MAPEAMENTO.format(transcricao=fallback_sample)}
                        ],
                        max_completion_tokens=10000
                    )
                    _record_openai_usage(response, model=self.openai_model)
                    content = response.choices[0].message.content.replace('```markdown', '').replace('```', '')
                    _elapsed = time.time() - _map_t0
                    print(f"{Fore.GREEN}   ‚úÖ Estrutura mapeada (OpenAI Fallback) em {_elapsed:.1f}s.")
                    return _sanitize_mapped_structure(content)
                except Exception as e_openai:
                    _elapsed = time.time() - _map_t0
                    print(f"{Fore.RED}‚ùå Falha tamb√©m no OpenAI ap√≥s {_elapsed:.1f}s: {e_openai}")
                    return None
            else:
                 _elapsed = time.time() - _map_t0
                 print(f"{Fore.RED}   ‚ùå Erro ao mapear estrutura e sem fallback ({_elapsed:.1f}s).")
                 return None

    async def _ai_reassign_tables(self, texto: str, *, max_tables: int = 3) -> tuple[str, list[str]]:
        """
        v2.34: Fallback de reatribui√ß√£o de tabelas via IA (decis√£o bin√°ria PARENT/CURRENT).
        """
        candidates = coletar_candidatos_reatribuicao_tabelas(texto, max_candidates=max_tables)
        if not candidates:
            return texto, []

        async def _decide_one(candidate: dict) -> str:
            prompt = (
                "Voc√™ √© um revisor de estrutura. Decida se a tabela pertence ao T√ìPICO ATUAL "
                "ou ao T√ìPICO PAI. Responda apenas com 'PARENT' ou 'CURRENT'.\n\n"
                f"T√ìPICO PAI: {candidate['parent_title']}\n"
                f"T√ìPICO ATUAL: {candidate['current_title']}\n\n"
                f"CONTEXTO PAI (antes do subt√≥pico): {candidate['parent_context']}\n"
                f"CONTEXTO ATUAL (antes da tabela): {candidate['current_context']}\n\n"
                "TABELA:\n"
                f"{candidate['table_text'][:3500]}\n"
            )
            if self.provider == "openai" and self.client:
                response = await self.client.chat.completions.create(
                    model=self.llm_model,
                    messages=[{"role": "system", "content": prompt}],
                    max_completion_tokens=128,
                )
                content = response.choices[0].message.content or ""
                return content.strip().upper()

            from google.genai import types
            def call_gemini():
                return self.client.models.generate_content(
                    model=self.llm_model,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        max_output_tokens=256,
                        thinking_config=types.ThinkingConfig(
                            include_thoughts=False,
                            thinking_level="LOW",
                        ),
                    ),
                )
            response = await asyncio.to_thread(call_gemini)
            _record_genai_usage(response, model=self.llm_model)
            return (response.text or "").strip().upper()

        lines = texto.split("\n")
        moves: list[dict] = []
        issues: list[str] = []
        for candidate in candidates:
            decision = await _decide_one(candidate)
            if "PARENT" in decision:
                moves.append(candidate)
                issues.append(
                    f"Tabela reatribu√≠da via IA: '{candidate['current_title']}' ‚Üí '{candidate['parent_title']}'"
                )

        if not moves:
            return texto, []

        moves.sort(key=lambda m: m["start"], reverse=True)
        for m in moves:
            block_lines = lines[m["start"]:m["end"]]
            del lines[m["start"]:m["end"]]
            insert_at = m["insert_at"]
            if insert_at > m["start"]:
                insert_at = max(0, insert_at - (m["end"] - m["start"]))
            for offset, bl in enumerate(block_lines):
                lines.insert(insert_at + offset, bl)
        return "\n".join(lines), issues

    def resolve_diarization_policy(
        self,
        mode: str,
        *,
        diarization: Optional[bool] = None,
        diarization_strict: Optional[bool] = None,
    ) -> tuple[bool, bool]:
        """
        Resolve pol√≠tica de diariza√ß√£o por modo.

        - `AUDIENCIA`/`REUNIAO`/`DEPOIMENTO`: diariza√ß√£o ON por padr√£o e STRICT (falha se indispon√≠vel).
        - `APOSTILA`/`FIDELIDADE`: diariza√ß√£o OFF por padr√£o; opt-in por configura√ß√£o.

        Opt-in env para apostilas:
        - `IUDEX_ENABLE_DIARIZATION_APOSTILA=1` (ou `ENABLE_DIARIZATION_APOSTILA=1`)

        Strictness:
        - `--diarization` (for√ßar ON) torna strict por padr√£o.
        - `IUDEX_DIARIZATION_STRICT=1` pode for√ßar strict quando habilitado por env.
        """
        mode_norm = (mode or "").strip().upper()

        if diarization is None:
            apostila_opt_in = bool(
                _env_truthy("IUDEX_ENABLE_DIARIZATION_APOSTILA", False)
                or _env_truthy("ENABLE_DIARIZATION_APOSTILA", False)
            )
            diarization_enabled = mode_norm in {"AUDIENCIA", "REUNIAO", "DEPOIMENTO"} or (
                mode_norm in {"APOSTILA", "FIDELIDADE"} and apostila_opt_in
            )
        else:
            diarization_enabled = bool(diarization)

        if not diarization_enabled:
            return False, False

        strict_env = _env_truthy("IUDEX_DIARIZATION_STRICT", None)
        if diarization_strict is not None:
            diarization_required = bool(diarization_strict)
        elif diarization is True:
            diarization_required = True
        elif mode_norm in {"AUDIENCIA", "REUNIAO", "DEPOIMENTO"}:
            diarization_required = True
        elif strict_env is not None:
            diarization_required = bool(strict_env)
        else:
            diarization_required = False

        return True, diarization_required

    def set_diarization_policy(self, *, enabled: bool, required: bool) -> None:
        self._diarization_enabled = bool(enabled)
        self._diarization_required = bool(required) if enabled else False
        if self._diarization_enabled:
            strict_label = "STRICT" if self._diarization_required else "SOFT"
            print(f"{Fore.CYAN}üó£Ô∏è  Diariza√ß√£o: ATIVA ({strict_label})")
        else:
            print(f"{Fore.CYAN}üó£Ô∏è  Diariza√ß√£o: DESATIVADA")

    def _get_hf_token(self) -> Optional[str]:
        return (os.getenv("HUGGING_FACE_TOKEN") or os.getenv("HF_TOKEN") or "").strip() or None

    def _diarization_available(self) -> tuple[bool, str]:
        # Se AssemblyAI est√° configurado como prim√°rio, diariza√ß√£o √© feita externamente
        aai_primary = os.getenv("ASSEMBLYAI_PRIMARY", "").strip().lower() in ("1", "true", "yes")
        aai_key = os.getenv("ASSEMBLYAI_API_KEY", "").strip()
        if aai_primary and aai_key:
            return True, "AssemblyAI (externo)"
        # Verificar diariza√ß√£o local (pyannote)
        if Pipeline and self._get_hf_token():
            return True, "pyannote (local)"
        # Fallback: RunPod diarize endpoint
        runpod_diarize = os.getenv("RUNPOD_DIARIZE_ENDPOINT_ID", "").strip()
        runpod_key = os.getenv("RUNPOD_API_KEY", "").strip()
        if runpod_diarize and runpod_key:
            return True, "RunPod (externo)"
        # Fallback: AssemblyAI (mesmo n√£o sendo prim√°rio, pode fazer diariza√ß√£o)
        if aai_key:
            return True, "AssemblyAI (externo, n√£o-prim√°rio)"
        # Nenhum provider dispon√≠vel
        if not Pipeline:
            return False, "pyannote.audio n√£o instalado"
        if not self._get_hf_token():
            return False, "HUGGING_FACE_TOKEN/HF_TOKEN n√£o configurado"
        return False, "nenhum provider de diariza√ß√£o dispon√≠vel"

    def _local_diarization_available(self) -> bool:
        """Verifica se diariza√ß√£o LOCAL (pyannote) est√° dispon√≠vel."""
        return bool(Pipeline and self._get_hf_token())

    def _ensure_diarization_available_or_raise(self) -> None:
        ok, reason = self._diarization_available()
        if ok:
            return
        if self._diarization_required:
            raise RuntimeError(
                "Diariza√ß√£o obrigat√≥ria, mas indispon√≠vel. "
                f"Motivo: {reason}. "
                "Instale `pyannote.audio` e `torch` e configure `HUGGING_FACE_TOKEN`."
            )

    def transcribe_file(
        self,
        audio_path: str,
        *,
        mode: str = "APOSTILA",
        high_accuracy: bool = False,
        diarization: Optional[bool] = None,
        diarization_strict: Optional[bool] = None,
        language: Optional[str] = None,
    ) -> str:
        """
        Transcri√ß√£o com pol√≠tica de diariza√ß√£o por modo (ponto √∫nico de entrada).
        Retorna apenas o texto. Use transcribe_file_full() para obter words tamb√©m.
        """
        result = self.transcribe_file_full(
            audio_path,
            mode=mode,
            high_accuracy=high_accuracy,
            diarization=diarization,
            diarization_strict=diarization_strict,
            language=language,
        )
        return result["text"]

    def transcribe_file_full(
        self,
        audio_path: str,
        *,
        mode: str = "APOSTILA",
        high_accuracy: bool = False,
        diarization: Optional[bool] = None,
        diarization_strict: Optional[bool] = None,
        language: Optional[str] = None,
    ) -> dict:
        """
        Transcri√ß√£o com pol√≠tica de diariza√ß√£o por modo.
        Retorna dict com: text, words, segments.

        Returns:
            dict: {
                "text": str,           # Texto formatado com timestamps a cada 60s
                "words": list,         # Lista de {word, start, end, speaker} para player
                "segments": list,      # Segmentos originais
            }
        """
        # Mant√©m o modo atual
        self._current_mode = (mode or "FIDELIDADE").strip().upper()
        self._current_language = (language or "pt").strip().lower()
        enabled, required = self.resolve_diarization_policy(
            mode, diarization=diarization, diarization_strict=diarization_strict
        )
        self.set_diarization_policy(enabled=enabled, required=required)

        if enabled:
            self._ensure_diarization_available_or_raise()
            if self._local_diarization_available():
                # Diariza√ß√£o local via pyannote
                if high_accuracy:
                    return self.transcribe_beam_with_segments(audio_path)
                return self.transcribe_with_segments(audio_path)
            else:
                # Diariza√ß√£o dispon√≠vel externamente (RunPod/AAI) ‚Äî transcrever sem diariza√ß√£o local,
                # o chamador (transcription_service) far√° a diariza√ß√£o via provider externo
                print(f"{Fore.YELLOW}‚ö†Ô∏è  pyannote indispon√≠vel localmente ‚Äî diariza√ß√£o ser√° feita externamente")
                original_diarization_enabled = self._diarization_enabled
                try:
                    self._diarization_enabled = False
                    if high_accuracy:
                        result = self.transcribe_with_segments(audio_path, beam_size=self._get_asr_beam_size())
                    else:
                        result = self.transcribe_with_segments(audio_path)
                    result["_needs_external_diarization"] = True
                    return result
                finally:
                    self._diarization_enabled = original_diarization_enabled

        # Sem diariza√ß√£o: ainda precisamos obter words para o player
        # Usar transcribe_with_segments mas for√ßar diariza√ß√£o desabilitada
        original_diarization_enabled = self._diarization_enabled
        try:
            self._diarization_enabled = False  # For√ßar desabilitado para n√£o rodar pyannote
            if high_accuracy:
                result = self.transcribe_with_segments(audio_path, beam_size=self._get_asr_beam_size())
            else:
                result = self.transcribe_with_segments(audio_path)
            return result
        finally:
            self._diarization_enabled = original_diarization_enabled

    def renumber_headings(self, text):
        """
        Post-processing: Enforces strictly sequential numbering (1, 2, 3...)
        for H2/H3/H4 headers using a STACK-BASED approach for correct nesting.

        v2.16: Fixed to properly reset child counters when parent level changes.
        v2.41: Added semantic title merge (SequenceMatcher) to fuse near-duplicates
               from chunk boundaries ‚Äî prevents title inflation.
        """
        from difflib import SequenceMatcher

        print(f"{Fore.CYAN}üî¢ Renumerando t√≥picos sequencialmente (Stack-Based v2.41)...")
        lines = text.split('\n')
        new_lines = []

        # Stack-based counters: [H1_count, H2_count, H3_count, H4_count]
        # Index 0 = H1 (usually title, skip), Index 1 = H2, etc.
        counters = [0, 0, 0, 0, 0]  # Extra slot for safety

        # Keywords to skip numbering (summary tables, etc.)
        skip_keywords = ['resumo', 'quadro', 'tabela', 's√≠ntese', 'esquema', 'bibliografia', 'refer√™ncias', 'sum√°rio']

        # Emoji pattern to detect decorative headers like "## üìã Sum√°rio"
        emoji_pattern = re.compile(r'^[\U0001F300-\U0001F9FF]')
        seen_h2_numbers = set()
        level_adjustments = 0

        # v2.41: Semantic merge tracking
        last_h2_text = ""
        last_h3_text = ""
        merge_count = 0

        for line in lines:
            stripped = line.strip()
            
            # Determine header level
            header_match = re.match(r'^(#{1,4})\s+(.*)$', stripped)
            
            if header_match:
                hashes = header_match.group(1)
                level = len(hashes)  # 1 for H1, 2 for H2, etc.
                raw_title = header_match.group(2).strip()

                # Heur√≠stica determin√≠stica (v2.17):
                # Se o header j√° cont√©m numera√ß√£o expl√≠cita (ex.: "5.5."), mas veio em n√≠vel errado (ex.: "## 5.5"),
                # ajusta o n√≠vel para preservar a hierarquia esperada antes de renumerar sequencialmente.
                #
                # Ex.: "## 5.5. Subt√≥pico" deve ser tratado como H3, para virar "### 5.5." ap√≥s a renumera√ß√£o stack-based.
                num_match = re.match(r'^(\d+(?:\.\d+)*)(?:\.)?\s+', raw_title)
                if num_match:
                    depth = num_match.group(1).count(".") + 1
                    desired_level = level
                    # Regra: t√≠tulos com numera√ß√£o decimal devem ser subt√≥picos quando j√° houve H2.
                    if depth == 1 and level > 2:
                        desired_level = 2
                    elif depth >= 2:
                        # Evitar criar "0.x" quando n√£o h√° H2 pr√©vio.
                        if counters[2] > 0:
                            desired_level = 3 if depth == 2 else 4
                            # Se n√£o houver H3 pr√©vio, evita H4 direto.
                            if desired_level == 4 and counters[3] == 0:
                                desired_level = 3
                    if desired_level != level and desired_level in (2, 3, 4):
                        level = desired_level
                        hashes = "#" * level
                        level_adjustments += 1
                
                # Clean existing numbers from title (e.g., "1.2.3. Title" -> "Title")
                title_text = re.sub(r'^(\d+(\.\d+)*\.?\s*)+', '', raw_title).strip()

                # Skip H1 (document title) - just clean and pass through
                if level == 1:
                    new_lines.append(f"# {title_text}")
                    continue

                # v2.41: Semantic merge ‚Äî fuse near-duplicate titles from chunk boundaries
                title_norm = re.sub(r'\s*\(Continua√ß√£o\)\s*$', '', title_text, flags=re.IGNORECASE).strip().lower()
                if level == 2 and last_h2_text:
                    ratio = SequenceMatcher(None, title_norm, last_h2_text).ratio()
                    if ratio > 0.85:
                        merge_count += 1
                        continue  # skip duplicate ‚Äî content flows under existing H2
                if level == 3 and last_h3_text:
                    ratio = SequenceMatcher(None, title_norm, last_h3_text).ratio()
                    if ratio > 0.85:
                        merge_count += 1
                        continue
                if level == 2:
                    last_h2_text = title_norm
                    last_h3_text = ""  # reset H3 tracker when H2 changes
                elif level == 3:
                    last_h3_text = title_norm

                # Check if this header should be skipped from numbering
                title_lower = title_text.lower()
                should_skip = (
                    any(k in title_lower for k in skip_keywords) or
                    emoji_pattern.match(title_text)  # Headers starting with emoji
                )

                if should_skip:
                    new_lines.append(f"{'#' * level} {title_text}")
                else:
                    # STACK LOGIC: Increment current level, reset all deeper levels
                    counters[level] += 1
                    for deeper_level in range(level + 1, len(counters)):
                        counters[deeper_level] = 0
                    
                    # Build hierarchical number (e.g., "2.3.1")
                    number_parts = [str(counters[lvl]) for lvl in range(2, level + 1)]
                    hierarchical_number = ".".join(number_parts)
                    
                    new_lines.append(f"{'#' * level} {hierarchical_number}. {title_text}")
                    if level == 2:
                        seen_h2_numbers.add(str(counters[2]))
            else:
                new_lines.append(line)
        
        if level_adjustments:
            print(f"{Fore.YELLOW}   ‚ÑπÔ∏è  Ajustes de n√≠vel aplicados: {level_adjustments}")
        if merge_count:
            print(f"{Fore.YELLOW}   üîÑ T√≠tulos duplicados mesclados: {merge_count}")
        print(f"{Fore.GREEN}   ‚úÖ Renumera√ß√£o conclu√≠da: {counters[2]} se√ß√µes H2, {counters[3]} H3, {counters[4]} H4")
        return '\n'.join(new_lines)


    def check_coverage(self, original, formatted):
        """Checks for missing laws or sumulas using ROBUST fingerprints (v2.10 Ported)."""
        print(f"{Fore.YELLOW}üîç Verificando fidelidade (Leis/S√∫mulas/Robust)...")
        
        # Use migrated robust helpers
        fp_original = extrair_fingerprints(original)
        contagem_original = contar_ocorrencias_robust(fp_original, original)
        contagem_formatado = contar_ocorrencias_robust(fp_original, formatted)
        
        omissoes = []
        duplicacoes = []
        
        for key, count_orig in contagem_original.items():
            count_fmt = contagem_formatado.get(key, 0)
            categoria, item = key.split(':', 1)
            
            if count_orig > 0 and count_fmt == 0:
                omissoes.append(f"[{categoria}] {item}")
            if count_fmt > count_orig:
                duplicacoes.append(f"[{categoria}] {item} (+{count_fmt - count_orig})")
        
        report = []
        if omissoes:
            report.append(f"‚ö†Ô∏è {len(omissoes)} POSS√çVEIS OMISS√ïES:")
            for o in omissoes[:15]: report.append(f"   - {o}")
            if len(omissoes) > 15: report.append(f"   ... e mais {len(omissoes)-15}")
            
        if duplicacoes:
            report.append(f"\n‚ÑπÔ∏è {len(duplicacoes)} CITA√á√ïES REFOR√áADAS (Agregadas):")
            for d in duplicacoes[:10]: report.append(f"   - {d}")

        if not report:
            print(f"{Fore.GREEN}‚úÖ Verifica√ß√£o OK: Nenhuma omiss√£o de Leis/S√∫mulas detectada.")
            return "Verifica√ß√£o OK: Nenhuma omiss√£o detectada."
        else:
            msg = "\n".join(report)
            print(f"{Fore.RED}{msg}")
            return msg

    def final_structure_audit(self, formatted_text, global_structure):
        """
        v2.16: Audita a estrutura final comparando com o mapeamento inicial.
        Retorna um relat√≥rio de discrep√¢ncias e, opcionalmente, tenta corrigir.
        """
        print(f"{Fore.CYAN}üîç Auditoria Final de Estrutura (v2.16)...")
        
        if not global_structure:
            print(f"{Fore.YELLOW}   ‚ö†Ô∏è Mapeamento global n√£o dispon√≠vel, pulando auditoria.")
            return formatted_text, []
        
        # Extract all H2/H3 titles from formatted text
        formatted_headers = []
        for line in formatted_text.split('\n'):
            match = re.match(r'^(#{2,3})\s+(?:\d+(?:\.\d+)*\.?\s*)?(.+)$', line.strip())
            if match:
                level = len(match.group(1))
                title = match.group(2).strip()
                # Normalize
                title_normalized = re.sub(r'\s*\(Continua√ß√£o\)\s*$', '', title, flags=re.IGNORECASE).strip().lower()
                formatted_headers.append((level, title_normalized, title))
        
        # Extract expected structure from global mapping
        # v2.41: Support both markdown (## Title) and numbered (1. Title, 1.1. Title) formats
        expected_headers = []
        for line in global_structure.split('\n'):
            stripped = line.strip()
            # Try markdown format first: ## Title, ### Title
            md_match = re.match(r'^(#{2,3})\s+(.+)$', stripped)
            if md_match:
                level = len(md_match.group(1))
                title = md_match.group(2).strip()
                # Remove ABRE/FECHA anchors if present
                title = re.sub(r'\s*\|\s*(?:ABRE|FECHA):\s*["\'][^"\']*["\']', '', title).strip()
                title_normalized = title.lower()
                expected_headers.append((level, title_normalized, title))
                continue
            # Try numbered format: 1. Title (level 2), 1.1. Title (level 3), 1.1.1. Title (level 4)
            num_match = re.match(r'^(\d+(?:\.\d+)*)\.\s+(.+)$', stripped)
            if num_match:
                depth = num_match.group(1).count('.') + 1  # 1. = depth 1 ‚Üí H2, 1.1. = depth 2 ‚Üí H3
                level = min(depth + 1, 4)  # map to markdown level: depth 1‚ÜíH2, depth 2‚ÜíH3, depth 3‚ÜíH4
                title = num_match.group(2).strip()
                # Remove ABRE/FECHA anchors if present
                title = re.sub(r'\s*\|\s*(?:ABRE|FECHA):\s*["\'][^"\']*["\']', '', title).strip()
                title_normalized = title.lower()
                if level <= 3:  # Only compare H2 and H3 for audit
                    expected_headers.append((level, title_normalized, title))
        
        # Find duplicates in formatted headers
        seen_titles = {}
        duplicates = []
        for idx, (level, title_norm, title_orig) in enumerate(formatted_headers):
            if title_norm in seen_titles:
                duplicates.append({
                    'title': title_orig,
                    'first_occurrence': seen_titles[title_norm],
                    'duplicate_occurrence': idx + 1
                })
            else:
                seen_titles[title_norm] = idx + 1
        
        issues = []
        if duplicates:
            issues.append(f"‚ö†Ô∏è {len(duplicates)} T√çTULOS DUPLICADOS DETECTADOS:")
            for d in duplicates[:5]:
                issues.append(f"   - '{d['title']}' (linhas ~{d['first_occurrence']} e ~{d['duplicate_occurrence']})")
        
        # Check coverage of expected structure
        expected_titles_set = {h[1] for h in expected_headers}
        formatted_titles_set = {h[1] for h in formatted_headers}
        missing_titles = expected_titles_set - formatted_titles_set
        
        if missing_titles:
            issues.append(f"\n‚ö†Ô∏è {len(missing_titles)} T√ìPICOS DO MAPEAMENTO N√ÉO ENCONTRADOS:")
            for t in list(missing_titles)[:5]:
                issues.append(f"   - '{t}'")
        
        if issues:
            report = "\n".join(issues)
            print(f"{Fore.YELLOW}{report}")
            return formatted_text, issues
        else:
            print(f"{Fore.GREEN}   ‚úÖ Estrutura auditada - Sem duplicatas ou omiss√µes de t√≥picos.")
            return formatted_text, []

    async def format_transcription_async(
        self,
        transcription,
        video_name,
        output_folder,
        mode="APOSTILA",
        custom_prompt=None,
        custom_prompt_scope: str = "tables_only",
        dry_run=False,
        progress_callback=None,
        skip_audit=False,
        skip_fidelity_audit=False,
        skip_sources_audit=False,
        hil_strict=False,
        include_timestamps: bool = True,
        allow_indirect: bool = False,
        allow_summary: bool = False,
        disable_tables: bool = False,
        segment_timeout_seconds: Optional[int] = None,
    ):
        """
        Orquestrador Principal com Checkpoint e Robustez (Sequential Mode)
        
        Args:
            transcription: Texto da transcri√ß√£o
            video_name: Nome do v√≠deo
            output_folder: Pasta de sa√≠da
            mode: "APOSTILA", "FIDELIDADE", "AUDIENCIA", "REUNIAO" ou "DEPOIMENTO"
            custom_prompt: Campo de customiza√ß√£o opcional (controlado por custom_prompt_scope).
            custom_prompt_scope: 'tables_only' (padr√£o) ‚Üí afeta apenas tabelas/extras em TODOS os modos;
                                 'style_and_tables' (avan√ßado, opt-in) ‚Üí substitui STYLE+TABLE layers.
            dry_run: Se True, apenas valida divis√£o de chunks
            skip_audit: Se True, pula a auditoria jur√≠dica
            skip_sources_audit: Se True, pula a auditoria de fontes integrada
            allow_indirect: Se True, permite discurso indireto em AUDIENCIA/REUNIAO/DEPOIMENTO.
            allow_summary: Se True, permite ata resumida em AUDIENCIA/REUNIAO/DEPOIMENTO.
        """
        async def emit(stage: str, progress: int, message: str):
            if not progress_callback:
                return
            try:
                result = progress_callback(stage, progress, message)
                if asyncio.iscoroutine(result):
                    await result
            except Exception:
                pass

        # v2.23: Store mode for dynamic file naming
        self._current_mode = mode.upper()
        mode_suffix = self._current_mode
        print(f"{Fore.MAGENTA}üß† Formatando com {self.llm_model} (Sequential Mode)...")
        
        # v2.22: Modular prompt composition
        # custom_prompt now only overrides the STYLE+TABLE layers, not the entire system prompt.
        self.prompt_apostila = self._build_system_prompt(
            mode=mode,
            custom_style_override=custom_prompt,
            custom_prompt_scope=custom_prompt_scope,
            disable_tables=bool(disable_tables),
            allow_indirect=allow_indirect,
            allow_summary=allow_summary,
            include_timestamps=include_timestamps,
        )
        
        if not custom_prompt:
            mode_label = (mode or "APOSTILA").upper()
            pretty_map = {
                "APOSTILA": "APOSTILA",
                "FIDELIDADE": "FIDELIDADE",
                "AUDIENCIA": "AUDI√äNCIA",
                "REUNIAO": "REUNI√ÉO",
                "DEPOIMENTO": "DEPOIMENTO",
            }
            pretty = pretty_map.get(mode_label, mode_label)
            icon = "üìö" if mode_label == "APOSTILA" else "üé®"
            print(f"{Fore.CYAN}{icon} Modo {pretty} ativo (Prompt modular)")
        
        # 0. Context Extraction
        pass
        # professors_info = self._extract_professors_context(transcription)

        await emit("formatting", 60, "Iniciando formata√ß√£o...")

        # 0.1 Global Structure Mapping (NEW) ‚Äî with heartbeat to prevent stall appearance
        _map_hb_done = asyncio.Event()
        _map_hb_start = time.time()
        async def _map_heartbeat():
            while not _map_hb_done.is_set():
                try:
                    await asyncio.wait_for(_map_hb_done.wait(), timeout=8)
                except asyncio.TimeoutError:
                    elapsed = time.time() - _map_hb_start
                    await emit("formatting", 64, f"Mapeando estrutura... ({elapsed:.0f}s)")
        _map_hb_task = asyncio.create_task(_map_heartbeat())
        try:
            global_structure = await self.map_structure(transcription)
        finally:
            _map_hb_done.set()
            _map_hb_task.cancel()
            try:
                await _map_hb_task
            except (asyncio.CancelledError, Exception):
                pass
        await emit("formatting", 68, "Estrutura global mapeada")

        # v2.41: Pr√©-filtro e separa√ß√£o cut vs hierarchy
        if global_structure:
            global_structure = filtrar_niveis_excessivos(global_structure, max_nivel=3)
            simplify_max_lines = _safe_int(os.getenv("IUDEX_MAP_SIMPLIFY_MAX_LINES")) or 120
            simplify_max_depth = _safe_int(os.getenv("IUDEX_MAP_SIMPLIFY_MAX_DEPTH")) or 3
            global_structure = simplificar_estrutura_se_necessario(
                global_structure,
                max_linhas=simplify_max_lines,
                max_nivel=simplify_max_depth,
            )

        # v2.42: Detectar tipo de conte√∫do e injetar addon SIMULADO se aplic√°vel
        _tipo_match = re.search(r'\[TIPO:\s*(SIMULADO|CORRE√á√ÉO|CORRECAO)\]', global_structure or '', re.IGNORECASE)
        if _tipo_match:
            _tipo = _tipo_match.group(1).upper()
            print(f"{Fore.MAGENTA}üéØ Tipo detectado: {_tipo} ‚Äî Injetando regras de quest√µes/simulado no prompt")
            self.prompt_apostila += self.PROMPT_SIMULADO_ADDON
        # Estrutura limpa (sem ABRE/FECHA) para guiar hierarquia nos chunks
        hierarchy_structure = limpar_estrutura_para_review(global_structure) if global_structure else None

        # 1. Sequential Slicing (v2.17: Com √¢ncoras de estrutura)
        mode_norm = (mode or "APOSTILA").strip().upper()
        print(f"üî™ Dividindo em chunks (v2.32)...")

        # Para AUDI√äNCIA/REUNI√ÉO/DEPOIMENTO: preferir chunking por blocos naturais (## Bloco XX ‚Äî ...),
        # evitando cortes no meio de um turno/ato.
        chunks_info = []
        if mode_norm in {"AUDIENCIA", "REUNIAO", "DEPOIMENTO"}:
            block_max_chars = int(os.getenv("IUDEX_HEARING_BLOCK_MAX_CHARS", 25000))
            block_overlap = int(os.getenv("IUDEX_HEARING_BLOCK_SPLIT_OVERLAP_CHARS", 300))
            block_prefix = os.getenv("IUDEX_HEARING_BLOCK_PREFIX_REGEX", None)
            chunks_info = dividir_por_blocos_markdown(
                transcription,
                max_chars=block_max_chars,
                block_prefix_pattern=block_prefix,
                split_overlap_chars=block_overlap,
            )

        # Fallback: slicing sequencial com √¢ncoras (aulas/apostilas e quando n√£o h√° blocos).
        if not chunks_info:
            print(f"   ‚ÑπÔ∏è  Usando divis√£o sequencial (com √¢ncoras v2.17)...")
            chunks_info = dividir_sequencial(transcription, chars_por_parte=15000, estrutura_global=global_structure)
        validar_chunks(chunks_info, transcription)
        
        total_segments = len(chunks_info)
        print(f"üìä Total de segmentos sequenciais: {total_segments}")
        await emit("formatting", 72, f"{total_segments} segmentos preparados")
        
        if dry_run:
            print(f"{Fore.YELLOW}üîç MODO DRY-RUN: Parando antes das chamadas de API.")
            print(f"   Exemplo do Chunk 1: {transcription[chunks_info[0]['inicio']:chunks_info[0]['inicio']+100]}...")
            return "# DRY RUN OUTPUT"
        
        # 2. Checkpoint Loading
        checkpoint_data = load_checkpoint(video_name, output_folder)
        results_map = {} # Map idx -> result
        
        if checkpoint_data:
            print(f"{Fore.CYAN}üìÇ Retomando via Checkpoint ({checkpoint_data.get('timestamp')})")
            if len(checkpoint_data.get('results', [])) > 0:
                saved_results = checkpoint_data['results']
                # Restore results map
                for idx, res in enumerate(saved_results):
                    if idx < total_segments:
                        results_map[idx] = res
                print(f"   ‚úÖ {len(results_map)} segmentos recuperados.")
        
        # v2.19: Context Caching Setup
        cached_context = None
        if total_segments > 1: # S√≥ cache se tiver m√∫ltiplos chunks
            # v2.41: cache deve receber estrutura limpa para orientar hierarquia (H2/H3),
            # mantendo ABRE/FECHA apenas para o corte de chunks.
            cached_context = self.create_context_cache(
                transcription,
                hierarchy_structure or global_structure,
            )
                
        # 3. Sequential Processing Loop
        ordered_results = []
        
        # Restore ordered results from map
        for i in range(len(results_map)):
            ordered_results.append(results_map[i])

        start_idx = len(ordered_results)
        
        # v2.40: Helper function for processing a single chunk
        async def _process_single_chunk(i: int, prev_result: Optional[str] = None) -> str:
            """Process a single chunk with optional context from previous result."""
            info = chunks_info[i]
            chunk_text = transcription[info['inicio']:info['fim']]
            raw_overlap_chars = int(
                os.getenv("IUDEX_RAW_CONTEXT_OVERLAP_CHARS", self.RAW_CONTEXT_OVERLAP_CHARS)
            )
            overlap_raw = ""
            if raw_overlap_chars > 0 and info.get("inicio", 0) > 0:
                start_overlap = max(0, info["inicio"] - raw_overlap_chars)
                overlap_raw = transcription[start_overlap:info["inicio"]].strip()

            # Context Management - use previous result if available
            contexto_estilo = ""
            if prev_result:
                raw_context = _extract_style_context(prev_result, max_chars=2500)
                if len(raw_context.split()) > 30 and "[!WARNING]" not in raw_context:
                    contexto_estilo = raw_context

            # Rate Limit
            await rate_limiter.wait_if_needed_async()

            # L√≥gica de Estrutura Local (Janela Deslizante)
            # v2.41: Usa hierarchy_structure (sem ABRE/FECHA) para guiar H2/H3
            estrutura_referencia = None
            _struct_source = hierarchy_structure or global_structure
            if _struct_source and not cached_context:
                max_lines = int(
                    os.getenv(
                        "IUDEX_MAP_MAX_LINES_PER_CHUNK",
                        self.MAP_MAX_LINES_PER_CHUNK,
                    )
                )
                itens_estrutura = [ln for ln in _struct_source.split('\n') if ln.strip()]
                if len(itens_estrutura) > 8 and total_segments > 1:
                    ratio = len(itens_estrutura) / total_segments
                    center_idx = int(i * ratio)
                    if len(itens_estrutura) > max_lines:
                        available = max(4, max_lines - 2)
                        half = max(2, available // 2)
                        start_idx_w = max(0, center_idx - half)
                        end_idx_w = min(len(itens_estrutura), start_idx_w + available)
                        start_idx_w = max(0, end_idx_w - available)
                        slice_itens = itens_estrutura[start_idx_w:end_idx_w]
                        if start_idx_w > 0:
                            slice_itens.insert(0, "[... T√≥picos anteriores ...]")
                        if end_idx_w < len(itens_estrutura):
                            slice_itens.append("[... T√≥picos posteriores ...]")
                        estrutura_referencia = '\n'.join(slice_itens)
                    else:
                        window_size = max(4, int(len(itens_estrutura) * 0.15))
                        start_idx_w = max(0, center_idx - window_size)
                        end_idx_w = min(len(itens_estrutura), center_idx + window_size + 2)
                        slice_itens = itens_estrutura[start_idx_w:end_idx_w]
                        if start_idx_w > 0:
                            slice_itens.insert(0, "[... T√≥picos anteriores ...]")
                        if end_idx_w < len(itens_estrutura):
                            slice_itens.append("[... T√≥picos posteriores ...]")
                        estrutura_referencia = '\n'.join(slice_itens)
                else:
                    estrutura_referencia = _struct_source

            # v2.26: Contexto de continuidade
            continuidade_nota = ""
            if info.get('instituto_continua') and info.get('instituto_nome'):
                continuidade_nota = f"\n\n‚ö†Ô∏è AVISO: O instituto '{info['instituto_nome']}' continua no pr√≥ximo chunk. N√ÉO gere o Quadro-s√≠ntese final ainda ‚Äî ele ser√° completado na pr√≥xima parte."

            # v2.27: Tabela aberta
            tabela_aberta_nota = ""
            if prev_result:
                table_state = self._detect_open_table_state(prev_result)
                if table_state.get('needs_table_continuation'):
                    tabela_aberta_nota = table_state.get('context_hint', '')

            contexto_final = contexto_estilo + continuidade_nota + tabela_aberta_nota

            formatted = await self.process_chunk_async(
                chunk_text=chunk_text,
                idx=i+1,
                total=total_segments,
                previous_context=contexto_final,
                depth=0,
                global_structure=estrutura_referencia,
                overlap_text=overlap_raw,
                cached_content=cached_context
            )
            return formatted

        # v2.40: Parallel or Sequential Processing
        parallel_chunks = int(os.getenv("IUDEX_PARALLEL_CHUNKS", self.PARALLEL_CHUNKS))
        try:
            heartbeat_every = float(os.getenv("IUDEX_PROGRESS_HEARTBEAT_SECONDS", "12"))
        except Exception:
            heartbeat_every = 12.0
        if segment_timeout_seconds is None:
            try:
                segment_timeout_seconds = int(os.getenv("IUDEX_FORMAT_SEGMENT_TIMEOUT_SECONDS", "0"))
            except Exception:
                segment_timeout_seconds = 0
        else:
            try:
                segment_timeout_seconds = int(segment_timeout_seconds)
            except Exception:
                segment_timeout_seconds = 0

        if start_idx < total_segments:
            if parallel_chunks <= 1 or total_segments - start_idx <= 2:
                # Sequential mode (original behavior)
                print(f"‚ñ∂ Iniciando processamento sequencial do segmento {start_idx + 1}...")

                for i in tqdm(range(start_idx, total_segments), desc="Processando Sequencial"):
                    base_msg = f"Formatando segmento {i+1}/{total_segments}..."
                    display_progress = 72
                    if total_segments:
                        progress = 72 + int(((i) / total_segments) * 23)
                        display_progress = min(progress, 95)
                        await emit("formatting", display_progress, base_msg)

                    hb_done = asyncio.Event()
                    hb_task = None
                    hb_start = time.time()
                    if heartbeat_every and heartbeat_every > 0:
                        async def _heartbeat():
                            while not hb_done.is_set():
                                try:
                                    await asyncio.wait_for(hb_done.wait(), timeout=heartbeat_every)
                                except asyncio.TimeoutError:
                                    elapsed = time.time() - hb_start
                                    msg = f"{base_msg} ({elapsed:.0f}s)"
                                    await emit("formatting", display_progress, msg)
                        hb_task = asyncio.create_task(_heartbeat())

                    try:
                        prev_result = ordered_results[-1] if ordered_results else None
                        if segment_timeout_seconds and segment_timeout_seconds > 0:
                            formatted = await asyncio.wait_for(
                                _process_single_chunk(i, prev_result),
                                timeout=segment_timeout_seconds,
                            )
                        else:
                            formatted = await _process_single_chunk(i, prev_result)

                        # Smart stitching
                        if ordered_results:
                            try:
                                formatted = limpar_inicio_redundante(formatted, ordered_results[-1])
                            except Exception:
                                pass

                        ordered_results.append(formatted)
                        if total_segments:
                            progress = 72 + int(((i + 1) / total_segments) * 23)
                            await emit("formatting", min(progress, 95), f"Segmento {i+1}/{total_segments} conclu√≠do")

                        save_checkpoint(video_name, output_folder, ordered_results, chunks_info, i + 1)

                    except Exception as e:
                        print(f"{Fore.RED}‚ùå Falha Fatal no segmento {i+1}: {e}")
                        save_checkpoint(video_name, output_folder, ordered_results, chunks_info, i)
                        raise e
                    finally:
                        hb_done.set()
                        if hb_task:
                            try:
                                await hb_task
                            except Exception:
                                pass
            else:
                # Parallel mode (v2.40): Process in batches with semaphore
                print(f"‚ñ∂ Iniciando processamento PARALELO ({parallel_chunks} workers) do segmento {start_idx + 1}...")
                semaphore = asyncio.Semaphore(parallel_chunks)

                async def process_with_semaphore(idx: int, prev_res: Optional[str]) -> tuple:
                    async with semaphore:
                        try:
                            if segment_timeout_seconds and segment_timeout_seconds > 0:
                                result = await asyncio.wait_for(
                                    _process_single_chunk(idx, prev_res),
                                    timeout=segment_timeout_seconds,
                                )
                            else:
                                result = await _process_single_chunk(idx, prev_res)
                            return (idx, result, None)
                        except Exception as e:
                            return (idx, None, e)

                # Process in waves: first chunk sequential, then batches
                remaining = list(range(start_idx, total_segments))

                while remaining:
                    batch_size = min(parallel_chunks, len(remaining))
                    batch = remaining[:batch_size]
                    remaining = remaining[batch_size:]

                    await emit("formatting", 72 + int((total_segments - len(remaining) - batch_size) / total_segments * 23),
                              f"Processando batch de {len(batch)} segmentos...")

                    # First chunk in batch gets context from previous result
                    first_idx = batch[0]
                    prev_result = ordered_results[-1] if ordered_results else None

                    # Process batch in parallel
                    tasks = []
                    for j, idx in enumerate(batch):
                        # Only first chunk gets previous context for better stitching
                        ctx = prev_result if j == 0 else None
                        tasks.append(process_with_semaphore(idx, ctx))

                    results = await asyncio.gather(*tasks)

                    # Sort by index and append
                    results_sorted = sorted(results, key=lambda x: x[0])
                    for idx, result, error in results_sorted:
                        if error:
                            print(f"{Fore.RED}‚ùå Falha no segmento {idx+1}: {error}")
                            save_checkpoint(video_name, output_folder, ordered_results, chunks_info, idx)
                            raise error

                        # Apply stitching
                        if ordered_results:
                            try:
                                result = limpar_inicio_redundante(result, ordered_results[-1])
                            except Exception:
                                pass

                        ordered_results.append(result)
                        print(f"   ‚úÖ Segmento {idx+1}/{total_segments} conclu√≠do")

                    # Checkpoint after batch
                    save_checkpoint(video_name, output_folder, ordered_results, chunks_info, batch[-1] + 1)

                await emit("formatting", 95, f"Todos os {total_segments} segmentos processados")
        
        await emit("formatting", 96, "Consolidando resultados...")

        # 4. Final Assembly
        print(f"\n{Fore.CYAN}üßπ Pipeline de Limpeza Final (v2.7)...")
        full_formatted = f"# {video_name}\n\n" + "\n\n".join(ordered_results)
        
        # 4.1 Limpar metadados de mapeamento que vazam para o output
        # Remove linhas como "[TIPO: AULA EXPOSITIVA]" ou "**[TIPO: SIMULADO]**"
        full_formatted = re.sub(r'^#?\s*\*?\*?\[TIPO:.*?\]\*?\*?\s*$', '', full_formatted, flags=re.MULTILINE)
        # Remove marcadores de bloco [BLOCO 01], [BLOCO 02], etc.
        full_formatted = re.sub(r'^\s*\[BLOCO\s*\d+\]\s*$', '', full_formatted, flags=re.MULTILINE)
        # Remove timestamps √≥rf√£os [HH:MM] ou [HH:MM:SS] no in√≠cio de linha
        full_formatted = re.sub(r'^\s*\[\d{1,2}:\d{2}(:\d{2})?\]\s*$', '', full_formatted, flags=re.MULTILINE)
        full_formatted = re.sub(r'\n{3,}', '\n\n', full_formatted)  # Remove linhas em branco extras
        
        # 5. Post-Processing Pipeline (v2.7 features)
        
        print("  Passada 1: Removendo duplica√ß√µes literais...")
        await emit("formatting", 96, "Passada 1: Removendo duplica√ß√µes literais...")
        # Uses the newly ported v2.7 logic
        full_formatted = remover_duplicacoes_literais(full_formatted)
        
        limiar_info = '70%' if mode == 'FIDELIDADE' else '60%'
        print(f"  Passada 2: Removendo se√ß√µes duplicadas (limiar {limiar_info})...")
        await emit("formatting", 96, f"Passada 2: Removendo se√ß√µes duplicadas ({limiar_info})...")
        full_formatted = remover_secoes_duplicadas(full_formatted, mode=mode)
        
        print("  Passada 2.5: Removendo par√°grafos duplicados (v2.17)...")
        await emit("formatting", 97, "Passada 2.5: Removendo par√°gr. duplicados...")
        full_formatted = remover_paragrafos_duplicados(full_formatted)
        
        print("  Passada 2.6: Removendo t√≠tulos √≥rf√£os (v2.17)...")
        full_formatted = remover_titulos_orfaos(full_formatted)
        
        print("  Passada 2.7: Mesclando tabelas divididas (v2.27)...")
        await emit("formatting", 97, "Passada 2.7: Mesclando tabelas divididas...")
        full_formatted = mesclar_tabelas_divididas(full_formatted)

        print("  Passada 2.8: Movendo tabelas para fim de se√ß√£o (v2.41)...")
        await emit("formatting", 97, "Passada 2.8: Reorganizando tabelas...")
        full_formatted = mover_tabelas_para_fim_de_secao(full_formatted)

        print("  Passada 2.9: Ajustando t√≠tulos de tabela de banca...")
        await emit("formatting", 97, "Passada 2.9: Ajustando t√≠tulos de tabela...")
        full_formatted = garantir_titulo_tabela_banca(full_formatted)
        
        print("  Passada 3: Normalizando t√≠tulos similares...")
        await emit("formatting", 97, "Passada 3: Normalizando t√≠tulos...")
        full_formatted = normalize_headings(full_formatted)
        
        if mode != "FIDELIDADE":
            print("  Passada 3.5: Reorganiza√ß√£o Estrutural Determin√≠stica...")
            await emit("formatting", 97, "Passada 3.5: Reorganiza√ß√£o Estrutural...")
            full_formatted = deterministic_structure_fix(full_formatted)
        else:
            print(f"{Fore.YELLOW}  ‚ÑπÔ∏è  Modo FIDELIDADE: Pulando reorganiza√ß√£o para preservar linearidade exata.")
        
        title_drift_telemetry = {
            "freeze_h2_h3": False,
            "headers_changed_count": 0,
            "headers_restored_count": 0,
            "headers_degraded_count": 0,
            "headers_diff": [],
        }

        if mode != "FIDELIDADE":
            print("  Passada 4: Revis√£o Sem√¢ntica por IA...")
            await emit("formatting", 98, "Passada 4: Revis√£o Sem√¢ntica por IA...")
            full_formatted = await ai_structure_review(full_formatted, self.client, self.llm_model, estrutura_mapeada=limpar_estrutura_para_review(global_structure))
        else:
            print(f"{Fore.MAGENTA}  Passada 4: Revis√£o Leve de Formata√ß√£o (Modo Fidelidade)...")
            await emit("formatting", 98, "Passada 4: Revis√£o Leve (Fidelidade)...")
            _fidelity_original_text = full_formatted
            _fidelity_reviewed_text = await ai_structure_review_lite(
                full_formatted,
                self.client,
                self.llm_model,
                estrutura_mapeada=limpar_estrutura_para_review(global_structure),
            )
            full_formatted, title_drift_telemetry = enforce_fidelity_heading_guard(
                _fidelity_original_text,
                _fidelity_reviewed_text,
                freeze_h2_h3=True,
            )
            if title_drift_telemetry.get("headers_restored_count", 0) > 0:
                print(
                    f"{Fore.YELLOW}   ‚ôªÔ∏è Heading guard: "
                    f"{title_drift_telemetry['headers_restored_count']} t√≠tulo(s) restaurado(s) "
                    f"de {title_drift_telemetry.get('headers_changed_count', 0)} alterado(s)."
                )
        
        # Passada 4.5: Renumera√ß√£o Determin√≠stica (Camada de Seguran√ßa)
        try:
            full_formatted = renumerar_secoes(full_formatted)
        except Exception as e:
            print(f"{Fore.YELLOW}‚ö†Ô∏è Falha na renumera√ß√£o determin√≠stica: {e}. Continuando...")
        
        print(f"\n{Fore.CYAN}üî¢ Renumerando t√≥picos (1..N) (Stack-Based v2.16)...")
        await emit("formatting", 98, "Renumerando t√≥picos (1..N)...")
        full_formatted = self.renumber_headings(full_formatted)

        # Passada 4.7: Auditoria determin√≠stica de hierarquia (subt√≥picos vs t√≥picos)
        strict_subtopic_fix = _env_truthy("IUDEX_STRICT_SUBTOPIC_FIX", default=True)
        strict_subtopic_fix = True if strict_subtopic_fix is None else bool(strict_subtopic_fix)
        mode_norm_fix = (mode or "").strip().upper()
        # Em modos de apostila/fidelidade, inconsist√™ncia de n√≠vel tende a degradar
        # toda a estrutura final; for√ßamos corre√ß√£o ativa por seguran√ßa.
        if mode_norm_fix in {"APOSTILA", "FIDELIDADE"} and not strict_subtopic_fix:
            print(
                f"{Fore.YELLOW}‚ö†Ô∏è IUDEX_STRICT_SUBTOPIC_FIX=0 ignorado para modo {mode_norm_fix}; "
                f"for√ßando corre√ß√£o de hierarquia.{Style.RESET_ALL}"
            )
            strict_subtopic_fix = True
        fixed_text, level_issues = audit_heading_levels(full_formatted, apply_fixes=strict_subtopic_fix)
        if level_issues:
            print(f"{Fore.YELLOW}‚ö†Ô∏è  {len(level_issues)} inconsist√™ncias de hierarquia detectadas")
            for issue in level_issues[:5]:
                print(f"{Fore.YELLOW}   - {issue}")
            if strict_subtopic_fix and fixed_text != full_formatted:
                full_formatted = fixed_text
                # Reaplica renumera√ß√£o para manter sequ√™ncia coerente ap√≥s corre√ß√µes de n√≠vel.
                full_formatted = self.renumber_headings(full_formatted)
        
        # 5.6 v2.18: Auto-Fix Pass - Corre√ß√µes autom√°ticas finais
        full_formatted, autofix_correcoes = aplicar_correcoes_automaticas(full_formatted, mode=mode)
        
        # 5.5 v2.16: Auditoria Final de Estrutura
        await emit("formatting", 99, "Auditoria Final de Estrutura...")
        full_formatted, audit_issues = self.final_structure_audit(full_formatted, global_structure)
        if level_issues:
            audit_issues = list(audit_issues or [])
            audit_issues.append("\n‚ö†Ô∏è PROBLEMAS DE HIERARQUIA (DETERMIN√çSTICO):")
            audit_issues.extend([f"   - {issue}" for issue in level_issues])

        # Passada 4.8: Reatribui√ß√£o determin√≠stica de tabelas por t√≥pico (subt√≥picos)
        full_formatted, table_reassign_issues = reatribuir_tabelas_por_topico(full_formatted, apply_fixes=True)
        if table_reassign_issues:
            audit_issues = list(audit_issues or [])
            audit_issues.append("\n‚ö†Ô∏è POSS√çVEL REATRIBUI√á√ÉO DE TABELAS (DETERMIN√çSTICO):")
            audit_issues.extend([f"   - {issue}" for issue in table_reassign_issues])

        # Passada 4.9: Reatribui√ß√£o cir√∫rgica via IA (fallback)
        ai_reassign_enabled = os.getenv("IUDEX_TABLE_REASSIGN_AI", "").strip().lower() in ("1", "true", "yes")
        if ai_reassign_enabled:
            try:
                full_formatted, ai_reassign_issues = await self._ai_reassign_tables(full_formatted)
                if ai_reassign_issues:
                    audit_issues = list(audit_issues or [])
                    audit_issues.append("\n‚ö†Ô∏è REATRIBUI√á√ÉO DE TABELAS (IA CIR√öRGICA):")
                    audit_issues.extend([f"   - {issue}" for issue in ai_reassign_issues])
            except Exception as e:
                print(f"{Fore.YELLOW}‚ö†Ô∏è Falha na reatribui√ß√£o de tabelas via IA: {e}")
        
        # 6. Validation & Coverage
        print(f"\n{Fore.CYAN}üõ°Ô∏è  Validando cobertura final...")
        await emit("formatting", 99, "Validando cobertura final...")
        coverage_report = self.check_coverage(transcription, full_formatted)
        
        # Save validation report
        report_path = Path(output_folder) / f"{video_name}_validacao.txt"
        with open(report_path, "w", encoding='utf-8') as f:
            f.write(coverage_report)
        print(f"üìÑ Relat√≥rio de valida√ß√£o salvo: {report_path.name}")
        
        # Save audit report if there are issues
        if audit_issues:
            audit_path = Path(output_folder) / f"{video_name}_{mode_suffix}_verificacao.txt"
            with open(audit_path, "w", encoding='utf-8') as f:
                f.write("# AUDITORIA DE ESTRUTURA (v2.16)\n\n")
                f.write(f"Cobertura: {coverage_report}\n\n")
                f.write("## Problemas Estruturais Detectados\n")
                for issue in audit_issues:
                    f.write(f"{issue}\n")
            print(f"üìÑ Relat√≥rio de auditoria salvo: {audit_path.name}")
        
        # 7. v2.16: Valida√ß√£o Full-Context LLM (Backup opcional)
        validation_result = None
        primary_fidelity_written = False
        if FIDELITY_BACKUP_ENABLED:
            print(f"\n{Fore.CYAN}üî¨ Valida√ß√£o Full-Context LLM (backup)...")
            validation_result = self.validate_completeness_full(
                transcription, full_formatted, video_name, global_structure
            )

            # Salvar relat√≥rio JSON do backup
            validation_report_path = Path(output_folder) / f"{video_name}_{mode_suffix}_fidelidade_backup.json"
            with open(validation_report_path, "w", encoding='utf-8') as f:
                json.dump(validation_result, f, ensure_ascii=False, indent=2)
            print(f"üìÑ Relat√≥rio de fidelidade (backup) salvo: {validation_report_path.name}")

            # Se houver problemas graves, gerar tamb√©m um markdown leg√≠vel
            # v2.30: Se houve erro de valida√ß√£o, default √© False (n√£o mascara falha)
            _default_aprovado = False if validation_result.get('erro_validacao') else True
            if not validation_result.get('aprovado', _default_aprovado):
                fidelity_md_path = Path(output_folder) / f"{video_name}_{mode_suffix}_REVISAO.md"
                with open(fidelity_md_path, "w", encoding='utf-8') as f:
                    f.write(f"# ‚ö†Ô∏è REVIS√ÉO NECESS√ÅRIA: {video_name}\n\n")
                    f.write(f"**Nota de Fidelidade:** {validation_result.get('nota', 0)}/10\n\n")
                    if validation_result.get('omissoes'):
                        f.write("## üìå Omiss√µes Detectadas\n")
                        for o in validation_result['omissoes']:
                            f.write(f"- {o}\n")
                        f.write("\n")
                    if validation_result.get('distorcoes'):
                        f.write("## ‚ö†Ô∏è Distor√ß√µes Detectadas\n")
                        for d in validation_result['distorcoes']:
                            f.write(f"- {d}\n")
                        f.write("\n")
                    if validation_result.get('problemas_estrutura'):
                        f.write("## üèóÔ∏è Problemas de Estrutura\n")
                        for p in validation_result['problemas_estrutura']:
                            f.write(f"- {p}\n")
                        f.write("\n")
                    if validation_result.get('observacoes'):
                        f.write(f"## Observa√ß√µes\n{validation_result['observacoes']}\n")
                print(f"{Fore.RED}üìÑ ATEN√á√ÉO: Documento requer revis√£o! Veja: {fidelity_md_path.name}")

            # 7.1 v2.18: Corretor IA Seguro (Safe Mode) - Corrige problemas estruturais
            # v2.30: Se houve erro de valida√ß√£o, default √© False (n√£o mascara falha)
            _default_aprovado2 = False if (validation_result or {}).get('erro_validacao') else True
            if validation_result and not validation_result.get('aprovado', _default_aprovado2):
                print(f"\n{Fore.CYAN}üîÅ Iniciando Auto-Fix Loop (Safe Mode)...")

                # Chama o novo corretor seguro
                full_formatted = await self.auto_fix_smart(full_formatted, validation_result, global_structure)

                # Re-validar ap√≥s corre√ß√£o
                print(f"{Fore.CYAN}üî¨ Re-validando ap√≥s corre√ß√£o autom√°tica...")
                revalidation_result = self.validate_completeness_full(
                    transcription, full_formatted, video_name, global_structure
                )

                # Atualizar o relat√≥rio com a revalida√ß√£o
                validation_result = revalidation_result
                validation_report_path = Path(output_folder) / f"{video_name}_{mode_suffix}_fidelidade_backup.json"
                with open(validation_report_path, "w", encoding='utf-8') as f:
                    json.dump(validation_result, f, ensure_ascii=False, indent=2)
                print(f"üìÑ Relat√≥rio de fidelidade (backup) atualizado: {validation_report_path.name}")
        
        def _build_fidelity_report(result):
            if not isinstance(result, dict):
                return ""
            nota = result.get("nota_fidelidade", result.get("nota", 0))
            aprovado = result.get("aprovado", True)
            omissoes = result.get("omissoes_graves", result.get("omissoes", [])) or []
            distorcoes = result.get("distorcoes", []) or []
            estrutura = result.get("problemas_estrutura", []) or []
            observacoes = result.get("observacoes", "") or ""

            def _sanitize(text):
                return str(text).replace("-->", "-- >").strip()

            lines = [
                "# Relat√≥rio de Fidelidade (RAW x Formatado)",
                f"Aprovado: {'sim' if aprovado else 'nao'}",
                f"Nota: {nota}/10",
            ]

            if omissoes:
                lines.append(f"Omiss√µes graves: {len(omissoes)}")
                lines.extend([f"- {_sanitize(item)}" for item in omissoes[:20]])
            else:
                lines.append("Omiss√µes graves: 0")

            if distorcoes:
                lines.append(f"Distor√ß√µes: {len(distorcoes)}")
                lines.extend([f"- {_sanitize(item)}" for item in distorcoes[:20]])
            else:
                lines.append("Distor√ß√µes: 0")

            if estrutura:
                lines.append(f"Problemas de estrutura: {len(estrutura)}")
                lines.extend([f"- {_sanitize(item)}" for item in estrutura[:20]])
            else:
                lines.append("Problemas de estrutura: 0")

            if observacoes:
                lines.append(f"Observa√ß√µes: {_sanitize(observacoes)}")

            return "\n".join(lines).strip()

        fidelity_report = _build_fidelity_report(validation_result)
        # Relat√≥rio salvo apenas em arquivo JSON separado, n√£o inclu√≠do no markdown
        # if fidelity_report:
        #     full_formatted += f"\n\n<!-- RELAT√ìRIO: {fidelity_report} -->"

        # v2.27: Auditoria Preventiva de Fidelidade (antes do DOCX)
        print(f"{Fore.CYAN}üìä [DIAG] Audit Check: AVAILABLE={FIDELITY_AUDIT_AVAILABLE}, ENABLED={FIDELITY_AUDIT_ENABLED}, skip={skip_fidelity_audit}, output_folder={output_folder}")
        if FIDELITY_AUDIT_AVAILABLE and FIDELITY_AUDIT_ENABLED and not skip_fidelity_audit:
            print(f"\n{Fore.CYAN}üî¨ Auditoria Preventiva de Fidelidade (v2.27)...")
            await emit("formatting", 99, "Auditoria preventiva de fidelidade...")

            preventive_json = Path(output_folder) / f"{video_name}_{mode_suffix}_AUDITORIA_FIDELIDADE.json"
            preventive_md = Path(output_folder) / f"{video_name}_{mode_suffix}_AUDITORIA_FIDELIDADE.md"

            try:
                preventive_result = auditar_fidelidade_preventiva(
                    self.client,
                    transcription,
                    full_formatted,
                    video_name,
                    str(preventive_json),
                    modo=mode_suffix,
                    include_sources=(SOURCES_AUDIT_ENABLED and not skip_sources_audit),
                )
                if not isinstance(preventive_result, dict):
                    preventive_result = {
                        "aprovado": False,
                        "nota_fidelidade": 0,
                        "gravidade_geral": "CR√çTICA",
                        "erro": f"Auditoria preventiva retornou tipo inv√°lido: {type(preventive_result)}",
                        "recomendacao_hil": {"pausar_para_revisao": True, "motivo": "Resultado inv√°lido", "areas_criticas": ["auditoria_preventiva"]},
                        "omissoes_criticas": [],
                        "distorcoes": [],
                        "alucinacoes": [],
                        "problemas_estruturais": [],
                        "problemas_contexto": [],
                        "metricas": {},
                    }
                try:
                    needs_persist = True
                    try:
                        needs_persist = (not preventive_json.exists()) or preventive_json.stat().st_size == 0
                    except Exception:
                        needs_persist = True
                    if needs_persist:
                        with open(preventive_json, "w", encoding="utf-8") as f:
                            json.dump(preventive_result, f, ensure_ascii=False, indent=2, default=str)
                except Exception as write_err:
                    print(f"{Fore.YELLOW}‚ö†Ô∏è Falha ao salvar JSON da auditoria preventiva: {write_err}")
                # Markdown generation should never abort/overwrite the JSON result.
                try:
                    gerar_relatorio_markdown_completo(preventive_result, str(preventive_md), video_name)
                except Exception as md_err:
                    try:
                        md_fallback = (
                            f"# üî¨ AUDITORIA PREVENTIVA DE FIDELIDADE: {video_name}\n\n"
                            f"**Status:** ‚ö†Ô∏è REQUER REVIS√ÉO\n"
                            f"**Nota de Fidelidade:** 0.0/10\n"
                            f"**Gravidade Geral:** N/A\n\n"
                            f"## ‚ùå Erro\n\nFalha ao gerar relat√≥rio Markdown: {md_err}\n"
                        )
                        with open(preventive_md, "w", encoding="utf-8") as f:
                            f.write(md_fallback)
                    except Exception as md_write_err:
                        print(f"{Fore.YELLOW}‚ö†Ô∏è Falha ao salvar markdown fallback: {md_write_err}")

                compat = (preventive_result or {}).get("compat_fidelidade")
                if isinstance(compat, dict) and compat:
                    fidelity_path = Path(output_folder) / f"{video_name}_{mode_suffix}_fidelidade.json"
                    with open(fidelity_path, "w", encoding="utf-8") as f:
                        json.dump(compat, f, ensure_ascii=False, indent=2)
                    print(f"üìÑ Relat√≥rio de fidelidade (preventiva) salvo: {fidelity_path.name}")
                    primary_fidelity_written = True

                recomendacao = (preventive_result or {}).get("recomendacao_hil", {}) or {}
                if hil_strict and recomendacao.get("pausar_para_revisao"):
                    save_hil_output(
                        full_formatted,
                        video_name,
                        output_folder,
                        mode_suffix,
                        reason="auditoria_preventiva",
                    )
                    raise HILCheckpointException(
                        f"Auditoria preventiva exige revis√£o humana. Veja: {preventive_md.name}"
                    )
            except HILCheckpointException:
                raise
            except Exception as e:
                import traceback
                print(f"{Fore.YELLOW}‚ö†Ô∏è Falha na auditoria preventiva: {e}. Continuando...")
                print(f"{Fore.RED}Traceback: {traceback.format_exc()}")
                # Save minimal error report so frontend doesn't show "unavailable"
                error_result = {
                    "aprovado": False,
                    "nota_fidelidade": 0,
                    "gravidade_geral": "N/A",
                    "erro": str(e),
                    "recomendacao_hil": {"pausar_para_revisao": False, "motivo": f"Falha na execu√ß√£o da auditoria: {str(e)}", "areas_criticas": []},
                    "compat_fidelidade": {"aprovado": False, "nota": 0, "erro": str(e)},
                    "omissoes_criticas": [],
                    "distorcoes": [],
                    "alucinacoes": [],
                    "problemas_estruturais": [],
                    "problemas_contexto": [],
                    "metricas": {},
                    "observacoes_gerais": f"Erro na auditoria: {str(e)}"
                }
                try:
                    with open(preventive_json, "w", encoding="utf-8") as f:
                        json.dump(error_result, f, ensure_ascii=False, indent=2)
                    gerar_relatorio_markdown_completo(error_result, str(preventive_md), video_name)
                except Exception as write_err:
                    print(f"{Fore.RED}‚ùå Falha ao salvar relat√≥rio de erro: {write_err}")

        elif not skip_fidelity_audit:
            # Audit wanted but unavailable/disabled - write placeholder
            print(f"{Fore.YELLOW}‚ö†Ô∏è Auditoria preventiva indispon√≠vel ou desativada. Gerando relat√≥rio de status.")
            preventive_json = Path(output_folder) / f"{video_name}_{mode_suffix}_AUDITORIA_FIDELIDADE.json"
            preventive_md = Path(output_folder) / f"{video_name}_{mode_suffix}_AUDITORIA_FIDELIDADE.md"
            
            reason = "M√≥dulo de auditoria n√£o encontrado ou falha na importa√ß√£o." if not FIDELITY_AUDIT_AVAILABLE else "Auditoria desativada por configura√ß√£o."
            
            placeholder_result = {
                "aprovado": True,
                "nota_fidelidade": 0,
                "gravidade_geral": "INFO",
                "recomendacao_hil": {
                    "pausar_para_revisao": False, 
                    "motivo": f"Auditoria n√£o executada: {reason}", 
                    "areas_criticas": []
                },
                "observacoes_gerais": f"A auditoria preventiva n√£o foi executada. {reason}",
                "compat_fidelidade": {"aprovado": True, "nota": 0},
                "omissoes_criticas": [],
                "distorcoes": [],
                "alucinacoes": [],
                "problemas_estruturais": [],
                "problemas_contexto": [],
                "metricas": {}
            }
            try:
                with open(preventive_json, "w", encoding="utf-8") as f:
                    json.dump(placeholder_result, f, ensure_ascii=False, indent=2)
                
                # Write simple markdown
                md_content = f"# Auditoria Preventiva\n\n**Status:** N√£o executada\n\n**Motivo:** {reason}"
                with open(preventive_md, "w", encoding="utf-8") as f:
                    f.write(md_content)
                    
            except Exception as e:
                print(f"{Fore.RED}‚ùå Falha ao salvar placeholder de auditoria: {e}")

        if not primary_fidelity_written and isinstance(validation_result, dict):
            fidelity_path = Path(output_folder) / f"{video_name}_{mode_suffix}_fidelidade.json"
            try:
                with open(fidelity_path, "w", encoding="utf-8") as f:
                    json.dump(validation_result, f, ensure_ascii=False, indent=2)
                print(f"üìÑ Relat√≥rio de fidelidade (fallback) salvo: {fidelity_path.name}")
            except Exception as e:
                print(f"{Fore.YELLOW}‚ö†Ô∏è Falha ao salvar relat√≥rio de fidelidade fallback: {e}")

        # Telemetria de drift de t√≠tulos (modo FIDELIDADE)
        if mode_suffix == "FIDELIDADE":
            try:
                drift_path = Path(output_folder) / f"{video_name}_{mode_suffix}_TITLE_DRIFT.json"
                with open(drift_path, "w", encoding="utf-8") as f:
                    json.dump(title_drift_telemetry, f, ensure_ascii=False, indent=2)
                print(f"üìÑ Telemetria de drift de t√≠tulos salva: {drift_path.name}")
            except Exception as e:
                print(f"{Fore.YELLOW}‚ö†Ô∏è Falha ao salvar telemetria de drift de t√≠tulos: {e}")

        # Checkpoint cleanup success
        delete_checkpoint(video_name, output_folder)
        
        # 8. v2.18: Auditoria Jur√≠dica P√≥s-Processamento
        if AUDIT_AVAILABLE and not skip_audit:
            print(f"\n{Fore.CYAN}üïµÔ∏è Auditoria Jur√≠dica P√≥s-Processamento...")
            audit_report_path = Path(output_folder) / f"{video_name}_{mode_suffix}_AUDITORIA.md"
            audit_content = auditar_consistencia_legal(
                self.client,
                full_formatted,
                str(audit_report_path),
                raw_transcript=transcription,
            )
            
            if audit_content:
                print(f"{Fore.GREEN}   üìé Relat√≥rio de auditoria salvo em arquivo separado...")
                # N√£o incluir no markdown para n√£o poluir a apostila final
                # full_formatted += f"\n\n<!-- RELAT√ìRIO: {audit_content} -->"
        
        # v2.19: Cleanup manual do cache para economia
        if cached_context:
            try:
                self.client.caches.delete(name=cached_context.name)
                print(f"{Fore.GREEN}üóëÔ∏è Cache {cached_context.name} deletado manualmente para economizar custos.")
            except Exception as e:
                print(f"{Fore.YELLOW}‚ö†Ô∏è N√£o foi poss√≠vel deletar o cache: {e}")

        # v2.30: Limpeza final de vocativos/g√≠rias (ex.: "Meu irm√£o, ...")
        try:
            full_formatted = remover_vocativos_girias(full_formatted)
        except Exception as e:
            print(f"{Fore.YELLOW}‚ö†Ô∏è Falha ao remover vocativos/g√≠rias: {e}")

        # v2.35: Normaliza√ß√µes finais usadas tamb√©m no preview/API (n√£o s√≥ no Word)
        try:
            full_formatted = normalizar_temas_markdown(full_formatted)
        except Exception as e:
            print(f"{Fore.YELLOW}‚ö†Ô∏è Falha ao normalizar Temas: {e}")
        try:
            full_formatted = remover_marcadores_continua(full_formatted)
        except Exception as e:
            print(f"{Fore.YELLOW}‚ö†Ô∏è Falha ao remover marcadores [continua]: {e}")

        await emit("formatting", 100, "Formata√ß√£o conclu√≠da")

        return full_formatted


    async def auto_fix_smart(self, formatted_text, validation_result, global_structure=None):
        """
        v2.18 (SAFE MODE): Corretor Estrutural Seguro.
        Foca EXCLUSIVAMENTE em problemas de estrutura (t√≠tulos, duplicatas, hierarquia).
        N√ÉO altera conte√∫do jur√≠dico para evitar alucina√ß√µes.
        """
        problemas_estrut = validation_result.get('problemas_estrutura', [])
        
        if not problemas_estrut:
            print(f"{Fore.GREEN}   ‚úÖ Nenhum problema estrutural para corrigir.")
            return formatted_text
        
        print(f"{Fore.CYAN}üîß Auto-Fix Safe: Corrigindo {len(problemas_estrut)} problemas estruturais...")
        
        report = "### PROBLEMAS ESTRUTURAIS:\n" + "\n".join([f"- {p}" for p in problemas_estrut]) + "\n"
            
        global_reference = (
            "## ESTRUTURA DE REFER√äNCIA (Guia):\n" + global_structure
            if global_structure
            else ""
        )

        PROMPT_FIX = f"""Voc√™ √© um editor t√©cnico de elite.
        
## TAREFA: LIMPEZA ESTRUTURAL (SEM ALTERAR CONTE√öDO)
Voc√™ deve corrigir APENAS a formata√ß√£o e estrutura do documento.

## REGRA DE OURO (SEGURAN√áA JUR√çDICA):
- **N√ÉO altere o texto dos par√°grafos.**
- **N√ÉO adicione nem remova informa√ß√µes jur√≠dicas.**
- **N√ÉO reescreva explica√ß√µes.**
- Sua permiss√£o √© APENAS para T√≠tulos, Hierarquia e Duplicatas exatas.

## INSTRU√á√ïES DE CORRE√á√ÉO:
1. **T√≠tulos Duplicados**: Se um t√≠tulo H2 aparece duas vezes, REMOVA a segunda ocorr√™ncia e mescle o conte√∫do sob o primeiro.
2. **Hierarquia**: Ajuste n√≠veis (H2, H3) para seguir a l√≥gica do conte√∫do.
3. **Par√°grafos Repetidos**: Delete duplica√ß√µes EXATAS de par√°grafos (copia-cola acidental).
4. **Renumera√ß√£o**: Garanta sequ√™ncia l√≥gica (1, 2, 3...) nos t√≠tulos.

{global_reference}

## RELAT√ìRIO DE ERROS:
{report}

## SA√çDA:
Retorne o documento COMPLETO corrigido em Markdown. Sem explica√ß√µes."""

        try:
            # Call Gemini Async
            def call_gemini():
                return self.client.models.generate_content(
                    model=self.llm_model,
                    contents=f"{PROMPT_FIX}\n\n## TEXTO A CORRIGIR:\n{formatted_text}",
                    config=types.GenerateContentConfig(
                        max_output_tokens=8192,
                        thinking_config=types.ThinkingConfig(
                            include_thoughts=False,
                            thinking_level="HIGH" 
                        )
                    )
                )

            response = await asyncio.to_thread(call_gemini)
            _record_genai_usage(response, model=self.llm_model)
            
            resultado = response.text.replace('```markdown', '').replace('```', '').strip()
            
            # Valida√ß√£o de seguran√ßa estrita (0.8 = 20% toler√¢ncia vs 30% antigo)
            if len(resultado) < len(formatted_text) * 0.8:
                print(f"{Fore.YELLOW}‚ö†Ô∏è Auto-Fix Safe cortou muito texto (>20%). Abortando por seguran√ßa.")
                return formatted_text
                
            print(f"{Fore.GREEN}   ‚úÖ Auto-Fix Estrutural conclu√≠do. ({len(formatted_text)} -> {len(resultado)} chars)")
            return resultado
            
        except Exception as e:
            print(f"{Fore.RED}‚ùå Falha no Auto-Fix Safe: {e}")
            return formatted_text


    def save_as_word(
        self,
        formatted_text,
        video_name,
        output_folder,
        mode=None,
        document_theme="classic",
        document_header=None,
        document_footer=None,
        document_margins="normal",
        document_font_family=None,
        document_font_size=None,
        document_line_height=None,
        document_paragraph_spacing=None,
    ):
        """Salva markdown formatado como documento Word (.docx) com estilo premium"""
        # v2.23: Dynamic mode suffix for file naming
        mode_suffix = mode.upper() if mode else getattr(self, '_current_mode', 'APOSTILA')
        try:
            from docx import Document
            from docx.shared import Pt, RGBColor, Inches, Cm
            from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
            from docx.enum.table import WD_TABLE_ALIGNMENT
            from docx.oxml.ns import qn
            from docx.oxml import OxmlElement
        except ImportError:
            print(f"{Fore.YELLOW}‚ö†Ô∏è python-docx n√£o instalado. Salvando apenas Markdown.")
            return None

        print(f"{Fore.CYAN}üìÑ Gerando documento Word profissional...")

        # v2.28: Sanitiza√ß√£o do markdown antes de converter
        try:
            formatted_text = sanitizar_markdown_final(formatted_text)
        except Exception as e:
            print(f"{Fore.YELLOW}‚ö†Ô∏è Erro na sanitiza√ß√£o: {e}. Continuando com texto original.")

        # v2.28: Corrigir tabelas que aparecem antes do conte√∫do terminar
        try:
            formatted_text = corrigir_tabelas_prematuras(formatted_text)
        except Exception as e:
            print(f"{Fore.YELLOW}‚ö†Ô∏è Erro ao corrigir tabelas prematuras: {e}.")

        # 1. Aplicar Smart Layout (opcional, mantido do Vomo para consist√™ncia)
        try:
            formatted_text = mover_tabelas_para_fim_de_secao(formatted_text)
        except Exception as e:
            print(f"{Fore.YELLOW}‚ö†Ô∏è Erro ao reorganizar tabelas: {e}. Usando layout padr√£o.")

        doc = Document()

        theme_norm = (document_theme or "classic").strip().lower()
        margins_norm = (document_margins or "normal").strip().lower()
        theme_presets = {
            "classic": {
                "font": "Arial",
                "title_color": RGBColor(0, 51, 102),
                "heading_color": RGBColor(0, 51, 102),
                "margins": (1, 1, 1.25, 1.25),
                "table": {
                    "default": {"header_bg": "0066CC", "header_text": RGBColor(255, 255, 255), "alt_row_bg": "F0F6FF"},
                    "quadro_sintese": {"header_bg": "0066CC", "header_text": RGBColor(255, 255, 255), "alt_row_bg": "E6F2FF"},
                    "pegadinhas": {"header_bg": "E67E00", "header_text": RGBColor(255, 255, 255), "alt_row_bg": "FFF5E6"},
                },
            },
            "minimal": {
                "font": "Arial",
                "title_color": RGBColor(55, 65, 81),
                "heading_color": RGBColor(55, 65, 81),
                "margins": (0.9, 0.9, 1.0, 1.0),
                "table": {
                    "default": {"header_bg": "F8FAFC", "header_text": RGBColor(31, 41, 55), "alt_row_bg": "FFFFFF"},
                    "quadro_sintese": {"header_bg": "F8FAFC", "header_text": RGBColor(31, 41, 55), "alt_row_bg": "FFFFFF"},
                    "pegadinhas": {"header_bg": "FFF7ED", "header_text": RGBColor(120, 53, 15), "alt_row_bg": "FFFFFF"},
                },
            },
            "executive": {
                "font": "Arial",
                "title_color": RGBColor(17, 24, 39),
                "heading_color": RGBColor(17, 24, 39),
                "margins": (1, 1, 1.1, 1.1),
                "table": {
                    "default": {"header_bg": "111827", "header_text": RGBColor(255, 255, 255), "alt_row_bg": "F3F4F6"},
                    "quadro_sintese": {"header_bg": "0F172A", "header_text": RGBColor(255, 255, 255), "alt_row_bg": "E5E7EB"},
                    "pegadinhas": {"header_bg": "B45309", "header_text": RGBColor(255, 255, 255), "alt_row_bg": "FEF3C7"},
                },
            },
            "academic": {
                "font": "Times New Roman",
                "title_color": RGBColor(55, 65, 81),
                "heading_color": RGBColor(55, 65, 81),
                "margins": (1.25, 1.25, 1.35, 1.35),
                "table": {
                    "default": {"header_bg": "ECEFF4", "header_text": RGBColor(31, 41, 55), "alt_row_bg": "FFFFFF"},
                    "quadro_sintese": {"header_bg": "E2E8F0", "header_text": RGBColor(31, 41, 55), "alt_row_bg": "FFFFFF"},
                    "pegadinhas": {"header_bg": "FFF7ED", "header_text": RGBColor(120, 53, 15), "alt_row_bg": "FFFFFF"},
                },
            },
        }
        theme = theme_presets.get(theme_norm, theme_presets["classic"])
        font_name = theme["font"]
        if document_font_family:
            font_name = str(document_font_family).strip() or font_name
        self._doc_font_name = font_name
        
        # 2. Configura√ß√µes Globais de Estilo (Arial + Justificado)
        style = doc.styles['Normal']
        font = style.font
        font.name = font_name
        base_font_size = None
        try:
            if document_font_size is not None:
                font_size_val = float(document_font_size)
                # UI usa px; converter para pontos (1px ‚âà 0.75pt)
                font.size = Pt(max(8, font_size_val * 0.75))
            else:
                font.size = Pt(11)
        except Exception:
            font.size = Pt(11)
        base_font_size = font.size or Pt(11)
        
        # Garantir Arial em documentos que ignoram o nome da fonte
        r = style.element.rPr.get_or_add_rFonts()
        r.set(qn('w:ascii'), font_name)
        r.set(qn('w:hAnsi'), font_name)
        
        style.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
        if document_line_height is not None:
            try:
                style.paragraph_format.line_spacing_rule = WD_LINE_SPACING.MULTIPLE
                style.paragraph_format.line_spacing = float(document_line_height)
            except Exception:
                style.paragraph_format.line_spacing_rule = WD_LINE_SPACING.ONE_POINT_FIVE
        else:
            style.paragraph_format.line_spacing_rule = WD_LINE_SPACING.ONE_POINT_FIVE

        if document_paragraph_spacing is not None:
            try:
                spacing_pt = float(document_paragraph_spacing) * 0.75
                style.paragraph_format.space_after = Pt(max(0, spacing_pt))
            except Exception:
                pass
        
        # Aplicar Arial tamb√©m aos t√≠tulos e outros estilos
        for style_name in [f'Heading {i}' for i in range(1, 6)] + ['Quote', 'List Bullet', 'List Number']:
            try:
                s = doc.styles[style_name]
                s.font.name = font_name
                r = s.element.rPr.get_or_add_rFonts()
                r.set(qn('w:ascii'), font_name)
                r.set(qn('w:hAnsi'), font_name)
            except KeyError:
                pass

        # Margens
        section = doc.sections[0]
        top_m, bottom_m, left_m, right_m = theme["margins"]
        if margins_norm == "compact":
            top_m, bottom_m, left_m, right_m = (0.9, 0.9, 1.0, 1.0)
        elif margins_norm == "wide":
            top_m, bottom_m, left_m, right_m = (1.25, 1.25, 1.35, 1.35)
        section.top_margin = Inches(top_m)
        section.bottom_margin = Inches(bottom_m)
        section.left_margin = Inches(left_m)
        section.right_margin = Inches(right_m)

        def _add_page_number(paragraph):
            run = paragraph.add_run()
            fldChar = OxmlElement('w:fldChar')
            fldChar.set(qn('w:fldCharType'), 'begin')
            run._r.append(fldChar)
            instrText = OxmlElement('w:instrText')
            instrText.set(qn('xml:space'), 'preserve')
            instrText.text = 'PAGE'
            run._r.append(instrText)
            fldChar = OxmlElement('w:fldChar')
            fldChar.set(qn('w:fldCharType'), 'end')
            run._r.append(fldChar)

        header = section.header
        header_para = header.paragraphs[0] if header.paragraphs else header.add_paragraph()
        header_text = (document_header or "").strip() or f"{video_name} ‚Äî {mode_suffix}"
        header_para.text = header_text
        header_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
        for run in header_para.runs:
            run.font.size = Pt(9)
            run.font.color.rgb = RGBColor(120, 120, 120)
            run.font.name = font_name

        footer = section.footer
        footer_para = footer.paragraphs[0] if footer.paragraphs else footer.add_paragraph()
        footer_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
        footer_para.text = ""
        footer_text = (document_footer or "").strip()
        if footer_text:
            run = footer_para.add_run(f"{footer_text} ‚Äî ")
            run.font.size = Pt(9)
            run.font.color.rgb = RGBColor(120, 120, 120)
            run.font.name = font_name
        _add_page_number(footer_para)
        for run in footer_para.runs:
            run.font.size = Pt(9)
            run.font.color.rgb = RGBColor(120, 120, 120)
            run.font.name = font_name
        
        # T√≠tulo principal
        title = doc.add_heading(video_name, level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        for run in title.runs:
            run.font.size = Pt(20)
            run.font.color.rgb = theme["title_color"]
            run.font.name = font_name
        
        # Data de gera√ß√£o e Modo
        date_para = doc.add_paragraph()
        date_run = date_para.add_run(f"Gerado em {time.strftime('%d/%m/%Y √†s %H:%M')} - Modo: {mode_suffix}")
        date_run.italic = True
        date_run.font.size = Pt(10)
        date_run.font.color.rgb = RGBColor(128, 128, 128)
        date_run.font.name = font_name
        date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        doc.add_paragraph()
        
        # Sum√°rio
        doc.add_heading('Sum√°rio', level=1)
        self.create_toc(doc)
        doc.add_page_break()
        
        # Processa conte√∫do markdown
        lines = formatted_text.split('\n')
        i = 0
        in_table = False
        table_rows = []
        current_table_cols = None
        current_table_type = "default"  # v2.28: Tipo de tabela atual

        def _is_table_separator(line: str) -> bool:
            return bool(re.match(r'^\s*\|[\s:|-]+\|[\s:|-]*$', line))

        def _count_table_cols(line: str) -> int:
            if '|' not in line:
                return 0
            return max(0, line.count('|') - 1)

        def _looks_like_table_header(idx: int) -> bool:
            if idx + 1 >= len(lines):
                return False
            if '|' not in lines[idx]:
                return False
            return _is_table_separator(lines[idx + 1].strip())

        def _detect_table_type_from_heading(heading_text: str) -> str:
            """v2.28: Detecta tipo de tabela pelo heading anterior."""
            if 'üìã' in heading_text or 'uadro' in heading_text.lower():
                return "quadro_sintese"
            elif 'üéØ' in heading_text or 'pegadinha' in heading_text.lower() or 'banca' in heading_text.lower():
                return "pegadinhas"
            return "default"

        while i < len(lines):
            line = lines[i].strip()

            if line in {"<!-- PAGE_BREAK -->", "<!--PAGE_BREAK-->"}:
                doc.add_page_break()
                i += 1
                continue
            
            if in_table:
                if not line:
                    if table_rows:
                        self._add_table_to_doc(doc, table_rows, current_table_type, theme_norm)
                    in_table = False
                    table_rows = []
                    current_table_cols = None
                    i += 1
                    continue

                if '|' in line:
                    if _looks_like_table_header(i):
                        candidate_cols = _count_table_cols(line)
                        if current_table_cols and table_rows and candidate_cols != current_table_cols:
                            self._add_table_to_doc(doc, table_rows, current_table_type, theme_norm)
                            table_rows = []
                            current_table_cols = None

                    is_separator = _is_table_separator(line)
                    if not is_separator:
                        row = [cell.strip() for cell in line.split('|')[1:-1]]
                        table_rows.append(row)
                        if row:
                            current_table_cols = max(current_table_cols or 0, len(row))

                    if i == len(lines) - 1:
                        if table_rows:
                            self._add_table_to_doc(doc, table_rows, current_table_type, theme_norm)
                        in_table = False
                        table_rows = []
                        current_table_cols = None
                    i += 1
                    continue

                if table_rows:
                    self._add_table_to_doc(doc, table_rows, current_table_type, theme_norm)
                in_table = False
                table_rows = []
                current_table_cols = None
                continue

            if not line:
                i += 1
                continue

            # Tabelas
            if '|' in line:
                in_table = True
                table_rows = []
                current_table_cols = None
                # v2.28: Tipo j√° foi definido pelo heading anterior
                continue

            # Headings
            if line.startswith('##### '):
                h = doc.add_heading('', level=5)
                self._format_inline_markdown(h.paragraphs[0], line[6:])
                # v2.28: Detectar tipo de tabela para heading level 5
                current_table_type = _detect_table_type_from_heading(line[6:])
            elif h_match := re.match(r'^(####|###|##|#)\s+(.*)', line):
                lvl = len(h_match.group(1))
                h_text = h_match.group(2)
                if lvl == 1 and h_text == video_name:
                    i += 1
                    continue
                h = doc.add_heading('', level=lvl)
                self._format_inline_markdown(h, h_text)
                for run in h.runs:
                    run.font.name = font_name
                    run.font.color.rgb = theme["heading_color"]
                # v2.28: Detectar tipo de tabela para heading level 4
                if lvl == 4:
                    current_table_type = _detect_table_type_from_heading(h_text)
            
            # Separadores
            elif line.strip() in ['---', '***', '___']:
                p = doc.add_paragraph()
                p.add_run('_' * 80).font.color.rgb = RGBColor(192, 192, 192)
            
            # Quotes
            elif line.startswith('>'):
                p = doc.add_paragraph(style='Quote')
                p.paragraph_format.left_indent = Cm(4.0)
                p.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
                self._format_inline_markdown(p, line[1:].strip())
                for run in p.runs:
                    run.italic = True
                    run.font.size = Pt(10)
            
            # Listas n√£o-ordenadas
            elif line.startswith('- ') or line.startswith('* '):
                p = doc.add_paragraph(style='List Bullet')
                p.paragraph_format.left_indent = Cm(1.5)
                p.paragraph_format.first_line_indent = Cm(-0.63)
                self._format_inline_markdown(p, line[2:])
                
            # Listas numeradas
            elif len(line) > 2 and line[0].isdigit() and line[1:3] in ['. ', ') ']:
                p = doc.add_paragraph(style='Normal')
                p.paragraph_format.left_indent = Cm(1.5)
                p.paragraph_format.first_line_indent = Cm(-0.63)
                self._format_inline_markdown(p, line)
                
            # Par√°grafo normal
            else:
                p = doc.add_paragraph()
                if document_line_height is not None:
                    try:
                        p.paragraph_format.line_spacing_rule = WD_LINE_SPACING.MULTIPLE
                        p.paragraph_format.line_spacing = float(document_line_height)
                    except Exception:
                        p.paragraph_format.line_spacing_rule = WD_LINE_SPACING.ONE_POINT_FIVE
                else:
                    p.paragraph_format.line_spacing_rule = WD_LINE_SPACING.ONE_POINT_FIVE
                p.paragraph_format.space_before = Pt(6)
                if document_paragraph_spacing is not None:
                    try:
                        spacing_pt = float(document_paragraph_spacing) * 0.75
                        p.paragraph_format.space_after = Pt(max(0, spacing_pt))
                    except Exception:
                        p.paragraph_format.space_after = Pt(6)
                else:
                    p.paragraph_format.space_after = Pt(6)
                p.paragraph_format.first_line_indent = Cm(1.0)
                p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
                self._format_inline_markdown(p, line)
                for run in p.runs:
                    run.font.size = base_font_size
            
            i += 1
            
        output_file = os.path.join(output_folder, f"{video_name}_{mode_suffix}.docx")
        doc.save(output_file)
        return output_file

    def _format_inline_markdown(self, paragraph, text):
        """Formata markdown inline avan√ßado (bold, italic, code, underline-style)"""
        from docx.shared import Pt, RGBColor
        paragraph.clear()
        font_name = getattr(self, "_doc_font_name", "Arial")
        
        # Regex robusta do format_transcription_gemini.py
        pattern = r'(\*{3}(.+?)\*{3}|_{3}(.+?)_{3}|\*{2}(.+?)\*{2}|_{2}(.+?)_{2}|(?<!\*)\*(?!\*)(.+?)(?<!\*)\*(?!\*)|(?<!_)_(?!(?:_|\s))(.+?)(?<!(?:_|\s))_(?!_)|`(.+?)`)'
        
        last_end = 0
        for match in re.finditer(pattern, text):
            if match.start() > last_end:
                run = paragraph.add_run(text[last_end:match.start()])
                run.font.name = font_name
            
            full_match = match.group(0)
            
            if full_match.startswith('***'):
                run = paragraph.add_run(match.group(2))
                run.bold = True
                run.italic = True
                run.font.name = font_name
            elif full_match.startswith('___'):
                run = paragraph.add_run(match.group(3))
                run.bold = True
                run.italic = True
                run.font.name = font_name
            elif full_match.startswith('**'):
                run = paragraph.add_run(match.group(4))
                run.bold = True
                run.font.name = font_name
            elif full_match.startswith('__'):
                run = paragraph.add_run(match.group(5))
                run.bold = True
                run.font.name = font_name
            elif full_match.startswith('*'):
                run = paragraph.add_run(match.group(6))
                run.italic = True
                run.font.name = font_name
            elif full_match.startswith('_'):
                run = paragraph.add_run(match.group(7))
                run.italic = True
                run.font.name = font_name
            elif full_match.startswith('`'):
                run = paragraph.add_run(match.group(8))
                run.font.name = 'Courier New'
                run.font.size = Pt(9)
                run.font.color.rgb = RGBColor(200, 0, 0)
            
            last_end = match.end()
        
        if last_end < len(text):
            run = paragraph.add_run(text[last_end:])
            run.font.name = font_name

    def _add_table_to_doc(self, doc, rows, table_type="default", document_theme="classic"):
        """
        v2.28: Adiciona tabela premium ao Word com estilos diferenciados.

        Args:
            doc: Documento Word
            rows: Lista de listas com dados das c√©lulas
            table_type: Tipo de tabela para estiliza√ß√£o diferenciada
                - "quadro_sintese" (üìã): Azul, 5 colunas, did√°tico
                - "pegadinhas" (üéØ): Laranja, 3 colunas, alerta
                - "default": Azul padr√£o
        """
        from docx.shared import RGBColor, Pt, Cm
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        from docx.enum.table import WD_TABLE_ALIGNMENT
        from docx.oxml import OxmlElement
        from docx.oxml.ns import qn

        if len(rows) < 2: return
        max_cols = max(len(row) for row in rows)
        if max_cols == 0: return

        theme_norm = (document_theme or "classic").strip().lower()
        palettes = {
            "classic": {
                "default": {"header_bg": "0066CC", "header_text": RGBColor(255, 255, 255), "alt_row_bg": "F0F6FF"},
                "quadro_sintese": {"header_bg": "0066CC", "header_text": RGBColor(255, 255, 255), "alt_row_bg": "E6F2FF"},
                "pegadinhas": {"header_bg": "E67E00", "header_text": RGBColor(255, 255, 255), "alt_row_bg": "FFF5E6"},
            },
            "minimal": {
                "default": {"header_bg": "F8FAFC", "header_text": RGBColor(31, 41, 55), "alt_row_bg": "FFFFFF"},
                "quadro_sintese": {"header_bg": "F8FAFC", "header_text": RGBColor(31, 41, 55), "alt_row_bg": "FFFFFF"},
                "pegadinhas": {"header_bg": "FFF7ED", "header_text": RGBColor(120, 53, 15), "alt_row_bg": "FFFFFF"},
            },
            "executive": {
                "default": {"header_bg": "111827", "header_text": RGBColor(255, 255, 255), "alt_row_bg": "F3F4F6"},
                "quadro_sintese": {"header_bg": "0F172A", "header_text": RGBColor(255, 255, 255), "alt_row_bg": "E5E7EB"},
                "pegadinhas": {"header_bg": "B45309", "header_text": RGBColor(255, 255, 255), "alt_row_bg": "FEF3C7"},
            },
            "academic": {
                "default": {"header_bg": "ECEFF4", "header_text": RGBColor(31, 41, 55), "alt_row_bg": "FFFFFF"},
                "quadro_sintese": {"header_bg": "E2E8F0", "header_text": RGBColor(31, 41, 55), "alt_row_bg": "FFFFFF"},
                "pegadinhas": {"header_bg": "FFF7ED", "header_text": RGBColor(120, 53, 15), "alt_row_bg": "FFFFFF"},
            },
        }

        palette = palettes.get(theme_norm, palettes["classic"])
        cores = palette.get(table_type, palette["default"])

        table = doc.add_table(rows=len(rows), cols=max_cols)
        table.style = 'Table Grid'
        table.alignment = WD_TABLE_ALIGNMENT.CENTER

        for i, row_data in enumerate(rows):
            for j in range(max_cols):
                cell = table.rows[i].cells[j]
                cell_text = row_data[j] if j < len(row_data) else ""

                p = cell.paragraphs[0]
                p.alignment = WD_ALIGN_PARAGRAPH.LEFT
                self._format_inline_markdown(p, cell_text)

                # Header styling
                if i == 0:
                    for para in cell.paragraphs:
                        for run in para.runs:
                            run.font.bold = True
                            run.font.color.rgb = cores["header_text"]
                            run.font.size = Pt(10)
                        para.alignment = WD_ALIGN_PARAGRAPH.CENTER

                    shading_elm = OxmlElement('w:shd')
                    shading_elm.set(qn('w:fill'), cores["header_bg"])
                    cell._element.get_or_add_tcPr().append(shading_elm)

                # v2.28: Zebra striping (linhas alternadas)
                elif i % 2 == 0:
                    shading_elm = OxmlElement('w:shd')
                    shading_elm.set(qn('w:fill'), cores["alt_row_bg"])
                    cell._element.get_or_add_tcPr().append(shading_elm)

        # v2.28: Ajustar largura das colunas baseado no tipo
        if table_type == "quadro_sintese" and max_cols == 5:
            # Propor√ß√µes: Item(15%), Defini√ß√£o(25%), Detalhes(25%), Base legal(15%), Dica(20%)
            widths = [Cm(2.5), Cm(4.0), Cm(4.0), Cm(2.5), Cm(3.5)]
            for j, width in enumerate(widths):
                for row in table.rows:
                    if j < len(row.cells):
                        row.cells[j].width = width
        elif table_type == "pegadinhas" and max_cols == 3:
            # Propor√ß√µes: Como cobra(35%), Resposta(30%), Erro comum(35%)
            widths = [Cm(5.5), Cm(4.5), Cm(5.5)]
            for j, width in enumerate(widths):
                for row in table.rows:
                    if j < len(row.cells):
                        row.cells[j].width = width

    def create_toc(self, doc):
        """Adiciona Sum√°rio nativo do Word"""
        from docx.oxml.ns import qn
        from docx.oxml import OxmlElement
        
        paragraph = doc.add_paragraph()
        run = paragraph.add_run()
        
        fldChar = OxmlElement('w:fldChar')
        fldChar.set(qn('w:fldCharType'), 'begin')
        run._r.append(fldChar)
        
        instrText = OxmlElement('w:instrText')
        instrText.set(qn('xml:space'), 'preserve')
        instrText.text = 'TOC \\o "1-3" \\h \\z \\u'
        run._r.append(instrText)
        
        fldChar = OxmlElement('w:fldChar')
        fldChar.set(qn('w:fldCharType'), 'separate')
        run._r.append(fldChar)
        
        fldChar = OxmlElement('w:fldChar')
        fldChar.set(qn('w:fldCharType'), 'end')
        run._r.append(fldChar)

def process_single_video(
    video_path,
    dry_run=False,
    mode="APOSTILA",
    skip_formatting=False,
    custom_prompt=None,
    high_accuracy=False,
    diarization: Optional[bool] = None,
    diarization_strict: bool = False,
    skip_audit=False,
    skip_fidelity_audit=False,
    skip_sources_audit=False,
    hil_strict=False,
    resume_hil=False,
    provider="gemini",
    word_only=False,
    auto_apply_fixes=False,
):
    def _is_public_url(value: str) -> bool:
        try:
            parsed = urlparse((value or "").strip())
            return parsed.scheme in {"http", "https"} and bool(parsed.netloc)
        except Exception:
            return False

    def _download_public_media(url: str) -> str:
        """
        Baixa m√≠dia de URL p√∫blica (ex.: YouTube) usando `yt-dlp`.

        - Faz cache por hash da URL no diret√≥rio configur√°vel.
        - Extrai √°udio para MP3 para acelerar o pipeline (FFmpeg ainda far√° WAV 16k mono).
        """
        download_dir = os.getenv("IUDEX_URL_DOWNLOAD_DIR", "tmp/url_imports").strip() or "tmp/url_imports"
        Path(download_dir).mkdir(parents=True, exist_ok=True)

        url_norm = (url or "").strip()
        url_hash = hashlib.sha256(url_norm.encode("utf-8")).hexdigest()[:12]
        base = f"url_{url_hash}"

        # Cache: se j√° existe MP3 baixado, reutiliza.
        cached_mp3 = Path(download_dir) / f"{base}.mp3"
        if cached_mp3.exists() and cached_mp3.stat().st_size > 1024:
            print(f"{Fore.CYAN}üåê URL cache: usando {cached_mp3.name}")
            return str(cached_mp3)

        ytdlp = (
            (os.getenv("IUDEX_YTDLP_PATH") or "").strip()
            or shutil.which("yt-dlp")
            or shutil.which("yt_dlp")
            or ("/opt/homebrew/bin/yt-dlp" if os.path.exists("/opt/homebrew/bin/yt-dlp") else None)
            or ("/usr/local/bin/yt-dlp" if os.path.exists("/usr/local/bin/yt-dlp") else None)
        )
        if not ytdlp:
            raise RuntimeError(
                "Para baixar v√≠deos de URL (ex.: YouTube), instale `yt-dlp`.\n"
                "macOS (Homebrew): `brew install yt-dlp`\n"
                "Python: `python3 -m pip install -U yt-dlp`\n"
            )

        outtmpl = str(Path(download_dir) / f"{base}.%(ext)s")
        cmd = [
            ytdlp,
            "--no-playlist",
            "--restrict-filenames",
            "-f",
            "bestaudio/best",
            "--extract-audio",
            "--audio-format",
            "mp3",
            "--audio-quality",
            "0",
            "-o",
            outtmpl,
            url_norm,
        ]
        print(f"{Fore.CYAN}üåê Baixando URL com yt-dlp...")
        subprocess.run(cmd, check=True)

        # yt-dlp pode gerar .mp3 ou outro ext dependendo de flags; procurar resultado.
        candidates = sorted(Path(download_dir).glob(f"{base}.*"), key=lambda p: p.stat().st_mtime, reverse=True)
        for path in candidates:
            if path.suffix.lower() == ".mp3" and path.stat().st_size > 1024:
                return str(path)
        # Fallback: se n√£o achou mp3, pega o mais recente.
        if candidates:
            return str(candidates[0])
        raise RuntimeError("Falha ao baixar URL: nenhum arquivo gerado.")

    if _is_public_url(video_path):
        video_path = _download_public_media(video_path)

    if not os.path.exists(video_path):
        print(f"{Fore.RED}‚ùå Arquivo n√£o encontrado: {video_path}")
        return

    folder = os.path.dirname(video_path)
    video_name = Path(video_path).stem
    
    try:
        vomo = VomoMLX(provider=provider)

        if word_only and video_path.lower().endswith('.md'):
            print(f"{Fore.CYAN}üìÑ Modo --word-only: Gerando Word a partir de MD existente...")
            with open(video_path, 'r', encoding='utf-8') as f:
                md_content = f.read()
            vomo.save_as_word(md_content, video_name, folder)
            print(f"{Fore.GREEN}‚úÖ DOCX gerado com sucesso!")
            return
        
        if video_path.lower().endswith(('.txt', '.md')):
            print(f"{Fore.CYAN}üìÑ Input √© arquivo de texto. Pulando transcri√ß√£o...")
            with open(video_path, 'r', encoding='utf-8') as f:
                transcription = f.read()
        else:
            if dry_run:
                print("‚ö†Ô∏è Dry run n√£o suporta √°udio direto ainda. Use arquivo .txt")
                return
            audio = vomo.optimize_audio(video_path)

            diar_enabled, _diar_required = vomo.resolve_diarization_policy(
                mode, diarization=diarization, diarization_strict=diarization_strict
            )
            raw_parts = [video_name, "RAW"]
            if diar_enabled:
                raw_parts.append("DIAR")
            if high_accuracy:
                raw_parts.append("BEAM")
            raw_path = os.path.join(folder, f"{'_'.join(raw_parts)}.txt")

            if os.path.exists(raw_path):
                with open(raw_path, 'r') as f:
                    transcription = f.read()
            else:
                # Escolhe backend de transcri√ß√£o
                transcription = vomo.transcribe_file(
                    audio,
                    mode=mode,
                    high_accuracy=high_accuracy,
                    diarization=diarization,
                    diarization_strict=diarization_strict,
                )
                with open(raw_path, 'w') as f:
                    f.write(transcription)
        
        if skip_formatting and not resume_hil:
            print(f"{Fore.GREEN}‚úÖ Transcri√ß√£o RAW conclu√≠da: {raw_path}")
            print(f"{Fore.YELLOW}   ‚ÑπÔ∏è  Formata√ß√£o pulada (--skip-formatting usado).{Style.RESET_ALL}")
            return

        formatted = None
        mode_suffix = (mode or "APOSTILA").upper()

        if resume_hil:
            hil_path = get_hil_output_path(video_name, folder, mode_suffix)
            if not hil_path.exists():
                print(f"{Fore.RED}‚ùå HIL checkpoint n√£o encontrado: {hil_path.name}")
                return
            with open(hil_path, 'r', encoding='utf-8') as f:
                formatted = f.read()
            print(f"{Fore.YELLOW}‚èØÔ∏è  Retomando a partir do HIL checkpoint: {hil_path.name}")
            primary_fidelity_written = False
            validation_result = None

            # v3.0: Carregar relat√≥rio unificado anterior para compara√ß√£o
            _previous_unified = None
            if UNIFIED_AUDIT_AVAILABLE:
                _prev_unified_path = Path(folder) / f"{video_name}_{mode_suffix}_UNIFIED_AUDIT.json"
                if _prev_unified_path.exists():
                    try:
                        _previous_unified = UnifiedReport.load_json(str(_prev_unified_path))
                        _prev_active = [f for f in _previous_unified.findings if f.verdict.value != "FALSO_POSITIVO"]
                        print(f"   üìä Relat√≥rio unificado anterior: {len(_prev_active)} findings carregados para compara√ß√£o")
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è Falha ao carregar relat√≥rio anterior: {e}")

            # Revalida√ß√£o preventiva (opcional) ap√≥s corre√ß√µes manuais
            if FIDELITY_AUDIT_AVAILABLE and FIDELITY_AUDIT_ENABLED and not skip_fidelity_audit:
                print(f"\n{Fore.CYAN}üî¨ Revalidando Auditoria Preventiva de Fidelidade...")
                preventive_json = Path(folder) / f"{video_name}_{mode_suffix}_AUDITORIA_FIDELIDADE.json"
                preventive_md = Path(folder) / f"{video_name}_{mode_suffix}_AUDITORIA_FIDELIDADE.md"
                try:
                    preventive_result = auditar_fidelidade_preventiva(
                        vomo.client,
                        transcription,
                        formatted,
                        video_name,
                        str(preventive_json),
                        modo=mode_suffix,
                        include_sources=(SOURCES_AUDIT_ENABLED and not skip_sources_audit),
                    )
                    gerar_relatorio_markdown_completo(preventive_result, str(preventive_md), video_name)
                    compat = (preventive_result or {}).get("compat_fidelidade")
                    if isinstance(compat, dict) and compat:
                        fidelity_path = Path(folder) / f"{video_name}_{mode_suffix}_fidelidade.json"
                        with open(fidelity_path, "w", encoding="utf-8") as f:
                            json.dump(compat, f, ensure_ascii=False, indent=2)
                        print(f"üìÑ Relat√≥rio de fidelidade (preventiva) salvo: {fidelity_path.name}")
                        primary_fidelity_written = True
                    recomendacao = (preventive_result or {}).get("recomendacao_hil", {}) or {}
                    if hil_strict and recomendacao.get("pausar_para_revisao"):
                        save_hil_output(
                            formatted,
                            video_name,
                            folder,
                            mode_suffix,
                            reason="auditoria_preventiva_resumo",
                        )
                        raise HILCheckpointException(
                            f"Auditoria preventiva exige revis√£o humana. Veja: {preventive_md.name}"
                        )
                except HILCheckpointException:
                    raise
                except Exception as e:
                    print(f"{Fore.YELLOW}‚ö†Ô∏è Falha na auditoria preventiva: {e}. Continuando...")

            # Valida√ß√£o Full-Context (backup)
            if FIDELITY_BACKUP_ENABLED:
                try:
                    print(f"\n{Fore.CYAN}üî¨ Valida√ß√£o Full-Context LLM (revalida√ß√£o/backup)...")
                    validation_result = vomo.validate_completeness_full(
                        transcription, formatted, video_name, None
                    )
                    validation_report_path = Path(folder) / f"{video_name}_{mode_suffix}_fidelidade_backup.json"
                    with open(validation_report_path, "w", encoding='utf-8') as f:
                        json.dump(validation_result, f, ensure_ascii=False, indent=2)
                    print(f"üìÑ Relat√≥rio de fidelidade (backup) salvo: {validation_report_path.name}")

                    # v2.30: Se houve erro de valida√ß√£o, default √© False (n√£o mascara falha)
                    _def_aprov = False if validation_result.get('erro_validacao') else True
                    if not validation_result.get('aprovado', _def_aprov):
                        fidelity_md_path = Path(folder) / f"{video_name}_{mode_suffix}_REVISAO.md"
                        with open(fidelity_md_path, "w", encoding='utf-8') as f:
                            f.write(f"# ‚ö†Ô∏è REVIS√ÉO NECESS√ÅRIA: {video_name}\n\n")
                            f.write(f"**Nota de Fidelidade:** {validation_result.get('nota', 0)}/10\n\n")
                            if validation_result.get('omissoes'):
                                f.write("## üìå Omiss√µes Detectadas\n")
                                for o in validation_result['omissoes']:
                                    f.write(f"- {o}\n")
                                f.write("\n")
                            if validation_result.get('distorcoes'):
                                f.write("## ‚ö†Ô∏è Distor√ß√µes Detectadas\n")
                                for d in validation_result['distorcoes']:
                                    f.write(f"- {d}\n")
                                f.write("\n")
                            if validation_result.get('problemas_estrutura'):
                                f.write("## üèóÔ∏è Problemas de Estrutura\n")
                                for p in validation_result['problemas_estrutura']:
                                    f.write(f"- {p}\n")
                                f.write("\n")
                            if validation_result.get('observacoes'):
                                f.write(f"## Observa√ß√µes\n{validation_result['observacoes']}\n")
                        print(f"{Fore.RED}üìÑ ATEN√á√ÉO: Documento requer revis√£o! Veja: {fidelity_md_path.name}")
                except Exception as e:
                    print(f"{Fore.YELLOW}‚ö†Ô∏è Erro na revalida√ß√£o Full-Context: {e}")

            if not primary_fidelity_written and isinstance(validation_result, dict):
                fidelity_path = Path(folder) / f"{video_name}_{mode_suffix}_fidelidade.json"
                try:
                    with open(fidelity_path, "w", encoding="utf-8") as f:
                        json.dump(validation_result, f, ensure_ascii=False, indent=2)
                    print(f"üìÑ Relat√≥rio de fidelidade (fallback) salvo: {fidelity_path.name}")
                except Exception as e:
                    print(f"{Fore.YELLOW}‚ö†Ô∏è Falha ao salvar relat√≥rio de fidelidade fallback: {e}")

            # Auditoria Jur√≠dica (opcional)
            if AUDIT_AVAILABLE and not skip_audit:
                print(f"\n{Fore.CYAN}üïµÔ∏è Auditoria Jur√≠dica P√≥s-Processamento...")
                audit_report_path = Path(folder) / f"{video_name}_{mode_suffix}_AUDITORIA.md"
                audit_content = auditar_consistencia_legal(
                    vomo.client,
                    formatted,
                    str(audit_report_path),
                    raw_transcript=transcription,
                )
                if audit_content:
                    print(f"{Fore.GREEN}   üìé Relat√≥rio de auditoria salvo em arquivo separado...")
        else:
            formatted = asyncio.run(vomo.format_transcription_async(
                transcription,
                video_name,
                folder,
                mode=mode,
                custom_prompt=custom_prompt,
                dry_run=dry_run,
                skip_audit=skip_audit,
                skip_fidelity_audit=skip_fidelity_audit,
                skip_sources_audit=skip_sources_audit,
                hil_strict=hil_strict,
            ))
        
        # v2.28+: Aplicar as mesmas corre√ß√µes de tabelas do DOCX tamb√©m no Markdown final (APOSTILA).
        # Caso contr√°rio, o usu√°rio v√™ no .md uma tabela "no meio do assunto" que s√≥ √© corrigida no DOCX.
        try:
            mode_label = (mode or "APOSTILA").upper()
        except Exception:
            mode_label = "APOSTILA"
        if formatted and mode_label == "APOSTILA":
            try:
                formatted = corrigir_tabelas_prematuras(formatted)
            except Exception as e:
                print(f"{Fore.YELLOW}‚ö†Ô∏è Erro ao corrigir tabelas prematuras (MD): {e}.")
            try:
                formatted = mover_tabelas_para_fim_de_secao(formatted)
            except Exception as e:
                print(f"{Fore.YELLOW}‚ö†Ô∏è Erro ao reorganizar tabelas (MD): {e}. Usando layout padr√£o.")

        with open(os.path.join(folder, f"{video_name}_{mode}.md"), 'w') as f:
            f.write(formatted)
            
        # Valida√ß√£o de Fidelidade (Portado)
        try:
             verificar_cobertura(transcription, formatted, os.path.join(folder, f"{video_name}_{mode}.md"))
        except Exception as e:
             print(f"{Fore.YELLOW}‚ö†Ô∏è Erro na valida√ß√£o de fidelidade: {e}")

        vomo.save_as_word(formatted, video_name, folder, mode=mode)
        
        # v2.24: Auto-Fix Post-Processing (Structural Analysis - HIL Mode)
        md_output_path = os.path.join(folder, f"{video_name}_{mode}.md")
        auto_apply_flag = auto_apply_fixes
        
        if AUTO_FIX_AVAILABLE:
            print(f"\n{Fore.CYAN}üîß Auto-Fix Structural Analysis (v2.24 HIL)...")
            try:
                issues = analyze_structural_issues(md_output_path)
                if issues['total_issues'] > 0:
                    print(f"{Fore.YELLOW}   ‚ö†Ô∏è {issues['total_issues']} problema(s) estrutural(is) detectado(s).")
                    print(f"   Se√ß√µes duplicadas: {len(issues['duplicate_sections'])}")
                    print(f"   Par√°grafos duplicados: {len(issues['duplicate_paragraphs'])}")
                    
                    # Save suggestions to JSON for HIL review
                    suggestions_path = os.path.join(folder, f"{video_name}_{mode}_SUGESTOES.json")
                    import json
                    with open(suggestions_path, 'w', encoding='utf-8') as f:
                        json.dump(issues, f, indent=2, ensure_ascii=False)
                    print(f"{Fore.CYAN}   üìã Sugest√µes salvas em: {os.path.basename(suggestions_path)}")
                    
                    if auto_apply_flag:
                        # Auto-apply fixes (only with explicit flag)
                        result = apply_structural_fixes_to_file(md_output_path, issues)
                        if result['fixes_applied']:
                            print(f"{Fore.GREEN}   ‚úÖ {len(result['fixes_applied'])} corre√ß√£o(√µes) aplicada(s) automaticamente.")
                            # Regenerate Word document with fixed content
                            with open(md_output_path, 'r', encoding='utf-8') as f:
                                fixed_content = f.read()
                            vomo.save_as_word(fixed_content, video_name, folder, mode=mode)
                    else:
                        print(f"{Fore.YELLOW}   ‚ÑπÔ∏è  Modo HIL: Revise as sugest√µes e use --auto-apply-fixes para aplicar.")
                else:
                    print(f"{Fore.GREEN}   ‚úÖ Nenhum problema estrutural detectado.")
            except Exception as e:
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è Erro no Auto-Fix: {e}")

        # v3.0: Relat√≥rio Unificado (cross-referencing entre camadas)
        if UNIFIED_AUDIT_AVAILABLE:
            try:
                engine = UnifiedAuditEngine(video_name, mode)
                # Ingerir auditoria preventiva (j√° salva em disco)
                prev_json = Path(folder) / f"{video_name}_{mode}_AUDITORIA_FIDELIDADE.json"
                if prev_json.exists():
                    with open(prev_json, "r", encoding="utf-8") as f:
                        engine.ingest_fidelity(json.load(f))
                # Ingerir backup
                backup_json = Path(folder) / f"{video_name}_{mode}_fidelidade_backup.json"
                if backup_json.exists():
                    with open(backup_json, "r", encoding="utf-8") as f:
                        engine.ingest_backup(json.load(f))
                # Ingerir structural (vari√°vel local do bloco acima)
                if AUTO_FIX_AVAILABLE and 'issues' in dir() and isinstance(issues, dict):
                    engine.ingest_structural(issues)
                unified_report = engine.build()
                # Salvar
                unified_json_path = os.path.join(folder, f"{video_name}_{mode}_UNIFIED_AUDIT.json")
                unified_md_path = os.path.join(folder, f"{video_name}_{mode}_UNIFIED_AUDIT.md")
                unified_report.save_json(unified_json_path)
                generate_unified_markdown(unified_report, unified_md_path)
                summary = unified_report.summary
                print(f"\n{Fore.CYAN}üìä Relat√≥rio Unificado (v3.0):")
                print(f"   Nota Geral: {unified_report.nota_geral:.1f}/10 (Fidelidade: {unified_report.nota_fidelidade:.1f} | Estrutural: {unified_report.nota_estrutural:.1f})")
                print(f"   Findings: {summary.get('total_findings', 0)} ativos ({summary.get('false_positives_removed', 0)} FP removidos)")
                by_sev = summary.get('by_severity', {})
                sev_parts = [f"{k}: {v}" for k, v in by_sev.items() if v > 0]
                if sev_parts:
                    print(f"   Severidade: {' | '.join(sev_parts)}")
                print(f"   Salvos: {os.path.basename(unified_json_path)} + {os.path.basename(unified_md_path)}")
                # Compara√ß√£o com relat√≥rio anterior (resume-hil)
                if '_previous_unified' in dir() and _previous_unified is not None:
                    try:
                        delta = compare_reports(_previous_unified, unified_report)
                        print(f"   üìà Delta HIL: {delta['resolved_count']} resolvidos | {delta['persistent_count']} persistentes | {delta['new_count']} novos")
                    except Exception:
                        pass
                # HIL unificado
                if hil_strict and unified_report.hil_recommendation.pausar:
                    save_hil_output(formatted, video_name, folder, mode_suffix, reason="unified_audit")
                    raise HILCheckpointException(
                        f"Auditoria unificada requer revis√£o. Veja: {os.path.basename(unified_md_path)}"
                    )
            except HILCheckpointException:
                raise
            except Exception as e:
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è Erro no relat√≥rio unificado: {e}")

        print(f"{Fore.GREEN}‚ú® SUCESSO!")
        
    except HILCheckpointException as e:
        print(f"{Fore.YELLOW}‚è∏Ô∏è  HIL checkpoint acionado: {e}")
        return
    except Exception as e:
        print(f"{Fore.RED}‚ùå Erro: {e}")
        traceback.print_exc()

def _parse_mode(value):
    normalized = value.strip().upper()
    allowed = {"APOSTILA", "FIDELIDADE", "AUDIENCIA", "REUNIAO", "DEPOIMENTO"}
    if normalized not in allowed:
        raise argparse.ArgumentTypeError(f"Modo invalido: {value}. Use {', '.join(sorted(allowed))}.")
    return normalized


def _parse_provider(value):
    normalized = value.strip().lower()
    allowed = {"gemini", "openai"}
    if normalized not in allowed:
        raise argparse.ArgumentTypeError(f"Provider invalido: {value}. Use {', '.join(sorted(allowed))}.")
    return normalized


def _load_custom_prompt(prompt_value):
    if not prompt_value:
        return None
    if prompt_value.endswith('.txt') and os.path.exists(prompt_value):
        with open(prompt_value, 'r', encoding='utf-8') as f:
            custom_prompt = f.read()
        print(f"{Fore.YELLOW}üìù Prompt carregado de arquivo: {prompt_value} ({len(custom_prompt):,} chars)")
        return custom_prompt
    custom_prompt = prompt_value
    print(f"{Fore.YELLOW}üìù Usando prompt direto ({len(custom_prompt):,} chars)")
    return custom_prompt


def _build_arg_parser():
    parser = argparse.ArgumentParser(
        description="MLX Vomo - transcricao e formatacao juridica.",
    )
    parser.add_argument("input_file", nargs="?", help="Arquivo de video/audio ou .txt/.md de entrada.")
    parser.add_argument("--mode", type=_parse_mode, default="FIDELIDADE",
                        help="Modo de formatacao: APOSTILA, FIDELIDADE, AUDIENCIA, REUNIAO, DEPOIMENTO.")
    parser.add_argument("--provider", type=_parse_provider, default="gemini",
                        help="Provider LLM: gemini ou openai.")
    parser.add_argument(
        "--prompt",
        help=(
            "Prompt customizado (texto direto ou arquivo .txt). "
            "Em APOSTILA/AUDIENCIA/REUNIAO: personaliza apenas TABELAS/EXTRAS (resumo/fluxograma/mapa mental/question√°rio), "
            "preservando tom/estilo/estrutura. Em outros modos: substitui STYLE+TABLE; HEAD/STRUCTURE/FOOTER s√£o preservados."
        ),
    )
    parser.add_argument("--dry-run", action="store_true", help="Executa apenas etapas locais.")
    parser.add_argument("--skip-formatting", action="store_true", help="Pula a formatacao final.")
    parser.add_argument("--high-accuracy", action="store_true", help="Usa beam search na transcricao.")
    diar_group = parser.add_mutually_exclusive_group()
    diar_group.add_argument(
        "--diarization",
        dest="diarization",
        action="store_true",
        help="For√ßa diariza√ß√£o ON (override do padr√£o por modo).",
    )
    diar_group.add_argument(
        "--no-diarization",
        dest="diarization",
        action="store_false",
        help="For√ßa diariza√ß√£o OFF (override do padr√£o por modo).",
    )
    parser.set_defaults(diarization=None)
    parser.add_argument(
        "--diarization-strict",
        action="store_true",
        help="Falha se diariza√ß√£o estiver indispon√≠vel (√∫til quando opt-in em APOSTILA/FIDELIDADE).",
    )
    parser.add_argument("--skip-fidelity-audit", action="store_true", help="Desativa auditoria de fidelidade.")
    parser.add_argument("--skip-sources-audit", action="store_true", help="Desativa auditoria de fontes.")
    parser.add_argument("--hil-strict", action="store_true", help="Habilita checkpoint estrito de HIL.")
    parser.add_argument("--resume-hil", action="store_true", help="Retoma a partir do checkpoint HIL.")
    parser.add_argument("--word-only", action="store_true", help="Gera DOCX a partir de um .md existente.")
    parser.add_argument("--auto-apply-fixes", action="store_true", help="Aplica correcoes estruturais automaticamente.")
    parser.add_argument("--no-audit", "--skip-legal-audit", "--skip-audit",
                        dest="skip_audit", action="store_true",
                        help="Desativa auditoria juridica.")
    return parser


if __name__ == "__main__":
    parser = _build_arg_parser()
    args = parser.parse_args()

    if not args.input_file:
        parser.print_usage()
        sys.exit(1)

    custom_prompt = _load_custom_prompt(args.prompt)
    print(f"{Fore.CYAN}üöÄ Iniciando MLX Vomo (v2.26)...")
    try:
        process_single_video(
            args.input_file,
            mode=args.mode,
            custom_prompt=custom_prompt,
            dry_run=args.dry_run,
            high_accuracy=args.high_accuracy,
            diarization=args.diarization,
            diarization_strict=args.diarization_strict,
            skip_formatting=args.skip_formatting,
            skip_audit=args.skip_audit,
            skip_fidelity_audit=args.skip_fidelity_audit,
            skip_sources_audit=args.skip_sources_audit,
            hil_strict=args.hil_strict,
            resume_hil=args.resume_hil,
            provider=args.provider,
            word_only=args.word_only,
            auto_apply_fixes=args.auto_apply_fixes,
        )
    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}‚ö†Ô∏è Interrup√ß√£o pelo usu√°rio.")
        sys.exit(0)
    except Exception as e:
        print(f"\n{Fore.RED}‚ùå Erro fatal: {e}")
        sys.exit(1)
