import os
import time
import asyncio
from pathlib import Path
from openai import OpenAI, AsyncOpenAI, APIError
from colorama import Fore, init
from tqdm import tqdm
import re
import traceback
import json
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

init(autoreset=True)

class VomoFormatter:
    def __init__(self):
        """Formatador de transcri√ß√µes Vomo (apenas texto) - VERS√ÉO OTIMIZADA"""
        
        # Cache para evitar re-identifica√ß√£o de speakers
        self.speaker_cache = {}
        print(f"{Fore.CYAN}üöÄ Inicializando Vomo Formatter (OpenAI GPT-5-mini)...")
        
        # Carrega vari√°vel de ambiente com override
        from dotenv import load_dotenv
        load_dotenv(override=True)
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError(f"{Fore.RED}‚ùå Configure: export OPENAI_API_KEY='sk-...'")
        
        # Cliente OpenAI direto (sem OpenRouter)
        self.async_client = AsyncOpenAI(
            api_key=api_key
        )
        
        # Modelo GPT-5-mini (OpenAI)
        self.model = "gpt-5-mini-2025-08-07"
        
        # Cliente s√≠ncrono (se necess√°rio)
        self.client = OpenAI(
            api_key=api_key
        )
        self.llm_client = self.client
    
    def _get_cache_path(self, file_path, cache_type="FORMATTED"):
        """Gera caminho do arquivo de cache baseado no arquivo original"""
        base_path = Path(file_path)
        cache_file = base_path.parent / f"{base_path.stem}_CACHE_{cache_type}.json"
        return str(cache_file)
    
    def _load_cache(self, cache_file):
        """Carrega resultado processado do cache se dispon√≠vel e v√°lido"""
        if not os.path.exists(cache_file):
            return None
        
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # Valida√ß√£o b√°sica
            required_fields = ['formatted_text', 'timestamp', 'version']
            if not all(field in cache_data for field in required_fields):
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è Cache inv√°lido (campos faltando), reprocessando...")
                return None
            
            # Verifica vers√£o (compatibilidade)
            if cache_data.get('version') != '0.3':
                print(f"{Fore.YELLOW}   ‚ö†Ô∏è Cache de vers√£o antiga ({cache_data.get('version')}), reprocessando...")
                return None
            
            print(f"{Fore.GREEN}   üìÇ Cache v√°lido encontrado ({cache_data['timestamp']})")
            return cache_data
        
        except Exception as e:
            print(f"{Fore.YELLOW}   ‚ö†Ô∏è Erro ao carregar cache ({e}), reprocessando...")
            return None
    
    def _save_cache(self, cache_file, formatted_text, original_file):
        """Salva resultado processado em cache JSON"""
        try:
            cache_data = {
                'formatted_text': formatted_text,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'original_file': os.path.basename(original_file),
                'version': '0.3',
                'model': self.model
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            print(f"{Fore.CYAN}   üíæ Cache salvo: {os.path.basename(cache_file)}")
        
        except Exception as e:
            print(f"{Fore.YELLOW}   ‚ö†Ô∏è Erro ao salvar cache ({e})")
    


    SYSTEM_PROMPT_FORMAT = """# SEU PAPEL
    Voc√™ √© um **revisor s√™nior de material did√°tico jur√≠dico** com 20 anos de experi√™ncia em preparat√≥rios para concursos de procuradoria (PGM/PGE). Seu trabalho √© transformar transcri√ß√µes brutas de aulas em apostilas de alta qualidade, mantendo 100% do conte√∫do t√©cnico.

    ---

    # MISS√ÉO

    Transformar a transcri√ß√£o de aula em uma **apostila did√°tica** com estilo formal, impessoal e expositivo, **PRESERVANDO TODO O CONTE√öDO ORIGINAL**.

    ‚ö†Ô∏è **REGRA ABSOLUTA: PRESERVE 100% DO CONTE√öDO - N√ÉO RESUMA, N√ÉO OMITA NADA.**

    ---

    # EXEMPLOS DE TRANSFORMA√á√ÉO (Few-Shot)

    ## Exemplo 1: Transforma√ß√£o de Linguagem
    **INPUT (Transcri√ß√£o bruta):**
    > "Ent√£o pessoal, a Lei 8.666, n√©, ela fala l√° no artigo 37 que a gente n√£o pode fazer contrata√ß√£o direta, t√°? Exceto em alguns casos, tipo... quando tem emerg√™ncia, sabe?"

    **OUTPUT (Apostila formatada):**
    > Observa-se que o *Art. 37 da Lei n¬∫ 8.666/93* estabelece a veda√ß√£o √† contrata√ß√£o direta como regra geral. Contudo, o legislador previu hip√≥teses excepcionais, notadamente os casos de emerg√™ncia, nos termos do *Art. 24, inciso IV* do referido diploma legal.

    ## Exemplo 2: Preserva√ß√£o de Dicas de Prova
    **INPUT:**
    > "Isso aqui cai muito em prova, hein! Aten√ß√£o total nesse artigo 37."

    **OUTPUT:**
    > Reveste-se de especial import√¢ncia este dispositivo para fins de certames p√∫blicos, sendo recorrentemente objeto de cobran√ßa em provas de procuradorias.

    ## Exemplo 3: Preserva√ß√£o de Exemplos Completos
    **INPUT:**
    > "Por exemplo, teve um caso no Rio que o prefeito tentou fazer uma contrata√ß√£o emergencial pra comprar uns equipamentos, mas o TCE glosou porque n√£o tinha a urg√™ncia real."

    **OUTPUT:**
    > A t√≠tulo ilustrativo, cumpre mencionar caso ocorrido no Estado do Rio de Janeiro, no qual determinado prefeito municipal tentou realizar contrata√ß√£o emergencial para aquisi√ß√£o de equipamentos. Contudo, o Tribunal de Contas do Estado glosou o procedimento, fundamentando sua decis√£o na aus√™ncia de urg√™ncia real que justificasse a dispensa de licita√ß√£o.

    ---

    # ‚ùå ERROS A EVITAR

    **ERRO 1 - Resumir exemplos:**
    - ‚ùå "O professor citou casos de jurisprud√™ncia"
    - ‚úÖ Transcreva cada caso completo com nome, ano e decis√£o

    **ERRO 2 - Generalizar refer√™ncias:**
    - ‚ùå "Conforme a legisla√ß√£o de licita√ß√µes"
    - ‚úÖ "Conforme o *Art. 37, caput, da Lei n¬∫ 8.666/93*"

    **ERRO 3 - Omitir dicas de prova:**
    - ‚ùå [Ignorar coment√°rio do professor sobre a prova]  
    - ‚úÖ Inclua: "Reveste-se de especial import√¢ncia para certames..."

    **ERRO 4 - Perder contexto hist√≥rico:**
    - ‚ùå "Houve mudan√ßas na lei"
    - ‚úÖ "A reforma introduzida pela Lei n¬∫ 14.133/2021 alterou significativamente..."

    ---

    # DIRETRIZES DE ESTILO (APOSTILA FORMAL)

    ## Tom e Linguagem
    ‚úÖ **USE SEMPRE:**
    - **Terceira pessoa impessoal**: "observa-se", "constata-se", "verifica-se", "cumpre destacar"
    - **Conectivos acad√™micos**: "nesse sentido", "ademais", "cumpre ressaltar", "destarte"
    - **Verbos formais**: "configura", "caracteriza", "evidencia", "compreende"

    ‚ùå **NUNCA USE:**
    - Primeira/segunda pessoa: "eu", "voc√™", "n√≥s", "a gente"
    - V√≠cios de fala: "n√©", "t√°", "a√≠", "ent√£o", "tipo", "beleza"
    - Express√µes informais: "p√¥", "cara", "galera", "pessoal"

    ---

    # ESTRUTURA OBRIGAT√ìRIA DO OUTPUT

    ```markdown
    # [T√≠tulo da Aula/Disciplina]

    ## 1. [Primeiro T√≥pico Principal]

    [Conte√∫do em prosa acad√™mica fluida, preservando 100% das informa√ß√µes]

    **S√çNTESE DO T√ìPICO:**

    | Conceito/Instituto | Defini√ß√£o | Fundamento Legal | Observa√ß√µes |
    |-------------------|-----------|-----------------|-------------|
    | [Nome] | [Descri√ß√£o] | [Art. X, Lei Y] | [Dicas/Exce√ß√µes] |

    ## 2. [Segundo T√≥pico Principal]
    ...
    ```

    ---

    # RESTRI√á√ïES OBRIGAT√ìRIAS

    1. **Tamanho:** Output deve ter entre **90% e 120%** do tamanho do input
    2. **Linguagem:** APENAS portugu√™s brasileiro formal
    3. **Formato:** APENAS Markdown v√°lido
    4. **Numera√ß√£o:** T√≥picos DEVEM ser numerados sequencialmente (1., 1.1, 1.2, 2., etc.)
    5. **Tabelas:** OBRIGAT√ìRIAS ao final de CADA t√≥pico principal

    ## PROIBIDO:
    - Coment√°rios meta ("Continua√ß√£o...", "Parte X...", "Chunk...")
    - Emojis (exceto em alertas de dica de prova)
    - Links externos inventados
    - Conte√∫do n√£o presente na transcri√ß√£o original
    - Reorganizar a ordem cronol√≥gica da aula

    ---

    # AUTO-VERIFICA√á√ÉO (EXECUTE ANTES DE ENVIAR)

    ‚ñ° Mantive TODOS os artigos de lei com n√∫meros corretos?
    ‚ñ° Mantive TODOS os exemplos e casos completos (n√£o resumidos)?
    ‚ñ° Mantive TODAS as dicas de prova e observa√ß√µes do professor?
    ‚ñ° Criei tabelas de s√≠ntese ao final de CADA t√≥pico principal?
    ‚ñ° Usei terceira pessoa impessoal em TODO o texto?
    ‚ñ° Output tem pelo menos 90% do tamanho do input?
    ‚ñ° Numera√ß√£o dos t√≥picos est√° sequencial e correta?

    **Se QUALQUER item for N√ÉO ‚Üí REVISE antes de enviar.**

    ---

    # FORMATO DE RESPOSTA

    Retorne **APENAS** a apostila em Markdown, sem meta-coment√°rios ou explica√ß√µes sobre o processo.
    """

    async def _summarize_chunk_async(self, text_chunk, idx, total):
        """Resume um bloco de texto formatado (Async)"""
        try:
            response = await self.async_client.chat.completions.create(
                model=self.model,  # Gemini 2.5 Flash via OpenRouter
                messages=[
                    {"role": "system", "content": self.SUMMARY_PROMPT},
                    {"role": "user", "content": f"Resuma a parte {idx+1}/{total}:\n\n{text_chunk}"}
                ]
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"{Fore.RED}‚ö†Ô∏è Erro ao resumir chunk {idx}: {e}")
            return text_chunk # Retorna original em caso de erro para n√£o perder

    def generate_summary_version(self, full_formatted_text):
        """Gera a vers√£o sint√©tica a partir da vers√£o completa"""
        print(f"\n{Fore.CYAN}üìâ Gerando vers√£o RESUMIDA (Sint√©tica)...")
        
        # Divide o texto completo em peda√ßos maiores (o resumo aceita mais contexto)
        # Usamos 25k caracteres pois o texto j√° est√° limpo
        chunks = self._smart_chunk(full_formatted_text, 25000)
        
        async def process_summary():
            results = []
            for i, chunk in enumerate(chunks):
                print(f"   Resumindo parte {i+1}/{len(chunks)}...")
                res = await self._summarize_chunk_async(chunk, i, len(chunks))
                results.append(res)
            return "\n\n---\n\n".join(results)
            
        summary_text = asyncio.run(process_summary())
        
        # Adiciona um cabe√ßalho distintivo
        header = "# RESUMO ESQUEM√ÅTICO (Revis√£o R√°pida)\n> Baseado na aula completa.\n\n"
        return header + summary_text

    def _smart_chunk(self, text, max_size):
        """Divide texto respeitando par√°grafos (ORIGINAL)"""
        if len(text) <= max_size:
            return [text]
        
        paragraphs = text.split('\n\n')
        chunks = []
        current = ""
        
        for para in paragraphs:
            if len(current) + len(para) + 2 <= max_size:
                current += para + "\n\n"
            else:
                if current:
                    chunks.append(current.strip())
                
                if len(para) > max_size:
                    sentences = para.replace('? ', '?|').replace('! ', '!|').replace('. ', '.|').split('|')
                    temp = ""
                    for s in sentences:
                        if len(temp) + len(s) <= max_size:
                            temp += s
                        else:
                            chunks.append(temp)
                            temp = s
                    if temp:
                        current = temp + "\n\n"
                    else:
                        current = ""
                else:
                    current = para + "\n\n"
        
        if current:
            chunks.append(current.strip())
            
        return chunks

    def _smart_chunk_overlapping(self, text, max_size=8000, overlap=1000):
        """
        Divide texto com SOBREPOSI√á√ÉO para evitar perda nas bordas
        """
        if len(text) <= max_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + max_size
            
            # Se n√£o for o √∫ltimo chunk, encontra quebra natural
            if end < len(text):
                # Procura por quebra de par√°grafo nos √∫ltimos 1000 chars
                search_zone = text[end-1000:end]
                last_break = search_zone.rfind('\n\n')
                
                if last_break != -1:
                    end = end - 1000 + last_break
            
            chunk = text[start:end]
            chunks.append(chunk)
            
            # Pr√≥ximo chunk come√ßa com sobreposi√ß√£o (exceto no final)
            if end < len(text):
                start = end - overlap  # 500 chars de overlap
            else:
                break
        
        return chunks

    def _validate_preservation_heuristics(self, original_text, formatted_text):
        """
        Valida√ß√£o autom√°tica baseada em contagem de elementos cr√≠ticos
        """
        import re
        
        print(f"\n{Fore.CYAN}üîç Valida√ß√£o Heur√≠stica de Preserva√ß√£o...")
        
        issues = []
        
        # 1. Contagem de Refer√™ncias Legais
        original_laws = set(re.findall(
            r'(?:Lei|lei)\s+n?¬∫?\s*\d+[\./]\d+|Art\.?\s*\d+|S√∫mula\s+\d+',
            original_text,
            re.IGNORECASE
        ))
        formatted_laws = set(re.findall(
            r'(?:Lei|lei)\s+n?¬∫?\s*\d+[\./]\d+|Art\.?\s*\d+|S√∫mula\s+\d+',
            formatted_text,
            re.IGNORECASE
        ))
        
        missing_laws = original_laws - formatted_laws
        if missing_laws:
            issues.append(f"‚ùå {len(missing_laws)} refer√™ncias legais omitidas: {list(missing_laws)[:3]}")
        else:
            print(f"{Fore.GREEN}   ‚úÖ Leis/Artigos: {len(formatted_laws)}/{len(original_laws)} preservados")
        
        # 2. Contagem de Autores
        original_authors = set(re.findall(
            r'(?:Prof|professor|Doutor|Dr\.?)\s+([A-Z√á√Å√Ä√É√â√ä][a-z√ß√°√†√£√©√™]+(?:\s+[A-Z√á√Å√Ä√É√â√ä][a-z√ß√°√†√£√©√™]+)+)',
            original_text
        ))
        formatted_authors = set(re.findall(
            r'(?:Prof|professor|Doutor|Dr\.?)\s+([A-Z√á√Å√Ä√É√â√ä][a-z√ß√°√†√£√©√™]+(?:\s+[A-Z√á√Å√Ä√É√â√ä][a-z√ß√°√†√£√©√™]+)+)',
            formatted_text
        ))
        
        missing_authors = original_authors - formatted_authors
        if missing_authors:
            issues.append(f"‚ùå {len(missing_authors)} autores omitidos: {list(missing_authors)[:3]}")
        else:
            print(f"{Fore.GREEN}   ‚úÖ Autores: {len(formatted_authors)}/{len(original_authors)} preservados")
        
        # 3. Contagem de Palavras-Chave de Dicas de Prova
        tip_keywords = [
            r'cai muito', r'aten√ß√£o', r'pegadinha', r'cuidado',
            r'importante', r'n√£o confund', r'diferen√ßa entre'
        ]
        
        original_tips = sum([len(re.findall(kw, original_text, re.IGNORECASE)) for kw in tip_keywords])
        formatted_tips = sum([len(re.findall(kw, formatted_text, re.IGNORECASE)) for kw in tip_keywords])
        
        if formatted_tips < original_tips * 0.8:  # Toler√¢ncia de 20%
            issues.append(f"‚ö†Ô∏è Poss√≠vel perda de dicas: {formatted_tips}/{original_tips} ocorr√™ncias")
        else:
            print(f"{Fore.GREEN}   ‚úÖ Dicas de prova: {formatted_tips}/{original_tips} preservadas")
        
        # 4. An√°lise de Comprimento Relativo
        original_words = len(original_text.split())
        formatted_words = len(formatted_text.split())
        ratio = formatted_words / original_words if original_words > 0 else 0
        
        if ratio < 0.70:  # Se o texto formatado for <70% do original
            issues.append(f"‚ö†Ô∏è ALERTA: Texto formatado √© {ratio:.1%} do original (esperado >70%)")
        else:
            print(f"{Fore.GREEN}   ‚úÖ Comprimento relativo: {ratio:.1%} (adequado)")
        
        # 5. Detec√ß√£o de Frases Cortadas
        truncation_patterns = [
            r'\.\.\.$',  # Texto terminando em ...
            r'\w+\s*$(?![.!?])',  # Palavra sem pontua√ß√£o final
            r'exemplo:|por exemplo:(?!\s*\w)',  # "exemplo:" sem continua√ß√£o
        ]
        
        for pattern in truncation_patterns:
            if re.search(pattern, formatted_text, re.MULTILINE):
                issues.append(f"‚ö†Ô∏è Poss√≠vel truncamento detectado (padr√£o: {pattern})")
        
        # RESULTADO
        if issues:
            print(f"\n{Fore.RED}‚îÅ‚îÅ‚îÅ PROBLEMAS DETECTADOS ‚îÅ‚îÅ‚îÅ")
            for issue in issues:
                print(f"{Fore.YELLOW}   {issue}")
            print(f"{Fore.RED}‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
            return False, issues
        else:
            print(f"\n{Fore.GREEN}‚úÖ Valida√ß√£o heur√≠stica: APROVADA (nenhuma omiss√£o detectada)")
            return True, []

    def _generate_audit_report(self, video_name, heuristic_issues, llm_issues):
        """Gera relat√≥rio de auditoria da formata√ß√£o"""
        
        report_path = f"audit_{video_name.replace(' ', '_')}.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# Relat√≥rio de Auditoria - {video_name}\n\n")
            f.write(f"**Data:** {time.strftime('%d/%m/%Y %H:%M')}\n\n")
            
            f.write("## Valida√ß√£o Heur√≠stica\n")
            if not heuristic_issues:
                f.write("‚úÖ Nenhuma omiss√£o detectada\n\n")
            else:
                for issue in heuristic_issues:
                    f.write(f"- {issue}\n")
                f.write("\n")
            
            f.write("## Valida√ß√£o LLM (3 janelas)\n")
            if not llm_issues:
                f.write("‚úÖ Nenhuma omiss√£o detectada\n\n")
            else:
                for issue in llm_issues:
                    f.write(f"- {issue}\n")
                f.write("\n")
            
            if not heuristic_issues and not llm_issues:
                f.write("## ‚úÖ CONCLUS√ÉO: Apostila APROVADA (preserva√ß√£o completa)\n")
            else:
                f.write("## ‚ö†Ô∏è CONCLUS√ÉO: Revisar se√ß√µes indicadas acima\n")
        
        print(f"{Fore.CYAN}üìÑ Relat√≥rio de auditoria salvo: {report_path}")
    
    def _segment_raw_transcription(self, raw_text):
        """Segmenta por SPEAKER (ORIGINAL - sem mudan√ßas)"""
        lines = raw_text.split('\n')
        speaker_pattern = re.compile(r'^SPEAKER \d+$')
        
        segments = []
        current_speaker = None
        current_content = []
        
        for line in lines:
            line = line.strip()
            if speaker_pattern.match(line):
                if current_speaker:
                    segments.append({
                        'speaker': current_speaker,
                        'content': "\n".join(current_content)
                    })
                current_speaker = line
                current_content = []
            else:
                if current_speaker:
                    current_content.append(line)
        
        if current_speaker:
            segments.append({
                'speaker': current_speaker,
                'content': "\n".join(current_content)
            })
        
        # Mesmas 3 etapas de merge/filtering do original
        merged_same_speaker = []
        if segments:
            current_seg = segments[0]
            for next_seg in segments[1:]:
                if next_seg['speaker'] == current_seg['speaker']:
                    current_seg['content'] += "\n" + next_seg['content']
                else:
                    merged_same_speaker.append(current_seg)
                    current_seg = next_seg
            merged_same_speaker.append(current_seg)
        
        filtered_segments = []
        if merged_same_speaker:
            current_seg = merged_same_speaker[0]
            for next_seg in merged_same_speaker[1:]:
                if len(next_seg['content']) < 100:
                    current_seg['content'] += "\n" + next_seg['content']
                else:
                    filtered_segments.append(current_seg)
                    current_seg = next_seg
            filtered_segments.append(current_seg)
        
        final_segments = []
        if filtered_segments:
            current_seg = filtered_segments[0]
            for next_seg in filtered_segments[1:]:
                if next_seg['speaker'] == current_seg['speaker']:
                    current_seg['content'] += "\n" + next_seg['content']
                else:
                    final_segments.append(current_seg)
                    current_seg = next_seg
            final_segments.append(current_seg)
        
        return final_segments
    
    # VERS√ïES ASYNC DOS M√âTODOS CR√çTICOS (mantendo o SYSTEM_PROMPT original)
    # Retry para robustez contra falhas de API
    @retry(
        retry=retry_if_exception_type((APIError, Exception)), 
        stop=stop_after_attempt(3), 
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def _format_chunk_async(self, chunk_text, chunk_idx, system_prompt):
        """VERS√ÉO ASYNC de _format_chunk_content_only - PRESERVA FORMATA√á√ÉO"""
        word_count = len(chunk_text.split())
        
        user_content = f"""‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚ö†Ô∏è MODO LITERAL ABSOLUTO - PRESERVA√á√ÉO DE CONTE√öDO ‚ö†Ô∏è
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
TAREFA: Formatar o texto abaixo mantendo ~{word_count} palavras originais.

üìè REQUISITOS:
1. Mantenha TODOS os exemplos, datas, nomes e cr√≠ticas.
2. Mantenha TODAS as dicas de prova ("Aten√ß√£o", "Cai muito").
3. N√ÉO resuma hist√≥rias ou anedotas.
4. Apenas limpe v√≠cios ("n√©", "tipo") e formate leis.

[TEXTO ORIGINAL - CHUNK {chunk_idx + 1}]
{chunk_text}
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
"""
        
        try:
            response = await self.async_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},  # PROMPT COMPLETO ORIGINAL
                    {"role": "user", "content": user_content}
                ],

                presence_penalty=0.0,  # Sem penalidade de repeti√ß√£o
                frequency_penalty=0.0   # Permite repetir termos t√©cnicos
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"{Fore.RED}‚ö†Ô∏è Erro ao formatar chunk {chunk_idx}: {e}")
            return chunk_text
    
    async def _identify_speaker_async(self, content, professors_info, speaker_label):
        """VERS√ÉO ASYNC de _identify_speaker_context com cache"""
        
        # Cache: Se j√° identificamos este speaker, reutiliza
        if speaker_label in self.speaker_cache:
            return self.speaker_cache[speaker_label]
        
        transition = self._detect_discipline_transition(content)
        
        prompt = f"""
        Analise o in√≠cio do texto abaixo e a lista de professores extra√≠da da introdu√ß√£o.
        Identifique quem √© o prov√°vel professor falando e qual a disciplina.
        
        IMPORTANTE:
        1. Se o texto indicar um professor diferente da lista (ex: g√™nero diferente, nome citado, autoapresenta√ß√£o), PREFIRA a informa√ß√£o do texto.
        2. Se o 'Falante (Label)' for "SPEAKER 2" (ou maior) e a lista tiver apenas um professor, ASSUMA QUE √â UM NOVO PROFESSOR.
        3. Se n√£o houver nome claro, mas houver indica√ß√£o de g√™nero (ex: "sou a professora"), use "Professora Desconhecida" ou tente inferir o nome pelo contexto.
        4. Identifique a disciplina pelo conte√∫do T√âCNICO-JUR√çDICO.
           - IGNORE avisos sobre editais, datas de prova, conselhos de estudo ou "conversa fiada".
           - Foque no tema substantivo (ex: se falar de "Poder Constituinte", a disciplina √© "Direito Constitucional").
           - SEJA ESPEC√çFICO (ex: "Direito Constitucional", "Direito Administrativo", "Processo Civil").

        Falante (Label): {speaker_label if speaker_label else "Desconhecido"}
        
        Lista de Professores (Contexto):
        {professors_info}
        
        Texto (In√≠cio do segmento - Amostra Ampliada):
        {content[:5000]}...
        
        Retorne APENAS um JSON no seguinte formato:
        {{
            "nome": "Nome do Professor",
            "disciplina": "Disciplina Espec√≠fica"
        }}
        """
        
        try:
            response = await self.async_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Voc√™ √© um assistente que identifica palestrantes."},
                    {"role": "user", "content": prompt}
                ]
            )
            content_json = response.choices[0].message.content
            
            # --- HEUR√çSTICA DE CORRE√á√ÉO ---
            # Se o LLM insistir no nome do primeiro professor para um SPEAKER diferente, for√ßamos "Professor(a) Convidado(a)"
            try:
                import json
                data = json.loads(content_json)
                detected_name = data.get("nome", "")
                
                if speaker_label and "SPEAKER" in speaker_label and not speaker_label.endswith("1"):
                    # Verifica se o nome detectado √© o mesmo do √∫nico professor conhecido
                    try:
                        prof_ctx = json.loads(professors_info)
                        prof_list = prof_ctx.get("professores", [])
                        if len(prof_list) == 1:
                            first_prof_name = prof_list[0].get("nome", "")
                            if detected_name == first_prof_name:
                                print(f"{Fore.YELLOW}‚ö†Ô∏è Detectado mesmo nome ({detected_name}) para {speaker_label}. Aplicando override.")
                                data["nome"] = "Professor(a) Convidado(a)"
                                content_json = json.dumps(data)
                    except Exception as e:
                        print(f"Erro ao aplicar heur√≠stica de professor: {e}")
            except:
                pass
            # ------------------------------
            
            # Armazena no cache para reutilizar\n            self.speaker_cache[speaker_label] = content_json\n            return content_json
        except Exception as e:
            print(f"Erro ao identificar speaker: {e}")
            return '{"nome": "Professor", "disciplina": "Disciplina"}'
    
    async def _generate_header_async(self, formatted_content, professor_context_json):
        """VERS√ÉO ASYNC de _generate_discipline_header"""
        try:
            import json
            try:
                prof_ctx = json.loads(professor_context_json)
                prof_name = prof_ctx.get("nome", "Professor")
                discipline = prof_ctx.get("disciplina", "Disciplina")
            except:
                prof_name = "Professor"
                discipline = "Disciplina"
            
            prompt = f"""
            Gere APENAS o t√≠tulo Markdown para esta se√ß√£o.
            Professor: {prof_name}
            Disciplina: {discipline}
            
            Conte√∫do:
            {formatted_content[:1000]}...
            
            FORMATO DE SA√çDA:
            # Prof. {prof_name} - {discipline}
            """
            
            response = await self.async_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Voc√™ √© um assistente que gera cabe√ßalhos de estudo."},
                    {"role": "user", "content": prompt}
                ]
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Erro ao gerar header: {e}")
            return f"# {prof_name} - {discipline}\n"
    
    def _extract_professors_context(self, full_text):
        """
        Extrai a lista de professores e disciplinas analisando todo o texto.
        Procura por padr√µes de introdu√ß√£o em todo o arquivo.
        """
        print(f"   üïµÔ∏è  Extraindo contexto de professores (Scan Completo)...")
        
        # 1. Pega o in√≠cio (padr√£o)
        intro_context = full_text[:5000]
        
        # 2. Busca por padr√µes de introdu√ß√£o no restante do texto
        keywords = [
            "meu nome √©", "sou o professor", "sou a professora", 
            "aqui √© o professor", "aqui √© a professora", 
            "comigo", "recebam", "passo a palavra", "agora com",
            "boa tarde", "bom dia", "boa noite", "ol√° pessoal", "ol√° a todos",
            "come√ßar a aula", "iniciar a aula", "mudando de assunto",
            "pr√≥ximo professor", "pr√≥xima professora"
        ]
        
        found_contexts = []
        lower_text = full_text.lower()
        
        for keyword in keywords:
            start_idx = 0
            while True:
                idx = lower_text.find(keyword, start_idx)
                if idx == -1:
                    break
                
                # Se encontrou, pega um contexto ao redor (500 chars antes e depois)
                start_ctx = max(0, idx - 500)
                end_ctx = min(len(full_text), idx + 500)
                found_contexts.append(full_text[start_ctx:end_ctx])
                
                start_idx = idx + len(keyword)
        
        # Combina tudo
        combined_context = intro_context + "\n\n... [TRECHOS RELEVANTES ENCONTRADOS] ...\n\n" + "\n\n".join(found_contexts)
        
        # Limita tamanho total para n√£o estourar contexto
        if len(combined_context) > 50000:
            combined_context = combined_context[:50000]

        system_prompt = """
        Voc√™ √© um assistente especializado em extrair informa√ß√µes de introdu√ß√µes de aulas.
        Analise o texto fornecido (que cont√©m o in√≠cio da aula e trechos onde professores podem ter se apresentado) e extraia a lista de TODOS os professores e suas respectivas disciplinas.
        
        Retorne APENAS um JSON no seguinte formato:
        {
            "professores": [
                {"nome": "Nome", "disciplina": "Disciplina", "ordem": "primeiro/segundo/√∫ltimo"}
            ]
        }
        Se n√£o encontrar informa√ß√µes, retorne {"professores": []}.
        """
        
        try:
            response = self.llm_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": combined_context}
                ],

                presence_penalty=0.0,
                frequency_penalty=0.0
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Erro ao extrair professores: {e}")
            return "{'professores': []}"
    
    def _check_transition(self, text_window):
        """MANT√âM S√çNCRONO - chamado internamente"""
        prompt = """
        Analise o texto abaixo e verifique se h√° uma transi√ß√£o expl√≠cita de disciplina ou professor.
        Exemplos: "Agora vamos falar de Direito Penal", "Passando a palavra para o professor X".
        
        Texto:
        {text}
        
        Retorne APENAS a transi√ß√£o encontrada ou "None".
        """
        try:
            response = self.llm_client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt.format(text=text_window)}]
            )
            content = response.choices[0].message.content.strip()
            return content if content != "None" else None
        except:
            return None
    
    def _detect_discipline_transition(self, chunk_text):
        """MANT√âM S√çNCRONO"""
        windows = [
            chunk_text[:10000],
            chunk_text[len(chunk_text)//2-5000:len(chunk_text)//2+5000],
            chunk_text[-10000:]
        ]
        
        for window in windows:
            if len(window) < 100: continue
            result = self._check_transition(window)
            if result:
                return result
        return None
    
    async def _extract_critical_entities(self, text):
        """Extrai entidades cr√≠ticas que DEVEM constar no output"""
        prompt = """
        Analise o texto jur√≠dico abaixo e liste TODAS as entidades cr√≠ticas que N√ÉO podem ser omitidas.
        
        Categorias obrigat√≥rias:
        1. LEGISLA√á√ÉO: Leis, Artigos, Par√°grafos, Incisos (ex: "Art. 37 da CF", "Lei 8.666")
        2. JURISPRUD√äNCIA: S√∫mulas, Temas de Repercuss√£o Geral, Nomes de Casos (ex: "S√∫mula Vinculante 13", "Tema 105", "Caso Sindispreve")
        3. PESSOAS/INSTITUI√á√ïES: Nomes de autores, pol√≠ticos, √≥rg√£os (ex: "Hely Lopes", "STF", "Fazenda P√∫blica")
        4. CONCEITOS/TERMOS-CHAVE: Termos t√©cnicos espec√≠ficos definidos ou explicados.
        4. CONCEITOS/TERMOS-CHAVE: Termos t√©cnicos espec√≠ficos definidos ou explicados.
        5. EXEMPLOS/HIST√ìRIAS: Resumo de 3-5 palavras de exemplos, casos hipot√©ticos ou hist√≥rias pessoais.
        
        Texto:
        {text}
        
        Retorne APENAS um JSON:
        {{
            "legislacao": ["item1", "item2"],
            "jurisprudencia": ["item1", "item2"],
            "entidades": ["item1", "item2"],
            "exemplos": ["item1", "item2"]
        }}
        """
        try:
            response = await self.async_client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt.format(text=text)}],
                response_format={"type": "json_object"}
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            print(f"{Fore.RED}‚ö†Ô∏è Erro ao extrair entidades: {e}")
            return {}

    def _verify_entities_presence(self, formatted_text, entities):
        """Verifica se as entidades extra√≠das est√£o presentes no texto formatado"""
        missing = []
        formatted_lower = formatted_text.lower()
        
        # Verifica Legisla√ß√£o e Jurisprud√™ncia (Prioridade Alta)
        for category in ["legislacao", "jurisprudencia", "entidades", "exemplos"]:
            if category in entities:
                for item in entities[category]:
                    # Busca flex√≠vel (case insensitive e parcial)
                    item_clean = item.lower().replace("art.", "").replace("lei", "").strip()
                    if len(item_clean) < 4: continue # Pula itens muito curtos
                    
                    # Verifica se o item (ou parte significativa dele) est√° no texto
                    if item.lower() not in formatted_lower:
                        # Tenta busca mais flex√≠vel (palavras-chave)
                        keywords = [w for w in item.lower().split() if len(w) > 3]
                        matches = sum(1 for k in keywords if k in formatted_lower)
                        
                        # Threshold de 60% para flexibilidade em exemplos (narrativas variam mais)
                        threshold = 0.6 if category == "exemplos" else 0.7
                        
                        if matches < len(keywords) * threshold:
                            missing.append(f"[{category.upper()}] {item}")
                            
        return missing

    async def _repair_omissions(self, formatted_text, missing_items, original_text):
        """Repara cirurgicamente o texto inserindo itens omitidos"""
        print(f"{Fore.MAGENTA}   üîß Iniciando REPARO CIR√öRGICO para {len(missing_items)} itens...")
        
        prompt = f"""
        CRITICAL RESTORATION TASK.
        The following items were OMITTED from the formatted text but are MANDATORY:
        {json.dumps(missing_items, ensure_ascii=False)}
        
        Source Context (where these items appear in original text):
        {original_text}
        
        Current Formatted Text:
        {formatted_text}
        
        INSTRUCTION:
        Rewrite the Formatted Text to include ALL the missing items naturally.
        - Maintain the current structure and flow.
        - Just weave the missing details back in where they belong.
        - DO NOT summarize. DO NOT remove anything else.
        - RETURN ONLY THE REPAIRED TEXT.
        """
        
        try:
            response = await self.async_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert legal text editor. Your goal is to restore missing information without altering the rest of the text."},
                    {"role": "user", "content": prompt}
                ]
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"{Fore.RED}   ‚ùå Erro no reparo: {e}")
            return formatted_text

    async def _format_with_validation(self, chunk_text, idx, system_prompt, max_retries=0):
        """Formata√ß√£o R√ÅPIDA sem valida√ß√£o (1 chamada LLM por chunk)"""
        try:
            response = await self.async_client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"FORMAT THIS TEXT:\n\n{chunk_text}"}
                ]
            )
            formatted = response.choices[0].message.content
            print(f"   ‚úÖ Chunk {idx+1} processado")
            return formatted
        except Exception as e:
            print(f"{Fore.RED}   ‚ùå Erro: {e}")
            return chunk_text

    
    def _merge_chunks_with_deduplication(self, formatted_chunks):
        """
        Remove duplica√ß√µes causadas por overlap entre chunks formatados.
        Usa difflib para encontrar sobreposi√ß√£o exata de subsequ√™ncias.
        
        Args:
            formatted_chunks (list): Lista de chunks j√° formatados pelo LLM
            
        Returns:
            str: Texto unificado sem redund√¢ncias
        """
        import difflib
        
        if not formatted_chunks:
            return ""
        
        if len(formatted_chunks) == 1:
            return formatted_chunks[0]
        
        print(f"{Fore.CYAN}   üîó Deduplicando overlap entre {len(formatted_chunks)} chunks...")
        
        merged = formatted_chunks[0]
        duplications_found = 0
        total_chars_removed = 0
        
        for i in range(1, len(formatted_chunks)):
            current_chunk = formatted_chunks[i]
            
            # Pega os √∫ltimos 2000 chars do anterior e os primeiros 2000 do atual
            lookback = 2000
            tail = merged[-lookback:] if len(merged) > lookback else merged
            head = current_chunk[:lookback] if len(current_chunk) > lookback else current_chunk
            
            # Encontra a maior substring comum sequencial (LCS)
            matcher = difflib.SequenceMatcher(None, tail, head)
            match = matcher.find_longest_match(0, len(tail), 0, len(head))
            
            # Se a sobreposi√ß√£o for significativa (> 100 chars), faz o merge cir√∫rgico
            if match.size > 100:
                # AN√ÅLISE DE PREFIXO (Evitar cortes indevidos)
                # Verifica se h√° conte√∫do no chunk atual ANTES do match que foi ignorado
                if match.b > 0:
                    head_prefix = current_chunk[:match.b]
                    tail_prefix = tail[:match.a]
                    
                    # Se o prefixo ignorado for muito diferente do final do anterior,
                    # provavelmente √© conte√∫do novo que foi omitido no anterior (ou alucina√ß√£o).
                    # Na d√∫vida, PRESERVA (melhor duplicar que omitir).
                    similarity = difflib.SequenceMatcher(None, tail_prefix, head_prefix).ratio()
                    
                    if similarity < 0.6: # 60% de similaridade
                        print(f"      ‚ö†Ô∏è Recuperando {len(head_prefix)} chars antes do overlap (Similaridade: {similarity:.2f})")
                        merged += head_prefix
                    else:
                        print(f"      ‚ÑπÔ∏è Ignorando prefixo redundante ({len(head_prefix)} chars, Sim: {similarity:.2f})")

                # Corta apenas o que j√° estava no chunk anterior (parte coincidente)
                unique_part = current_chunk[match.b + match.size:]
                merged += unique_part
                duplications_found += 1
                total_chars_removed += match.size
                print(f"      Chunk {i+1}: Overlap exato de {match.size} chars removido")
            else:
                # Fallback: Concatena com quebra de par√°grafo se n√£o achar overlap claro
                merged += "\n\n" + current_chunk
        
        if duplications_found > 0:
            print(f"{Fore.GREEN}   ‚úÖ {duplications_found} overlaps removidos ({total_chars_removed} chars)")
        else:
            print(f"{Fore.GREEN}   ‚úÖ Nenhum overlap detectado")
        
        return merged

    # NOVO M√âTODO: Processa 1 segmento inteiro em PARALELO
    async def _process_segment_parallel(self, segment, professors_info, idx, system_prompt):
        """
        Processa 1 segmento completo (chunks em paralelo) - PRESERVA FORMATA√á√ÉO MARKDOWN
        """
        speaker = segment['speaker']
        content = segment['content']
        
        print(f"\n{Fore.YELLOW}‚ñ∂ Segmento {idx+1} ({speaker})...")
        
        # Chunks otimizados (8k chars = ~2k tokens) com overlap para evitar perda
        chunks = self._smart_chunk_overlapping(content, max_size=8000, overlap=1500)
        print(f"   {len(chunks)} chunks de ~8k chars (com 1.5k overlap)")
        
        # 1. Identifica contexto (paralelo)
        context_task = self._identify_speaker_async(content[:5000], professors_info, speaker)
        
        # 2. Formata chunks SEQUENCIALMENTE com VALIDA√á√ÉO
        # Cada chunk √© verificado quanto √† preserva√ß√£o de conte√∫do (>80%)
        formatted_parts = []
        import gc
        
        for j, chunk in enumerate(chunks):
            print(f"   Processando chunk {j+1}/{len(chunks)}...")
            part = await self._format_with_validation(chunk, j, system_prompt, max_retries=0)
            formatted_parts.append(part)
            
            # Limpeza for√ßada de mem√≥ria ap√≥s cada chunk
            gc.collect()
        
        # Aguarda contexto (se ainda n√£o terminou)
        prof_context = await context_task
        
        # Pass 1: Formatar Conte√∫do (Chunking com overlap para preservar contexto)
        # üéØ TAMANHO REDUZIDO: 8k chars (~2k tokens) com 1k overlap
        # Isso evita perda de contexto nas bordas e previne compress√£o do output
        chunks = self._smart_chunk_overlapping(content, max_size=8000)
        formatted_segment_parts = []
        

        
        print(f"   {len(chunks)} chunks de ~8k chars (com 1k overlap)")
        
        # 1. Identifica contexto (paralelo)
        context_task = self._identify_speaker_async(content[:5000], professors_info, speaker)
        
        # 2. Formata TODOS os chunks em paralelo (MANT√âM O SYSTEM_PROMPT COMPLETO)
        chunk_tasks = [
            self._format_chunk_async(chunk, j, system_prompt)
            for j, chunk in enumerate(chunks)
        ]
        
        # Aguarda tudo em paralelo
        prof_context, *formatted_parts = await asyncio.gather(context_task, *chunk_tasks)
        
        # 3. Gera header
        # Concatena com deduplica√ß√£o de overlaps
        full_content = self._merge_chunks_with_deduplication(formatted_parts)
        header = await self._generate_header_async(full_content[:10000], prof_context)
        
        return f"{header}\n\n{full_content}\n\n---\n\n", prof_context
    
    def format_transcription(self, transcript_text, video_name, output_folder):
        """
        VERS√ÉO OTIMIZADA: Processa segmentos em PARALELO mantendo formata√ß√£o original
        """
        print(f"{Fore.MAGENTA}üß† Formatando com GPT-5-mini (Async Otimizado)...")
        
        # 1. Extrai contexto da intro (1 chamada s√≠ncrona)
        # USANDO NOVO M√âTODO DE SCAN COMPLETO
        professors_info = self._extract_professors_context(transcript_text)
        print(f"   üìã Contexto: {professors_info}")
        
        # 2. Segmenta por speaker
        segments = []
        if any(re.match(r'^SPEAKER \d+$', line.strip()) for line in transcript_text.split('\n')):
            print(f"   üó£Ô∏è  Usando segmentos de falante...")
            segments = self._segment_raw_transcription(transcript_text)
        else:
            print(f"   ‚ö†Ô∏è  Sem tags de falante. Processando como bloco √∫nico.")
            segments = [{'speaker': 'SPEAKER 1', 'content': transcript_text}]
        
        # 3. Processa segmentos em PARALELO (m√°ximo 5 concorrentes)
        async def process_all():
            semaphore = asyncio.Semaphore(5)  # Limite de rate
            
            async def process_with_limit(seg, i):
                async with semaphore:
                    return await self._process_segment_parallel(seg, professors_info, i, self.SYSTEM_PROMPT_FORMAT)
            
            results = await asyncio.gather(*[
                process_with_limit(seg, i) for i, seg in enumerate(segments)
            ])
            
            return results
        
        # Executa tudo
        results = asyncio.run(process_all())
        
        # 4. Monta resultado final
        formatted_chunks = [result[0] for result in results]
        full_formatted = f"# {video_name}\n\n" + "\n\n".join(formatted_chunks)
        
        # üÜï RENUMERA√á√ÉO AUTOM√ÅTICA (garante sequ√™ncia correta)
        print(f"\n{Fore.CYAN}üî¢ Renumerando t√≥picos sequencialmente...")
        full_formatted = self._renumber_topics(full_formatted)
        
        # VALIDA√á√ÉO EM DUAS CAMADAS
        print(f"\n{Fore.MAGENTA}üîí Iniciando valida√ß√£o de preserva√ß√£o...")
        
        # Camada 1: Heur√≠stica (r√°pida)
        h_passed, h_issues = self._validate_preservation_heuristics(transcript_text, full_formatted)
        
        # Camada 2: LLM (precisa, mas lenta)
        full_formatted, llm_issues = self.validate_completeness_enhanced(transcript_text, full_formatted, video_name)
        
        # Gera relat√≥rio
        self._generate_audit_report(video_name, h_issues, llm_issues)
        
        return full_formatted
    
    def _renumber_topics(self, markdown_text):
        """
        Renumera TODOS os t√≥picos e subt√≥picos sequencialmente,
        garantindo numera√ß√£o correta independente do LLM.
        """
        import re
        
        lines = markdown_text.split('\n')
        output_lines = []
        
        # Contadores para cada n√≠vel
        # counters[0] = ## (n√≠vel 2)
        # counters[1] = ### (n√≠vel 3)
        # counters[2] = #### (n√≠vel 4)
        counters = [0, 0, 0]
        
        # Regex para detectar headings numerados
        # Captura: ## 1.2.3 T√≠tulo ou ## 1. T√≠tulo ou ## T√≠tulo
        heading_pattern = re.compile(r'^(#{2,4})\s*(?:[\d\.]+\s+)?(.+)$')
        
        for line in lines:
            match = heading_pattern.match(line)
            
            if match:
                hashes = match.group(1)  # ##, ###, ou ####
                title = match.group(2).strip()  # Texto do t√≠tulo
                
                level = len(hashes) - 2  # ## = 0, ### = 1, #### = 2
                
                if level > 2:
                    # Ignora n√≠veis muito profundos (##### ou mais)
                    output_lines.append(line)
                    continue
                
                # Incrementa contador do n√≠vel atual
                counters[level] += 1
                
                # Reseta contadores dos n√≠veis inferiores
                for i in range(level + 1, 3):
                    counters[i] = 0
                
                # Constr√≥i n√∫mero sequencial
                if level == 0:  # ##
                    number = f"{counters[0]}"
                elif level == 1:  # ###
                    number = f"{counters[0]}.{counters[1]}"
                elif level == 2:  # ####
                    number = f"{counters[0]}.{counters[1]}.{counters[2]}"
                
                # Reconstr√≥i linha com n√∫mero correto
                new_line = f"{hashes} {number}. {title}"
                output_lines.append(new_line)
                
            else:
                # Linha normal (n√£o √© heading)
                output_lines.append(line)
        
        renumbered_text = '\n'.join(output_lines)
        
        # Conta quantos t√≥picos foram renumerados
        topic_count = counters[0]
        print(f"{Fore.GREEN}   ‚úÖ {topic_count} t√≥picos principais renumerados sequencialmente")
        
        return renumbered_text
    
    def validate_completeness_enhanced(self, raw_transcript, formatted_text, video_name):
        """
        Valida√ß√£o LLM-as-Judge com amostragem estrat√©gica
        """
        print(f"{Fore.YELLOW}üîç Valida√ß√£o LLM (Amostragem M√∫ltipla)...")
        
        # Em vez de apenas os primeiros 50k, valida 3 JANELAS diferentes
        windows = [
            ("IN√çCIO", raw_transcript[:50000], formatted_text[:50000]),
            ("MEIO", raw_transcript[len(raw_transcript)//2-25000:len(raw_transcript)//2+25000],
                      formatted_text[len(formatted_text)//2-25000:len(formatted_text)//2+25000]),
            ("FIM", raw_transcript[-50000:], formatted_text[-50000:])
        ]
        
        validation_prompt = """# TAREFA DE VALIDA√á√ÉO
Voc√™ receber√°:
1. TRANSCRI√á√ÉO BRUTA (amostra)
2. APOSTILA FORMATADA (amostra correspondente)

Identifique conte√∫do OMITIDO na apostila.

## O QUE PROCURAR (PRIORIDADE ALTA):
‚úÖ Exemplos pr√°ticos, casos, hist√≥rias
‚úÖ Cr√≠ticas do professor
‚úÖ Especula√ß√µes, "apostas"
‚úÖ Argumentos estrat√©gicos
‚úÖ Contexto hist√≥rico/pol√≠tico
‚úÖ Dicas de prova
‚úÖ Nuances t√©cnicas

## FORMATO DE RESPOSTA:
Se TUDO preservado:
```
VALIDA√á√ÉO: COMPLETA
```

Se houver omiss√µes:
```
OMISS√ïES DETECTADAS:
1. [Tipo]: [Descri√ß√£o concisa]
2. [Tipo]: [Descri√ß√£o]
```

Seja RIGOROSO mas CONCISO (m√°x 5 itens)."""
        
        all_issues = []
        
        for window_name, raw_window, formatted_window in windows:
            print(f"   Validando janela: {window_name}...")
            
            try:
                response = self.llm_client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": validation_prompt},
                        {"role": "user", "content": f"RAW:\n{raw_window}\n\nFORMATTED:\n{formatted_window}"}
                    ],
                    timeout=120
                )
                
                result = response.choices[0].message.content
                
                if "VALIDA√á√ÉO: COMPLETA" not in result:
                    all_issues.append(f"[{window_name}] {result}")
            
            except Exception as e:
                print(f"{Fore.RED}   ‚ùå Erro na valida√ß√£o {window_name}: {e}")
        
        # RESULTADO CONSOLIDADO
        if not all_issues:
            print(f"{Fore.GREEN}   ‚úÖ Todas as janelas validadas: Nenhuma omiss√£o detectada")
        else:
            print(f"\n{Fore.YELLOW}‚îÅ‚îÅ‚îÅ OMISS√ïES DETECTADAS (LLM Validation) ‚îÅ‚îÅ‚îÅ")
            for issue in all_issues:
                print(f"{Fore.YELLOW}   {issue}")
            print(f"{Fore.YELLOW}‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
            print(f"{Fore.CYAN}   üí° Revise manualmente o arquivo .md")
        
        return formatted_text, all_issues
    
    def save_as_word(self, formatted_text, video_name, output_folder):
        """Salva a apostila formatada em Word (.docx) com formata√ß√£o profissional"""
        from docx import Document
        from docx.shared import Pt, RGBColor, Inches, Cm
        from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
        from docx.oxml.ns import qn
        from docx.oxml import OxmlElement
        
        print(f"{Fore.CYAN}üìÑ Gerando documento Word profissional...")
        
        doc = Document()
        
        # Configura√ß√µes do documento
        section = doc.sections[0]
        section.top_margin = Inches(1)
        section.bottom_margin = Inches(1)
        section.left_margin = Inches(1.25)
        section.right_margin = Inches(1.25)
        
        # T√≠tulo Principal
        title = doc.add_heading(video_name, level=0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        for run in title.runs:
            run.font.size = Pt(20)
            run.font.color.rgb = RGBColor(0, 51, 102)
        
        # Data
        date_para = doc.add_paragraph()
        date_run = date_para.add_run(f"Gerado em {time.strftime('%d/%m/%Y √†s %H:%M')}")
        date_run.italic = True
        date_run.font.size = Pt(10)
        date_run.font.color.rgb = RGBColor(128, 128, 128)
        date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        doc.add_paragraph()  # Espa√ßo
        
        # Processa o texto linha por linha
        lines = formatted_text.split('\n')
        i = 0
        in_table = False
        table_rows = []
        
        while i < len(lines):
            line = lines[i].strip()
            
            # Linha vazia ou c√≥digo markdown - PULA
            if not line or line.startswith('```'):
                i += 1
                continue
            
            # Detecta tabelas Markdown
            if '|' in line and not in_table:
                in_table = True
                table_rows = []
            
            if in_table:
                if '|' in line and not line.startswith('|--'):  # Ignora separador
                    table_rows.append([cell.strip() for cell in line.split('|')[1:-1]])
                
                # Fim da tabela
                if '|' not in line or i == len(lines) - 1:
                    if len(table_rows) > 0:
                        self._add_table_to_doc(doc, table_rows)
                    in_table = False
                    table_rows = []
                    if '|' not in line:
                        continue
                i += 1
                continue
            
            # Se√ß√µes especiais (Summary, Key Takeaways, Action Items)
            if line.startswith('## Summary') or line.startswith('## Key Takeaways') or line.startswith('## Action Items'):
                heading = doc.add_heading(line.replace('## ', ''), level=2)
                for run in heading.runs:
                    run.font.color.rgb = RGBColor(0, 102, 204)
                i += 1
                continue
            
            # Headings
            if line.startswith('#### '):
                doc.add_heading(line.replace('#### ', ''), level=4)
                i += 1
                continue
            elif line.startswith('### '):
                doc.add_heading(line.replace('### ', ''), level=3)
                i += 1
                continue
            elif line.startswith('## '):
                doc.add_heading(line.replace('## ', ''), level=2)
                i += 1
                continue
            elif line.startswith('# ') and line != f"# {video_name}":  # Evita duplicar t√≠tulo
                doc.add_heading(line.replace('# ', ''), level=1)
                i += 1
                continue
            
            # Separador horizontal
            if line.startswith('---'):
                p = doc.add_paragraph()
                p.add_run('_' * 80).font.color.rgb = RGBColor(192, 192, 192)
                i += 1
                continue
            
            # Blockquote
            if line.startswith('>'):
                p = doc.add_paragraph(line.replace('> ', ''), style='Quote')
                p.paragraph_format.left_indent = Inches(0.5)
                p.paragraph_format.line_spacing_rule = WD_LINE_SPACING.ONE_POINT_FIVE
                for run in p.runs:
                    run.font.italic = True
                    run.font.color.rgb = RGBColor(64, 64, 64)
                i += 1
                continue
            
            # Listas com bullets
            if line.startswith('- ') or line.startswith('* '):
                p = doc.add_paragraph(line[2:], style='List Bullet')
                p.paragraph_format.line_spacing_rule = WD_LINE_SPACING.ONE_POINT_FIVE
                p.paragraph_format.space_after = Pt(6)
                self._format_inline_markdown(p, line[2:])
                i += 1
                continue
            
            # Listas numeradas
            if len(line) > 2 and line[0].isdigit() and line[1:3] in ['. ', ') ']:
                text = line.split('. ', 1)[1] if '. ' in line else line.split(') ', 1)[1]
                p = doc.add_paragraph(text, style='List Number')
                p.paragraph_format.line_spacing_rule = WD_LINE_SPACING.ONE_POINT_FIVE
                p.paragraph_format.space_after = Pt(6)
                self._format_inline_markdown(p, text)
                i += 1
                continue
            
            # Texto normal (Par√°grafo de conte√∫do)
            p = doc.add_paragraph()
            
            # Formata√ß√£o solicitada:
            # - Espa√ßamento 1.5
            # - 6pt antes e depois
            # - Recuo na primeira linha (1.25 cm)
            p.paragraph_format.line_spacing_rule = WD_LINE_SPACING.ONE_POINT_FIVE
            p.paragraph_format.space_before = Pt(6)
            p.paragraph_format.space_after = Pt(6)
            p.paragraph_format.first_line_indent = Cm(1.25)
            p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY  # Justificado fica melhor em apostila
            
            self._format_inline_markdown(p, line)
            i += 1
        
        # Salva o documento
        docx_file = os.path.join(output_folder, f"{video_name}_APOSTILA.docx")
        doc.save(docx_file)
        
        return docx_file
    
    def _add_table_to_doc(self, doc, rows):
        """Adiciona tabela formatada ao documento"""
        from docx.shared import RGBColor
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        from docx.oxml.ns import qn
        from docx.oxml import OxmlElement
        
        if len(rows) < 2:
            return
        
        # Determina o n√∫mero m√°ximo de colunas
        max_cols = max(len(row) for row in rows)
        
        table = doc.add_table(rows=len(rows), cols=max_cols)
        table.style = 'Table Grid'
        
        for i, row_data in enumerate(rows):
            for j, cell_text in enumerate(row_data):
                cell = table.rows[i].cells[j]
                cell.text = cell_text
                
                # Formata cabe√ßalho
                if i == 0:
                    for paragraph in cell.paragraphs:
                        for run in paragraph.runs:
                            run.font.bold = True
                            run.font.color.rgb = RGBColor(255, 255, 255)
                        paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
                    
                    # Cor de fundo do cabe√ßalho
                    shading_elm = OxmlElement('w:shd')
                    shading_elm.set(qn('w:fill'), '0066CC')
                    cell._element.get_or_add_tcPr().append(shading_elm)
    
    def _format_inline_markdown(self, paragraph, text):
        """Formata markdown inline (negrito, it√°lico) em um par√°grafo existente usando Regex"""
        paragraph.clear()
        
        # Regex que captura **bold**, __bold__, *italic*, _italic_
        # A ordem importa: verifica bold primeiro
        tokens = re.split(r'(\*\*.*?\*\*|__.*?__|\*.*?\*|_.*?_)', text)
        
        for token in tokens:
            if not token: continue
            
            run = paragraph.add_run()
            if (token.startswith('**') and token.endswith('**')) or \
               (token.startswith('__') and token.endswith('__')):
                run.bold = True
                run.text = token[2:-2] # Remove marcadores
            elif (token.startswith('*') and token.endswith('*')) or \
                 (token.startswith('_') and token.endswith('_')):
                run.italic = True
                run.text = token[1:-1] # Remove marcadores
            else:
                run.text = token


def process_text_file(file_path):
    """Processa um arquivo de texto espec√≠fico gerando vers√µes completa e resumida"""
    if not os.path.exists(file_path):
        print(f"{Fore.RED}‚ùå Arquivo n√£o encontrado: {file_path}")
        return
    
    folder = os.path.dirname(file_path)
    video_name = Path(file_path).stem.replace('_RAW', '').replace('_TRANSCRICAO', '')
    
    print(f"\n{Fore.CYAN}{'='*60}")
    print(f"{Fore.CYAN}üìÑ Processando: {video_name}")
    print(f"{Fore.CYAN}{'='*60}\n")
    
    try:
        formatter = VomoFormatter()
        
        with open(file_path, "r", encoding="utf-8") as f:
            transcription = f.read()
        
        # =============================================================
        # ETAPA 1: VERS√ÉO COMPLETA (com cache)
        # =============================================================
        print(f"\n{Fore.BLUE}{'='*60}")
        print(f"{Fore.BLUE}ETAPA 1: VERS√ÉO COMPLETA")
        print(f"{Fore.BLUE}{'='*60}")
        
        # Verifica cache primeiro
        cache_file = formatter._get_cache_path(file_path, cache_type="FORMATTED")
        cached_data = formatter._load_cache(cache_file)
        
        if cached_data:
            # Usa vers√£o em cache
            print(f"{Fore.GREEN}‚ö° Usando vers√£o em cache (processamento instant√¢neo)")
            formatted_text = cached_data['formatted_text']
            validation_issues = []  # Cache j√° foi validado anteriormente
        else:
            # Processa do zero
            print(f"{Fore.CYAN}üîÑ Processando transcri√ß√£o (pode levar alguns minutos)...")
            formatted_text = formatter.format_transcription(transcription, video_name, folder)
            
            # Valida completude comparando com transcri√ß√£o bruta
            formatted_text, validation_issues = formatter.validate_completeness_enhanced(transcription, formatted_text, video_name)
            
            # Salva cache para pr√≥xima execu√ß√£o
            formatter._save_cache(cache_file, formatted_text, file_path)
        
        # Salva MD Completo
        output_md = os.path.join(folder, f"{video_name}_APOSTILA_COMPLETA.md")
        with open(output_md, 'w', encoding='utf-8') as f:
            f.write(f"# {video_name}\n\n{formatted_text}")
        print(f"{Fore.GREEN}üìù MD Completo salvo: {output_md}")
        
        # Salva Word Completo
        docx_completo = formatter.save_as_word(formatted_text, f"{video_name}_COMPLETA", folder)
        print(f"{Fore.GREEN}üìÑ Word Completo salvo: {docx_completo}")

        
        # =============================================================
        # ETAPA 2: VERS√ÉO RESUMIDA (com cache)
        # =============================================================
        print(f"\n{Fore.BLUE}{'='*60}")
        print(f"{Fore.BLUE}ETAPA 2: VERS√ÉO RESUMIDA")
        print(f"{Fore.BLUE}{'='*60}")
        
        # Verifica cache do resumo
        summary_cache_file = formatter._get_cache_path(file_path, cache_type="SUMMARY")
        cached_summary = formatter._load_cache(summary_cache_file)
        
        if cached_summary:
            # Usa resumo em cache
            print(f"{Fore.GREEN}‚ö° Usando resumo em cache (processamento instant√¢neo)")
            summary_text = cached_summary['formatted_text']
        else:
            # Gera resumo do zero
            print(f"{Fore.CYAN}üîÑ Gerando vers√£o resumida...")
            summary_text = formatter.generate_summary_version(formatted_text)
            
            # Salva cache do resumo
            formatter._save_cache(summary_cache_file, summary_text, file_path)
        
        # Salva MD Resumido
        output_md_summary = os.path.join(folder, f"{video_name}_RESUMO.md")
        with open(output_md_summary, 'w', encoding='utf-8') as f:
            f.write(summary_text)
        print(f"{Fore.GREEN}üìù MD Resumo salvo: {output_md_summary}")
        
        # Salva Word Resumido
        docx_resumo = formatter.save_as_word(summary_text, f"{video_name}_RESUMO", folder)
        print(f"{Fore.GREEN}üìÑ Word Resumo salvo: {docx_resumo}")

        
        # =============================================================
        # SUCESSO FINAL
        # =============================================================
        print(f"\n{Fore.GREEN}{'='*60}")
        print(f"{Fore.GREEN}‚ú® SUCESSO! Apostila Completa + Resumo gerados.")
        print(f"{Fore.GREEN}üìÑ Completa: {docx_completo}")
        print(f"{Fore.GREEN}üìÑ Resumo: {docx_resumo}")
        print(f"{Fore.GREEN}{'='*60}")
        
    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}‚ö†Ô∏è  Interrompido pelo usu√°rio")
        
    except Exception as e:
        print(f"{Fore.RED}‚ùå Erro: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        file_path = sys.argv[1]
        process_text_file(file_path)
    else:
        print(f"{Fore.YELLOW}‚ö†Ô∏è  Uso: python format_only.py <caminho_do_arquivo.txt>")
        print(f"{Fore.CYAN}üí° O script gerar√° automaticamente duas vers√µes:")
        print(f"{Fore.CYAN}   1. COMPLETA: Apostila detalhada com todo o conte√∫do")
        print(f"{Fore.CYAN}   2. RESUMO: Vers√£o esquem√°tica para revis√£o r√°pida")
