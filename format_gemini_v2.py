#!/usr/bin/env python3
"""
Script para processar transcri√ß√µes de aulas usando Gemini via OpenRouter
Formata conforme diretrizes espec√≠ficas para concursos de procuradorias
VERS√ÉO COM CHUNKS MENORES E PROMPT ANTI-RESUMO
"""

import os
import sys
import time
import re
import requests
from pathlib import Path
from docx import Document
from typing import List

# Configura√ß√£o
OPENROUTER_API_KEY = "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"
OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1/chat/completions"
MODEL_NAME = "google/gemini-2.0-flash-001"
MAX_OUTPUT_TOKENS = 65_536
# Chunks menores para garantir que o modelo formate sem resumir
MAX_CHARS_PER_CHUNK = 30_000  # ~7.5k tokens
OUTPUT_DIR = Path('/Users/nicholasjacob/Documents/Aplicativos/Iudex/Aulas_PGM_RJ')

PROMPT_FORMATACAO = """TAREFA: REVISAR E FORMATAR TRANSCRI√á√ÉO - N√ÉO RESUMIR!

Voc√™ √© um revisor de transcri√ß√µes para concursos de procuradorias.

‚ö†Ô∏è REGRA CR√çTICA: N√ÉO RESUMA! O TEXTO DE SA√çDA DEVE TER TAMANHO SIMILAR OU MAIOR QUE O DE ENTRADA!

Sua tarefa √© APENAS:
1. Corrigir erros gramaticais e de pontua√ß√£o
2. Melhorar a formata√ß√£o e legibilidade
3. Organizar em t√≥picos/subt√≥picos
4. Adicionar tabelas de s√≠ntese ao final de cada t√≥pico principal

DIRETRIZES OBRIGAT√ìRIAS:
- Mantenha TODAS as ideias, exemplos, explica√ß√µes, dicas e casos
- Mantenha o modo em primeira pessoa
- Corrija linguagem coloquial para portugu√™s padr√£o formal
- Elimine v√≠cios de oralidade MAS MANTENHA TODO O CONTE√öDO
- Preserve a sequ√™ncia exata das ideias
- Use par√°grafos bem definidos (evite par√°grafos muito longos)
- Enumere t√≥picos e subt√≥picos (1, 1.1, 2, 2.1...)
- Use negrito para conceitos importantes
- USE TEXTO CORRIDO - mantenha todo o conte√∫do explicativo

‚ö†Ô∏è PROIBIDO:
- N√ÉO resuma
- N√ÉO omita informa√ß√µes
- N√ÉO pule exemplos ou casos pr√°ticos
- N√ÉO remova dicas de prova
- N√ÉO corte explica√ß√µes

Ao final de CADA t√≥pico principal, adicione uma tabela:

| Conceito | Defini√ß√£o | Fundamento Legal | Observa√ß√µes |
|----------|-----------|------------------|-------------|
| [Termo] | [Explica√ß√£o] | [Art./Lei] | [Dicas] |

{contexto_numeracao}

<transcri√ß√£o>
{texto}
</transcri√ß√£o>"""


def extrair_texto(caminho_arquivo: str) -> str:
    """Extrai texto de arquivo txt ou docx"""
    path = Path(caminho_arquivo)
    if path.suffix.lower() == '.docx':
        doc = Document(caminho_arquivo)
        return '\n\n'.join([p.text for p in doc.paragraphs if p.text.strip()])
    else:
        with open(caminho_arquivo, 'r', encoding='utf-8') as f:
            return f.read()


def dividir_texto_em_chunks(texto: str, max_chars: int = MAX_CHARS_PER_CHUNK) -> List[str]:
    """Divide o texto em chunks em pontos naturais (par√°grafos/quebras duplas)."""
    paragrafos = texto.split('\n\n')
    chunks = []
    chunk_atual = []
    tamanho_atual = 0
    
    for paragrafo in paragrafos:
        tamanho_paragrafo = len(paragrafo)
        
        if tamanho_atual + tamanho_paragrafo > max_chars and chunk_atual:
            chunks.append('\n\n'.join(chunk_atual))
            chunk_atual = [paragrafo]
            tamanho_atual = tamanho_paragrafo
        else:
            chunk_atual.append(paragrafo)
            tamanho_atual += tamanho_paragrafo
    
    if chunk_atual:
        chunks.append('\n\n'.join(chunk_atual))
    
    return chunks


def processar_com_openrouter(texto: str, parte_num: int, total_partes: int, ultimo_topico: int) -> str:
    """Processa um chunk de texto com Gemini via OpenRouter"""
    
    if parte_num == 1:
        contexto_numeracao = "Esta √© a PRIMEIRA parte. Inicie a numera√ß√£o em 1."
    else:
        contexto_numeracao = f"PARTE {parte_num}/{total_partes}. Continue a numera√ß√£o a partir do t√≥pico {ultimo_topico + 1}."
    
    prompt_completo = PROMPT_FORMATACAO.format(
        texto=texto,
        contexto_numeracao=contexto_numeracao
    )
    
    print(f"\n{'='*60}")
    print(f"üìù Processando parte {parte_num}/{total_partes}")
    print(f"   Entrada: {len(texto):,} chars")
    print(f"{'='*60}")
    
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://iudex.app",
        "X-Title": "Iudex Formatter"
    }
    
    payload = {
        "model": MODEL_NAME,
        "messages": [{"role": "user", "content": prompt_completo}],
        "temperature": 0.1,
        "max_tokens": MAX_OUTPUT_TOKENS,
        "top_p": 0.95
    }
    
    response = requests.post(
        OPENROUTER_BASE_URL,
        headers=headers,
        json=payload,
        timeout=600
    )
    response.raise_for_status()
    
    result = response.json()['choices'][0]['message']['content']
    print(f"   Sa√≠da: {len(result):,} chars")
    
    return result


def extrair_ultimo_topico(texto: str) -> int:
    """Extrai o n√∫mero do √∫ltimo t√≥pico principal"""
    matches = re.findall(r'(?:^|\n)(?:#+ |\*\*)?(\d+)\.', texto)
    return max(int(m) for m in matches) if matches else 0


def salvar_markdown(texto: str, caminho: Path):
    """Salva como Markdown"""
    with open(caminho, 'w', encoding='utf-8') as f:
        f.write(texto)


def salvar_docx(texto: str, caminho: Path):
    """Salva como DOCX simples"""
    doc = Document()
    for linha in texto.split('\n'):
        if linha.strip():
            doc.add_paragraph(linha)
    doc.save(str(caminho))


def main():
    if len(sys.argv) > 1:
        arquivo_entrada = Path(sys.argv[1])
    else:
        arquivo_entrada = Path('/Users/nicholasjacob/Documents/Aplicativos/Iudex/Aulas_PGM_RJ/Administrativo.txt')
    
    if not arquivo_entrada.exists():
        print(f"‚ùå Arquivo n√£o encontrado: {arquivo_entrada}")
        return
    
    print(f"üìÑ Lendo: {arquivo_entrada}")
    texto_original = extrair_texto(str(arquivo_entrada))
    print(f"‚úÖ {len(texto_original):,} caracteres")
    
    chunks = dividir_texto_em_chunks(texto_original)
    print(f"üìä Dividido em {len(chunks)} partes")
    
    resultados = []
    ultimo_topico = 0
    
    for i, chunk in enumerate(chunks, 1):
        print(f"\nüîÑ Parte {i}/{len(chunks)}...")
        
        try:
            resultado = processar_com_openrouter(chunk, i, len(chunks), ultimo_topico)
            resultados.append(resultado)
            ultimo_topico = extrair_ultimo_topico(resultado)
            print(f"‚úÖ Conclu√≠do (√∫ltimo t√≥pico: {ultimo_topico})")
            
            if i < len(chunks):
                time.sleep(1)
                
        except Exception as e:
            print(f"‚ùå Erro: {e}")
            continue
    
    texto_final = '\n\n'.join(resultados)
    nome_base = arquivo_entrada.stem
    
    print(f"\nüíæ Salvando...")
    
    arquivo_md = OUTPUT_DIR / f'{nome_base}_GEMINI_V2.md'
    salvar_markdown(texto_final, arquivo_md)
    print(f"‚úÖ Markdown: {arquivo_md}")
    
    arquivo_docx = OUTPUT_DIR / f'{nome_base}_GEMINI_V2.docx'
    salvar_docx(texto_final, arquivo_docx)
    print(f"‚úÖ DOCX: {arquivo_docx}")
    
    print(f"\n{'='*60}")
    print(f"üéâ CONCLU√çDO!")
    print(f"   Entrada: {len(texto_original):,} chars")
    print(f"   Sa√≠da: {len(texto_final):,} chars")
    print(f"   Raz√£o: {len(texto_final)/len(texto_original)*100:.1f}%")
    print(f"{'='*60}")


if __name__ == '__main__':
    main()
