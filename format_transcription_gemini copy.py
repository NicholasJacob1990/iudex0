#!/usr/bin/env python3
"""
Script v2.10 - FormataÃ§Ã£o de TranscriÃ§Ãµes com Gemini 2.5 Flash
MELHORIAS: Smart Stitching Anti-DuplicaÃ§Ã£o CirÃºrgica

MudanÃ§as v2.10 vs v2.9:
- remover_eco_do_contexto: Remove eco do contexto na resposta da API
- titulos_sao_similares: Fuzzy matching para detecÃ§Ã£o de tÃ­tulos duplicados
- limpar_inicio_redundante: Limpeza na junÃ§Ã£o de chunks
- InjeÃ§Ã£o dinÃ¢mica de ultimo_titulo no prompt para prevenir repetiÃ§Ã£o

Uso: python format_transcription_gemini.py <entrada.txt> [saida]
"""


import os
import sys
import time
import random
import json
import re
import threading
from pathlib import Path
from time import sleep
from difflib import SequenceMatcher
import hashlib

try:
    from audit_module import auditar_consistencia_legal
    AUDIT_AVAILABLE = True
except ImportError:
    AUDIT_AVAILABLE = False
    logger = logging.getLogger(__name__) if 'logging' in locals() else None
    if logger: logger.warning("âš ï¸  MÃ³dulo de auditoria nÃ£o encontrado.")
    else: print("âš ï¸  MÃ³dulo de auditoria nÃ£o encontrado.")

try:
    from google import genai
    from google.genai import types
except ImportError:
    print("âŒ Erro: Biblioteca google-genai nÃ£o instalada.")
    print("   Instale com: pip install google-genai")
    sys.exit(1)

try:
    from tqdm import tqdm
except ImportError:
    print("âš ï¸ Aviso: tqdm nÃ£o instalado. Progress bar desabilitada.")
    tqdm = None

try:
    from docx import Document
    from docx.shared import Pt, RGBColor, Inches, Cm
    from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
    from docx.enum.table import WD_TABLE_ALIGNMENT
    from docx.oxml.ns import qn
    from docx.oxml import OxmlElement
    DOCX_AVAILABLE = True
except ImportError:
    print("âš ï¸ Aviso: python-docx nÃ£o disponÃ­vel. SaÃ­da em Word desabilitada.")
    DOCX_AVAILABLE = False

import logging

# =============================================================================
# CONFIGURAÃ‡Ã•ES v2.7
# =============================================================================

CHARS_POR_PARTE = 20000
CONTEXTO_ESTILO = 3000
OUTPUT_TOKEN_LIMIT = 32000
CACHE_TTL = '7200s'
MIN_CHARS_PARA_CACHE = 20000
MAX_RETRIES = 3
MAX_RPM = 10 
# v2.7: FORÃ‡AR delimitadores visÃ­veis para evitar confusÃ£o
USE_FANCY_DELIMITERS = True

# PreÃ§os API Gemini 2.5 Flash (Dezembro 2025)
PRECO_INPUT_SEM_CACHE = 0.30
PRECO_INPUT_COM_CACHE = 0.03
PRECO_OUTPUT = 2.50

# =============================================================================
# LOGGING
# =============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S',
    handlers=[
        logging.FileHandler('formatacao.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# =============================================================================
# PROMPTS v2.7 - INSTRUÃ‡Ã•ES ANTI-DUPLICAÃ‡ÃƒO REFORÃ‡ADAS
# =============================================================================

PROMPT_FIDELIDADE = """# DIRETRIZES DE FORMATAÃ‡ÃƒO E REVISÃƒO

## PAPEL
VOCÃŠ Ã‰ UM EXCELENTISSIMO REDATOR JURÃDICO E DIDÃTICO

 **Tom:** didÃ¡tico, como o professor explicando em aula.  
- **Pessoa:** manter a pessoa original da transcriÃ§Ã£o (1Âª pessoa se for assim na fala).  
- **Estilo:** texto corrido, com parÃ¡grafos curtos, sem â€œinventarâ€ doutrina nova.  
- **Objetivo:** reproduzir a aula em forma escrita, clara e organizada, mas ainda com a â€œvozâ€ do professor.


## OBJETIVO
Transformar a transcriÃ§Ã£o em um texto claro, legÃ­vel e coeso, em PortuguÃªs PadrÃ£o, MANTENDO A FIDELIDADE TOTAL ao conteÃºdo original.



## ğŸš« O QUE NÃƒO FAZER
1. **NÃƒO RESUMA**. O tamanho do texto de saÃ­da deve ser prÃ³ximo ao de entrada.
2. **NÃƒO OMITA** informaÃ§Ãµes, exemplos, casos concretos ou explicaÃ§Ãµes.
3. **NÃƒO ALTERE** o significado ou a sequÃªncia das ideias e das falas do professor.
4. **NÃƒO CRIE MUITOS BULLET POINTS** ou frases curtas demasiadamente. PREFIRA UM FORMATO DE MANUAL DIDÃTICO, nÃ£o checklist.
5. **NÃƒO USE NEGRITOS EM EXCESSO**. Use apenas para conceitos-chave realmente importantes.
6. **NÃƒO RESUMA e NÃƒO OMITA**. VocÃª pode reescrever frases em portuguÃªs padrÃ£o para melhorar a fluidez, preservar a ordem, os detalhes tÃ©cnicos e os exemplos, mas **REMOVA** pausas excessivas e hesitaÃ§Ãµes.


## âŒ PRESERVE OBRIGATORIAMENTE
- **NÃšMEROS EXATOS**: Artigos, Leis, SÃºmulas, Julgados (REDI/Informativos). **NUNCA OMITA NÃšMEROS DE LEIS OU SÃšMULAS**.
- **TODO o conteÃºdo tÃ©cnico**: exemplos, explicaÃ§Ãµes, analogias, raciocÃ­nios
- **ReferÃªncias**: leis, artigos, jurisprudÃªncia, autores, casos citados
- **ÃŠnfases intencionais**: "isso Ã© MUITO importante" (mantenha o destaque)
- **ObservaÃ§Ãµes pedagÃ³gicas**: "cuidado com isso!", "ponto polÃªmico"

## âœ… DIRETRIZES DE ESTILO
1. **CorreÃ§Ã£o Gramatical**: Corrija erros gramaticais, regÃªncias, ortogrÃ¡ficos e de pontuaÃ§Ã£o, tornando o texto gramaticalmente correto e claro.
2. **Limpeza Profunda:**
   - **REMOVA** marcadores de oralidade: "nÃ©", "tÃ¡?", "entende?", "veja bem", "tipo assim".
   - **REMOVA** interaÃ§Ãµes diretas com a turma/alunos e logÃ­stica: "Isso mesmo", "A colega perguntou", "JÃ¡ estÃ£o me vendo?", "EstÃ£o ouvindo?", "Como ele disse ali atrÃ¡s".
   - **REMOVA** redundÃ¢ncias: "subir para cima", "criaÃ§Ã£o nova".
   - **TRANSFORME** perguntas retÃ³ricas em afirmaÃ§Ãµes quando possÃ­vel (ex: "E o que isso significa?" -> "Isso significa que...").
3. **CoesÃ£o**: Utilize conectivos necessÃ¡rios para tornar o texto mais fluido. Aplique a pontuaÃ§Ã£o devida para deixar o texto coeso e coerente.
4. **Legibilidade**:
   - **USE TEXTO CORRIDO NA MEDIDA DO POSSÃVEL**
   - Utilize formataÃ§Ã£o e estrutura com parÃ¡grafos bem definidos, facilitando a leitura e compreensÃ£o
   - Evite parÃ¡grafos longos (mÃ¡ximo 3-4 linhas visuais)
   - Evite blocos de texto maciÃ§os, quebre os blocos de texto em parÃ¡grafos menores
   - Seja didÃ¡tico sem perder detalhes e conteÃºdo
5. **Linguagem**: Ajuste a linguagem coloquial para um portuguÃªs padrÃ£o, mantendo o significado original.
6. **CitaÃ§Ãµes**: Use itÃ¡lico para citaÃ§Ãµes curtas e recuo em itÃ¡lico para citaÃ§Ãµes longas.
7. -Use **negrito** para destacar conceitos-chave (sem exagero).
8. **FormataÃ§Ã£o DidÃ¡tica** (use com moderaÃ§Ã£o, sem excesso):
   - **Bullet points** para enumerar elementos, requisitos ou caracterÃ­sticas
   - **Listas numeradas** (1., 2., 3.) para enumerar itens, etapas, correntes ou exemplos
   - **Marcadores relacionais** como "â†’" para indicar relaÃ§Ãµes, transiÃ§Ãµes, ou consequÃªncias lÃ³gicas
   - Exemplo: "Processo entre A e B â†’ prova usada contra C"
9. **QuestÃµes e ExercÃ­cios**:
   - Se o professor ditar uma questÃ£o, exercÃ­cio ou caso hipotÃ©tico para julgar, **ILHE-O** em um bloco de citaÃ§Ã£o:
   > **QuestÃ£o:** O prazo para agravo de petiÃ§Ã£o Ã© de...
   - Separe claramente o enunciado da questÃ£o da explicaÃ§Ã£o/gabarito subsequente.

## ğŸ“ ESTRUTURA
- Mantenha a sequÃªncia exata das falas.
- Use TÃ­tulos Markdown (##, ###) para organizar os tÃ³picos, se identificÃ¡veis a partir do contexto.

## ğŸš« TÃTULOS E SUBTÃ“PICOS (IMPORTANTE)
- **NÃƒO crÃ­e subtÃ³picos para frases soltas.**
- Use tÃ­tulos (##, ###) **APENAS** para mudanÃ§as reais de assunto.
- Se uma frase parece um tÃ­tulo mas nÃ£o inicia uma nova seÃ§Ã£o, mantenha como texto normal e use **negrito** se necessÃ¡rio.

## ğŸ“Š TABELA DE SÃNTESE (FLEXÃVEL)
Ao final de cada **bloco temÃ¡tico relevante** (ou capÃ­tulo), produza uma tabela de sÃ­ntese (modelo flexÃ­vel).
Exemplo de estrutura (adapte conforme o conteÃºdo):

```
### ğŸ“‹ Tabela de sÃ­ntese do tÃ³pico

| Conceito/Instituto | DefiniÃ§Ã£o (conforme a aula) | Fundamento Legal (se citado) | ObservaÃ§Ãµes (alertas/exceÃ§Ãµes/juris) |
| :--- | :--- | :--- | :--- |
| ...  | ...  | Art. X, Lei Y / "â€”" | ... |
```

***REGRAS CRÃTICAS PARA TABELAS:**
1. **Limite de conteÃºdo por cÃ©lula:** mÃ¡ximo ~50 palavras. Se precisar de mais, divida em mÃºltiplas linhas da tabela
2. **PROIBIDO usar blocos de cÃ³digo (```) dentro de cÃ©lulas** - use texto simples
3. **NUNCA deixe o tÃ­tulo "ğŸ“‹ Resumo do TÃ³pico" sozinho** - se nÃ£o houver dados para tabela, NÃƒO escreva o tÃ­tulo
4. **POSICIONAMENTO ESTRITO:**
   - A tabela deve vir **APENAS AO FINAL** de um bloco concluÃ­do.
   - **PROIBIDO** inserir tabela no meio de uma frase ou interrompendo uma explicaÃ§Ã£o.
   - Se o texto continuar sobre o mesmo assunto, **termine o texto primeiro** e coloque a tabela depois.


## âš ï¸ REGRA ANTI-DUPLICAÃ‡ÃƒO (CRÃTICA)
Se vocÃª receber um CONTEXTO de referÃªncia (entre delimitadores â”â”â”):
- Este contexto Ã© APENAS para vocÃª manter o mesmo ESTILO DE ESCRITA
- **NUNCA formate novamente esse contexto**
- **NUNCA inclua esse contexto na sua resposta**
- **NUNCA repita informaÃ§Ãµes que jÃ¡ estÃ£o no contexto**
- Formate APENAS o texto que estÃ¡ entre as tags <texto_para_formatar>
- Se o texto_para_formatar comeÃ§ar com algo similar ao fim do contexto, NÃƒO duplique, apenas continue naturalmente
"""

PROMPT_APOSTILA = """# DIRETRIZES DE REDAÃ‡ÃƒO: MANUAL JURÃDICO DIDÃTICO (MODO APOSTILA)
## PAPEL
VOCÃŠ Ã‰ UM EXCELENTISSIMO REDATOR JURÃDICO E DIDÃTICO
- **Tom:** doutrinÃ¡rio, impessoal, estilo manual de Direito.  
- **Pessoa:** 3Âª pessoa ou construÃ§Ãµes impessoais (â€œobserva-seâ€, â€œentende-seâ€).  
- **Estilo:** prosa mais densa, porÃ©m com parÃ¡grafos curtos e didÃ¡ticos.  
- **Objetivo:** transformar o conteÃºdo da aula em texto de apostila/livro, sem alterar o conteÃºdo e sem inventar informaÃ§Ãµes.


## OBJETIVO
Transformar a transcriÃ§Ã£o em um texto claro, legÃ­vel e coeso, em PortuguÃªs PadrÃ£o, em formato de apostila/manual didÃ¡tico

## ğŸš« O QUE NÃƒO FAZER
1. **NÃƒO RESUMA**. O tamanho do texto de saÃ­da deve ser prÃ³ximo ao de entrada.
2. **NÃƒO OMITA** informaÃ§Ãµes, exemplos, casos concretos ou explicaÃ§Ãµes.
3. **NÃƒO ALTERE** o significado ou a sequÃªncia das ideias 


âŒ PRESERVE obrigatoriamente:
- **NÃšMEROS EXATOS**: Artigos, Leis, Artigos SÃºmulas, Julgados (REDI/Informativos). **NUNCA OMITA NÃšMEROS DE LEIS OU SÃšMULAS**.
- **TODO o conteÃºdo tÃ©cnico**: exemplos, explicaÃ§Ãµes, analogias, raciocÃ­nios
- **ReferÃªncias**: leis, artigos, jurisprudÃªncia, autores, casos citados
- **ÃŠnfases intencionais**: "isso Ã© MUITO importante" (mantenha o destaque)
- **ObservaÃ§Ãµes pedagÃ³gicas**: "cuidado com isso!", "ponto polÃªmico"

## âœ… DIRETRIZES DE ESTILO
1. **CorreÃ§Ã£o Gramatical**: Ajuste a linguagem coloquial para o padrÃ£o culto.
2. **Limpeza**: Remova gÃ­rias, cacoetes ("nÃ©", "tipo assim", "entÃ£o") e vÃ­cios de oralidade.
3. **CoesÃ£o**: Use conectivos e pontuaÃ§Ã£o adequada para tornar o texto fluido.
4. **Legibilidade**:
   - Use parÃ¡grafos bem definidos e curtos (mÃ¡ximo 3-4 linhas visuais).
    - Evite blocos de texto maciÃ§os, quebre os blocos de texto em parÃ¡grafos menores
   - Use **negrito** para destacar conceitos-chave (sem exagero).
5. **FormataÃ§Ã£o DidÃ¡tica** (use com moderaÃ§Ã£o, sem excesso):
   - **Bullet points** para enumerar elementos, requisitos ou caracterÃ­sticas
   - **Listas numeradas** (1., 2., 3.) para enumerar itens, etapas, correntes doutrinÃ¡rias ou exemplos
   - **Marcadores relacionais** como "â†’" para indicar relaÃ§Ãµes, transiÃ§Ãµes, ou consequÃªncias lÃ³gicas
   - Exemplo: "Processo entre Pedro e JosÃ© â†’ prova usada contra Ana"

## ğŸ“ ESTRUTURA
- Mantenha a sequÃªncia exata das falas.
- Use TÃ­tulos Markdown (##, ###) para organizar os tÃ³picos, se identificÃ¡veis a partir do contexto.

## ğŸ“Š TABELA DE SÃNTESE (FLEXÃVEL)
Ao final de cada **bloco temÃ¡tico relevante** (ou capÃ­tulo), produza uma tabela de sÃ­ntese (modelo flexÃ­vel).
Exemplo de estrutura (adapte conforme o conteÃºdo):

```
### ğŸ“‹ Tabela de sÃ­ntese do tÃ³pico

| Conceito/Instituto | DefiniÃ§Ã£o (conforme a aula) | Fundamento Legal (se citado) | ObservaÃ§Ãµes (alertas/exceÃ§Ãµes/juris) |
| :--- | :--- | :--- | :--- |
| ...  | ...  | Art. X, Lei Y / "â€”" | ... |
```

***REGRAS CRÃTICAS PARA TABELAS:**
1. **Limite de conteÃºdo por cÃ©lula:** mÃ¡ximo ~50 palavras. Se precisar de mais, divida em mÃºltiplas linhas da tabela
2. **PROIBIDO usar blocos de cÃ³digo (```) dentro de cÃ©lulas** - use texto simples
3. **NUNCA deixe o tÃ­tulo "ğŸ“‹ Resumo do TÃ³pico" sozinho** - se nÃ£o houver dados para tabela, NÃƒO escreva o tÃ­tulo
4. **POSICIONAMENTO:** A tabela deve vir **APENAS AO FINAL** da explicaÃ§Ã£o completa dos tÃ³picos ou blocos temÃ¡ticos relevantes 
   - **NUNCA** insira a tabela no meio de uma explicaÃ§Ã£o.
   - **NUNCA** resuma um tÃ³pico que vocÃª ainda nÃ£o acabou de explicar no texto.
   - A tabela deve ser o **fechamento** lÃ³gico da seÃ§Ã£o, antes de iniciar um novo tÃ­tulo ou tÃ³pico (## ou ###).

## âš ï¸ REGRA ANTI-DUPLICAÃ‡ÃƒO (CRÃTICA)
Se vocÃª receber um CONTEXTO de referÃªncia (entre delimitadores â”â”â”):
- Este contexto Ã© APENAS para vocÃª manter o mesmo ESTILO DE ESCRITA
- **NUNCA formate novamente esse contexto**
- **NUNCA inclua esse contexto na sua resposta**
- **NUNCA repita informaÃ§Ãµes que jÃ¡ estÃ£o no contexto**
- Formate APENAS o texto que estÃ¡ entre as tags <texto_para_formatar>
- Se o texto_para_formatar comeÃ§ar com algo similar ao fim do contexto, NÃƒO duplique, apenas continue naturalmente
"""


# =============================================================================
# ESCOLHA O MODO
# =============================================================================

PROMPT_FORMATACAO = PROMPT_FIDELIDADE
# PROMPT_FORMATACAO = PROMPT_APOSTILA

# =============================================================================
# DETECÃ‡ÃƒO DE MODO
# =============================================================================

FIDELIDADE_MODE = "NÃƒO RESUMA" in PROMPT_FORMATACAO
APOSTILA_MODE = "MANUAL JURÃDICO" in PROMPT_FORMATACAO

if APOSTILA_MODE:
    THRESHOLD_MINIMO = 0.75
    THRESHOLD_CRITICO = 0.65
    MODO_NOME = "APOSTILA"
elif FIDELIDADE_MODE:
    THRESHOLD_MINIMO = 0.75
    THRESHOLD_CRITICO = 0.70
    MODO_NOME = "FIDELIDADE"
else:
    THRESHOLD_MINIMO = 0.70
    THRESHOLD_CRITICO = 0.60
    MODO_NOME = "PADRÃƒO"

logger.info(f"ğŸ¯ Modo: {MODO_NOME} (threshold={THRESHOLD_MINIMO:.0%})")
logger.info(f"ğŸ›¡ï¸  Anti-duplicaÃ§Ã£o: ATIVADA (v2.7)")

# =============================================================================
# RATE LIMITER
# =============================================================================

class RateLimiter:
    def __init__(self, max_requests_per_minute=MAX_RPM):
        self.max_rpm = max_requests_per_minute
        self.requests = []
        self.lock = threading.Lock()
    
    def wait_if_needed(self):
        with self.lock:
            now = time.time()
            self.requests = [t for t in self.requests if now - t < 60]
            
            if len(self.requests) >= self.max_rpm:
                oldest = min(self.requests)
                wait_time = 60 - (now - oldest) + 0.5
                logger.info(f"â±ï¸  Rate limit: aguardando {wait_time:.1f}s...")
                sleep(wait_time)
                self.requests = [t for t in self.requests if time.time() - t < 60]
            
            self.requests.append(time.time())

rate_limiter = RateLimiter()

# =============================================================================
# CHECKPOINT/RESUME
# =============================================================================

def get_checkpoint_path(input_file):
    return Path(input_file).with_suffix('.checkpoint.json')

def save_checkpoint(input_file, resultados, chunks_info, secao_atual):
    checkpoint_path = get_checkpoint_path(input_file)
    checkpoint_data = {
        'input_file': str(input_file),
        'secao_atual': secao_atual,
        'total_secoes': len(chunks_info),
        'chunks_info': chunks_info,
        'resultados': resultados,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'version': '2.7',
        'modo': MODO_NOME
    }
    with open(checkpoint_path, 'w', encoding='utf-8') as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)

def load_checkpoint(input_file):
    checkpoint_path = get_checkpoint_path(input_file)
    if checkpoint_path.exists():
        try:
            with open(checkpoint_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if data.get('version') not in ('2.6', '2.7'):
                    logger.warning("Checkpoint de versÃ£o anterior. Reiniciando...")
                    return None
                return data
        except Exception as e:
            logger.error(f"Erro ao carregar checkpoint: {e}")
    return None

def delete_checkpoint(input_file):
    checkpoint_path = get_checkpoint_path(input_file)
    if checkpoint_path.exists():
        checkpoint_path.unlink()
        logger.info("ğŸ§¹ Checkpoint removido")

# =============================================================================
# DIVISÃƒO SEQUENCIAL
# =============================================================================

def dividir_sequencial(transcricao_completa):
    """Divide documento em chunks SEQUENCIAIS sem sobreposiÃ§Ã£o"""
    chunks = []
    tamanho_total = len(transcricao_completa)
    inicio = 0
    
    while inicio < tamanho_total:
        fim = min(inicio + CHARS_POR_PARTE, tamanho_total)
        
        if fim < tamanho_total:
            janela = transcricao_completa[max(0, fim - 500):min(tamanho_total, fim + 500)]
            titulo_match = re.search(r'\n(#{2,4}\s+.+)\n', janela)
            
            if titulo_match:
                pos_titulo = janela.find(titulo_match.group(0))
                fim = max(0, fim - 500) + pos_titulo
            else:
                quebra = transcricao_completa.rfind('\n\n', fim - 300, fim + 300)
                if quebra != -1 and quebra > inicio:
                    fim = quebra
                else:
                    quebra = transcricao_completa.rfind('. ', fim - 150, fim + 150)
                    if quebra != -1 and quebra > inicio:
                        fim = quebra + 1
        
        chunks.append({'inicio': inicio, 'fim': fim})
        inicio = fim
    
    return chunks

def validar_chunks(chunks, transcricao_completa):
    """v2.7: ValidaÃ§Ã£o rigorosa de chunks sequenciais"""
    logger.info("ğŸ” Validando chunks sequenciais...")
    
    for i in range(len(chunks)):
        chunk = chunks[i]
        
        # Verifica se inÃ­cio == fim do anterior
        if i > 0:
            anterior = chunks[i-1]
            if chunk['inicio'] != anterior['fim']:
                logger.error(f"âŒ Gap/Overlap no chunk {i+1}!")
                logger.error(f"   Anterior termina em: {anterior['fim']}")
                logger.error(f"   Atual comeÃ§a em: {chunk['inicio']}")
                logger.error(f"   DiferenÃ§a: {chunk['inicio'] - anterior['fim']} chars")
                
                # Mostra preview
                if chunk['inicio'] < anterior['fim']:
                    overlap_text = transcricao_completa[chunk['inicio']:anterior['fim']]
                    logger.error(f"   OVERLAP: '{overlap_text[:100]}...'")
                
                return False
    
    logger.info(f"âœ… {len(chunks)} chunks validados (sequenciais, sem overlap)")
    return True

# =============================================================================
# FUNÃ‡Ã•ES AUXILIARES
# =============================================================================

def limpar_tags_xml(texto):
    texto = re.sub(r'</?[a-z_][\w\-]*>', '', texto, flags=re.IGNORECASE)
    texto = re.sub(r'<[a-z_][\w\-]*\s+[^>]+>', '', texto, flags=re.IGNORECASE)
    return texto

def carregar_transcricao(arquivo):
    try:
        with open(arquivo, 'r', encoding='utf-8-sig') as f:
            conteudo = f.read()
        
        if not conteudo.strip():
            logger.error("Arquivo estÃ¡ vazio.")
            sys.exit(1)
            
        return conteudo
    except FileNotFoundError:
        logger.error(f"Arquivo '{arquivo}' nÃ£o encontrado.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Erro ao ler arquivo: {e}")
        sys.exit(1)

def estimar_custo(transcricao, usar_cache, num_chunks=1):
    tokens_in = len(transcricao) // 4
    
    if APOSTILA_MODE:
        tokens_out_estimado = int(tokens_in * 0.65)
    elif FIDELIDADE_MODE:
        tokens_out_estimado = int(tokens_in * 1.00)
    else:
        tokens_out_estimado = int(tokens_in * 0.85)
    
    tokens_prompt = len(PROMPT_FORMATACAO) // 4
    tokens_in_total = tokens_in + (tokens_prompt * num_chunks)
    
    if usar_cache:
        custo_input = (tokens_in * PRECO_INPUT_COM_CACHE + tokens_prompt * num_chunks * PRECO_INPUT_SEM_CACHE) / 1_000_000
        custo_output = (tokens_out_estimado * PRECO_OUTPUT) / 1_000_000
        custo = custo_input + custo_output
    else:
        custo = (tokens_in_total * PRECO_INPUT_SEM_CACHE + tokens_out_estimado * PRECO_OUTPUT) / 1_000_000
    
    logger.info(f"ğŸ’° Custo estimado: ${custo:.4f} USD (modo {MODO_NOME})")

# v2.9: Cache REABILITADO com hash inteligente
def criar_cache_contexto(client, transcricao_completa):
    """Cria cache de contexto com hash estÃ¡vel para reutilizaÃ§Ã£o"""
    
    # Cache sÃ³ vale a pena para documentos grandes
    if len(transcricao_completa) < MIN_CHARS_PARA_CACHE:
        logger.info(f"ğŸ“¦ Documento pequeno ({len(transcricao_completa):,} chars), cache nÃ£o necessÃ¡rio")
        return None
    
    try:
        # Hash do prompt base para cache estÃ¡vel entre execuÃ§Ãµes
        prompt_hash = hashlib.sha256(PROMPT_FORMATACAO.encode()).hexdigest()[:16]
        cache_name = f"fmt_{prompt_hash}"
        
        # v2.9: Tenta encontrar cache existente vÃ¡lido
        try:
            for c in client.caches.list(page_size=100):
                if c.display_name == cache_name:
                    logger.info(f"â™»ï¸  Reusando cache existente: {cache_name} ({c.name})")
                    return c
        except Exception as e:
            logger.warning(f"Cache lookup warning: {e}")

        # Prepara conteÃºdo para cache: prompt do sistema
        cache_content = f"""{PROMPT_FORMATACAO}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“š CONTEXTO GLOBAL
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Modo: {MODO_NOME}
"""
        
        # Cria cache usando a API do google-genai
        cache = client.caches.create(
            model='gemini-2.5-flash',
            config=types.CreateCachedContentConfig(
                contents=[cache_content],
                ttl=CACHE_TTL,
                display_name=cache_name
            )
        )
        
        logger.info(f"âœ… Cache criado: {cache_name} (hash: {prompt_hash}, TTL: {CACHE_TTL})")
        return cache
        
    except Exception as e:
        logger.warning(f"âš ï¸ Falha ao criar cache: {e}. Continuando sem cache.")
        return None

# =============================================================================
# MAPEAMENTO ESTRUTURAL (v2.8)
# =============================================================================

PROMPT_MAPEAMENTO = """VocÃª Ã© um especialista em organizaÃ§Ã£o de conteÃºdo jurÃ­dico.

Analise a transcriÃ§Ã£o abaixo e extraia a ESTRUTURA DE TÃ“PICOS do documento.

## INSTRUÃ‡Ã•ES:
1. Identifique os TÃ“PICOS PRINCIPAIS da aula (temas macro abordados)
2. Se houver mÃºltiplas disciplinas/matÃ©rias, cada uma Ã© um tÃ³pico de nÃ­vel 1
3. Se for uma aula sobre um Ãºnico tema, organize por subtÃ³picos lÃ³gicos
4. **MÃXIMO 3 NÃVEIS**: Use apenas 1., 1.1., 1.1.1. (nunca 1.1.1.1.)
5. Seja conciso - apenas tÃ­tulos, nÃ£o explicaÃ§Ãµes
6. Mantenha a ORDEM em que aparecem na transcriÃ§Ã£o
7. Mapeie do inÃ­cio ao fim, sem omitir partes

## FORMATO:
```
1. [TÃ³pico Principal]
   1.1. [SubtÃ³pico]
      1.1.1. [Detalhamento]
```

## TRANSCRIÃ‡ÃƒO:
{transcricao}

Retorne APENAS a estrutura hierÃ¡rquica (mÃ¡x 3 nÃ­veis), sem texto adicional.
"""

def mapear_estrutura(client, transcricao_completa):
    """Analisa o documento completo e extrai a estrutura de tÃ³picos"""
    logger.info("ğŸ—ºï¸  Mapeando estrutura do documento...")
    
    rate_limiter.wait_if_needed()
    
    # Gemini 2.5 Flash suporta 1M tokens (~4M chars)
    # Limite de 3.5M chars para deixar margem para output (20k tokens)
    max_chars_mapeamento = 3_500_000
    
    if len(transcricao_completa) > max_chars_mapeamento:
        logger.warning(f"âš ï¸  Documento EXTREMAMENTE grande ({len(transcricao_completa):,} chars). Cortando final para caber no contexto.")
        # Ã‰ melhor cortar o final do que picotar o meio para estrutura
        texto_para_mapear = transcricao_completa[:max_chars_mapeamento]
    else:
        texto_para_mapear = transcricao_completa
        logger.info(f"   Mapeando documento completo ({len(transcricao_completa):,} chars)")
    
    prompt = PROMPT_MAPEAMENTO.format(transcricao=texto_para_mapear)
    
    try:
        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=prompt,
            config=types.GenerateContentConfig(
                max_output_tokens=20000,  # Aumentado para documentos grandes
                safety_settings=[
                    types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                ]
            )
        )
        
        
        if not response.text:
            logger.warning("âš ï¸  Resposta vazia do mapeamento.")
            logger.warning(f"   Response object: {response}")
            if hasattr(response, 'candidates') and response.candidates:
                logger.warning(f"   Finish reason: {response.candidates[0].finish_reason}")
                if hasattr(response.candidates[0], 'safety_ratings'):
                    logger.warning(f"   Safety ratings: {response.candidates[0].safety_ratings}")
            logger.warning("   Continuando sem estrutura prÃ©via.")
            return None
        
        estrutura = response.text.strip()
        
        # Remove markdown code blocks se presentes
        if estrutura.startswith('```'):
            linhas = estrutura.split('\n')
            estrutura = '\n'.join(linhas[1:-1]) if len(linhas) > 2 else estrutura
        
        linhas = [l for l in estrutura.split('\n') if l.strip()]
        
        # ValidaÃ§Ã£o da estrutura
        if len(linhas) < 3:
            logger.warning(f"âš ï¸  Estrutura muito curta ({len(linhas)} linhas). Pode estar incompleta.")
            return None
        
        tem_numeracao = any(re.match(r'^\d+\.', l.strip()) for l in linhas)
        if not tem_numeracao:
            logger.warning("âš ï¸  Estrutura sem numeraÃ§Ã£o hierÃ¡rquica. Pode estar mal formatada.")
            return None
        
        logger.info(f"âœ… Estrutura mapeada: {len(linhas)} tÃ³picos identificados")
        
        # Log preview (primeiras 10 linhas + total)
        for linha in linhas[:10]:
            logger.info(f"   {linha}")
        if len(linhas) > 10:
            logger.info(f"   ... e mais {len(linhas) - 10} tÃ³picos")
        
        return estrutura
        
    except Exception as e:
        logger.warning(f"âš ï¸  Falha no mapeamento: {e}. Continuando sem estrutura prÃ©via.")
        return None

def simplificar_estrutura_se_necessario(estrutura, max_linhas=60):
    """
    Se a estrutura for muito longa (> max_linhas), mantÃ©m apenas:
    - NÃ­vel 1: 1. Assunto
    - NÃ­vel 2: 1.1. Subassunto

    Sempre inclui todos os nÃ­veis 1, e todos os nÃ­veis 2,
    e depois corta no mÃ¡ximo max_linhas, preservando a ordem original.
    """
    if not estrutura:
        return estrutura

    linhas = [l for l in estrutura.strip().split("\n") if l.strip()]
    if len(linhas) <= max_linhas:
        # JÃ¡ estÃ¡ razoÃ¡vel, mantÃ©m atÃ© nÃ­vel 3 (serÃ¡ filtrado depois por filtrar_niveis_excessivos)
        return estrutura

    logger.info(f"ğŸ“‰ Estrutura muito longa ({len(linhas)} itens). Simplificando para nÃ­veis 1 e 2, mÃ¡x {max_linhas} linhas...")

    nivel1 = []
    nivel2 = []

    for l in linhas:
        s = l.strip()
        # 1. Processo do Trabalho
        if re.match(r"^\d+\.\s", s):
            nivel1.append(l)
        # 1.1. Recursos Trabalhistas
        elif re.match(r"^\d+\.\d+\.\s", s):
            nivel2.append(l)

    # Se por algum motivo nÃ£o identificou quase nada, devolve original para nÃ£o quebrar
    if len(nivel1) + len(nivel2) < 5:
        logger.warning("âš ï¸ SimplificaÃ§Ã£o deixou poucos tÃ³picos. Mantendo estrutura original.")
        return estrutura

    # Monta nova estrutura: primeiro todos os nÃ­veis 1, depois os nÃ­veis 2, respeitando ordem de aparecimento
    nova = []
    vistos = set()

    for l in linhas:
        if l in vistos:
            continue
        if l in nivel1 or l in nivel2:
            nova.append(l)
            vistos.add(l)

    if len(nova) > max_linhas:
        nova = nova[:max_linhas]

    logger.info(f"âœ… Estrutura simplificada: {len(linhas)} -> {len(nova)} linhas (nÃ­veis 1 e 2).")
    return "\n".join(nova)

def filtrar_niveis_execessivos(estrutura, max_nivel=3):
    """
    Remove itens da estrutura que sejam mais profundos que max_nivel.
    Ex: se max_nivel=3, remove 1.1.1.1
    """
    if not estrutura:
        return estrutura
        
    linhas = estrutura.strip().split('\n')
    linhas_filtradas = []
    itens_removidos = 0
    
    # Regex para validar nÃ­vel. 
    # NÃ­vel 1: \d+\.
    # NÃ­vel 2: \d+\.\d+
    # NÃ­vel 3: \d+\.\d+\.\d+
    # O regex verifica se tem no mÃ¡ximo (max_nivel-1) pontos internos entre nÃºmeros
    
    for linha in linhas:
        # Conta quantos grupos de nÃºmeros existem
        match = re.match(r'^(\d+(?:\.\d+)*)', linha.strip())
        if match:
            numeracao = match.group(1)
            nivel = numeracao.count('.') + 1
            if linha.strip().endswith('.'): # Se terminar com ponto (1.1.), nÃ£o conta como nÃ­vel extra
                 nivel = numeracao.count('.')
            
            # Ajuste robusto: contar nÃºmeros separados por ponto
            partes = [p for p in numeracao.split('.') if p.isdigit()]
            nivel_real = len(partes)
            
            if nivel_real <= max_nivel:
                linhas_filtradas.append(linha)
            else:
                itens_removidos += 1
        else:
            # Linhas sem numeraÃ§Ã£o (tÃ­tulos soltos?) mantÃ©m por seguranÃ§a ou remove?
            # Vamos manter para nÃ£o quebrar formataÃ§Ã£o estranha
            linhas_filtradas.append(linha)
            
    if itens_removidos > 0:
        logger.info(f"âœ‚ï¸  Filtrados {itens_removidos} itens com nÃ­vel > {max_nivel}")
    
    return '\n'.join(linhas_filtradas)

# =============================================================================
# PROCESSAMENTO
# =============================================================================

def processar_simples(client, transcricao_bruta):
    logger.info("ğŸ“„ Documento pequeno - processando em requisiÃ§Ã£o Ãºnica...")
    
    prompt = f"""{PROMPT_FORMATACAO}

<texto_para_formatar>
{transcricao_bruta}
</texto_para_formatar>

Retorne APENAS o Markdown formatado."""
    
    for tentativa in range(MAX_RETRIES):
        try:
            response = client.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt,
                config=types.GenerateContentConfig(
                    max_output_tokens=OUTPUT_TOKEN_LIMIT,
                    temperature=0,
                    safety_settings=[
                        types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                    ]
                )
            )
            resultado = response.text
            return limpar_tags_xml(resultado)
        except Exception as e:
            if tentativa < MAX_RETRIES - 1:
                wait = (2 ** tentativa) + random.uniform(0, 1)
                logger.warning(f"Erro, retry em {wait:.1f}s...")
                sleep(wait)
            else:
                raise

def processar_chunk(client, cache, texto_chunk, numero, total, contexto_estilo="", estrutura_global=None, ultimo_titulo=None, profundidade=0):
    rate_limiter.wait_if_needed()
    
    # RecursÃ£o infinita protection e limite mÃ­nimo
    MIN_CHUNK_CHARS = 4000
    if len(texto_chunk) < MIN_CHUNK_CHARS:
        logger.warning(f"âš ï¸ Chunk {numero} muito pequeno ({len(texto_chunk)} chars). Processando sem dividir.")
    elif profundidade > 2:
        logger.warning(f"âš ï¸ Chunk {numero}: Profundidade de recursÃ£o {profundidade} atingida. Processando sem dividir.")

    # v2.8: SeÃ§Ã£o de estrutura global
    secao_estrutura = ""
    if estrutura_global:
        secao_estrutura = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ ESTRUTURA GLOBAL DA AULA (SIGA ESTA HIERARQUIA)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{estrutura_global}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ USE esta estrutura para nomear seus tÃ­tulos (##, ###)
   Os tÃ­tulos devem corresponder aos tÃ³picos listados acima.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
    
    # v2.10: Aviso sobre Ãºltimo tÃ­tulo (anti-duplicaÃ§Ã£o)
    aviso_titulo = ""
    if ultimo_titulo:
        aviso_titulo = f"""
ğŸš« O bloco anterior TERMINOU no tÃ³pico: "{ultimo_titulo}"
   NÃƒO inicie sua resposta repetindo este tÃ­tulo.
   Continue o conteÃºdo ou inicie o PRÃ“XIMO subtÃ³pico.
"""
    
    # v2.7: Delimitadores MUITO visÃ­veis e instruÃ§Ãµes reforÃ§adas
    secao_contexto = ""
    if contexto_estilo:
        secao_contexto = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”’ CONTEXTO ANTERIOR (SOMENTE REFERÃŠNCIA DE ESTILO)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{contexto_estilo}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ ATENÃ‡ÃƒO: O bloco acima JÃ FOI FORMATADO anteriormente.
- NÃƒO formate novamente esse conteÃºdo
- NÃƒO inclua esse conteÃºdo na sua resposta
- Use APENAS como referÃªncia de estilo de escrita
{aviso_titulo}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ NOVO TEXTO PARA FORMATAR (comece aqui):
"""
    
    # v2.9: Cache Support - Se usar cache, nÃ£o repete PROMPT_FORMATACAO
    instructions_body = f"""
{secao_estrutura}
{secao_contexto}

<texto_para_formatar>
{texto_chunk}
</texto_para_formatar>

**INSTRUÃ‡Ã•ES FINAIS**:
- Esta Ã© a parte {numero} de {total} (Profundidade: {profundidade})
- Formate APENAS o texto entre <texto_para_formatar>
- Se houver ESTRUTURA GLOBAL acima, use os mesmos nomes de tÃ³picos
- Se houver contexto acima, NÃƒO o reprocesse
- **ANTI-REPETIÃ‡ÃƒO DE TÃTULOS**: Se o contexto anterior termina com um tÃ­tulo (ex: "## HomologaÃ§Ã£o"), NÃƒO repita esse tÃ­tulo no inÃ­cio da sua resposta. Continue o conteÃºdo diretamente ou inicie o PRÃ“XIMO tÃ³pico diferente.
- Retorne APENAS o Markdown formatado do NOVO texto
"""

    if cache:
        prompt = instructions_body
    else:
        prompt = f"{PROMPT_FORMATACAO}\n{instructions_body}"

    for tentativa in range(MAX_RETRIES):
        try:
            safety_config = [
                types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
            ]
            
            # ConfiguraÃ§Ã£o dinÃ¢mica para suportar cache
            gen_config_args = {
                "max_output_tokens": OUTPUT_TOKEN_LIMIT,
                "temperature": 0.1,
                "safety_settings": safety_config
            }
            if cache:
                gen_config_args['cached_content'] = cache.name

            response = client.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt,
                config=types.GenerateContentConfig(**gen_config_args)
            )
            
            # --- DiagnÃ³stico finishReason e Usage ---
            finish_reason = "UNKNOWN"
            usage_tokens = 0
            
            try:
                if hasattr(response, 'usage_metadata'):
                    usage = response.usage_metadata
                    # Tenta acessar atributos (pode variar entre SDKs/Vertex)
                    # Vertex API retorna candidates_token_count
                    cand_tokens = getattr(usage, 'candidates_token_count', 0)
                    prompt_tokens = getattr(usage, 'prompt_token_count', 0)
                    usage_tokens = cand_tokens
                    logger.info(f"ğŸ“Š Usage: Prompt={prompt_tokens} | Candidates={cand_tokens}")
                
                if hasattr(response, 'candidates') and response.candidates:
                    cand = response.candidates[0]
                    if hasattr(cand, 'finish_reason'):
                         finish_reason = str(cand.finish_reason) # ex: FinishReason.STOP ou "STOP"
            except Exception as ex_usage:
                logger.warning(f"âš ï¸ Erro ao ler metadados: {ex_usage}")

            # Captura texto (lidando com caso de .text vazio mas content presente)
            resultado = ""
            try:
                resultado = response.text
            except ValueError:
                # O SDK levanta ValueError se finish_reason for SAFETY ou se nÃ£o houver text field padrÃ£o
                pass
            
            if not resultado and hasattr(response, 'candidates') and response.candidates:
                # Tenta extrair parts[0].text manualmente se .text falhou
                try:
                    parts = response.candidates[0].content.parts
                    if parts:
                        resultado = parts[0].text
                except:
                    pass
            
            if not resultado:
                logger.warning(f"âš ï¸  Resposta vazia na tentativa {tentativa+1}. Reason: {finish_reason}")
                if tentativa < MAX_RETRIES - 1:
                    sleep(2 * (tentativa + 1))
                    continue
                else:
                    return f"[ERRO SEÃ‡ÃƒO {numero}: RESPOSTA VAZIA]"

            # ValidaÃ§Ã£o bÃ¡sica de tamanho (compressÃ£o)
            razao = len(resultado) / len(texto_chunk) if len(texto_chunk) > 0 else 0
            
            problema_detectado = False
            msg_problema = ""
            
            compressao_excessiva_severa = False
            
            if razao < THRESHOLD_CRITICO: # < 0.70 por padrÃ£o
                problema_detectado = True
                msg_problema = f"CompressÃ£o excessiva ({razao:.0%})"
                # Flag para chunking imediato se for muito baixo (ex < 0.65)
                # O usuÃ¡rio pediu < THRESHOLD_CRITICO, vamos ser assertivos.
                compressao_excessiva_severa = True
                
            if problema_detectado:
                logger.warning(f"âš ï¸ SeÃ§Ã£o {numero}: {msg_problema}. Reason: {finish_reason}. (Tentativa {tentativa+1}/{MAX_RETRIES})")
                
                # SEÃ‡ÃƒO CRÃTICA: DecisÃ£o de Chunking Adaptativo
                
                # LÃ³gica antiga: sÃ³ no final ou MAX_TOKENS
                # LÃ³gica nova: divide cedo se compressÃ£o for severa
                
                deve_dividir = (
                    len(texto_chunk) > MIN_CHUNK_CHARS 
                    and profundidade < 2
                    and (
                        ("MAX_TOKENS" in str(finish_reason)) or 
                        (tentativa == MAX_RETRIES - 1) or
                        (compressao_excessiva_severa) # NOVO: Divide jÃ¡!
                    )
                )
                
                if deve_dividir:
                    motivo = "MAX_TOKENS" if "MAX_TOKENS" in str(finish_reason) else "COMPRESSÃƒO"
                    logger.info(f"âœ‚ï¸  ATIVANDO CHUNKING ADAPTATIVO para SeÃ§Ã£o {numero} (Motivo: {motivo} | Profundidade {profundidade} -> {profundidade+1})")
                    return dividir_e_reprocessar(client, cache, texto_chunk, numero, total, contexto_estilo, estrutura_global, ultimo_titulo, profundidade)
                
                if tentativa < MAX_RETRIES - 1:
                    continue  # Tenta de novo (se nÃ£o foi severo o suficiente para dividir)
                else:
                    logger.error(f"SeÃ§Ã£o {numero}: Falha apÃ³s {MAX_RETRIES} tentativas. Retornando melhor esforÃ§o.")
            
            return resultado
            
        except Exception as e:
            erro_msg = str(e)
            is_recoverable = any(code in erro_msg for code in ['503', '429', '500', 'RESOURCE_EXHAUSTED', 'InternalServerError']) or "Resposta vazia" in erro_msg
            
            if tentativa < MAX_RETRIES - 1 and is_recoverable:
                wait = (2 ** (tentativa + 2)) + random.uniform(1, 3)
                if '429' in erro_msg or 'RATE_LIMIT' in erro_msg:
                    wait = 30 + random.uniform(0, 5)
                    logger.warning(f"ğŸ›‘ Rate Limit (429) detectado na seÃ§Ã£o {numero}. Pausa longa de {wait:.1f}s...")
                else:
                    logger.warning(f"Erro seÃ§Ã£o {numero}: {erro_msg}. Retry {tentativa+2}/{MAX_RETRIES} em {wait:.1f}s")
                sleep(wait)
            else:
                logger.error(f"Falha seÃ§Ã£o {numero}: {erro_msg}")
                return f"\n\n> [!WARNING]\n> Falha ao processar seÃ§Ã£o {numero}. Texto original:\n\n{texto_chunk}"
    
    return texto_chunk

def dividir_e_reprocessar(client, cache, texto_chunk, numero, total, contexto_estilo, estrutura_global, ultimo_titulo, profundidade):
    """
    Divide um chunk grande em dois menores e processa recursivamente.
    Tenta dividir em quebras de parÃ¡grafo (\n\n) prÃ³ximas ao meio.
    """
    meio = len(texto_chunk) // 2
    
    # Procura quebra ideal (\n\n) num raio de 20% do meio
    margem = int(len(texto_chunk) * 0.2)
    inicio_busca = max(0, meio - margem)
    fim_busca = min(len(texto_chunk), meio + margem)
    
    janela_busca = texto_chunk[inicio_busca:fim_busca]
    pos_relativa = janela_busca.find('\n\n')
    
    if pos_relativa != -1:
        ponto_corte = inicio_busca + pos_relativa + 2 # +2 para incluir os \n\n no primeiro bloco ou pular? Vamos cortar DEPOIS dos \n\n
    else:
        # Tenta quebra simples \n
        pos_relativa_n = janela_busca.find('\n')
        if pos_relativa_n != -1:
            ponto_corte = inicio_busca + pos_relativa_n + 1
        else:
            # Corte seco no espaÃ§o mais prÃ³ximo
            pos_relativa_espaco = janela_busca.find(' ')
            if pos_relativa_espaco != -1:
                ponto_corte = inicio_busca + pos_relativa_espaco + 1
            else:
                ponto_corte = meio # Corte arbitrÃ¡rio
    
    parte_a = texto_chunk[:ponto_corte]
    parte_b = texto_chunk[ponto_corte:]
    
    logger.info(f"   Splitting chunk {numero}: Part A ({len(parte_a)} chars) + Part B ({len(parte_b)} chars)")
    
    # Processa Parte A
    resultado_a = processar_chunk(
        client, cache, parte_a, f"{numero}.A", total, 
        contexto_estilo, estrutura_global, ultimo_titulo, profundidade + 1
    )
    
    # Usa o final de A como contexto para B? Talvez seja excessivo e caro.
    # Vamos manter o contexto original para B por seguranÃ§a, 
    # ou usar resultado_a[-1000:] como novo contexto_estilo.
    # Usar resultado_a Ã© melhor para continuidade.
    
    novo_contexto = resultado_a[-2000:] if len(resultado_a) > 2000 else resultado_a
    
    # Processa Parte B
    resultado_b = processar_chunk(
        client, cache, parte_b, f"{numero}.B", total, 
        novo_contexto, estrutura_global, None, profundidade + 1 # ultimo_titulo None pois A jÃ¡ tratou disso
    )
    
    return f"{resultado_a}\n\n{resultado_b}"

def extrair_titulos_h2(texto):
    """Extrai todos os tÃ­tulos de nÃ­vel 2 (##) do texto"""
    titulos = []
    for linha in texto.split('\n'):
        if linha.strip().startswith('##') and not linha.strip().startswith('###'):
            titulo_limpo = re.sub(r'^##\s*\d+\.\s*', '', linha.strip())
            titulos.append(titulo_limpo.lower())
    return titulos

# =============================================================================
# SMART STITCHING (v2.10) - Anti-DuplicaÃ§Ã£o CirÃºrgica
# =============================================================================

def remover_eco_do_contexto(resposta_api, contexto_enviado):
    """
    Remove o inÃ­cio da resposta se for apenas um 'eco' do final do contexto.
    """
    if not contexto_enviado or not resposta_api:
        return resposta_api

    final_contexto = contexto_enviado.strip()[-300:]
    inicio_resposta = resposta_api.strip()[:300]

    matcher = SequenceMatcher(None, final_contexto, inicio_resposta)
    match = matcher.find_longest_match(0, len(final_contexto), 0, len(inicio_resposta))

    if match.size > 50:
        logger.info(f"âœ‚ï¸ Eco detectado! Removendo {match.size} chars repetidos no inÃ­cio.")
        return resposta_api.strip()[match.size:].strip()
    
    return resposta_api

def titulos_sao_similares(t1, t2, threshold=0.90):
    """Verifica se dois tÃ­tulos sÃ£o semanticamente iguais (fuzzy matching)."""
    def normalizar(t):
        # Remove apenas caracteres nÃ£o alfanumÃ©ricos, mas MANTÃ‰M tamanho relativo
        return re.sub(r'[^a-z0-9 ]', '', t.lower())
    
    nt1 = normalizar(t1)
    nt2 = normalizar(t2)
    
    if not nt1 or not nt2:
        return False
    
    # PROTEÃ‡ÃƒO 1: Se um tÃ­tulo for muito maior que o outro, nÃ£o sÃ£o duplicatas
    nt1_compact = nt1.replace(' ', '')
    nt2_compact = nt2.replace(' ', '')
    len_ratio = min(len(nt1_compact), len(nt2_compact)) / max(len(nt1_compact), len(nt2_compact))
    if len_ratio < 0.8:  # Se a diferenÃ§a de tamanho for > 20%, assume que sÃ£o diferentes
        return False
    
    # PROTEÃ‡ÃƒO 2: VerificaÃ§Ã£o por palavras - se houver palavras exclusivas significativas
    palavras1 = set(nt1.split())
    palavras2 = set(nt2.split())
    diferenca = palavras1.symmetric_difference(palavras2)
    
    # Se as palavras diferentes forem longas (nÃ£o apenas 'e', 'do', 'da'), assume diferenÃ§a real
    if any(len(w) > 3 for w in diferenca):
        return False
        
    return SequenceMatcher(None, nt1_compact, nt2_compact).ratio() > threshold

def limpar_inicio_redundante(texto_novo, texto_acumulado):
    """
    Remove tÃ­tulo no inÃ­cio do novo chunk se similar ao Ãºltimo tÃ­tulo do texto acumulado.
    """
    if not texto_acumulado.strip():
        return texto_novo

    ultimas_linhas = texto_acumulado.strip().split('\n')[-30:]
    ultimo_titulo = None
    for linha in reversed(ultimas_linhas):
        if linha.strip().startswith('##'):
            ultimo_titulo = re.sub(r'^#+\s*(?:\d+(?:\.\d+)*\.?)?\s*', '', linha).strip()
            break
    
    if not ultimo_titulo:
        return texto_novo

    linhas_novas = texto_novo.strip().split('\n')
    
    for i, linha in enumerate(linhas_novas[:10]):  # v2.11: Busca mais profunda (era 5)
        if linha.strip().startswith('##'):
            novo_titulo = re.sub(r'^#+\s*(?:\d+(?:\.\d+)*\.?)?\s*', '', linha).strip()
            
            if titulos_sao_similares(ultimo_titulo, novo_titulo):
                logger.info(f"âœ‚ï¸ TÃ­tulo duplicado na junÃ§Ã£o: '{novo_titulo}' â‰ˆ '{ultimo_titulo}'")
                return '\n'.join(linhas_novas[i+1:])
    
    return texto_novo

def detectar_secoes_duplicadas(texto):
    """v2.10: Detecta seÃ§Ãµes duplicadas por tÃ­tulos (Fuzzy Matching)"""
    logger.info("ğŸ” Detectando seÃ§Ãµes duplicadas (fuzzy)...")
    
    linhas = texto.split('\n')
    titulos_vistos = []
    secoes_duplicadas = []
    
    for i, linha in enumerate(linhas):
        if linha.strip().startswith('##') and not linha.strip().startswith('###'):
            titulo_normalizado = re.sub(r'^##\s*\d+\.?\s*', '', linha.strip())
            titulo_normalizado = re.sub(r'[ğŸ“‹ğŸ“ŠğŸ—‚ï¸]', '', titulo_normalizado).strip()
            
            duplicado = False
            for t_visto, linha_visto in titulos_vistos:
                if titulos_sao_similares(titulo_normalizado, t_visto):
                    logger.warning(f"âš ï¸  Duplicado (fuzzy): '{linha.strip()}' â‰ˆ '{t_visto}'")
                    secoes_duplicadas.append({
                        'titulo': titulo_normalizado,
                        'primeira_linha': linha_visto,
                        'duplicada_linha': i
                    })
                    duplicado = True
                    break
            
            if not duplicado:
                titulos_vistos.append((titulo_normalizado, i))
    
    if secoes_duplicadas:
        logger.error(f"âŒ {len(secoes_duplicadas)} seÃ§Ãµes duplicadas detectadas!")
    else:
        logger.info("âœ… Nenhuma seÃ§Ã£o duplicada detectada")
    
    return secoes_duplicadas

def remover_secoes_duplicadas(texto):
    """v2.7: Remove seÃ§Ãµes duplicadas mantendo apenas a primeira ocorrÃªncia"""
    secoes_dup = detectar_secoes_duplicadas(texto)
    
    if not secoes_dup:
        return texto
    
    logger.info("ğŸ§¹ Removendo seÃ§Ãµes duplicadas...")
    
    linhas = texto.split('\n')
    linhas_para_remover = set()
    
    for dup in secoes_dup:
        # Marca para remoÃ§Ã£o todas as linhas da seÃ§Ã£o duplicada
        inicio_remocao = dup['duplicada_linha']
        
        # Encontra onde a seÃ§Ã£o duplicada termina (prÃ³ximo ## ou fim do arquivo)
        fim_remocao = len(linhas)
        for i in range(inicio_remocao + 1, len(linhas)):
            if linhas[i].strip().startswith('##') and not linhas[i].strip().startswith('###'):
                fim_remocao = i
                break
        
        logger.info(f"   Removendo linhas {inicio_remocao}-{fim_remocao} (seÃ§Ã£o duplicada)")
        for i in range(inicio_remocao, fim_remocao):
            linhas_para_remover.add(i)
    
    # ReconstrÃ³i texto sem as linhas duplicadas
    linhas_limpas = [linha for i, linha in enumerate(linhas) if i not in linhas_para_remover]
    
    logger.info(f"âœ… {len(linhas_para_remover)} linhas removidas")
    return '\n'.join(linhas_limpas)

def remover_duplicacoes_literais(texto):
    """Remove parÃ¡grafos individuais duplicados"""
    paragrafos = texto.split('\n\n')
    paragrafos_limpos = []
    dup_count = 0
    
    for i, para in enumerate(paragrafos):
        if i == 0:
            paragrafos_limpos.append(para)
            continue
        
        if len(para.strip()) < 80 or para.strip().startswith('#'):
            paragrafos_limpos.append(para)
            continue
        
        is_duplicate = False
        para_norm = ' '.join(para.split()).lower()
        
        for j in range(max(0, len(paragrafos_limpos) - 3), len(paragrafos_limpos)):
            para_ant = paragrafos_limpos[j]
            para_ant_norm = ' '.join(para_ant.split()).lower()
            
            ratio = SequenceMatcher(None, para_norm, para_ant_norm).ratio()
            
            if ratio > 0.95:
                is_duplicate = True
                dup_count += 1
                break
        
        if not is_duplicate:
            paragrafos_limpos.append(para)
    
    if dup_count > 5:
        logger.warning(f"âš ï¸  {dup_count} parÃ¡grafos duplicados removidos")
    
    return '\n\n'.join(paragrafos_limpos)

def numerar_titulos(texto):
    """Adiciona numeraÃ§Ã£o sequencial aos tÃ­tulos"""
    linhas = texto.split('\n')
    linhas_numeradas = []
    
    contador_h2 = 0
    contador_h3 = 0
    contador_h4 = 0
    
    titulo_pattern = re.compile(r'^(#{2,4})\s+(?:\d+(?:\.\d+)*\.?\s+)?(.+)$')
    
    # Variaveis para rastrear Ãºltimos tÃ­tulos e evitar repetiÃ§Ãµes
    ultimo_h2_texto = ""
    ultimo_h3_texto = ""
    
    for linha in linhas:
        match = titulo_pattern.match(linha)
        
        if match:
            nivel = len(match.group(1))
            texto_titulo = match.group(2).strip()
            
            # NÃ£o numera tÃ­tulos de resumo/quadros
            if any(keyword in texto_titulo.lower() for keyword in ['resumo', 'quadro', 'esquema', 'ğŸ“‹', 'ğŸ“Š', 'ğŸ—‚ï¸']):
                linhas_numeradas.append(linha)
                continue
            
            # MERGE INTELIGENTE DE TÃTULOS REPETIDOS (v2.11)
            # Se o tÃ­tulo atual for muito similar ao anterior do mesmo nÃ­vel ("continuaÃ§Ã£o" de chunk), ignoramos o novo
            # para que o texto flua como um Ãºnico tÃ³pico.
            
            from difflib import SequenceMatcher
            eh_duplicado = False
            
            if nivel == 2:
                # Verifica similaridade com Ãºltimo H2
                ratio = SequenceMatcher(None, texto_titulo.lower(), ultimo_h2_texto.lower()).ratio()
                if ratio > 0.9:
                    eh_duplicado = True
                    logger.info(f"ğŸ”„ TÃ­tulo H2 mesclado: '{texto_titulo}' â‰ˆ '{ultimo_h2_texto}'")
                else:
                    ultimo_h2_texto = texto_titulo
            elif nivel == 3:
                 # Verifica similaridade com Ãºltimo H3
                ratio = SequenceMatcher(None, texto_titulo.lower(), ultimo_h3_texto.lower()).ratio()
                if ratio > 0.9:
                    eh_duplicado = True
                    logger.info(f"ğŸ”„ TÃ­tulo H3 mesclado: '{texto_titulo}' â‰ˆ '{ultimo_h3_texto}'")
                else:
                    ultimo_h3_texto = texto_titulo
            
            if eh_duplicado:
                continue # Pula a linha do tÃ­tulo, fundindo o conteÃºdo
            
            if nivel == 2:
                contador_h2 += 1
                contador_h3 = 0
                contador_h4 = 0
                nova_linha = f"## {contador_h2}. {texto_titulo}"
            elif nivel == 3:
                contador_h3 += 1
                contador_h4 = 0
                nova_linha = f"### {contador_h2}.{contador_h3}. {texto_titulo}"
            elif nivel == 4:
                contador_h4 += 1
                nova_linha = f"#### {contador_h2}.{contador_h3}.{contador_h4}. {texto_titulo}"
            else:
                nova_linha = linha
            
            linhas_numeradas.append(nova_linha)
        else:
            linhas_numeradas.append(linha)
    
    return '\n'.join(linhas_numeradas)

# =============================================================================
# VERIFICAÃ‡ÃƒO DE COBERTURA E DUPLICAÃ‡Ã•ES
# =============================================================================

def normalizar_fingerprint(texto, tipo):
    """Normaliza texto para comparaÃ§Ã£o (ex: 'Lei 11.100' -> 'lei 11100')"""
    texto = texto.lower().strip()
    
    if tipo == 'leis':
        # MantÃ©m apenas 'lei' e nÃºmeros
        nums = re.findall(r'\d+', texto)
        if nums:
            # ReconstrÃ³i como 'lei 12345'
            # Filtra leis com menos de 4 dÃ­gitos para evitar ruÃ­do (ex: lei 10, lei 13)
            num_full = ''.join(nums)
            if len(num_full) >= 4:
                return f"lei {num_full}"
            return None
            
    elif tipo == 'sumulas':
        nums = re.findall(r'\d+', texto)
        if nums:
            return f"sÃºmula {''.join(nums)}"
            
    elif tipo == 'artigos':
        nums = re.findall(r'\d+', texto)
        if nums:
            return f"artigo {''.join(nums)}"
            
    return re.sub(r'[^\w\s]', '', texto)

def extrair_fingerprints(texto):
    """Extrai 'fingerprints' Ãºnicos e normalizados do texto"""
    fingerprints = {
        'leis': set(),
        'sumulas': set(),
        'artigos': set(),
        'julgados': set()
    }
    
    # Regex melhorado para capturar variaÃ§Ãµes
    lei_pattern = re.compile(r'\b(?:lei|l\.)\s*n?Âº?\s*([\d\.]+)', re.IGNORECASE)
    sumula_pattern = re.compile(r'\bsÃºmula\s*(?:vinculante)?\s*n?Âº?\s*(\d+)', re.IGNORECASE)
    
    # Extrai e normaliza
    for match in lei_pattern.finditer(texto):
        fp = normalizar_fingerprint(f"lei {match.group(1)}", 'leis')
        if fp: fingerprints['leis'].add(fp)
    
    for match in sumula_pattern.finditer(texto):
        fp = normalizar_fingerprint(f"sÃºmula {match.group(1)}", 'sumulas')
        if fp: fingerprints['sumulas'].add(fp)
        
    return fingerprints

def contar_ocorrencias_robust(fingerprints, texto):
    """Conta ocorrÃªncias com suporte a formataÃ§Ã£o jurÃ­dica formal (Lei nÂº X)"""
    contagens = {}
    
    # CORREÃ‡ÃƒO 1: NÃ£o remover pontuaÃ§Ã£o indiscriminadamente, apenas normalizar espaÃ§os
    # Mantemos barras e pontos para evitar fusÃ£o de nÃºmeros (11.101/2005)
    texto_lower = texto.lower()
    
    for categoria, items in fingerprints.items():
        for item in items:
            key = f"{categoria}:{item}"
            
            if categoria == 'leis':
                # item ex: "lei 4320" -> extrai "4320"
                # Remove pontuaÃ§Ã£o do item para garantir match limpo no nÃºmero
                num_bruto = item.split()[-1] 
                num = re.sub(r'[^\d]', '', num_bruto)
                
                # Permite pontos opcionais entre dÃ­gitos (ex: 4.320 match com 4320)
                num_regex = r"\.?".join(list(num))
                
                # CORREÃ‡ÃƒO 2: Regex flexÃ­vel que aceita "n", "nÂº", "no", "num" no meio
                # Aceita: "Lei 4320", "Lei nÂº 4.320", "Lei n. 4320"
                # O \W* permite pontos/barras entre Lei e o nÃºmero
                # Adicionado \b no final para evitar matches parciais (Lei 10 != Lei 100)
                pattern = f"lei(?:\\s+|\\.|\\,|nÂº|n\\.|n\\s|num\\.?)*{num_regex}\\b"
                
                # Usamos findall no texto original (lower) para pegar variaÃ§Ãµes com pontuaÃ§Ã£o
                matches = re.findall(pattern, texto_lower)
                contagens[key] = len(matches)
                
            elif categoria == 'sumulas':
                num = item.split()[-1]
                num_regex = r"\.?".join(list(num)) # SÃºmulas raramente tem ponto, mas por garantia
                
                # Mesma lÃ³gica para sÃºmulas (SÃºmula Vinculante nÂº 10)
                pattern = f"sÃºmula(?:\\s+|\\.|\\,|vinculante|nÂº|n\\.|n\\s)*{num_regex}\\b"
                matches = re.findall(pattern, texto_lower)
                contagens[key] = len(matches)
                
            else:
                # Fallback para outros tipos (busca literal simples)
                contagens[key] = texto_lower.count(item)
                
    return contagens

def verificar_cobertura(texto_original, texto_formatado, arquivo_saida=None):
    """Verifica omissÃµes e duplicaÃ§Ãµes artificiais entre original e formatado"""
    logger.info("ğŸ” Verificando cobertura e duplicaÃ§Ãµes...")
    
    # Extrai fingerprints do original
    fp_original = extrair_fingerprints(texto_original)
    
    # Conta ocorrÃªncias em ambos
    contagem_original = contar_ocorrencias_robust(fp_original, texto_original)
    contagem_formatado = contar_ocorrencias_robust(fp_original, texto_formatado)
    
    omissoes = []
    duplicacoes = []
    
    for key, count_orig in contagem_original.items():
        count_fmt = contagem_formatado.get(key, 0)
        categoria, item = key.split(':', 1)
        
        # OmissÃ£o: estava no original mas sumiu
        if count_orig > 0 and count_fmt == 0:
            omissoes.append({
                'categoria': categoria,
                'item': item,
                'original': count_orig,
                'formatado': count_fmt
            })
        
        # DuplicaÃ§Ã£o (agora considerada positiva em materiais didÃ¡ticos)
        if count_fmt > count_orig:
            duplicacoes.append({
                'categoria': categoria,
                'item': item,
                'original': count_orig,
                'formatado': count_fmt,
                'extra': count_fmt - count_orig
            })
    
    # Gera relatÃ³rio
    total_items = len([k for k, v in contagem_original.items() if v > 0])
    items_preservados = total_items - len(omissoes)
    cobertura = items_preservados / total_items * 100 if total_items > 0 else 100
    
    logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    logger.info(f"ğŸ“Š RELATÃ“RIO DE VERIFICAÃ‡ÃƒO")
    logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    logger.info(f"âœ… Cobertura: {cobertura:.1f}% ({items_preservados}/{total_items} referÃªncias)")
    
    if omissoes:
        logger.warning(f"\nâŒ POSSÃVEIS OMISSÃ•ES ({len(omissoes)}):")
        for o in omissoes[:10]:  # Limita a 10
            logger.warning(f"   - [{o['categoria']}] {o['item']}")
        if len(omissoes) > 10:
            logger.warning(f"   ... e mais {len(omissoes) - 10} omissÃµes")
    else:
        logger.info("âœ… Nenhuma omissÃ£o detectada")
    
    if duplicacoes:
        logger.info(f"\nâ„¹ï¸ CITAÃ‡Ã•ES REFORÃ‡ADAS (Tabelas/Resumos) ({len(duplicacoes)}):")
        for d in duplicacoes[:10]:
            logger.info(f"   - [{d['categoria']}] {d['item']}: {d['original']}x â†’ {d['formatado']}x (+{d['extra']})")
        if len(duplicacoes) > 10:
            logger.info(f"   ... e mais {len(duplicacoes) - 10} citaÃ§Ãµes extras")
    else:
        logger.info("â„¹ï¸ Nenhuma citaÃ§Ã£o extra detectada")
    
    logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    # Salva relatÃ³rio em arquivo se especificado
    if arquivo_saida:
        relatorio_path = arquivo_saida.replace('.md', '_verificacao.txt')
        with open(relatorio_path, 'w', encoding='utf-8') as f:
            f.write(f"RELATÃ“RIO DE VERIFICAÃ‡ÃƒO\n")
            f.write(f"{'='*50}\n\n")
            f.write(f"Cobertura: {cobertura:.1f}% ({items_preservados}/{total_items})\n\n")
            
            if omissoes:
                f.write(f"OMISSÃ•ES ({len(omissoes)}):\n")
                for o in omissoes:
                    f.write(f"  - [{o['categoria']}] {o['item']}\n")
                f.write("\n")
            
            if duplicacoes:
                f.write(f"DUPLICAÃ‡Ã•ES ARTIFICIAIS ({len(duplicacoes)}):\n")
                for d in duplicacoes:
                    f.write(f"  - [{d['categoria']}] {d['item']}: {d['original']}x original â†’ {d['formatado']}x formatado\n")
        
        logger.info(f"ğŸ“„ RelatÃ³rio salvo: {relatorio_path}")
    
    return {
        'cobertura': cobertura,
        'omissoes': omissoes,
        'duplicacoes': duplicacoes
    }

# =============================================================================
# V2.10: FUNÃ‡Ã•ES DE PÃ“S-PROCESSAMENTO ESTRUTURAL (Tabelas e ParÃ¡grafos)
# =============================================================================

def mover_tabelas_para_fim_de_secao(texto):
    """
    v2.11: Reorganiza tabelas movendo-as para o final do BLOCO ATUAL (H2 ou H3).
    Corrige bug de tabelas sumindo ou ficando muito longe do contexto.
    """
    logger.info("ğŸ“Š Reorganizando tabelas (Smart Layout v2.11)...")
    
    linhas = texto.split('\n')
    resultado = []
    tabelas_pendentes = [] 
    
    i = 0
    while i < len(linhas):
        linha = linhas[i]
        linha_strip = linha.strip()
        
        # 1. DETECTAR SE Ã‰ UM TÃTULO (H1, H2, H3...)
        # Se encontrarmos um novo tÃ­tulo, hora de "despejar" as tabelas acumuladas do bloco anterior
        if linha_strip.startswith('#'):
            # Despeja tabelas antes de iniciar o novo tÃ³pico
            if tabelas_pendentes:
                resultado.append('') # EspaÃ§o antes
                for t_info in tabelas_pendentes:
                    if t_info['titulo']:
                        resultado.append(t_info['titulo'])
                    resultado.extend(t_info['linhas'])
                    resultado.append('') # EspaÃ§o depois
                tabelas_pendentes = []
            
            resultado.append(linha)
            i += 1
            continue

        # 2. DETECTAR INÃCIO DE TABELA
        # CritÃ©rio: Linha tem pipe '|' E parece estrutura de tabela (nÃ£o apenas citaÃ§Ã£o)
        eh_inicio_tabela = False
        if '|' in linha_strip:
            # Verifica se Ã© uma linha de markdown table vÃ¡lida (tem pipe e chars)
            # E se a prÃ³xima linha ou a seguinte tem o separador '---'
            has_separator = False
            for lookahead in range(1, 3): # Olha atÃ© 2 linhas pra frente (ignora 1 linha vazia)
                if i + lookahead < len(linhas):
                    prox = linhas[i + lookahead].strip()
                    if set(prox).issubset(set('|- :')): # SÃ³ contem caracteres de estrutura de tabela
                         has_separator = True
                         break
            
            if has_separator or (linha_strip.startswith('|') and linha_strip.endswith('|')):
                eh_inicio_tabela = True

        if eh_inicio_tabela:
            # --- Captura da Tabela ---
            tabela_linhas = []
            titulo_tabela = None
            
            # Tenta recuperar o tÃ­tulo da tabela que ficou na linha anterior (ou resultado)
            # Verifica se a Ãºltima linha adicionada ao resultado parece um tÃ­tulo de tabela
            if resultado and len(resultado) > 0:
                last_line = resultado[-1].strip()
                # PadrÃµes comuns de tÃ­tulo de tabela gerados pela IA
                if (last_line.startswith('###') or last_line.startswith('**')) and \
                   any(x in last_line.lower() for x in ['tabela', 'resumo', 'quadro', 'sÃ­ntese', 'esquema', 'ğŸ“‹']):
                    titulo_tabela = resultado.pop() # Remove do fluxo principal para agrupar com a tabela

            # Captura as linhas da tabela
            j = i
            while j < len(linhas):
                curr = linhas[j].strip()
                # Continua se tiver pipe ou for linha vazia no meio da tabela (mas cuidado com fim)
                if '|' in curr:
                    tabela_linhas.append(linhas[j])
                    j += 1
                elif not curr:
                    # Linha vazia: verifica se a prÃ³xima volta a ter pipe
                    if j + 1 < len(linhas) and '|' in linhas[j+1]:
                        tabela_linhas.append(linhas[j]) # MantÃ©m linha vazia interna
                        j += 1
                    else:
                        break # Fim da tabela
                else:
                    break # Texto normal, fim da tabela

            # Verifica se capturou algo Ãºtil
            if len(tabela_linhas) > 0:
                tabelas_pendentes.append({
                    'titulo': titulo_tabela,
                    'linhas': tabela_linhas
                })
                i = j # Pula as linhas processadas
                continue
            else:
                # Falso positivo? Devolve o tÃ­tulo se tinhamos pego
                if titulo_tabela:
                    resultado.append(titulo_tabela)
        
        # Se nÃ£o for tabela nem tÃ­tulo, adiciona linha normal
        resultado.append(linha)
        i += 1
    
    # 3. FINAL DO DOCUMENTO
    # Se sobraram tabelas no buffer, despeja agora
    if tabelas_pendentes:
        resultado.append('')
        for t_info in tabelas_pendentes:
            if t_info['titulo']:
                resultado.append(t_info['titulo'])
            resultado.extend(t_info['linhas'])
            resultado.append('')
            
    return '\n'.join(resultado)

def quebrar_paragrafos_longos(texto, max_chars=400, max_sentencas=4):
    """
    Quebra parÃ¡grafos que excedem limite de chars OU nÃºmero de sentenÃ§as.
    Preserva listas, tabelas, citaÃ§Ãµes e blocos especiais.
    """
    logger.info(f"âœ‚ï¸ Quebrando parÃ¡grafos > {max_chars} chars ou > {max_sentencas} sentenÃ§as...")
    
    paragrafos = texto.split('\n\n')
    resultado = []
    quebras = 0
    
    for para in paragrafos:
        linha_strip = para.strip()
        
        # PRESERVAR: tÃ­tulos, listas, tabelas, citaÃ§Ãµes, blocos de cÃ³digo
        if (linha_strip.startswith('#') or 
            linha_strip.startswith('-') or 
            linha_strip.startswith('* ') or 
            linha_strip.startswith('|') or
            linha_strip.startswith('>') or
            linha_strip.startswith('```') or
            re.match(r'^\d+\.', linha_strip)):
            resultado.append(para)
            continue
        
        # Verifica se precisa quebrar
        num_chars = len(para)
        sentencas = re.split(r'(?<=[.!?])\s+', para)
        num_sentencas = len(sentencas)
        
        if num_chars <= max_chars and num_sentencas <= max_sentencas:
            resultado.append(para)
            continue
        
        # QUEBRAR: Agrupa em blocos de atÃ© max_sentencas
        quebras += 1
        subparagrafos = []
        bloco_atual = []
        chars_atual = 0
        sentencas_no_bloco = 0
        
        for sentenca in sentencas:
            teste_chars = chars_atual + len(sentenca)
            
            # Se adicionar essa sentenÃ§a ultrapassar AMBOS os limites ou o limite de sentenÃ§as
            # E jÃ¡ temos algo no bloco...
            if (teste_chars > max_chars or sentencas_no_bloco >= max_sentencas) and bloco_atual:
                subparagrafos.append(' '.join(bloco_atual).strip())
                bloco_atual = [sentenca]
                chars_atual = len(sentenca)
                sentencas_no_bloco = 1
            else:
                bloco_atual.append(sentenca)
                chars_atual = teste_chars
                sentencas_no_bloco += 1
        
        # Adiciona Ãºltimo bloco
        if bloco_atual:
            subparagrafos.append(' '.join(bloco_atual).strip())
        
        resultado.append('\n\n'.join(subparagrafos))
        
    if quebras > 0:
        logger.info(f"   âœ… {quebras} parÃ¡grafos foram ajustados.")
        
    return '\n\n'.join(resultado)

# =============================================================================
# FLUXO PRINCIPAL
# =============================================================================

def formatar_transcricao(transcricao_completa, usar_cache=True, input_file=None):
    # âš¡ FORÃ‡AR USO DE VERTEX AI - AI Studio desabilitado
    rate_limiter.max_rpm = 60
    # Tenta pegar o projeto do ambiente, se nÃ£o tiver, usa o hardcoded como fallback
    project_id = os.getenv("GOOGLE_CLOUD_PROJECT", "gen-lang-client-0727883752")
    logger.info(f" Usando Vertex AI (Project: {project_id})")
    logger.info(" Rate Limit: 60 RPM")
    logger.info("ğŸ”¥ AI Studio DESABILITADO - Usando apenas Vertex AI")
    
    # Verificar se as credenciais do Vertex AI estÃ£o configuradas
    if os.getenv("GOOGLE_APPLICATION_CREDENTIALS") is None:
        logger.error("âŒ Nenhuma autenticaÃ§Ã£o configurada para Vertex AI.")
        logger.error("Configure a variÃ¡vel de ambiente GOOGLE_APPLICATION_CREDENTIALS:")
        logger.error("  export GOOGLE_APPLICATION_CREDENTIALS='/path/to/service-account.json'")
        sys.exit(1)

    client = genai.Client(
        vertexai=True,
        project=project_id,
        location="us-central1"
    )
    
    # Dry-run mode
    if '--dry-run' in sys.argv:
        logger.info("ğŸ” MODO DRY-RUN: Validando divisÃ£o de chunks")
        chunks = dividir_sequencial(transcricao_completa)
        validar_chunks(chunks, transcricao_completa)
        for i, c in enumerate(chunks):
            print(f"\n  Chunk {i+1}/{len(chunks)}:")
            print(f"    PosiÃ§Ã£o: {c['inicio']:,} â†’ {c['fim']:,} ({c['fim']-c['inicio']:,} chars)")
            inicio_preview = transcricao_completa[c['inicio']:c['inicio']+80].replace('\n', 'â†µ')
            fim_preview = transcricao_completa[max(0, c['fim']-80):c['fim']].replace('\n', 'â†µ')
            print(f"    InÃ­cio: {inicio_preview}...")
            print(f"    Fim: ...{fim_preview}")
        sys.exit(0)
    
    tamanho_total = len(transcricao_completa)
    logger.info(f"ğŸ“Š Tamanho: {tamanho_total:,} caracteres")
    
    chunks = dividir_sequencial(transcricao_completa)
    
    # v2.7: ValidaÃ§Ã£o rigorosa
    if not validar_chunks(chunks, transcricao_completa):
        logger.error("âŒ Chunks invÃ¡lidos! Abortando.")
        sys.exit(1)
    
    num_partes = len(chunks)
    chunks_info = [{'inicio': c['inicio'], 'fim': c['fim']} for c in chunks]
    
    estimar_custo(transcricao_completa, usar_cache, num_partes)
    
    if num_partes == 1:
        return processar_simples(client, transcricao_completa), None
    
    checkpoint = None
    resultados = []
    inicio_secao = 0
    
    if input_file:
        checkpoint = load_checkpoint(input_file)
        if checkpoint:
            logger.info(f"ğŸ“ Checkpoint: seÃ§Ã£o {checkpoint['secao_atual']}/{checkpoint['total_secoes']}")
            resposta = input("   Continuar? (s/n): ").strip().lower()
            if resposta == 's':
                resultados = checkpoint['resultados']
                inicio_secao = checkpoint['secao_atual']
                chunks_info = checkpoint['chunks_info']
            else:
                delete_checkpoint(input_file)
    
    # v2.9: Cache DESABILITADO (Flash Ã© barato e input < 32k tokens nÃ£o compensa)
    cache = None
    
    # v2.8: Mapeamento estrutural prÃ©vio
    estrutura_global = []
    if num_partes > 1:  # Mapeia sempre que houver mÃºltiplas partes (mesmo retomando, pois checkpoint nÃ£o salva estrutura)
        estrutura_global = mapear_estrutura(client, transcricao_completa)
        
        # v2.10: ForÃ§ar limite de 3 nÃ­veis (pedido usuÃ¡rio)
        estrutura_global = filtrar_niveis_execessivos(estrutura_global, max_nivel=3)
        
        # v2.10: Simplifica se for muito grande
        estrutura_global = simplificar_estrutura_se_necessario(estrutura_global)
    
    try:
        iterator = range(inicio_secao, num_partes)
        if tqdm:
            iterator = tqdm(iterator, desc="Formatando", initial=inicio_secao, total=num_partes)
        
        for i in iterator:
            chunk = chunks_info[i]
            texto_chunk = transcricao_completa[chunk['inicio']:chunk['fim']]
            
            # Contexto com validaÃ§Ã£o
            contexto_estilo = ""
            if i > 0 and resultados:
                raw_context = resultados[-1][-CONTEXTO_ESTILO:]
                if len(raw_context.split()) < 50 or "[!WARNING]" in raw_context:
                    logger.warning(f"Contexto chunk {i+1} descartado")
                else:
                    contexto_estilo = raw_context
            
            # v2.10: Extrair Ãºltimo tÃ­tulo do chunk anterior para anti-duplicaÃ§Ã£o
            ultimo_titulo = None
            if resultados:
                texto_anterior = resultados[-1]
                for linha in reversed(texto_anterior.split('\n')[-30:]):
                    if linha.strip().startswith('##'):
                        ultimo_titulo = re.sub(r'^#+\s*(?:\d+(?:\.\d+)*\.?)?\s*', '', linha).strip()
                        break
            
            resultado = processar_chunk(
                client, cache, texto_chunk,
                i + 1, num_partes,
                contexto_estilo=contexto_estilo,
                estrutura_global=estrutura_global,
                ultimo_titulo=ultimo_titulo
            )
            
            # v2.10: Smart Stitching - Remove eco do contexto
            if contexto_estilo:
                resultado = remover_eco_do_contexto(resultado, contexto_estilo)
            
            # v2.10: Smart Stitching - Remove tÃ­tulo duplicado na fronteira
            texto_acumulado = '\n\n'.join(resultados) if resultados else ""
            resultado = limpar_inicio_redundante(resultado, texto_acumulado)
            
            resultados.append(resultado)
            
            if input_file:
                save_checkpoint(input_file, resultados, chunks_info, i + 1)
            
            if not tqdm:
                logger.info(f"âœ… SeÃ§Ã£o {i+1}/{num_partes}")
        
        # v2.7: Post-processing em mÃºltiplas passadas
        logger.info("ğŸ§¹ Iniciando limpeza (v2.7)...")
        
        texto_final = '\n\n'.join(resultados)
        
        logger.info("  Passada 1: Removendo duplicaÃ§Ãµes literais...")
        texto_final = remover_duplicacoes_literais(texto_final)
        
        # logger.info("  Passada 2: Detectando seÃ§Ãµes duplicadas...")
        # texto_final = remover_secoes_duplicadas(texto_final)  # DESATIVADO: causava falsos positivos
        
        
        # v2.10: ReordenaÃ§Ã£o do Pipeline (Tabelas -> NumeraÃ§Ã£o -> ParÃ¡grafos)
        
        logger.info("  Passada 2: Reorganizando tabelas (Smart Layout)...")
        texto_final = mover_tabelas_para_fim_de_secao(texto_final)
        
        logger.info("  Passada 3: Numerando tÃ­tulos...")
        texto_final = numerar_titulos(texto_final)
        
        logger.info("  Passada 4: Ajustando parÃ¡grafos longos...")
        texto_final = quebrar_paragrafos_longos(texto_final, max_chars=400, max_sentencas=4)
        
        # ValidaÃ§Ã£o final
        palavras_in = len(transcricao_completa.split())
        palavras_out = len(texto_final.split())
        razao = palavras_out / palavras_in if palavras_in > 0 else 1.0
        
        logger.info(f"âœ… ValidaÃ§Ã£o: {razao:.0%} do original ({palavras_out:,}/{palavras_in:,} palavras)")
        
        if razao < THRESHOLD_CRITICO:
            if FIDELIDADE_MODE:
                logger.error(f"âŒ POSSÃVEL PERDA DE CONTEÃšDO ({razao:.0%})")
                logger.error(f"   Esperado: >{THRESHOLD_CRITICO:.0%} | Obtido: {razao:.0%}")
            elif APOSTILA_MODE:
                logger.warning(f"âš ï¸  Texto condensado: {razao:.0%}")
                logger.info(f"   âœ… Esperado no modo {MODO_NOME}")
            
            if razao < 0.30 or (FIDELIDADE_MODE and razao < THRESHOLD_CRITICO):
                resposta = input("\n   Continuar? (s/n): ").strip().lower()
                if resposta != 's':
                    logger.info("Cancelado.")
                    sys.exit(1)
        
        if input_file:
            delete_checkpoint(input_file)
        
        return texto_final, cache, client
    
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  Interrompido. Checkpoint salvo.")
        sys.exit(1)

# =============================================================================
# EXPORTAÃ‡ÃƒO WORD
# =============================================================================

def create_toc(doc):
    """Adiciona SumÃ¡rio (Table of Contents) nativo do Word"""
    paragraph = doc.add_paragraph()
    run = paragraph.add_run()
    
    fldChar = OxmlElement('w:fldChar')
    fldChar.set(qn('w:fldCharType'), 'begin')
    run._r.append(fldChar)
    
    instrText = OxmlElement('w:instrText')
    instrText.set(qn('xml:space'), 'preserve')
    instrText.text = 'TOC \\o "1-3" \\h \\z \\u'
    run._r.append(instrText)
    
    fldChar = OxmlElement('w:fldChar')
    fldChar.set(qn('w:fldCharType'), 'separate')
    run._r.append(fldChar)
    
    fldChar = OxmlElement('w:fldChar')
    fldChar.set(qn('w:fldCharType'), 'end')
    run._r.append(fldChar)

def _format_inline_markdown(paragraph, text):
    """Formata markdown inline (negrito, itÃ¡lico, cÃ³digo)"""
    paragraph.clear()
    
    pattern = r'(\*\*\*(.+?)\*\*\*|\*\*(.+?)\*\*|\*(.+?)\*|`(.+?)`)'
    
    last_end = 0
    for match in re.finditer(pattern, text):
        if match.start() > last_end:
            paragraph.add_run(text[last_end:match.start()])
        
        if match.group(0).startswith('***'):
            run = paragraph.add_run(match.group(2))
            run.bold = True
            run.italic = True
        elif match.group(0).startswith('**'):
            run = paragraph.add_run(match.group(3))
            run.bold = True
        elif match.group(0).startswith('*'):
            run = paragraph.add_run(match.group(4))
            run.italic = True
        elif match.group(0).startswith('`'):
            run = paragraph.add_run(match.group(5))
            run.font.name = 'Courier New'
            run.font.size = Pt(9)
            run.font.color.rgb = RGBColor(200, 0, 0)
        
        last_end = match.end()
    
    if last_end < len(text):
        paragraph.add_run(text[last_end:])

def _add_table_to_doc(doc, rows):
    """Adiciona tabela formatada ao documento Word"""
    if len(rows) < 2:
        return
    
    max_cols = max(len(row) for row in rows)
    if max_cols == 0:
        return
    
    table = doc.add_table(rows=len(rows), cols=max_cols)
    table.style = 'Light Grid Accent 1'
    
    for i, row_data in enumerate(rows):
        for j in range(max_cols):
            cell = table.rows[i].cells[j]
            cell_text = row_data[j] if j < len(row_data) else ""
            
            # Formata markdown dentro da cÃ©lula
            _format_inline_markdown(cell.paragraphs[0], cell_text)
            
            # Alinhamento padrÃ£o: Esquerda
            cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.LEFT

            if i == 0:
                for paragraph in cell.paragraphs:
                    for run in paragraph.runs:
                        run.font.bold = True
                        run.font.color.rgb = RGBColor(255, 255, 255)
                    paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
                
                shading_elm = OxmlElement('w:shd')
                shading_elm.set(qn('w:fill'), '0066CC')
                cell._element.get_or_add_tcPr().append(shading_elm)
            else:
                 # ConteÃºdo normal da tabela: Esquerda
                 for paragraph in cell.paragraphs:
                    paragraph.alignment = WD_ALIGN_PARAGRAPH.LEFT
    
    table.alignment = WD_TABLE_ALIGNMENT.CENTER

def save_as_word(formatted_text, video_name, output_file):
    """Salva markdown formatado como documento Word (.docx)"""
    if not DOCX_AVAILABLE:
        logger.warning("python-docx nÃ£o disponÃ­vel. Salvando apenas Markdown.")
        return None
    
    logger.info("ğŸ“„ Gerando documento Word...")
    
    doc = Document()
    
    # Margens
    section = doc.sections[0]
    section.top_margin = Inches(1)
    section.bottom_margin = Inches(1)
    section.left_margin = Inches(1.25)
    section.right_margin = Inches(1.25)
    
    # TÃ­tulo principal
    title = doc.add_heading(video_name, level=0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    for run in title.runs:
        run.font.size = Pt(20)
        run.font.color.rgb = RGBColor(0, 51, 102)
    
    # Data de geraÃ§Ã£o
    date_para = doc.add_paragraph()
    date_run = date_para.add_run(f"Gerado em {time.strftime('%d/%m/%Y Ã s %H:%M')} - Modo: {MODO_NOME}")
    date_run.italic = True
    date_run.font.size = Pt(10)
    date_run.font.color.rgb = RGBColor(128, 128, 128)
    date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    doc.add_paragraph()
    
    # SumÃ¡rio
    doc.add_heading('SumÃ¡rio', level=1)
    create_toc(doc)
    doc.add_page_break()
    
    # Processa conteÃºdo markdown
    lines = formatted_text.split('\n')
    i = 0
    in_table = False
    table_rows = []
    
    while i < len(lines):
        line = lines[i].strip()
        
        if not line:
            i += 1
            continue
        
        # Tabelas
        if '|' in line and not in_table:
            in_table = True
            table_rows = []
        
        if in_table:
            # Ignora linha separadora (ex: |---| ou | :--- | :--- | :--- |)
            is_separator = re.match(r'^\s*\|[\s:|-]+\|[\s:|-]*$', line)
            
            if '|' in line and not is_separator:
                table_rows.append([cell.strip() for cell in line.split('|')[1:-1]])
            
            if '|' not in line or i == len(lines) - 1:
                if len(table_rows) > 0:
                    _add_table_to_doc(doc, table_rows)
                in_table = False
                table_rows = []
                if '|' not in line:
                    continue
            i += 1
            continue
        
        # Headings
        if line.startswith('##### '):
            doc.add_heading(line[6:], level=5)
        elif line.startswith('#### '):
            doc.add_heading(line[5:], level=4)
        elif line.startswith('### '):
            doc.add_heading(line[4:], level=3)
        elif line.startswith('## '):
            doc.add_heading(line[3:], level=2)
        elif line.startswith('# ') and line != f"# {video_name}":
            doc.add_heading(line[2:], level=1)
        # Separadores
        elif line.strip() in ['---', '***', '___']:
            p = doc.add_paragraph()
            p.add_run('_' * 80).font.color.rgb = RGBColor(192, 192, 192)
        # Quotes
        elif line.startswith('>'):
            p = doc.add_paragraph(style='Quote')
            p.paragraph_format.left_indent = Cm(4.0)  # Recuo de 4 cm da margem esquerda
            p.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
            
            run = p.add_run(line[1:].strip())
            run.italic = True
            run.font.size = Pt(10) # Geralmente citaÃ§Ãµes longas tÃªm fonte menor
        # Listas nÃ£o-ordenadas
        elif line.startswith('- ') or line.startswith('* '):
            p = doc.add_paragraph(line[2:], style='List Bullet')
            # ForÃ§ar recuo de 1,5cm
            p.paragraph_format.left_indent = Cm(1.5)
            p.paragraph_format.first_line_indent = Cm(-0.63)
            
            _format_inline_markdown(p, line[2:])
            
        # Listas numeradas
        elif len(line) > 2 and line[0].isdigit() and line[1:3] in ['. ', ') ']:
            p = doc.add_paragraph(style='Normal')
            # Recuo padronizado de 1,5cm
            p.paragraph_format.left_indent = Cm(1.5)
            p.paragraph_format.first_line_indent = Cm(-0.63) # MantÃ©m hanging indent para o nÃºmero
            _format_inline_markdown(p, line)
            
        # ParÃ¡grafo normal
        else:
            p = doc.add_paragraph()
            # EspaÃ§amento 1.5 (Line Spacing = 1.5)
            p.paragraph_format.line_spacing_rule = WD_LINE_SPACING.ONE_POINT_FIVE
            # EspaÃ§amento antes e apÃ³s parÃ¡grafo (6pt)
            p.paragraph_format.space_before = Pt(6)
            p.paragraph_format.space_after = Pt(6) 
            
            # Recuo de 1Âª linha (1cm)
            p.paragraph_format.first_line_indent = Cm(1.0)
            
            # Justificado
            p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
            
            _format_inline_markdown(p, line)
            
            # ForÃ§ar fonte 12pt para texto normal
            for run in p.runs:
                run.font.size = Pt(12)
        
        i += 1
    
    # Flush final (caso o arquivo termine com tabela seguida de linhas em branco)
    if in_table and len(table_rows) > 0:
        _add_table_to_doc(doc, table_rows)
        
    doc.save(output_file)
    logger.info(f"âœ… Word salvo: {output_file}")
    return output_file

def salvar_resultado(conteudo, arquivo):
    with open(arquivo, 'w', encoding='utf-8') as f:
        f.write(conteudo)
    logger.info(f"âœ… Markdown salvo: {arquivo}")

# =============================================================================
# MAIN
# =============================================================================

def main():
    if len(sys.argv) < 2 or '--help' in sys.argv:
        print("=" * 70)
        print("FORMATADOR v2.7 - ANTI-DUPLICAÃ‡ÃƒO")
        print("=" * 70)
        print("\nUso: python format_transcription_gemini.py <entrada.txt> [saida]")
        print("\nOpÃ§Ãµes:")
        print("  --dry-run    Valida chunks e mostra preview")
        print("  --help       Mostra esta mensagem")
        print("\nğŸ›¡ï¸  CORREÃ‡Ã•ES v2.7:")
        print("  â€¢ DetecÃ§Ã£o agressiva de seÃ§Ãµes duplicadas")
        print("  â€¢ ValidaÃ§Ã£o rigorosa de chunks sequenciais")
        print("  â€¢ Delimitadores de contexto mais visÃ­veis")
        print("  â€¢ Post-processing em mÃºltiplas passadas")
        print("  â€¢ Cache desabilitado (debug)")
        print("\nDependÃªncias:")
        print("  pip install google-genai python-docx tqdm")
        sys.exit(1)
    
    arquivo_entrada = sys.argv[1]
    
    if len(sys.argv) > 2 and not sys.argv[2].startswith('--'):
        arquivo_saida = sys.argv[2]
    else:
        base = arquivo_entrada.replace('.txt', '_formatada_v2.7')
        arquivo_saida = f"{base}.md"
    
    video_name = Path(arquivo_entrada).stem
    
    logger.info("=" * 60)
    logger.info(f"FORMATADOR v2.7 ANTI-DUPLICAÃ‡ÃƒO - Modo {MODO_NOME}")
    logger.info("=" * 60)
    logger.info(f"ğŸ“‚ Entrada: {arquivo_entrada}")
    logger.info(f"ğŸ“ SaÃ­da: {arquivo_saida}")
    
    transcricao = carregar_transcricao(arquivo_entrada)
    
    try:
        resultado, cache, client = formatar_transcricao(transcricao, input_file=arquivo_entrada)
    except Exception as e:
        logger.error(f"\nâŒ Falha: {e}", exc_info=True)
        sys.exit(1)
    
    salvar_resultado(resultado, arquivo_saida)
    
    # v2.8: VerificaÃ§Ã£o de cobertura e duplicaÃ§Ãµes
    verificar_cobertura(transcricao, resultado, arquivo_saida)

    # v2.9: Auditoria Legal PÃ³s-Processamento
    if AUDIT_AVAILABLE:
        report_path = arquivo_saida.replace('.md', '_RELATORIO_AUDITORIA.md')
        auditar_consistencia_legal(client, resultado, report_path)

    if DOCX_AVAILABLE:
        arquivo_docx = arquivo_saida.replace('.md', '.docx')
        save_as_word(resultado, video_name, arquivo_docx)
    
    tokens_in = len(transcricao) // 4
    tokens_out = len(resultado) // 4
    custo = (tokens_in * PRECO_INPUT_SEM_CACHE + tokens_out * PRECO_OUTPUT) / 1_000_000
    
    logger.info("=" * 60)
    logger.info(f"ğŸ’° Custo: ${custo:.4f} USD")
    logger.info(f"âœ¨ ConcluÃ­do! (v2.8 com verificaÃ§Ã£o)")
    logger.info("=" * 60)

if __name__ == "__main__":
    main()
