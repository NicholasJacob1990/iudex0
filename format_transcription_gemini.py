#!/usr/bin/env python3
"""
Script v2.11 - FormataÃ§Ã£o de TranscriÃ§Ãµes com Gemini 3 Flash Preview
MELHORIAS: Smart Stitching Anti-DuplicaÃ§Ã£o CirÃºrgica

MudanÃ§as v2.10 vs v2.9:
- remover_eco_do_contexto: Remove eco do contexto na resposta da API
- titulos_sao_similares: Fuzzy matching para detecÃ§Ã£o de tÃ­tulos duplicados
- limpar_inicio_redundante: Limpeza na junÃ§Ã£o de chunks
- InjeÃ§Ã£o dinÃ¢mica de ultimo_titulo no prompt para prevenir repetiÃ§Ã£o

Uso: python format_transcription_gemini.py <entrada.txt> [saida]
"""


import os
import sys
import time
import random
import json
import re
import threading
from pathlib import Path
from time import sleep
from difflib import SequenceMatcher
import hashlib

try:
    from audit_module import auditar_consistencia_legal
    AUDIT_AVAILABLE = True
except ImportError:
    AUDIT_AVAILABLE = False
    logger = logging.getLogger(__name__) if 'logging' in locals() else None
    if logger: logger.warning("âš ï¸  MÃ³dulo de auditoria nÃ£o encontrado.")
    else: print("âš ï¸  MÃ³dulo de auditoria nÃ£o encontrado.")

try:
    from google import genai
    from google.genai import types
except ImportError:
    print("âŒ Erro: Biblioteca google-genai nÃ£o instalada.")
    print("   Instale com: pip install google-genai")
    sys.exit(1)

try:
    from tqdm import tqdm
except ImportError:
    print("âš ï¸ Aviso: tqdm nÃ£o instalado. Progress bar desabilitada.")
    tqdm = None

try:
    from docx import Document
    from docx.shared import Pt, RGBColor, Inches, Cm
    from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
    from docx.enum.table import WD_TABLE_ALIGNMENT
    from docx.oxml.ns import qn
    from docx.oxml import OxmlElement
    DOCX_AVAILABLE = True
except ImportError:
    print("âš ï¸ Aviso: python-docx nÃ£o disponÃ­vel. SaÃ­da em Word desabilitada.")
    DOCX_AVAILABLE = False

# =============================================================================
# SETUP CREDENCIAIS (v2.11)
# =============================================================================
CREDENTIALS_PATH = "/Users/nicholasjacob/Documents/Aplicativos/Transcritor/vertex_credentials.json"
if os.path.exists(CREDENTIALS_PATH) and not os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = CREDENTIALS_PATH
    # print(f"ğŸ”‘ Credenciais carregadas: {CREDENTIALS_PATH}")

import logging

# =============================================================================
# CONFIGURAÃ‡Ã•ES v2.7
# =============================================================================

CHARS_POR_PARTE = 15000
CONTEXTO_ESTILO = 3000
OUTPUT_TOKEN_LIMIT = 32000
CACHE_TTL = '7200s'
MIN_CHARS_PARA_CACHE = 150000
MAX_RETRIES = 3
MAX_RPM = 60 
# v2.7: FORÃ‡AR delimitadores visÃ­veis para evitar confusÃ£o
USE_FANCY_DELIMITERS = True

# Modelo Gemini (centralizado para fÃ¡cil atualizaÃ§Ã£o)
GEMINI_MODEL = 'gemini-3-flash-preview'

# PreÃ§os API Gemini 3 Flash Preview (Estimativa)
PRECO_INPUT_SEM_CACHE = 0.50
PRECO_INPUT_COM_CACHE = 0.05  # Estimado a 10% do input (manter proporÃ§Ã£o anterior)
PRECO_OUTPUT = 3.00

# =============================================================================
# LOGGING
# =============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S',
    handlers=[
        logging.FileHandler('formatacao.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# =============================================================================
# MÃ‰TRICAS DE EXECUÃ‡ÃƒO (v2.10)
# =============================================================================
class MetricsCollector:
    """Coleta e reporta mÃ©tricas de execuÃ§Ã£o para otimizaÃ§Ã£o de custos."""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.api_calls = 0
        self.gemini_calls = 0
        self.openai_calls = 0
        self.cache_hits = 0
        self.adaptive_splits = 0
        self.total_prompt_tokens = 0
        self.total_cached_tokens = 0
        self.total_completion_tokens = 0
        self.total_time_seconds = 0.0
        self.chunks_processed = 0
        self.start_time = None
    
    def start_timer(self):
        self.start_time = time.time()
    
    def stop_timer(self):
        if self.start_time:
            self.total_time_seconds = time.time() - self.start_time
    
    def record_api_call(self, prompt_tokens=0, completion_tokens=0, cached_tokens=0, provider='gemini'):
        # Garantir que sÃ£o inteiros (v2.17.1: fix NoneType)
        prompt_tokens = prompt_tokens or 0
        completion_tokens = completion_tokens or 0
        cached_tokens = cached_tokens or 0
        
        self.api_calls += 1
        if provider == 'gemini':
            self.gemini_calls += 1
        elif provider == 'openai':
            self.openai_calls += 1
            
        # Se houve tokens cacheados, considera cache hit
        if cached_tokens > 0:
            self.cache_hits += 1
            
        self.total_prompt_tokens += (prompt_tokens - cached_tokens)
        self.total_cached_tokens += cached_tokens
        self.total_completion_tokens += completion_tokens
    
    def record_adaptive_split(self):
        self.adaptive_splits += 1

    def record_cache_hit(self):
        self.cache_hits += 1
    
    def get_cost(self):
        """Calcula custo estimado (Gemini 3 Flash Preview)."""
        # PreÃ§os por 1M tokens ($0.50 Input / $3.00 Output)
        input_price = 0.50       # USD (Standard)
        cached_price = 0.05      # USD (Cached - Est. 10%)
        output_price = 3.00      # USD (Output)
        
        cost = (
            (self.total_prompt_tokens * input_price) + 
            (self.total_cached_tokens * cached_price) +
            (self.total_completion_tokens * output_price)
        ) / 1_000_000
        return cost
    
    def get_report(self):
        avg_time = self.total_time_seconds / self.api_calls if self.api_calls > 0 else 0
        cost = self.get_cost()
        
        return f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š MÃ‰TRICAS DE EXECUÃ‡ÃƒO (v2.10)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   ğŸ“¡ Total de Chamadas API: {self.api_calls}
      - Gemini: {self.gemini_calls}
      - OpenAI: {self.openai_calls}
      - Cache Hits: {self.cache_hits}
   âœ‚ï¸ DivisÃµes Adaptativas: {self.adaptive_splits}
   ğŸ¯ Tokens Usados:
      - Prompt (Regular): {self.total_prompt_tokens:,}
      - Prompt (Cached):  {self.total_cached_tokens:,}
      - Completion:       {self.total_completion_tokens:,}
      - Total Geral:      {self.total_prompt_tokens + self.total_cached_tokens + self.total_completion_tokens:,}
   â±ï¸ Tempo Total: {self.total_time_seconds:.1f}s (mÃ©dia: {avg_time:.2f}s/chamada)
   ğŸ’° Custo Real: ${cost:.6f} USD
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

# Global metrics instance
metrics = MetricsCollector()

# =============================================================================
# PROMPTS v2.7 - INSTRUÃ‡Ã•ES ANTI-DUPLICAÃ‡ÃƒO REFORÃ‡ADAS
# =============================================================================

PROMPT_FIDELIDADE = """# DIRETRIZES DE FORMATAÃ‡ÃƒO E REVISÃƒO

## PAPEL
VOCÃŠ Ã‰ UM EXCELENTISSIMO REDATOR TÃ‰CNICO E DIDÃTICO

 **Tom:** didÃ¡tico, como o professor explicando em aula.  
- **Pessoa:** manter a pessoa original da transcriÃ§Ã£o (1Âª pessoa se for assim na fala).  
- **Estilo:** texto corrido, com parÃ¡grafos curtos, sem â€œinventarâ€ doutrina nova.  
- **Objetivo:** reproduzir a aula em forma escrita, clara e organizada, mas ainda com a â€œvozâ€ do professor.


## OBJETIVO
-Transformar a transcriÃ§Ã£o em um texto claro, legÃ­vel e coeso, em PortuguÃªs PadrÃ£o, MANTENDO A FIDELIDADE TOTAL ao conteÃºdo original.
-- **Tamanho:** a saÃ­da deve ficar **entre 95% e 115%** do tamanho do trecho de entrada (salvo remoÃ§Ã£o de muletas e logÃ­stica).

## ğŸš« O QUE NÃƒO FAZER
1. **NÃƒO RESUMA**. O tamanho do texto de saÃ­da deve ser prÃ³ximo ao de entrada.
2. **NÃƒO OMITA** informaÃ§Ãµes, exemplos, casos concretos ou explicaÃ§Ãµes.
3. **NÃƒO ALTERE** o significado ou a sequÃªncia das ideias e das falas do professor.
4. **NÃƒO CRIE MUITOS BULLET POINTS** ou frases curtas demasiadamente. PREFIRA UM FORMATO DE MANUAL DIDÃTICO, nÃ£o checklist.
5. **NÃƒO USE NEGRITOS EM EXCESSO**. Use apenas para conceitos-chave realmente importantes.
6. **NÃƒO RESUMA e NÃƒO OMITA**. VocÃª pode reescrever frases em portuguÃªs padrÃ£o para melhorar a fluidez, preservar a ordem, os detalhes tÃ©cnicos e os exemplos, mas **REMOVA** pausas excessivas e hesitaÃ§Ãµes.


## âŒ PRESERVE OBRIGATORIAMENTE
- **NÃšMEROS EXATOS**: Artigos, Leis, SÃºmulas, Julgados (REDI/Informativos). **NUNCA OMITA NÃšMEROS DE LEIS OU SÃšMULAS**.
- **TODO o conteÃºdo tÃ©cnico**: exemplos, explicaÃ§Ãµes, analogias, raciocÃ­nios
- **ReferÃªncias**: leis, artigos, jurisprudÃªncia, autores, casos citados
- **ÃŠnfases intencionais**: "isso Ã© MUITO importante" (mantenha o destaque)
- **ObservaÃ§Ãµes pedagÃ³gicas**: "cuidado com isso!", "ponto polÃªmico"

## ğŸ¯ PRESERVAÃ‡ÃƒO ESPECIAL: DICAS DE PROVA E EXAMINADORES (CRÃTICO)
Aulas presenciais frequentemente contÃªm informaÃ§Ãµes valiosas sobre:
1. **ReferÃªncias a Examinadores**: Nomes de examinadores de concursos, suas preferÃªncias, posicionamentos ou temas favoritos. **PRESERVE INTEGRALMENTE**.
   - Exemplo: "O examinador Fulano costuma cobrar..." â†’ MANTER
   - Exemplo: "Esse tema foi cobrado pelo professor X na prova..." â†’ MANTER
2. **Dicas de Prova**: OrientaÃ§Ãµes sobre o que costuma cair em provas, pegadinhas comuns, temas recorrentes.
   - Exemplo: "Isso cai muito em prova..." â†’ MANTER
   - Exemplo: "AtenÃ§Ã£o: essa Ã© uma pegadinha clÃ¡ssica..." â†’ MANTER
3. **EstratÃ©gias de Estudo**: SugestÃµes do professor sobre priorizaÃ§Ã£o, macetes, formas de memorizaÃ§Ã£o.
   - Exemplo: "Gravem isso: na dÃºvida, marquem..." â†’ MANTER
   - Exemplo: "Para PGM, foquem em..." â†’ MANTER
4. **Casos PrÃ¡ticos e HistÃ³rias Reais**: Exemplos de situaÃ§Ãµes reais, casos julgados, histÃ³rias ilustrativas.
   - **NUNCA RESUMA** histÃ³rias ou exemplos prÃ¡ticos. Preserve na Ã­ntegra.

> âš ï¸ **ESSAS INFORMAÃ‡Ã•ES SÃƒO O DIFERENCIAL DE UMA AULA AO VIVO.** Sua omissÃ£o representa perda irreparÃ¡vel de valor didÃ¡tico.


## âœ… DIRETRIZES DE ESTILO
1. **CorreÃ§Ã£o Gramatical**: Corrija erros gramaticais, regÃªncias, ortogrÃ¡ficos e de pontuaÃ§Ã£o, tornando o texto gramaticalmente correto e claro.
2. **Limpeza Profunda:**
   - **REMOVA** marcadores de oralidade: "nÃ©", "tÃ¡?", "entende?", "veja bem", "tipo assim".
   - **REMOVA** interaÃ§Ãµes diretas com a turma/alunos e logÃ­stica: "Isso mesmo", "A colega perguntou", "JÃ¡ estÃ£o me vendo?", "EstÃ£o ouvindo?", "Como ele disse ali atrÃ¡s".
   - **REMOVA** redundÃ¢ncias: "subir para cima", "criaÃ§Ã£o nova".
   - **TRANSFORME** perguntas retÃ³ricas em afirmaÃ§Ãµes quando possÃ­vel (ex: "E o que isso significa?" -> "Isso significa que...").
3. **CoesÃ£o**: Utilize conectivos necessÃ¡rios para tornar o texto mais fluido. Aplique a pontuaÃ§Ã£o devida para deixar o texto coeso e coerente.
4. **Legibilidade**:
   - **USE TEXTO CORRIDO NA MEDIDA DO POSSÃVEL**
   - Utilize formataÃ§Ã£o e estrutura com parÃ¡grafos bem definidos, facilitando a leitura e compreensÃ£o
   - Evite parÃ¡grafos longos (mÃ¡ximo 3-4 linhas visuais)
   - Evite blocos de texto maciÃ§os, quebre os blocos de texto em parÃ¡grafos menores
   - Seja didÃ¡tico sem perder detalhes e conteÃºdo
5. **Linguagem**: Ajuste a linguagem coloquial para um portuguÃªs padrÃ£o, mantendo o significado original.
6. **CitaÃ§Ãµes**: Use itÃ¡lico para citaÃ§Ãµes curtas e recuo em itÃ¡lico para citaÃ§Ãµes longas.
7. -Use **negrito** para destacar conceitos-chave (sem exagero).
8. **FormataÃ§Ã£o DidÃ¡tica** (use com moderaÃ§Ã£o, sem excesso):
   - **Bullet points** para enumerar elementos, requisitos ou caracterÃ­sticas
   - **Listas numeradas** (1., 2., 3.) para enumerar itens, etapas, correntes ou exemplos
   - **Marcadores relacionais** como "â†’" para indicar relaÃ§Ãµes, transiÃ§Ãµes, ou consequÃªncias lÃ³gicas
   - Exemplo: "Processo entre A e B â†’ prova usada contra C"
9. **QuestÃµes e ExercÃ­cios**:
   - Se o professor ditar uma questÃ£o, exercÃ­cio ou caso hipotÃ©tico para julgar, **ILHE-O** em um bloco de citaÃ§Ã£o:
   > **QuestÃ£o:** O prazo para agravo de petiÃ§Ã£o Ã© de...
   - Separe claramente o enunciado da questÃ£o da explicaÃ§Ã£o/gabarito subsequente.
10. **Destaques com Emojis** (use com moderaÃ§Ã£o para facilitar escaneamento visual):
   - ğŸ’¡ **Dica de Prova** ou **ObservaÃ§Ã£o PedagÃ³gica**: Quando o professor der uma dica especÃ­fica para provas ou concursos.
   - âš ï¸ **AtenÃ§Ã£o** ou **Cuidado**: Para alertas, pegadinhas ou pontos polÃªmicos.
   - ğŸ“Œ **Ponto Importante**: Para conceitos-chave que merecem destaque especial.
   - Exemplo de uso: `> ğŸ’¡ **Dica de Prova:** Esse tema caiu 3 vezes na PGM-Rio.`

## ğŸ“ ESTRUTURA
- Mantenha a sequÃªncia exata das falas.
- Use TÃ­tulos Markdown (##, ###) para organizar os tÃ³picos, se identificÃ¡veis a partir do contexto.

## ğŸš« TÃTULOS E SUBTÃ“PICOS (IMPORTANTE)
- **NÃƒO crÃ­e subtÃ³picos para frases soltas.**
- Use tÃ­tulos (##, ###) **APENAS** para mudanÃ§as reais de assunto.
- Se uma frase parece um tÃ­tulo mas nÃ£o inicia uma nova seÃ§Ã£o, mantenha como texto normal e use **negrito** se necessÃ¡rio.

## ğŸ“Š TABELA DE SÃNTESE (FLEXÃVEL)
Ao final de cada **bloco temÃ¡tico relevante** (ou capÃ­tulo), produza uma tabela de sÃ­ntese completa (modelo flexÃ­vel).
Exemplo de estrutura (adapte conforme o conteÃºdo):

```
### ğŸ“‹ Tabela de sÃ­ntese do tÃ³pico

| Conceito/Instituto | DefiniÃ§Ã£o (conforme a aula) | Fundamento Legal (se citado) | ObservaÃ§Ãµes (alertas/exceÃ§Ãµes/juris) |
| :--- | :--- | :--- | :--- |
| ...  | ...  | Art. X, Lei Y / "â€”" | ... |
```

***REGRAS CRÃTICAS PARA TABELAS:**
1. **Limite de conteÃºdo por cÃ©lula:** mÃ¡ximo ~50 palavras. Se precisar de mais, divida em mÃºltiplas linhas da tabela
2. **PROIBIDO usar blocos de cÃ³digo (```) dentro de cÃ©lulas** - use texto simples
3. **NUNCA deixe o tÃ­tulo "ğŸ“‹ Resumo do TÃ³pico" sozinho** - se nÃ£o houver dados para tabela, NÃƒO escreva o tÃ­tulo
4. **POSICIONAMENTO ESTRITO:**
   - A tabela deve vir **APENAS AO FINAL** de um bloco concluÃ­do.
   - **PROIBIDO** inserir tabela no meio de uma frase ou interrompendo uma explicaÃ§Ã£o.
   - Se o texto continuar sobre o mesmo assunto, **termine o texto primeiro** e coloque a tabela depois.


## âš ï¸ REGRA ANTI-DUPLICAÃ‡ÃƒO (CRÃTICA)
Se vocÃª receber um CONTEXTO de referÃªncia (entre delimitadores â”â”â”):
- Este contexto Ã© APENAS para vocÃª manter o mesmo ESTILO DE ESCRITA
- **NUNCA formate novamente esse contexto**
- **NUNCA inclua esse contexto na sua resposta**
- **NUNCA repita informaÃ§Ãµes que jÃ¡ estÃ£o no contexto**
- Formate APENAS o texto que estÃ¡ entre as tags <texto_para_formatar>
- Se o texto_para_formatar comeÃ§ar com algo similar ao fim do contexto, NÃƒO duplique, apenas continue naturalmente
"""

PROMPT_APOSTILA = """# DIRETRIZES DE REDAÃ‡ÃƒO: MANUAL JURÃDICO DIDÃTICO (MODO APOSTILA)
## PAPEL
VOCÃŠ Ã‰ UM EXCELENTISSIMO REDATOR TÃ‰CNICO E DIDÃTICO
- **Tom:** doutrinÃ¡rio, impessoal, estilo manual de Direito.  
- **Pessoa:** 3Âª pessoa ou construÃ§Ãµes impessoais (â€œobserva-seâ€, â€œentende-seâ€).  
- **Estilo:** prosa mais densa, porÃ©m com parÃ¡grafos curtos e didÃ¡ticos.  
- **Objetivo:** transformar o conteÃºdo da aula em texto de apostila/livro, sem alterar o conteÃºdo e sem inventar informaÃ§Ãµes.


## OBJETIVO
Transformar a transcriÃ§Ã£o em um texto claro, legÃ­vel e coeso, em PortuguÃªs PadrÃ£o, em formato de apostila/manual didÃ¡tico

## ğŸš« O QUE NÃƒO FAZER
1. **NÃƒO RESUMA**. O tamanho do texto de saÃ­da deve ser prÃ³ximo ao de entrada.
2. **NÃƒO OMITA** informaÃ§Ãµes, exemplos, casos concretos ou explicaÃ§Ãµes.
3. **NÃƒO ALTERE** o significado ou a sequÃªncia das ideias 


âŒ PRESERVE obrigatoriamente:
- **NÃšMEROS EXATOS**: Artigos, Leis, Artigos SÃºmulas, Julgados (REDI/Informativos). **NUNCA OMITA NÃšMEROS DE LEIS OU SÃšMULAS**.
- **TODO o conteÃºdo tÃ©cnico**: exemplos, explicaÃ§Ãµes, analogias, raciocÃ­nios
- **ReferÃªncias**: leis, artigos, jurisprudÃªncia, autores, casos citados
- **ÃŠnfases intencionais**: "isso Ã© MUITO importante" (mantenha o destaque)
- **ObservaÃ§Ãµes pedagÃ³gicas**: "cuidado com isso!", "ponto polÃªmico"

## ğŸ¯ PRESERVAÃ‡ÃƒO ESPECIAL: DICAS DE PROVA E EXAMINADORES (CRÃTICO)
Aulas presenciais frequentemente contÃªm informaÃ§Ãµes valiosas sobre:
1. **ReferÃªncias a Examinadores**: Nomes de examinadores de concursos, suas preferÃªncias, posicionamentos ou temas favoritos. **PRESERVE INTEGRALMENTE**.
   - Exemplo: "O examinador Fulano costuma cobrar..." â†’ MANTER
   - Exemplo: "Esse tema foi cobrado pelo professor X na prova..." â†’ MANTER
2. **Dicas de Prova**: OrientaÃ§Ãµes sobre o que costuma cair em provas, pegadinhas comuns, temas recorrentes.
   - Exemplo: "Isso cai muito em prova..." â†’ MANTER
   - Exemplo: "AtenÃ§Ã£o: essa Ã© uma pegadinha clÃ¡ssica..." â†’ MANTER
3. **EstratÃ©gias de Estudo**: SugestÃµes do professor sobre priorizaÃ§Ã£o, macetes, formas de memorizaÃ§Ã£o.
   - Exemplo: "Gravem isso: na dÃºvida, marquem..." â†’ MANTER
   - Exemplo: "Para PGM, foquem em..." â†’ MANTER
4. **Casos PrÃ¡ticos e HistÃ³rias Reais**: Exemplos de situaÃ§Ãµes reais, casos julgados, histÃ³rias ilustrativas.
   - **NUNCA RESUMA** histÃ³rias ou exemplos prÃ¡ticos. Preserve na Ã­ntegra.

> âš ï¸ **ESSAS INFORMAÃ‡Ã•ES SÃƒO O DIFERENCIAL DE UMA AULA AO VIVO.** Sua omissÃ£o representa perda irreparÃ¡vel de valor didÃ¡tico.


## âœ… DIRETRIZES DE ESTILO
1. **CorreÃ§Ã£o Gramatical**: Ajuste a linguagem coloquial para o padrÃ£o culto.
2. **Limpeza**: Remova gÃ­rias, cacoetes ("nÃ©", "tipo assim", "entÃ£o") e vÃ­cios de oralidade.
3. **CoesÃ£o**: Use conectivos e pontuaÃ§Ã£o adequada para tornar o texto fluido.
4. **Legibilidade**:
   - Use parÃ¡grafos bem definidos e curtos (mÃ¡ximo 3-4 linhas visuais).
    - Evite blocos de texto maciÃ§os, quebre os blocos de texto em parÃ¡grafos menores
   - Use **negrito** para destacar conceitos-chave (sem exagero).
5. **FormataÃ§Ã£o DidÃ¡tica** (use com moderaÃ§Ã£o, sem excesso):
   - **Bullet points** para enumerar elementos, requisitos ou caracterÃ­sticas
   - **Listas numeradas** (1., 2., 3.) para enumerar itens, etapas, correntes doutrinÃ¡rias ou exemplos
   - **Marcadores relacionais** como "â†’" para indicar relaÃ§Ãµes, transiÃ§Ãµes, ou consequÃªncias lÃ³gicas
   - Exemplo: "Processo entre Pedro e JosÃ© â†’ prova usada contra Ana"
6. **Destaques com Emojis** (use com moderaÃ§Ã£o para facilitar escaneamento visual):
   - ğŸ’¡ **Dica de Prova** ou **ObservaÃ§Ã£o PedagÃ³gica**: Quando houver uma dica especÃ­fica para provas ou concursos.
   - âš ï¸ **AtenÃ§Ã£o** ou **Cuidado**: Para alertas, pegadinhas ou pontos polÃªmicos.
   - ğŸ“Œ **Ponto Importante**: Para conceitos-chave que merecem destaque especial.
   - Exemplo de uso: `> ğŸ’¡ **Dica de Prova:** Esse tema caiu 3 vezes na PGM-Rio.`

## ğŸ“ ESTRUTURA
- Mantenha a sequÃªncia exata das falas.
- Use TÃ­tulos Markdown (##, ###) para organizar os tÃ³picos, se identificÃ¡veis a partir do contexto.

## ğŸ“Š TABELA DE SÃNTESE (FLEXÃVEL)
Ao final de cada **bloco temÃ¡tico relevante** (ou capÃ­tulo), produza uma tabela de sÃ­ntese (modelo flexÃ­vel).
Exemplo de estrutura (adapte conforme o conteÃºdo):

```
### ğŸ“‹ Tabela de sÃ­ntese do tÃ³pico

| Conceito/Instituto | DefiniÃ§Ã£o (conforme a aula) | Fundamento Legal (se citado) | ObservaÃ§Ãµes (alertas/exceÃ§Ãµes/juris) |
| :--- | :--- | :--- | :--- |
| ...  | ...  | Art. X, Lei Y / "â€”" | ... |
```

***REGRAS CRÃTICAS PARA TABELAS:**
1. **Limite de conteÃºdo por cÃ©lula:** mÃ¡ximo ~50 palavras. Se precisar de mais, divida em mÃºltiplas linhas da tabela
2. **PROIBIDO usar blocos de cÃ³digo (```) dentro de cÃ©lulas** - use texto simples
3. **NUNCA deixe o tÃ­tulo "ğŸ“‹ Resumo do TÃ³pico" sozinho** - se nÃ£o houver dados para tabela, NÃƒO escreva o tÃ­tulo
4. **POSICIONAMENTO:** A tabela deve vir **APENAS AO FINAL** da explicaÃ§Ã£o completa dos tÃ³picos ou blocos temÃ¡ticos relevantes 
   - **NUNCA** insira a tabela no meio de uma explicaÃ§Ã£o.
   - **NUNCA** resuma um tÃ³pico que vocÃª ainda nÃ£o acabou de explicar no texto.
   - A tabela deve ser o **fechamento** lÃ³gico da seÃ§Ã£o, antes de iniciar um novo tÃ­tulo ou tÃ³pico (## ou ###).

## âš ï¸ REGRA ANTI-DUPLICAÃ‡ÃƒO (CRÃTICA)
Se vocÃª receber um CONTEXTO de referÃªncia (entre delimitadores â”â”â”):
- Este contexto Ã© APENAS para vocÃª manter o mesmo ESTILO DE ESCRITA
- **NUNCA formate novamente esse contexto**
- **NUNCA inclua esse contexto na sua resposta**
- **NUNCA repita informaÃ§Ãµes que jÃ¡ estÃ£o no contexto**
- Formate APENAS o texto que estÃ¡ entre as tags <texto_para_formatar>
- Se o texto_para_formatar comeÃ§ar com algo similar ao fim do contexto, NÃƒO duplique, apenas continue naturalmente
"""


# =============================================================================
# ESCOLHA O MODO
# =============================================================================

PROMPT_FORMATACAO = PROMPT_FIDELIDADE
# PROMPT_FORMATACAO = PROMPT_APOSTILA

# =============================================================================
# DETECÃ‡ÃƒO DE MODO
# =============================================================================

FIDELIDADE_MODE = "NÃƒO RESUMA" in PROMPT_FORMATACAO
APOSTILA_MODE = "MANUAL JURÃDICO" in PROMPT_FORMATACAO

if APOSTILA_MODE:
    THRESHOLD_MINIMO = 0.75
    THRESHOLD_CRITICO = 0.65
    MODO_NOME = "APOSTILA"
elif FIDELIDADE_MODE:
    THRESHOLD_MINIMO = 0.75
    THRESHOLD_CRITICO = 0.70
    MODO_NOME = "FIDELIDADE"
else:
    THRESHOLD_MINIMO = 0.70
    THRESHOLD_CRITICO = 0.60
    MODO_NOME = "PADRÃƒO"

logger.info(f"ğŸ¯ Modo: {MODO_NOME} (threshold={THRESHOLD_MINIMO:.0%})")
logger.info(f"ğŸ›¡ï¸  Anti-duplicaÃ§Ã£o: ATIVADA (v2.7)")

# Limiares adaptativos por camada de deduplicaÃ§Ã£o
# 7-DIFF (chunk overlaps): pode ser agressivo, overlaps sÃ£o quase sempre erros
LIMIAR_7DIFF = 0.85 if MODO_NOME == "FIDELIDADE" else 0.80
# SeÃ§Ãµes duplicadas: mais cuidado, professor pode repetir propositalmente
LIMIAR_SECOES = 0.70 if MODO_NOME == "FIDELIDADE" else 0.60

logger.info(f"ğŸ“Š Limiares: 7-DIFF={LIMIAR_7DIFF:.0%} | SeÃ§Ãµes={LIMIAR_SECOES:.0%}")

# =============================================================================
# RATE LIMITER
# =============================================================================

class RateLimiter:
    def __init__(self, max_requests_per_minute=MAX_RPM):
        self.max_rpm = max_requests_per_minute
        self.requests = []
        self.lock = threading.Lock()
    
    def wait_if_needed(self):
        with self.lock:
            now = time.time()
            self.requests = [t for t in self.requests if now - t < 60]
            
            if len(self.requests) >= self.max_rpm:
                oldest = min(self.requests)
                wait_time = 60 - (now - oldest) + 0.5
                logger.info(f"â±ï¸  Rate limit: aguardando {wait_time:.1f}s...")
                sleep(wait_time)
                self.requests = [t for t in self.requests if time.time() - t < 60]
            
            self.requests.append(time.time())

rate_limiter = RateLimiter()

# =============================================================================
# CHECKPOINT/RESUME
# =============================================================================

def get_checkpoint_path(input_file):
    return Path(input_file).with_suffix('.checkpoint.json')

def save_checkpoint(input_file, resultados, chunks_info, secao_atual):
    checkpoint_path = get_checkpoint_path(input_file)
    checkpoint_data = {
        'input_file': str(input_file),
        'secao_atual': secao_atual,
        'total_secoes': len(chunks_info),
        'chunks_info': chunks_info,
        'resultados': resultados,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'version': '2.7',
        'modo': MODO_NOME
    }
    with open(checkpoint_path, 'w', encoding='utf-8') as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)

def load_checkpoint(input_file):
    checkpoint_path = get_checkpoint_path(input_file)
    if checkpoint_path.exists():
        try:
            with open(checkpoint_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if data.get('version') not in ('2.6', '2.7'):
                    logger.warning("Checkpoint de versÃ£o anterior. Reiniciando...")
                    return None
                return data
        except Exception as e:
            logger.error(f"Erro ao carregar checkpoint: {e}")
    return None

def delete_checkpoint(input_file):
    checkpoint_path = get_checkpoint_path(input_file)
    if checkpoint_path.exists():
        checkpoint_path.unlink()
        logger.info("ğŸ§¹ Checkpoint removido")

# =============================================================================
# DIVISÃƒO SEQUENCIAL
# =============================================================================

def dividir_sequencial(transcricao_completa, estrutura_global=None):
    """
    v2.17: Divide documento com InteligÃªncia de Ã‚ncoras (Anchor-Based Chunking).
    Se estrutura_global for fornecida, tenta alinhar cortes com inÃ­cio de tÃ³picos.
    """
    chunks = []
    tamanho_total = len(transcricao_completa)
    inicio = 0
    
    # Prepara Ã¢ncoras (keywords dos tÃ­tulos)
    ancoras = []
    if estrutura_global:
        for item in estrutura_global.split('\n'):
            clean_item = re.sub(r'^\d+(\.\d+)*\.?\s*', '', item.strip())
            if len(clean_item) > 10:
                # Pega as primeiras 4 palavras do tÃ­tulo como "Ã¢ncora"
                keywords = ' '.join(clean_item.split()[:4])
                ancoras.append(keywords)
    
    while inicio < tamanho_total:
        target_fim = inicio + CHARS_POR_PARTE
        fim = min(target_fim, tamanho_total)
        
        if fim < tamanho_total:
            # Janela de busca para ajuste fino
            janela_busca = transcricao_completa[max(0, fim - 1000):min(tamanho_total, fim + 1000)]
            offset_janela = max(0, fim - 1000)
            
            melhor_corte = -1
            
            # 1. Tenta encontrar uma Ã‚NCORA de TÃ³pico (Prioridade Alta)
            if ancoras:
                for ancora in ancoras:
                    # Busca fuzzy ou exata da Ã¢ncora na janela
                    # SimplificaÃ§Ã£o: busca exata case-insensitive
                    pos_ancora = janela_busca.lower().find(ancora.lower())
                    if pos_ancora != -1:
                        corte_proposto = offset_janela + pos_ancora
                        # Se o corte sugerido pela Ã¢ncora estiver dentro de um range aceitÃ¡vel
                        if abs(corte_proposto - fim) < 800: # Aceita desvio de atÃ© 800 chars
                            melhor_corte = corte_proposto
                            logger.info(f"   âš“ Ã‚ncora encontrada: '{ancora}' (ajustando corte)")
                            break
            
            # 2. Se nÃ£o achou Ã¢ncora, busca quebra estrutural forte (## TÃ­tulo)
            if melhor_corte == -1:
                titulo_match = re.search(r'\n(#{2,4}\s+.+)\n', janela_busca)
                if titulo_match:
                    melhor_corte = offset_janela + janela_busca.find(titulo_match.group(0))
            
            # 3. Fallback: Quebra de parÃ¡grafo duplo
            if melhor_corte == -1:
                quebra = transcricao_completa.rfind('\n\n', fim - 300, fim + 300)
                if quebra != -1 and quebra > inicio:
                    melhor_corte = quebra
            
            # 4. Ãšltimo recurso: Ponto final
            if melhor_corte == -1:
                quebra = transcricao_completa.rfind('. ', fim - 150, fim + 150)
                if quebra != -1 and quebra > inicio:
                    melhor_corte = quebra + 1
            
            # Aplica o melhor corte encontrado
            if melhor_corte != -1:
                fim = melhor_corte
        
        chunks.append({'inicio': inicio, 'fim': fim})
        inicio = fim
    
    return chunks

def validar_chunks(chunks, transcricao_completa):
    """v2.7: ValidaÃ§Ã£o rigorosa de chunks sequenciais"""
    logger.info("ğŸ” Validando chunks sequenciais...")
    
    for i in range(len(chunks)):
        chunk = chunks[i]
        
        # Verifica se inÃ­cio == fim do anterior
        if i > 0:
            anterior = chunks[i-1]
            if chunk['inicio'] != anterior['fim']:
                logger.error(f"âŒ Gap/Overlap no chunk {i+1}!")
                logger.error(f"   Anterior termina em: {anterior['fim']}")
                logger.error(f"   Atual comeÃ§a em: {chunk['inicio']}")
                logger.error(f"   DiferenÃ§a: {chunk['inicio'] - anterior['fim']} chars")
                
                # Mostra preview
                if chunk['inicio'] < anterior['fim']:
                    overlap_text = transcricao_completa[chunk['inicio']:anterior['fim']]
                    logger.error(f"   OVERLAP: '{overlap_text[:100]}...'")
                
                return False
    
    logger.info(f"âœ… {len(chunks)} chunks validados (sequenciais, sem overlap)")
    return True

# =============================================================================
# FUNÃ‡Ã•ES AUXILIARES
# =============================================================================

def limpar_tags_xml(texto):
    texto = re.sub(r'</?[a-z_][\w\-]*>', '', texto, flags=re.IGNORECASE)
    texto = re.sub(r'<[a-z_][\w\-]*\s+[^>]+>', '', texto, flags=re.IGNORECASE)
    return texto

def carregar_transcricao(arquivo):
    try:
        with open(arquivo, 'r', encoding='utf-8-sig') as f:
            conteudo = f.read()
        
        if not conteudo.strip():
            logger.error("Arquivo estÃ¡ vazio.")
            sys.exit(1)
            
        return conteudo
    except FileNotFoundError:
        logger.error(f"Arquivo '{arquivo}' nÃ£o encontrado.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Erro ao ler arquivo: {e}")
        sys.exit(1)

def estimar_custo(transcricao, usar_cache, num_chunks=1):
    tokens_in = len(transcricao) // 4
    
    if APOSTILA_MODE:
        tokens_out_estimado = int(tokens_in * 0.65)
    elif FIDELIDADE_MODE:
        tokens_out_estimado = int(tokens_in * 1.00)
    else:
        tokens_out_estimado = int(tokens_in * 0.85)
    
    tokens_prompt = len(PROMPT_FORMATACAO) // 4
    tokens_in_total = tokens_in + (tokens_prompt * num_chunks)
    
    if usar_cache:
        custo_input = (tokens_in * PRECO_INPUT_COM_CACHE + tokens_prompt * num_chunks * PRECO_INPUT_SEM_CACHE) / 1_000_000
        custo_output = (tokens_out_estimado * PRECO_OUTPUT) / 1_000_000
        custo = custo_input + custo_output
    else:
        custo = (tokens_in_total * PRECO_INPUT_SEM_CACHE + tokens_out_estimado * PRECO_OUTPUT) / 1_000_000
    
    logger.info(f"ğŸ’° Custo estimado: ${custo:.4f} USD (modo {MODO_NOME})")

# v2.9: Cache REABILITADO com hash inteligente
def criar_cache_contexto(client, transcricao_completa, system_prompt, estrutura_global=None):
    """Cria cache de contexto com hash estÃ¡vel para reutilizaÃ§Ã£o"""
    
    # Cache sÃ³ vale a pena para documentos grandes
    if len(transcricao_completa) < MIN_CHARS_PARA_CACHE:
        logger.info(f"ğŸ“¦ Documento pequeno ({len(transcricao_completa):,} chars), cache nÃ£o necessÃ¡rio")
        return None
    
    try:
        # Hash do prompt + estrutura para garantir unicidade por documento
        combined_content = system_prompt + (estrutura_global or "")
        prompt_hash = hashlib.sha256(combined_content.encode()).hexdigest()[:16]
        cache_name = f"fmt_{prompt_hash}"
        
        # v2.9: Tenta encontrar cache existente vÃ¡lido
        try:
            for c in client.caches.list(page_size=100):
                if c.display_name == cache_name:
                    logger.info(f"â™»ï¸  Reusando cache existente: {cache_name} ({c.name})")
                    return c
        except Exception as e:
            logger.warning(f"Cache lookup warning: {e}")

        # Adiciona a estrutura global se disponÃ­vel
        estrutura_text = f"\n\n## ESTRUTURA GLOBAL:\n{estrutura_global}" if estrutura_global else ""
        
        cache_content = f"""{system_prompt}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“š CONTEXTO GLOBAL
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Modo: {MODO_NOME}{estrutura_text}
"""
        
        # v2.19: TTL DinÃ¢mico (User Request)
        # Estimativa: 1 hora a cada 500k chars + 1h margem
        # Isso evita que o cache expire no meio de um documento longo
        tempo_estimado_segundos = int((len(transcricao_completa) / 500000) * 3600) + 3600
        dinamico_ttl = f"{max(3600, tempo_estimado_segundos)}s"
        
        # Cria cache usando a API do google-genai
        cache = client.caches.create(
            model=GEMINI_MODEL,
            config=types.CreateCachedContentConfig(
                contents=[cache_content],
                ttl=dinamico_ttl,
                display_name=cache_name
            )
        )
        
        logger.info(f"âœ… Cache criado: {cache_name} (hash: {prompt_hash}, TTL: {CACHE_TTL})")
        return cache
        
    except Exception as e:
        logger.warning(f"âš ï¸ Falha ao criar cache: {e}. Continuando sem cache.")
        return None

# =============================================================================
# MAPEAMENTO ESTRUTURAL (v2.8)
# =============================================================================

PROMPT_MAPEAMENTO = """VocÃª Ã© um especialista em organizaÃ§Ã£o de conteÃºdo educacional acadÃªmico.

## ETAPA 1: IDENTIFICAR O TIPO DE CONTEÃšDO
Analise a transcriÃ§Ã£o e determine qual Ã© a **natureza predominante** do material:

| Tipo | Pistas no Texto | Estrutura Ideal |
|------|-----------------|-----------------|
| **SIMULADO** | "questÃ£o 1", "questÃ£o 2", "espelho de correÃ§Ã£o", "correÃ§Ã£o do simulado", "vamos corrigir" | Organizar por QUESTÃ•ES numeradas |
| **AULA EXPOSITIVA** | explicaÃ§Ãµes contÃ­nuas de um tema, teoria, doutrina, sem questÃµes especÃ­ficas | Organizar por TEMAS/MATÃ‰RIAS |
| **REVISÃƒO** | "revisÃ£o", "resumo", mÃºltiplos temas curtos, "pontos importantes" | Organizar por TÃ“PICOS de revisÃ£o |
| **CORREÃ‡ÃƒO DE PROVA** | "gabarito", "alternativa correta", "item certo/errado" | Organizar por QUESTÃ•ES com gabarito |

## ETAPA 2: EXTRAIR A ESTRUTURA

### Se for SIMULADO ou CORREÃ‡ÃƒO DE PROVA:
```
1. OrientaÃ§Ãµes Gerais / IntroduÃ§Ã£o
2. QuestÃ£o 1: [TÃ­tulo descritivo] â€” [Ãrea do Direito]
   2.1. Enunciado e Contexto
   2.2. FundamentaÃ§Ã£o (Doutrina/JurisprudÃªncia)
   2.3. Pontos do Espelho / Resposta
3. QuestÃ£o 2: [TÃ­tulo descritivo] â€” [Ãrea do Direito]
   3.1. ...
[Continue para cada questÃ£o]
N. ConsideraÃ§Ãµes Finais / DÃºvidas
```

### Se for AULA EXPOSITIVA:
```
1. IntroduÃ§Ã£o
2. [MatÃ©ria 1: ex. Direito Administrativo]
   2.1. [Subtema]
      2.1.1. [Detalhamento]
3. [MatÃ©ria 2: ex. Direito Civil]
   3.1. ...
```

### Se for REVISÃƒO:
```
1. [Tema 1]
   1.1. Pontos-chave
   1.2. JurisprudÃªncia/SÃºmulas
2. [Tema 2]
   2.1. ...
```

## REGRAS GERAIS:
1. **MÃXIMO 3 NÃVEIS** de hierarquia (1., 1.1., 1.1.1.)
2. **Seja descritivo** nos tÃ­tulos â€” inclua o assunto real, nÃ£o apenas "QuestÃ£o 1"
3. **Mantenha a ORDEM** cronolÃ³gica da transcriÃ§Ã£o
4. **Mapeie do INÃCIO ao FIM** â€” nÃ£o omita partes
5. **Identifique a ÃREA DO DIREITO** de cada bloco quando possÃ­vel

## ğŸ›ï¸ REGRA ESPECIAL: MARCOS LEGAIS (v2.17)
Quando identificar marcos legais importantes, crie subtÃ³picos especÃ­ficos:
- **SÃºmulas** (STF, STJ, Vinculantes): Criar subtÃ³pico "X.Y. SÃºmula [NÃºmero] do [Tribunal]"
- **Teses (RepercussÃ£o Geral/Repetitivos)**: Criar subtÃ³pico "X.Y. Tese/Tema [NÃºmero] do STJ/STF"
- **Artigos de Lei Central**: Se um artigo Ã© explicado em profundidade, criar subtÃ³pico "X.Y. Art. [NÃºmero] da [Lei]"

Exemplo:
```
2. ExecuÃ§Ã£o Fiscal
   2.1. Procedimento da LEF (Lei 6.830/80)
   2.2. SÃºmula 314 do STJ (CitaÃ§Ã£o por Hora Certa)
   2.3. Tema 444 do STJ (Redirecionamento)
```

## RESPOSTA:
Primeiro, indique em uma linha: `[TIPO: SIMULADO/EXPOSITIVA/REVISÃƒO/CORREÃ‡ÃƒO]`
Depois, retorne APENAS a estrutura hierÃ¡rquica (mÃ¡x 3 nÃ­veis).
"""

def mapear_estrutura(client, transcricao_completa):
    """Analisa o documento completo e extrai a estrutura de tÃ³picos"""
    logger.info("ğŸ—ºï¸  Mapeando estrutura do documento...")
    
    rate_limiter.wait_if_needed()
    
    # Gemini 2.5 Flash suporta 1M tokens (~4M chars)
    # Limite de 3.5M chars para deixar margem para output (20k tokens)
    max_chars_mapeamento = 3_500_000
    
    if len(transcricao_completa) > max_chars_mapeamento:
        logger.warning(f"âš ï¸  Documento EXTREMAMENTE grande ({len(transcricao_completa):,} chars). Cortando final para caber no contexto.")
        # Ã‰ melhor cortar o final do que picotar o meio para estrutura
        texto_para_mapear = transcricao_completa[:max_chars_mapeamento]
    else:
        texto_para_mapear = transcricao_completa
        logger.info(f"   Mapeando documento completo ({len(transcricao_completa):,} chars)")
    
    prompt = PROMPT_MAPEAMENTO.format(transcricao=texto_para_mapear)
    
    try:
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=prompt,
            config=types.GenerateContentConfig(
                max_output_tokens=20000,  # Aumentado para documentos grandes
                thinking_config={"include_thoughts": False, "thinking_level": "HIGH"}, # Mapeamento: HIGH
                safety_settings=[
                    types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                ]
            )
        )
        
        
        if not response.text:
            logger.warning("âš ï¸  Resposta vazia do mapeamento.")
            logger.warning(f"   Response object: {response}")
            if hasattr(response, 'candidates') and response.candidates:
                logger.warning(f"   Finish reason: {response.candidates[0].finish_reason}")
                if hasattr(response.candidates[0], 'safety_ratings'):
                    logger.warning(f"   Safety ratings: {response.candidates[0].safety_ratings}")
            logger.warning("   Continuando sem estrutura prÃ©via.")
            return None
        
        estrutura = response.text.strip()
        
        # Remove markdown code blocks se presentes
        if estrutura.startswith('```'):
            linhas = estrutura.split('\n')
            estrutura = '\n'.join(linhas[1:-1]) if len(linhas) > 2 else estrutura
        
        linhas = [l for l in estrutura.split('\n') if l.strip()]
        
        # ValidaÃ§Ã£o da estrutura
        if len(linhas) < 3:
            logger.warning(f"âš ï¸  Estrutura muito curta ({len(linhas)} linhas). Pode estar incompleta.")
            return None
        
        tem_numeracao = any(re.match(r'^\d+\.', l.strip()) for l in linhas)
        if not tem_numeracao:
            logger.warning("âš ï¸  Estrutura sem numeraÃ§Ã£o hierÃ¡rquica. Pode estar mal formatada.")
            return None
        
        logger.info(f"âœ… Estrutura mapeada: {len(linhas)} tÃ³picos identificados")
        
        # Log preview (primeiras 10 linhas + total)
        for linha in linhas[:10]:
            logger.info(f"   {linha}")
        if len(linhas) > 10:
            logger.info(f"   ... e mais {len(linhas) - 10} tÃ³picos")
        
        return estrutura
        
    except Exception as e:
        logger.warning(f"âš ï¸  Falha no mapeamento: {e}. Continuando sem estrutura prÃ©via.")
        return None

def simplificar_estrutura_se_necessario(estrutura, max_linhas=60):
    """
    Se a estrutura for muito longa (> max_linhas), mantÃ©m apenas:
    - NÃ­vel 1: 1. Assunto
    - NÃ­vel 2: 1.1. Subassunto

    Sempre inclui todos os nÃ­veis 1, e todos os nÃ­veis 2,
    e depois corta no mÃ¡ximo max_linhas, preservando a ordem original.
    """
    if not estrutura:
        return estrutura

    linhas = [l for l in estrutura.strip().split("\n") if l.strip()]
    if len(linhas) <= max_linhas:
        # JÃ¡ estÃ¡ razoÃ¡vel, mantÃ©m atÃ© nÃ­vel 3 (serÃ¡ filtrado depois por filtrar_niveis_excessivos)
        return estrutura

    logger.info(f"ğŸ“‰ Estrutura muito longa ({len(linhas)} itens). Simplificando para nÃ­veis 1 e 2, mÃ¡x {max_linhas} linhas...")

    nivel1 = []
    nivel2 = []

    for l in linhas:
        s = l.strip()
        # 1. Processo do Trabalho
        if re.match(r"^\d+\.\s", s):
            nivel1.append(l)
        # 1.1. Recursos Trabalhistas
        elif re.match(r"^\d+\.\d+\.\s", s):
            nivel2.append(l)

    # Se por algum motivo nÃ£o identificou quase nada, devolve original para nÃ£o quebrar
    if len(nivel1) + len(nivel2) < 5:
        logger.warning("âš ï¸ SimplificaÃ§Ã£o deixou poucos tÃ³picos. Mantendo estrutura original.")
        return estrutura

    # Monta nova estrutura: primeiro todos os nÃ­veis 1, depois os nÃ­veis 2, respeitando ordem de aparecimento
    nova = []
    vistos = set()

    for l in linhas:
        if l in vistos:
            continue
        if l in nivel1 or l in nivel2:
            nova.append(l)
            vistos.add(l)

    if len(nova) > max_linhas:
        nova = nova[:max_linhas]

    logger.info(f"âœ… Estrutura simplificada: {len(linhas)} -> {len(nova)} linhas (nÃ­veis 1 e 2).")
    return "\n".join(nova)

def filtrar_niveis_execessivos(estrutura, max_nivel=3):
    """
    Remove itens da estrutura que sejam mais profundos que max_nivel.
    Ex: se max_nivel=3, remove 1.1.1.1
    """
    if not estrutura:
        return estrutura
        
    linhas = estrutura.strip().split('\n')
    linhas_filtradas = []
    itens_removidos = 0
    
    # Regex para validar nÃ­vel. 
    # NÃ­vel 1: \d+\.
    # NÃ­vel 2: \d+\.\d+
    # NÃ­vel 3: \d+\.\d+\.\d+
    # O regex verifica se tem no mÃ¡ximo (max_nivel-1) pontos internos entre nÃºmeros
    
    for linha in linhas:
        # Conta quantos grupos de nÃºmeros existem
        match = re.match(r'^(\d+(?:\.\d+)*)', linha.strip())
        if match:
            numeracao = match.group(1)
            nivel = numeracao.count('.') + 1
            if linha.strip().endswith('.'): # Se terminar com ponto (1.1.), nÃ£o conta como nÃ­vel extra
                 nivel = numeracao.count('.')
            
            # Ajuste robusto: contar nÃºmeros separados por ponto
            partes = [p for p in numeracao.split('.') if p.isdigit()]
            nivel_real = len(partes)
            
            if nivel_real <= max_nivel:
                linhas_filtradas.append(linha)
            else:
                itens_removidos += 1
        else:
            # Linhas sem numeraÃ§Ã£o (tÃ­tulos soltos?) mantÃ©m por seguranÃ§a ou remove?
            # Vamos manter para nÃ£o quebrar formataÃ§Ã£o estranha
            linhas_filtradas.append(linha)
            
    if itens_removidos > 0:
        logger.info(f"âœ‚ï¸  Filtrados {itens_removidos} itens com nÃ­vel > {max_nivel}")
    
    return '\n'.join(linhas_filtradas)

# =============================================================================
# PROCESSAMENTO
# =============================================================================

def processar_simples(client, transcricao_bruta, system_prompt):
    logger.info("ğŸ“„ Documento pequeno - processando em requisiÃ§Ã£o Ãºnica...")
    
    prompt = f"""{system_prompt}

<texto_para_formatar>
{transcricao_bruta}
</texto_para_formatar>

Retorne APENAS o Markdown formatado."""
    
    for tentativa in range(MAX_RETRIES):
        try:
            response = client.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt,
                config=types.GenerateContentConfig(
                    max_output_tokens=OUTPUT_TOKEN_LIMIT,
                    temperature=0,
                    thinking_config={"include_thoughts": False, "thinking_level": "LOW"}, # FormataÃ§Ã£o: LOW
                    safety_settings=[
                        types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                    ]
                )
            )
            resultado = response.text
            return limpar_tags_xml(resultado)
        except Exception as e:
            if tentativa < MAX_RETRIES - 1:
                wait = (2 ** tentativa) + random.uniform(0, 1)
                logger.warning(f"Erro, retry em {wait:.1f}s...")
                sleep(wait)
            else:
                raise

def processar_chunk(client, cache, system_prompt, texto_chunk, numero, total, contexto_estilo="", estrutura_global=None, ultimo_titulo=None, profundidade=0):
    rate_limiter.wait_if_needed()
    
    # RecursÃ£o infinita protection e limite mÃ­nimo
    MIN_CHUNK_CHARS = 4000
    if len(texto_chunk) < MIN_CHUNK_CHARS:
        logger.warning(f"âš ï¸ Chunk {numero} muito pequeno ({len(texto_chunk)} chars). Processando sem dividir.")
    elif profundidade > 2:
        logger.warning(f"âš ï¸ Chunk {numero}: Profundidade de recursÃ£o {profundidade} atingida. Processando sem dividir.")

    # v2.8: SeÃ§Ã£o de estrutura global
    secao_estrutura = ""
    if estrutura_global:
        secao_estrutura = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ ESTRUTURA GLOBAL DA AULA (GUIA)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{estrutura_global}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ REGRA DE OURO - PRIORIDADE DO CONTEÃšDO REAL:
O Mapeamento acima Ã© apenas um guia inicial. SE houver divergÃªncia 
entre o Mapeamento e a TranscriÃ§Ã£o Real (ex: o professor mudou de 
assunto, ou o tÃ­tulo nÃ£o existe na fala), SIGA A TRANSCRIÃ‡ÃƒO REAL.
A fidelidade ao que foi *falado* Ã© mais importante que seguir 
cegamente a estrutura.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
    
    # v2.10: Aviso sobre Ãºltimo tÃ­tulo (anti-duplicaÃ§Ã£o)
    aviso_titulo = ""
    if ultimo_titulo:
        aviso_titulo = f"""
ğŸš« O bloco anterior TERMINOU no tÃ³pico: "{ultimo_titulo}"
   NÃƒO inicie sua resposta repetindo este tÃ­tulo.
   Continue o conteÃºdo ou inicie o PRÃ“XIMO subtÃ³pico.
"""
    
    # v2.7: Delimitadores MUITO visÃ­veis e instruÃ§Ãµes reforÃ§adas
    secao_contexto = ""
    if contexto_estilo:
        secao_contexto = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”’ CONTEXTO ANTERIOR (SOMENTE REFERÃŠNCIA DE ESTILO)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{contexto_estilo}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ ATENÃ‡ÃƒO: O bloco acima JÃ FOI FORMATADO anteriormente.
- NÃƒO formate novamente esse conteÃºdo
- NÃƒO inclua esse conteÃºdo na sua resposta
- Use APENAS como referÃªncia de estilo de escrita
{aviso_titulo}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ NOVO TEXTO PARA FORMATAR (comece aqui):
"""
    
    # v2.9: Cache Support - Se usar cache, nÃ£o repete PROMPT_FORMATACAO
    instructions_body = f"""
{secao_estrutura}
{secao_contexto}

<texto_para_formatar>
{texto_chunk}
</texto_para_formatar>

**INSTRUÃ‡Ã•ES FINAIS**:
- Esta Ã© a parte {numero} de {total} (Profundidade: {profundidade})
- Formate APENAS o texto entre <texto_para_formatar>
- Se houver ESTRUTURA GLOBAL acima, use os mesmos nomes de tÃ³picos
- Se houver contexto acima, NÃƒO o reprocesse
- **ANTI-REPETIÃ‡ÃƒO DE TÃTULOS**: Se o contexto anterior termina com um tÃ­tulo (ex: "## HomologaÃ§Ã£o"), NÃƒO repita esse tÃ­tulo no inÃ­cio da sua resposta. Continue o conteÃºdo diretamente ou inicie o PRÃ“XIMO tÃ³pico diferente.
- Retorne APENAS o Markdown formatado do NOVO texto
"""

    if cache:
        prompt = instructions_body
    else:
        prompt = f"{system_prompt}\n{instructions_body}"

    for tentativa in range(MAX_RETRIES):
        try:
            safety_config = [
                types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
            ]
            
            # ConfiguraÃ§Ã£o dinÃ¢mica para suportar cache
            gen_config_args = {
                "max_output_tokens": OUTPUT_TOKEN_LIMIT,
                "temperature": 0.1,
                "thinking_config": {"include_thoughts": False, "thinking_level": "LOW"}, # FormataÃ§Ã£o: LOW
                "safety_settings": safety_config
            }
            if cache:
                gen_config_args['cached_content'] = cache.name

            response = client.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt,
                config=types.GenerateContentConfig(**gen_config_args)
            )
            
            # --- DiagnÃ³stico finishReason e Usage ---
            finish_reason = "UNKNOWN"
            usage_tokens = 0
            
            try:
                if hasattr(response, 'usage_metadata'):
                    usage = response.usage_metadata
                    # Tenta acessar atributos (pode variar entre SDKs/Vertex)
                    # v2.17.1: Garantir inteiros (getattr pode retornar None se o atributo existe mas Ã© None)
                    candidates_token_count = getattr(usage, 'candidates_token_count', 0) or 0
                    prompt_token_count = getattr(usage, 'prompt_token_count', 0) or 0
                    cached_content_token_count = getattr(usage, 'cached_content_token_count', 0) or 0
                    
                    usage_tokens = candidates_token_count
                    
                    logger.info(f"ğŸ“Š Usage: Prompt={prompt_token_count} (Cached: {cached_content_token_count}) | Candidates={candidates_token_count}")
                    
                    # Acumular mÃ©tricas globais (v2.10)
                    metrics.record_api_call(
                        prompt_tokens=prompt_token_count, 
                        completion_tokens=candidates_token_count, 
                        cached_tokens=cached_content_token_count,
                        provider='gemini'
                    )
                
                if hasattr(response, 'candidates') and response.candidates:
                    cand = response.candidates[0]
                    if hasattr(cand, 'finish_reason'):
                         finish_reason = str(cand.finish_reason) # ex: FinishReason.STOP ou "STOP"
            except Exception as ex_usage:
                logger.warning(f"âš ï¸ Erro ao ler metadados: {ex_usage}")

            # Captura texto (lidando com caso de .text vazio mas content presente)
            resultado = ""
            try:
                resultado = response.text
            except ValueError:
                # O SDK levanta ValueError se finish_reason for SAFETY ou se nÃ£o houver text field padrÃ£o
                pass
            
            if not resultado and hasattr(response, 'candidates') and response.candidates:
                # Tenta extrair parts[0].text manualmente se .text falhou
                try:
                    parts = response.candidates[0].content.parts
                    if parts:
                        resultado = parts[0].text
                except:
                    pass
            
            if not resultado:
                logger.warning(f"âš ï¸  Resposta vazia na tentativa {tentativa+1}. Reason: {finish_reason}")
                if tentativa < MAX_RETRIES - 1:
                    sleep(2 * (tentativa + 1))
                    continue
                else:
                    return f"[ERRO SEÃ‡ÃƒO {numero}: RESPOSTA VAZIA]"

            # ValidaÃ§Ã£o bÃ¡sica de tamanho (compressÃ£o)
            razao = len(resultado) / len(texto_chunk) if len(texto_chunk) > 0 else 0
            
            problema_detectado = False
            msg_problema = ""
            
            compressao_excessiva_severa = False
            
            if razao < THRESHOLD_CRITICO: # < 0.70 por padrÃ£o
                problema_detectado = True
                msg_problema = f"CompressÃ£o excessiva ({razao:.0%})"
                # Flag para chunking imediato se for muito baixo (ex < 0.65)
                # O usuÃ¡rio pediu < THRESHOLD_CRITICO, vamos ser assertivos.
                compressao_excessiva_severa = True
                
            if problema_detectado:
                logger.warning(f"âš ï¸ SeÃ§Ã£o {numero}: {msg_problema}. Reason: {finish_reason}. (Tentativa {tentativa+1}/{MAX_RETRIES})")
                
                # SEÃ‡ÃƒO CRÃTICA: DecisÃ£o de Chunking Adaptativo
                
                # LÃ³gica antiga: sÃ³ no final ou MAX_TOKENS
                # LÃ³gica nova: divide cedo se compressÃ£o for severa
                
                deve_dividir = (
                    len(texto_chunk) > MIN_CHUNK_CHARS 
                    and profundidade < 2
                    and (
                        ("MAX_TOKENS" in str(finish_reason)) or 
                        (tentativa == MAX_RETRIES - 1) or
                        (compressao_excessiva_severa) # NOVO: Divide jÃ¡!
                    )
                )
                
                if deve_dividir:
                    motivo = "MAX_TOKENS" if "MAX_TOKENS" in str(finish_reason) else "COMPRESSÃƒO"
                    logger.info(f"âœ‚ï¸  ATIVANDO CHUNKING ADAPTATIVO para SeÃ§Ã£o {numero} (Motivo: {motivo} | Profundidade {profundidade} -> {profundidade+1})")
                    return dividir_e_reprocessar(client, cache, system_prompt, texto_chunk, numero, total, contexto_estilo, estrutura_global, ultimo_titulo, profundidade)
                
                if tentativa < MAX_RETRIES - 1:
                    continue  # Tenta de novo (se nÃ£o foi severo o suficiente para dividir)
                else:
                    logger.error(f"SeÃ§Ã£o {numero}: Falha apÃ³s {MAX_RETRIES} tentativas. Retornando melhor esforÃ§o.")
            
            return resultado
            
        except Exception as e:
            erro_msg = str(e)
            is_recoverable = any(code in erro_msg for code in ['503', '429', '500', 'RESOURCE_EXHAUSTED', 'InternalServerError']) or "Resposta vazia" in erro_msg
            
            if tentativa < MAX_RETRIES - 1 and is_recoverable:
                wait = (2 ** (tentativa + 2)) + random.uniform(1, 3)
                if '429' in erro_msg or 'RATE_LIMIT' in erro_msg:
                    wait = 30 + random.uniform(0, 5)
                    logger.warning(f"ğŸ›‘ Rate Limit (429) detectado na seÃ§Ã£o {numero}. Pausa longa de {wait:.1f}s...")
                else:
                    logger.warning(f"Erro seÃ§Ã£o {numero}: {erro_msg}. Retry {tentativa+2}/{MAX_RETRIES} em {wait:.1f}s")
                sleep(wait)
            else:
                logger.error(f"Falha seÃ§Ã£o {numero}: {erro_msg}")
                return f"\n\n> [!WARNING]\n> Falha ao processar seÃ§Ã£o {numero}. Texto original:\n\n{texto_chunk}"
    
    return texto_chunk

def dividir_e_reprocessar(client, cache, system_prompt, texto_chunk, numero, total, contexto_estilo, estrutura_global, ultimo_titulo, profundidade):
    """
    Divide um chunk grande em dois menores e processa recursivamente.
    Tenta dividir em quebras de parÃ¡grafo (\n\n) prÃ³ximas ao meio.
    """
    # v2.10: Registrar divisÃ£o adaptativa
    metrics.record_adaptive_split()
    
    meio = len(texto_chunk) // 2
    
    # Procura quebra ideal (\n\n) num raio de 20% do meio
    margem = int(len(texto_chunk) * 0.2)
    inicio_busca = max(0, meio - margem)
    fim_busca = min(len(texto_chunk), meio + margem)
    
    janela_busca = texto_chunk[inicio_busca:fim_busca]
    pos_relativa = janela_busca.find('\n\n')
    
    if pos_relativa != -1:
        ponto_corte = inicio_busca + pos_relativa + 2 # +2 para incluir os \n\n no primeiro bloco ou pular? Vamos cortar DEPOIS dos \n\n
    else:
        # Tenta quebra simples \n
        pos_relativa_n = janela_busca.find('\n')
        if pos_relativa_n != -1:
            ponto_corte = inicio_busca + pos_relativa_n + 1
        else:
            # Corte seco no espaÃ§o mais prÃ³ximo
            pos_relativa_espaco = janela_busca.find(' ')
            if pos_relativa_espaco != -1:
                ponto_corte = inicio_busca + pos_relativa_espaco + 1
            else:
                ponto_corte = meio # Corte arbitrÃ¡rio
    
    parte_a = texto_chunk[:ponto_corte]
    parte_b = texto_chunk[ponto_corte:]
    
    logger.info(f"   Splitting chunk {numero}: Part A ({len(parte_a)} chars) + Part B ({len(parte_b)} chars)")
    
    # Processa Parte A
    resultado_a = processar_chunk(
        client, cache, system_prompt, parte_a, f"{numero}.A", total, 
        contexto_estilo, estrutura_global, ultimo_titulo, profundidade + 1
    )
    
    # Usa o final de A como contexto para B? Talvez seja excessivo e caro.
    # Vamos manter o contexto original para B por seguranÃ§a, 
    # ou usar resultado_a[-1000:] como novo contexto_estilo.
    # Usar resultado_a Ã© melhor para continuidade.
    
    novo_contexto = resultado_a[-2000:] if len(resultado_a) > 2000 else resultado_a
    
    # Processa Parte B
    resultado_b = processar_chunk(
        client, cache, system_prompt, parte_b, f"{numero}.B", total, 
        novo_contexto, estrutura_global, None, profundidade + 1 # ultimo_titulo None pois A jÃ¡ tratou disso
    )
    
    return f"{resultado_a}\n\n{resultado_b}"

def extrair_titulos_h2(texto):
    """Extrai todos os tÃ­tulos de nÃ­vel 2 (##) do texto"""
    titulos = []
    for linha in texto.split('\n'):
        if linha.strip().startswith('##') and not linha.strip().startswith('###'):
            titulo_limpo = re.sub(r'^##\s*\d+\.\s*', '', linha.strip())
            titulos.append(titulo_limpo.lower())
    return titulos

# =============================================================================
# SMART STITCHING (v2.10) - Anti-DuplicaÃ§Ã£o CirÃºrgica
# =============================================================================

def remover_eco_do_contexto(resposta_api, contexto_enviado):
    """
    Remove o inÃ­cio da resposta se for apenas um 'eco' do final do contexto.
    """
    if not contexto_enviado or not resposta_api:
        return resposta_api

    final_contexto = contexto_enviado.strip()[-300:]
    inicio_resposta = resposta_api.strip()[:300]

    matcher = SequenceMatcher(None, final_contexto, inicio_resposta)
    match = matcher.find_longest_match(0, len(final_contexto), 0, len(inicio_resposta))

    if match.size > 50:
        logger.info(f"âœ‚ï¸ Eco detectado! Removendo {match.size} chars repetidos no inÃ­cio.")
        return resposta_api.strip()[match.size:].strip()
    
    return resposta_api

def titulos_sao_similares(t1, t2, threshold=None):
    """Verifica se dois tÃ­tulos sÃ£o semanticamente iguais (fuzzy matching).
    Usa LIMIAR_SECOES global se threshold nÃ£o for especificado.
    """
    if threshold is None:
        threshold = LIMIAR_SECOES  # Usa limiar de seÃ§Ãµes (0.70 Fidelidade / 0.60 Apostila)
        
    def normalizar(t):
        # Remove apenas caracteres nÃ£o alfanumÃ©ricos, mas MANTÃ‰M tamanho relativo
        return re.sub(r'[^a-z0-9 ]', '', t.lower())
    
    nt1 = normalizar(t1)
    nt2 = normalizar(t2)
    
    if not nt1 or not nt2:
        return False
    
    # PROTEÃ‡ÃƒO 1: Se um tÃ­tulo for muito maior que o outro, nÃ£o sÃ£o duplicatas
    nt1_compact = nt1.replace(' ', '')
    nt2_compact = nt2.replace(' ', '')
    len_ratio = min(len(nt1_compact), len(nt2_compact)) / max(len(nt1_compact), len(nt2_compact))
    if len_ratio < 0.8:  # Se a diferenÃ§a de tamanho for > 20%, assume que sÃ£o diferentes
        return False
    
    # PROTEÃ‡ÃƒO 2: VerificaÃ§Ã£o por palavras - se houver palavras exclusivas significativas
    palavras1 = set(nt1.split())
    palavras2 = set(nt2.split())
    diferenca = palavras1.symmetric_difference(palavras2)
    
    # Se as palavras diferentes forem longas (nÃ£o apenas 'e', 'do', 'da'), assume diferenÃ§a real
    if any(len(w) > 3 for w in diferenca):
        return False
        
    return SequenceMatcher(None, nt1_compact, nt2_compact).ratio() > threshold

def limpar_inicio_redundante(texto_novo, texto_acumulado):
    """
    Remove tÃ­tulo no inÃ­cio do novo chunk se similar ao Ãºltimo tÃ­tulo do texto acumulado.
    """
    if not texto_acumulado.strip():
        return texto_novo

    ultimas_linhas = texto_acumulado.strip().split('\n')[-30:]
    ultimo_titulo = None
    for linha in reversed(ultimas_linhas):
        if linha.strip().startswith('##'):
            ultimo_titulo = re.sub(r'^#+\s*(?:\d+(?:\.\d+)*\.?)?\s*', '', linha).strip()
            break
    
    if not ultimo_titulo:
        return texto_novo

    linhas_novas = texto_novo.strip().split('\n')
    
    for i, linha in enumerate(linhas_novas[:10]):  # v2.11: Busca mais profunda (era 5)
        if linha.strip().startswith('##'):
            novo_titulo = re.sub(r'^#+\s*(?:\d+(?:\.\d+)*\.?)?\s*', '', linha).strip()
            
            if titulos_sao_similares(ultimo_titulo, novo_titulo):
                logger.info(f"âœ‚ï¸ TÃ­tulo duplicado na junÃ§Ã£o: '{novo_titulo}' â‰ˆ '{ultimo_titulo}'")
                return '\n'.join(linhas_novas[i+1:])
    
    return texto_novo

def detectar_secoes_duplicadas(texto):
    """v2.15: Detecta seÃ§Ãµes duplicadas por tÃ­tulos em ## e ### (Fuzzy Matching)"""
    logger.info("ğŸ” Detectando seÃ§Ãµes duplicadas (fuzzy, H2+H3)...")
    
    linhas = texto.split('\n')
    titulos_vistos = []  # (titulo_normalizado, linha_idx)
    secoes_duplicadas = []
    
    for i, linha in enumerate(linhas):
        linha_strip = linha.strip()
        # Match both ## (H2) and ### (H3)
        if linha_strip.startswith('##'):
            # Extract level
            match_nivel = re.match(r'^(#+)', linha_strip)
            nivel = len(match_nivel.group(1)) if match_nivel else 2
            
            # Normalize title: remove ## prefix, numbers, emojis, and "(ContinuaÃ§Ã£o)"
            titulo_normalizado = re.sub(r'^#{2,4}\s*\d+(?:\.\d+)*\.?\s*', '', linha_strip)
            titulo_normalizado = re.sub(r'[ğŸ“‹ğŸ“ŠğŸ—‚ï¸]', '', titulo_normalizado).strip()
            titulo_normalizado = re.sub(r'\s*\(ContinuaÃ§Ã£o\)\s*$', '', titulo_normalizado, flags=re.IGNORECASE).strip()
            
            duplicado = False
            for t_visto, linha_visto in titulos_vistos:
                if titulos_sao_similares(titulo_normalizado, t_visto):
                    logger.warning(f"âš ï¸  Duplicado (fuzzy): '{linha_strip}' â‰ˆ '{t_visto}'")
                    secoes_duplicadas.append({
                        'titulo': titulo_normalizado,
                        'primeira_linha': linha_visto,
                        'duplicada_linha': i
                    })
                    duplicado = True
                    break
            
            if not duplicado:
                titulos_vistos.append((titulo_normalizado, i))
    
    if secoes_duplicadas:
        logger.error(f"âŒ {len(secoes_duplicadas)} seÃ§Ãµes duplicadas detectadas!")
    else:
        logger.info("âœ… Nenhuma seÃ§Ã£o duplicada detectada")
    
    return secoes_duplicadas

def remover_secoes_duplicadas(texto):
    """v2.14: Remove seÃ§Ãµes duplicadas com COMPARAÃ‡ÃƒO JANELADA (Fix DiluiÃ§Ã£o)"""
    from difflib import SequenceMatcher
    
    secoes_dup = detectar_secoes_duplicadas(texto)
    if not secoes_dup: return texto
    
    print("ğŸ§¹ Removendo seÃ§Ãµes duplicadas (Smart Dedupe v2.14)...")
    linhas = texto.split('\n')
    
    # Rastreia o ÃšLTIMO segmento adicionado para cada tÃ­tulo (para evitar diluiÃ§Ã£o na comparaÃ§Ã£o)
    ultimo_segmento_visto = {}  # titulo_normalizado -> Ãºltimo texto adicionado
    linhas_para_remover = set()
    linhas_para_adicionar_separador = set()
    
    for dup in secoes_dup:
        # --- 1. Extrair ConteÃºdo Original ---
        idx_orig = dup['primeira_linha']
        header_orig = linhas[idx_orig].strip()
        match_orig = re.match(r'^(#+)', header_orig)
        nivel_orig = len(match_orig.group(1)) if match_orig else 2
        
        # Extrai conteÃºdo da seÃ§Ã£o original
        content_orig = []
        for i in range(idx_orig + 1, len(linhas)):
            line = linhas[i].strip()
            if line.startswith('#'):
                match_now = re.match(r'^(#+)', line)
                if match_now and len(match_now.group(1)) <= nivel_orig: break
            content_orig.append(linhas[i])
        text_orig = "\n".join(content_orig)
        
        titulo_key = re.sub(r'^#{2,4}\s*\d+(?:\.\d+)*\.?\s*', '', header_orig)
        titulo_key = re.sub(r'\s*\(ContinuaÃ§Ã£o\)\s*$', '', titulo_key, flags=re.IGNORECASE).strip()
        
        # Inicializa o rastreamento se for a primeira vez
        if titulo_key not in ultimo_segmento_visto:
            ultimo_segmento_visto[titulo_key] = text_orig

        # --- 2. Extrair ConteÃºdo Duplicado ---
        idx_dup = dup['duplicada_linha']
        header_dup = linhas[idx_dup].strip()
        match_dup = re.match(r'^(#+)', header_dup)
        nivel_dup = len(match_dup.group(1)) if match_dup else 2
        
        fim_dup_idx = len(linhas)
        content_dup = []
        for i in range(idx_dup + 1, len(linhas)):
            line = linhas[i].strip()
            if line.startswith('#'):
                match_now = re.match(r'^(#+)', line)
                if match_now and len(match_now.group(1)) <= nivel_dup:
                    fim_dup_idx = i
                    break
            content_dup.append(linhas[i])
        text_dup = "\n".join(content_dup)
        
        # --- 3. Comparar ConteÃºdo (LÃ³gica Janelada v2.15) ---
        # Compara APENAS com o Ãºltimo segmento conhecido dessa seÃ§Ã£o
        texto_referencia = ultimo_segmento_visto.get(titulo_key, text_orig)
        
        len_dup = len(text_dup.strip())
        len_ref = len(texto_referencia.strip())
        
        # LÃ³gica de decisÃ£o baseada em tamanho
        if len_dup < 50:
            # Duplicado curto demais = lixo, deletar
            sim = 1.0
            print(f"   âš ï¸  SeÃ§Ã£o duplicada muito curta ({len_dup} chars) - marcando para remoÃ§Ã£o")
        elif len_ref < 50 and len_dup >= 200:
            # Original curto, mas duplicado substancial = original estava incompleto
            # Manter o duplicado como novo conteÃºdo
            sim = 0.0
            print(f"   â„¹ï¸  Original curto ({len_ref}c), duplicado substancial ({len_dup}c) - mantendo novo conteÃºdo")
        else:
            sim = SequenceMatcher(None, texto_referencia, text_dup).ratio()
            
        print(f"   Similaridade: {sim:.1%} | Linha {idx_dup} | '{titulo_key[:40]}...'")
        
        if sim > LIMIAR_SECOES:  # Usa limiar de seÃ§Ãµes (0.70 Fidelidade / 0.60 Apostila) 
            print(f"   ğŸ—‘ï¸  Removendo SEÃ‡ÃƒO INTEIRA (Duplicata confirmada)")
            for i in range(idx_dup, fim_dup_idx):
                linhas_para_remover.add(i)
        else:
            print(f"   ğŸ”— Mesclando conteÃºdo (Nova informaÃ§Ã£o detectada)")
            linhas_para_remover.add(idx_dup)
            if idx_dup + 1 < len(linhas):
                linhas_para_adicionar_separador.add(idx_dup + 1)
            
            # ATUALIZA o Ãºltimo segmento visto para a prÃ³xima comparaÃ§Ã£o
            ultimo_segmento_visto[titulo_key] = text_dup

    # --- 4. ReconstruÃ§Ã£o ---
    linhas_limpas = []
    for i, linha in enumerate(linhas):
        if i in linhas_para_remover:
            continue
        if i in linhas_para_adicionar_separador:
            linhas_limpas.append("") 
        linhas_limpas.append(linha)
        
    print(f"âœ… {len(linhas_para_remover)} linhas removidas")
    return '\n'.join(linhas_limpas)

def remover_duplicacoes_literais(texto):
    """Remove parÃ¡grafos individuais duplicados"""
    paragrafos = texto.split('\n\n')
    paragrafos_limpos = []
    dup_count = 0
    
    for i, para in enumerate(paragrafos):
        if i == 0:
            paragrafos_limpos.append(para)
            continue
        
        if len(para.strip()) < 80 or para.strip().startswith('#'):
            paragrafos_limpos.append(para)
            continue
        
        is_duplicate = False
        para_norm = ' '.join(para.split()).lower()
        
        for j in range(max(0, len(paragrafos_limpos) - 3), len(paragrafos_limpos)):
            para_ant = paragrafos_limpos[j]
            para_ant_norm = ' '.join(para_ant.split()).lower()
            
            ratio = SequenceMatcher(None, para_norm, para_ant_norm).ratio()
            
            if ratio > 0.95:
                is_duplicate = True
                dup_count += 1
                break
        
        if not is_duplicate:
            paragrafos_limpos.append(para)
    
    if dup_count > 5:
        logger.warning(f"âš ï¸  {dup_count} parÃ¡grafos duplicados removidos")
    
    return '\n\n'.join(paragrafos_limpos)

# =============================================================================
# V2.17: DEDUPLICAÃ‡ÃƒO ROBUSTA (7-DIFF Strategy) - Portado de mlx_vomo.py
# =============================================================================

def remover_overlap_duplicado(resultados):
    """
    v2.17: Remove duplicaÃ§Ã£o entre chunks usando detecÃ§Ã£o ROBUSTA de conteÃºdo.
    EstratÃ©gia 7-DIFF: Compara tÃ­tulo + conteÃºdo com janela deslizante de 20 seÃ§Ãµes.
    """
    if isinstance(resultados, str):
        resultados = [resultados]
    if len(resultados) <= 1:
        return resultados[0] if resultados else ""
    
    from difflib import SequenceMatcher
    
    def normalize_text(text):
        if not text: return ""
        text = re.sub(r'[#*-]', ' ', text)
        text = re.sub(r'[^\w\s]', '', text)
        return ' '.join(text.lower().split())

    def calculate_similarity(text1, text2):
        if not text1 or not text2:
            return 0.0
        if len(text1) < 50:
            return 1.0 if text1 in text2 or text2 in text1 else 0.0
        return SequenceMatcher(None, text1, text2).quick_ratio()

    def extract_unique_paragraphs(sec_curr_content, sec_prev_content):
        if not sec_curr_content: return []
        unique = []
        paras_curr = sec_curr_content.split('\n\n')
        paras_prev_norm = [normalize_text(p) for p in sec_prev_content.split('\n\n')]
        
        for p in paras_curr:
            p_clean = p.strip()
            if not p_clean or len(p_clean) < 20: continue
            
            p_norm = normalize_text(p_clean)
            is_present = False
            for pp_norm in paras_prev_norm:
                if calculate_similarity(p_norm, pp_norm) > 0.85:
                    is_present = True
                    break
            
            if not is_present:
                unique.append(p_clean)
        return unique

    logger.info("ğŸ§¹ Iniciando deduplicaÃ§Ã£o robusta (7-DIFF Strategy)...")

    # 1. Junta e Parseia
    texto_bruto = '\n\n'.join(resultados)
    lines = texto_bruto.split('\n')
    
    sections = []
    current_section = None
    header_pattern = re.compile(r'^(#{1,6})\s+(.+)$')
    
    captured_lines = []
    intro_lines = []
    has_started = False
    
    for line in lines:
        match = header_pattern.match(line)
        if match:
            has_started = True
            if current_section:
                current_section['content'] = '\n'.join(captured_lines).strip()
                sections.append(current_section)
                captured_lines = []
            
            title_text = match.group(2).strip()
            title_clean = re.sub(r'^\d+(?:\.\d+)*\.?\s*', '', title_text)
            
            current_section = {
                'title_clean': title_clean,
                'level': len(match.group(1)),
                'full_header': line,
                'content': ""
            }
        else:
            if has_started:
                captured_lines.append(line)
            else:
                intro_lines.append(line)
    
    if current_section:
        current_section['content'] = '\n'.join(captured_lines).strip()
        sections.append(current_section)
        
    logger.info(f"   ğŸ“Š Analisando {len(sections)} seÃ§Ãµes...")

    # 2. DetecÃ§Ã£o e RemoÃ§Ã£o
    indices_to_remove = set()
    MAX_WINDOW = 20
    
    for i in range(len(sections)):
        if i in indices_to_remove: continue
        sec_curr = sections[i]
        
        start_check = max(0, i - MAX_WINDOW)
        for j in range(start_check, i):
            if j in indices_to_remove: continue
            sec_prev = sections[j]
            
            if sec_curr['level'] != sec_prev['level']: continue
            
            sim_title = calculate_similarity(normalize_text(sec_curr['title_clean']), normalize_text(sec_prev['title_clean']))
            sim_content = calculate_similarity(normalize_text(sec_curr['content']), normalize_text(sec_prev['content']))
            
            is_duplicate = False
            
            if sim_title > 0.9 and sim_content > 0.6:
                is_duplicate = True
            elif sim_content > LIMIAR_7DIFF:  # Usa limiar 7-DIFF (0.85 Fidelidade / 0.80 Apostila)
                 is_duplicate = True
            elif sim_title > 0.95 and len(sec_curr['content']) < 100:
                 is_duplicate = True
            
            if is_duplicate:
                logger.info(f"   ğŸ—‘ï¸  Duplicata detectada: '{sec_curr['title_clean'][:40]}...'")
                
                unique_paras = extract_unique_paragraphs(sec_curr['content'], sec_prev['content'])
                if unique_paras:
                    sections[j]['content'] += '\n\n' + '\n\n'.join(unique_paras)
                
                indices_to_remove.add(i)
                break
    
    # 3. ReconstruÃ§Ã£o
    final_lines = list(intro_lines)
    for i, sec in enumerate(sections):
        if i in indices_to_remove: continue
        final_lines.append(sec['full_header'])
        if sec['content']:
            final_lines.append(sec['content'])
        final_lines.append("")
    
    logger.info(f"   âœ… {len(indices_to_remove)} seÃ§Ãµes duplicadas removidas/mescladas")
    return '\n'.join(final_lines)

def deterministic_structure_fix(text):
    """
    v2.17: ReorganizaÃ§Ã£o Estrutural DeterminÃ­stica.
    Detecta se usa H1 ou H2 como nÃ­vel principal e reorganiza o documento.
    """
    logger.info("ğŸ§© Executando ReorganizaÃ§Ã£o Estrutural DeterminÃ­stica...")
    
    lines = text.split('\n')
    
    # DetecÃ§Ã£o de Hierarquia
    has_h1 = any(re.match(r'^#\s+', line) for line in lines)
    header_level_regex = r'^#\s+' if has_h1 else r'^##\s+'
    logger.info(f"   â„¹ï¸  NÃ­vel principal detectado: {'H1 (#)' if has_h1 else 'H2 (##)'}")

    content_map = {
        "PREAMBULO": [],
        "DISCIPLINAS": {}, 
        "ENCERRAMENTO": []
    }
    
    current_area = "PREAMBULO"
    current_block = []
    disciplinas_order = [] 
    
    re_disciplina = re.compile(rf'{header_level_regex}(?!QuestÃ£o|Q\.)([^0-9\.]+.*)', re.IGNORECASE)
    re_encerramento = re.compile(rf'{header_level_regex}(?:ENCERRAMENTO|CONSIDERAÃ‡Ã•ES|CONCLUSÃƒO)', re.IGNORECASE)
    
    def flush_block(area, block_lines):
        if not block_lines: return
        block_text = '\n'.join(block_lines)
        
        if area == "PREAMBULO":
            content_map["PREAMBULO"].append(block_text)
        elif area == "ENCERRAMENTO":
            content_map["ENCERRAMENTO"].append(block_text)
        else:
            if area not in content_map["DISCIPLINAS"]:
                content_map["DISCIPLINAS"][area] = []
                disciplinas_order.append(area)
            content_map["DISCIPLINAS"][area].append(block_text)

    for line in lines:
        match_disc = re_disciplina.match(line)
        if match_disc:
            flush_block(current_area, current_block)
            current_block = []
            
            raw_area = match_disc.group(1).strip().upper()
            
            if "DIREITO" not in raw_area and len(raw_area) < 50:
                 if any(x in raw_area for x in ["CIVIL", "PENAL", "TRABALHO", "ADMINISTRATIVO", "CONSTITUCIONAL"]):
                     current_area = f"DIREITO {raw_area}"
                 else:
                     current_area = raw_area
            else:
                 current_area = raw_area
            continue 
            
        if re_encerramento.match(line):
            flush_block(current_area, current_block)
            current_block = []
            current_area = "ENCERRAMENTO"
            continue

        current_block.append(line)
        
    flush_block(current_area, current_block)
    
    # ReconstruÃ§Ã£o
    final_output = []
    
    if content_map["PREAMBULO"]:
        final_output.append("# ORIENTAÃ‡Ã•ES GERAIS / INTRODUÃ‡ÃƒO")
        final_output.extend(content_map["PREAMBULO"])
        final_output.append("")

    for area in disciplinas_order:
        area_clean = area.replace("#", "").strip()
        final_output.append(f"# {area_clean}")
        for block in content_map["DISCIPLINAS"][area]:
            final_output.append(block)
        final_output.append("")
        
    if content_map["ENCERRAMENTO"]:
        final_output.append("# CONSIDERAÃ‡Ã•ES FINAIS")
        final_output.extend(content_map["ENCERRAMENTO"])
        
    num_identified = len(disciplinas_order)
    logger.info(f"   âœ… Reorganizado: {num_identified} seÃ§Ãµes principais identificadas.")
    
    if num_identified == 0 and len(content_map["PREAMBULO"]) > 0:
        logger.warning("   âš ï¸ Nenhuma estrutura detectada. Mantendo original.")
        return text
        
    return '\n'.join(final_output)

def normalize_headings(texto):
    """
    v1.0: Normaliza tÃ­tulos semanticamente similares para uma versÃ£o Ãºnica.
    - Agrupa tÃ­tulos por similaridade
    - Escolhe o tÃ­tulo mais descritivo de cada grupo
    - Remove sufixos como "(ContinuaÃ§Ã£o)"
    """
    from difflib import SequenceMatcher
    
    print("ğŸ”¤ Normalizando tÃ­tulos similares...")
    linhas = texto.split('\n')
    
    # 1. Extrair todos os tÃ­tulos com info de nÃ­vel e posiÃ§Ã£o
    titulos = []
    for i, linha in enumerate(linhas):
        stripped = linha.strip()
        if stripped.startswith('##'):
            match = re.match(r'^(#+)\s*', stripped)
            nivel = len(match.group(1)) if match else 2
            # Extrai tÃ­tulo limpo (sem # e sem numeraÃ§Ã£o)
            titulo_limpo = re.sub(r'^#{2,4}\s*\d+(?:\.\d+)*\.?\s*', '', stripped).strip()
            # Remove "(ContinuaÃ§Ã£o)" para comparaÃ§Ã£o
            titulo_base = re.sub(r'\s*\(ContinuaÃ§Ã£o\)\s*$', '', titulo_limpo, flags=re.IGNORECASE).strip()
            titulos.append({
                'linha': i,
                'nivel': nivel,
                'original': stripped,
                'limpo': titulo_limpo,
                'base': titulo_base
            })
    
    if not titulos:
        return texto
    
    # 2. Agrupar tÃ­tulos similares (mesmo nÃ­vel + similaridade > LIMIAR)
    grupos = []
    usados = set()
    
    for i, t1 in enumerate(titulos):
        if i in usados:
            continue
        grupo = [t1]
        usados.add(i)
        
        for j, t2 in enumerate(titulos):
            if j in usados or j <= i:
                continue
            if t1['nivel'] == t2['nivel']:
                sim = SequenceMatcher(None, t1['base'].lower(), t2['base'].lower()).ratio()
                if sim > LIMIAR_SECOES:
                    grupo.append(t2)
                    usados.add(j)
        
        if len(grupo) > 1:
            grupos.append(grupo)
    
    if not grupos:
        # Nada para normalizar, mas ainda remove "(ContinuaÃ§Ã£o)"
        texto_limpo = re.sub(r'\s*\(ContinuaÃ§Ã£o\)\s*(?=\n|$)', '', texto, flags=re.IGNORECASE)
        return texto_limpo
    
    # 3. Para cada grupo, escolher o "melhor" tÃ­tulo (mais curto entre os mais longos)
    #    LÃ³gica: preferir tÃ­tulos sem "(ContinuaÃ§Ã£o)" e com descriÃ§Ã£o completa
    substituicoes = {}
    for grupo in grupos:
        # Ordenar por: nÃ£o ter "(ContinuaÃ§Ã£o)" primeiro, depois por comprimento (prefer mÃ©dio)
        candidatos = sorted(grupo, key=lambda x: (
            '(ContinuaÃ§Ã£o)' in x['limpo'],  # False (0) vem antes de True (1)
            abs(len(x['limpo']) - 40)  # Preferir tÃ­tulos de ~40 chars (nem muito curto nem muito longo)
        ))
        
        melhor = candidatos[0]['limpo']
        print(f"   ğŸ“ Grupo de {len(grupo)} tÃ­tulos similares â†’ padronizando para: '{melhor[:50]}...'")
        
        for t in grupo:
            if t['limpo'] != melhor:
                substituicoes[t['linha']] = (t['nivel'], melhor)
    
    # 4. Aplicar substituiÃ§Ãµes
    novas_linhas = []
    for i, linha in enumerate(linhas):
        if i in substituicoes:
            nivel, novo_titulo = substituicoes[i]
            # Preservar numeraÃ§Ã£o existente se houver
            match_num = re.match(r'^(#+\s*\d+(?:\.\d+)*\.?\s*)', linhas[i].strip())
            if match_num:
                prefixo = match_num.group(1)
                # Remove a numeraÃ§Ã£o do novo tÃ­tulo se ele tiver uma
                novo_titulo = re.sub(r'^\d+(?:\.\d+)*\.?\s*', '', novo_titulo)
                novas_linhas.append(f"{prefixo}{novo_titulo}")
            else:
                novas_linhas.append(f"{'#' * nivel} {novo_titulo}")
        else:
            # Remove "(ContinuaÃ§Ã£o)" de qualquer tÃ­tulo restante
            if linha.strip().startswith('##'):
                linha = re.sub(r'\s*\(ContinuaÃ§Ã£o\)\s*(?=\n|$)', '', linha, flags=re.IGNORECASE)
            novas_linhas.append(linha)
    
    print(f"   âœ… {len(substituicoes)} tÃ­tulos normalizados, '(ContinuaÃ§Ã£o)' removidos")
    return '\n'.join(novas_linhas)

# =============================================================================
# REVISÃƒO SEMÃ‚NTICA DE ESTRUTURA POR IA (v2.16)
# =============================================================================

# =============================================================================
# REVISÃƒO SEMÃ‚NTICA, MAPEAMENTO E ESTRUTURA (v2.16)
# =============================================================================

PROMPT_MAPEAMENTO = """VocÃª Ã© um especialista em organizaÃ§Ã£o de conteÃºdo educacional acadÃªmico (v2.17).

## ETAPA 1: IDENTIFICAR O TIPO DE CONTEÃšDO
Analise a transcriÃ§Ã£o e determine qual Ã© a **natureza predominante** do material:

| Tipo | Pistas no Texto | Estrutura Ideal |
|------|-----------------|-----------------|
| **SIMULADO** | "questÃ£o 1", "questÃ£o 2", "espelho de correÃ§Ã£o", "correÃ§Ã£o do simulado", "vamos corrigir" | Organizar por QUESTÃ•ES numeradas |
| **AULA EXPOSITIVA** | explicaÃ§Ãµes contÃ­nuas de um tema, teoria, doutrina, sem questÃµes especÃ­ficas | Organizar por TEMAS/MATÃ‰RIAS e MARCOS LEGAIS |
| **REVISÃƒO** | "revisÃ£o", "resumo", mÃºltiplos temas curtos, "pontos importantes" | Organizar por TÃ“PICOS de revisÃ£o |
| **CORREÃ‡ÃƒO DE PROVA** | "gabarito", "alternativa correta", "item certo/errado" | Organizar por QUESTÃ•ES com gabarito |

## ETAPA 2: EXTRAIR A ESTRUTURA

### Se for SIMULADO ou CORREÃ‡ÃƒO DE PROVA:
```
1. OrientaÃ§Ãµes Gerais / IntroduÃ§Ã£o
2. QuestÃ£o 1: [TÃ­tulo descritivo] â€” [Ãrea do Direito]
   2.1. Enunciado e Contexto
   2.2. FundamentaÃ§Ã£o (Doutrina/JurisprudÃªncia)
   2.3. Pontos do Espelho / Resposta
3. QuestÃ£o 2: [TÃ­tulo descritivo] â€” [Ãrea do Direito]
   3.1. ...
[Continue para cada questÃ£o]
N. ConsideraÃ§Ãµes Finais / DÃºvidas
```

### Se for AULA EXPOSITIVA (ATENÃ‡ÃƒO AOS MARCOS LEGAIS):
VocÃª DEVE identificar **Marcos Legais e Jurisprudenciais** importantes e elevÃ¡-los Ã  categoria de SUBTÃ“PICOS.
Exemplos de marcos: "SÃºmula X", "Artigo Y do CC", "Tese de RepercussÃ£o Geral Z", "Julgado X do STF".

```
1. IntroduÃ§Ã£o
2. [MatÃ©ria 1: ex. Direito Administrativo]
   2.1. [Subtema]
      2.1.1. [Detalhamento]
3. [MatÃ©ria 2: ex. Direito Civil]
   3.1. ...
```

### Se for REVISÃƒO:
```
1. [Tema 1]
   1.1. Pontos-chave
   1.2. JurisprudÃªncia/SÃºmulas
2. [Tema 2]
   2.1. ...
```

## REGRAS GERAIS:
1. **MÃXIMO 3 NÃVEIS** de hierarquia (1., 1.1., 1.1.1.)
2. **Seja descritivo** nos tÃ­tulos â€” inclua o assunto real, nÃ£o apenas "QuestÃ£o 1"
3. **Mantenha a ORDEM** cronolÃ³gica da transcriÃ§Ã£o
4. **Mapeie do INÃCIO ao FIM** â€” nÃ£o omita partes
5. **Identifique a ÃREA DO DIREITO** de cada bloco quando possÃ­vel

## TRANSCRIÃ‡ÃƒO:
{transcricao}

## RESPOSTA:
Primeiro, indique em uma linha: `[TIPO: SIMULADO/EXPOSITIVA/REVISÃƒO/CORREÃ‡ÃƒO]`
Depois, retorne APENAS a estrutura hierÃ¡rquica (mÃ¡x 3 nÃ­veis)."""

PROMPT_STRUCTURE_REVIEW = """VocÃª Ã© um revisor especializado em estrutura de documentos jurÃ­dicos educacionais.

## ESTRUTURA DE MAPEAMENTO INICIAL (ReferÃªncia - se disponÃ­vel):
{estrutura_mapeada}

## TAREFA
Revise a ESTRUTURA (headers/tÃ­tulos) do documento abaixo, COMPARANDO com o mapeamento acima (se disponÃ­vel). Sua missÃ£o Ã© harmonizar o mapeamento planejado com o CONTEÃšDO REAL da aula.

## âœ… O QUE VOCÃŠ DEVE FAZER:

### 1. COMPARAR E REFINAR TÃTULOS (CRÃTICO)
Verifique se los tÃ­tulos refletem os tÃ³picos do mapeamento. Se um tÃ­tulo for genÃ©rico, torne-o descritivo.
- ERRADO: "### QuestÃ£o" ou "### TÃ³pico"
- CORRETO: "### QuestÃ£o 1: Responsabilidade Civil" (Descritivo conforme mapa/conteÃºdo)
- **CRÃTICO:** Evite tÃ­tulos idÃªnticos em seÃ§Ãµes "irmÃ£s". Diferencie-os pelo conteÃºdo especÃ­fico de cada uma.

### 2. VALIDAR HIERARQUIA E PROMOÃ‡ÃƒO DE TÃ“PICOS
Confirme se a estrutura segue lÃ³gica consistente (##, ###, ####). 
- **PROMOÃ‡ÃƒO:** Se um sub-subtÃ³pico (ex: 9.19.5) for extenso e tratar de um tema central (ex: ExecuÃ§Ã£o Fiscal), PROMOVA-O a um nÃ­vel superior (ex: ### 9.20) para evitar fragmentaÃ§Ã£o excessiva e respeitar o limite de nÃ­veis.

### 3. RENUMERAÃ‡ÃƒO SEQUENCIAL OBRIGATÃ“RIA
Se vocÃª criar, deletar ou promover uma seÃ§Ã£o, vocÃª DEVE renumerar TODAS as seÃ§Ãµes subsequentes daquela mesma hierarquia para manter a sequÃªncia lÃ³gica (ex: se 9.20 foi criado, o antigo 9.20 vira 9.21, e assim por diante).

### 4. MESCLAR QUESTÃ•ES DUPLICADAS
Se duas seÃ§Ãµes tÃªm o mesmo nÃºmero de questÃ£o na mesma Ã¡rea, MESCLE-AS.
- ERRADO: "2.1. QuestÃ£o 1: TAC" + "2.2. QuestÃ£o 1: TAC" 
- CORRETO: "2.1. QuestÃ£o 1: TAC" (Ãšnica, com todo o conteÃºdo unificado)

### 5. PRIORIDADE DO CONTEÃšDO REAL (DECIDIR ESTRUTURA)
Se houver conflito entre mapeamento e documento, escolha a estrutura que melhor reflete o CONTEÃšDO REAL. O mapeamento Ã© apenas um guia.

### 6. LIMPEZA TÃ‰CNICA (SINTAXE MARKDOWN)
Corrija problemas detalhados de formataÃ§Ã£o:
- **Tabelas:** Alinhar colunas e adicionar separadores faltantes.
- **Listas:** Corrigir bullets ou numeraÃ§Ã£o mal formatada.
- **EspaÃ§amento:** Padronizar linhas em branco entre seÃ§Ãµes (mÃ­nimo uma linha).
- **Headers Vazios:** Remover tÃ­tulos sem conteÃºdo abaixo.
- **Metadados:** Remover headers ou tags inline como "[TIPO: ...]", "**[TIPO: ...]**", ou "[BLOCO 0X]".

## âŒ O QUE VOCÃŠ NÃƒO DEVE FAZER:
1. **NÃƒO ALTERE O CONTEÃšDO** dos parÃ¡grafos - apenas os tÃ­tulos e a organizaÃ§Ã£o.
2. **NUNCA RESUMA** ou encurte o texto (mesmo tamanho de entrada/saÃ­da Ã© obrigatÃ³rio).
3. **NÃƒO INVENTE** fatos jurÃ­dicos.
4. **NÃƒO REMOVA** trechos tÃ©cnicos ou exemplos.

## REGRAS CRÃTICAS DE HIERARQUIA:
- Use **MÃXIMO 3** nÃ­veis de hierarquia (##, ###, ####).
- Nunca use # (H1) para subtÃ³picos (apenas para o tÃ­tulo principal do documento).
- Preserve a ordem cronolÃ³gica geral.

## DOCUMENTO PARA REVISAR:
{documento}

## ğŸ“ RELATÃ“RIO ESPERADO:
Ao final do documento, inclua um bloco de comentÃ¡rio indicando:
- Quantos tÃ­tulos foram refinados
- PromoÃ§Ãµes de seÃ§Ãµes e renumeraÃ§Ãµes realizadas
- DiscrepÃ¢ncias com o mapeamento (se houver)

Formato:
<!-- RELATÃ“RIO: X tÃ­tulos refinados | Y seÃ§Ãµes promovidas/renumeradas | DiscrepÃ¢ncias: [Nenhuma/Lista] -->

## RESPOSTA:
Retorne o documento COMPLETO E INTEGRAL (mesmo tamanho do original) com os tÃ­tulos/headers corrigidos e o relatÃ³rio no final. NÃƒO RESUMA."""


def map_structure(client, full_text):
    """Creates a global structure skeleton to guide the formatting."""
    logger.info("ğŸ—ºï¸  Mapeando estrutura global do documento...")
    
    # Limit input to avoid context overflow (200k chars is plenty for structure)
    input_sample = full_text[:200000] 
    
    try:
        rate_limiter.wait_if_needed()
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=PROMPT_MAPEAMENTO.format(transcricao=input_sample),
            config=types.GenerateContentConfig(
                max_output_tokens=20000,
                thinking_config={"include_thoughts": False, "thinking_level": "HIGH"}
            )
        )
        content = response.text.replace('```markdown', '').replace('```', '')
        logger.info("   âœ… Estrutura mapeada com sucesso.")
        return content

    except Exception as e:
        logger.warning(f"   âš ï¸  Falha no mapeamento via Gemini: {e}")
        return None

def ai_structure_review(client, texto, estrutura_mapeada=None):
    """
    v2.0: RevisÃ£o semÃ¢ntica de estrutura usando IA com VALIDAÃ‡ÃƒO CRUZADA.
    Compara o documento com a estrutura de mapeamento inicial.
    Corrige: questÃµes duplicadas, subtÃ³picos Ã³rfÃ£os, fragmentaÃ§Ã£o excessiva.
    """
    logger.info("ğŸ§  RevisÃ£o SemÃ¢ntica de Estrutura (IA v2.0)...")
    
    # Gemini 3 Flash suporta 1M tokens (~4M chars) - usar atÃ© 500k chars
    max_doc_chars = 500000
    if len(texto) > max_doc_chars:
        logger.warning(f"   âš ï¸ Documento muito longo ({len(texto)} chars), truncando para {max_doc_chars//1000}k...")
        texto_para_revisao = texto[:max_doc_chars] + "\n\n[... documento truncado para revisÃ£o estrutural ...]"
    else:
        texto_para_revisao = texto
    
    # Preparar estrutura mapeada (se disponÃ­vel)
    if estrutura_mapeada:
        estrutura_str = estrutura_mapeada[:50000]  # Limitar estrutura a 50k chars
        logger.info(f"   ğŸ“‹ Usando estrutura de mapeamento inicial ({len(estrutura_mapeada)} chars) para validaÃ§Ã£o cruzada.")
    else:
        estrutura_str = "[Estrutura de mapeamento nÃ£o disponÃ­vel - analisar documento autonomamente]"
        logger.info("   â„¹ï¸  Sem mapeamento inicial, IA revisarÃ¡ estrutura autonomamente.")
    
    try:
        rate_limiter.wait_if_needed()
        
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=PROMPT_STRUCTURE_REVIEW.format(
                estrutura_mapeada=estrutura_str,
                documento=texto_para_revisao
            ),
            config=types.GenerateContentConfig(
                max_output_tokens=65536,  # MÃ¡ximo permitido
                thinking_config={"include_thoughts": False, "thinking_level": "HIGH"},
                safety_settings=[
                    types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                ]
            )
        )
        
        resultado = response.text.replace('```markdown', '').replace('```', '').strip()
        
        # Extrair e exibir relatÃ³rio da IA (se presente)
        relatorio_match = re.search(r'<!--\s*RELATÃ“RIO:\s*(.+?)\s*-->', resultado, re.IGNORECASE)
        if relatorio_match:
            relatorio = relatorio_match.group(1)
            logger.info(f"   ğŸ“Š RelatÃ³rio da IA: {relatorio}")
            # Remover o comentÃ¡rio do resultado final
            resultado = re.sub(r'<!--\s*RELATÃ“RIO:.+?-->\s*', '', resultado, flags=re.IGNORECASE).strip()
        
        # ValidaÃ§Ã£o bÃ¡sica: o resultado deve ter pelo menos 70% do tamanho original
        if len(resultado) < len(texto) * 0.7:
            logger.warning(f"   âš ï¸ RevisÃ£o retornou texto muito curto ({len(resultado)} vs {len(texto)}). Mantendo original.")
            return texto
        
        # Contar quantos headers foram alterados
        headers_original = len(re.findall(r'^#{2,4}\s', texto, re.MULTILINE))
        headers_revisado = len(re.findall(r'^#{2,4}\s', resultado, re.MULTILINE))
        diff = abs(headers_original - headers_revisado)
        
        logger.info(f"   âœ… Estrutura revisada: {headers_original} â†’ {headers_revisado} headers (Î”{diff})")
        return resultado
        
    except Exception as e:
        logger.warning(f"   âš ï¸ Erro na revisÃ£o por IA: {e}. Mantendo estrutura original.")
        return texto

# =============================================================================
# REVISÃƒO LEVE DE FORMATAÃ‡ÃƒO (MODO FIDELIDADE) v2.0
# =============================================================================

PROMPT_STRUCTURE_REVIEW_LITE = """VocÃª Ã© um revisor editorial especializado em ESTRUTURA e FORMATAÃ‡ÃƒO de documentos educacionais. VocÃª receberÃ¡:
1. Uma **Estrutura de Mapeamento Inicial** (planejada antes da formataÃ§Ã£o)
2. O **Documento Processado** (resultado da formataÃ§Ã£o por chunks)

Sua tarefa Ã© analisar ambos e garantir que os tÃ­tulos estejam **descritivos, hierarquicamente corretos e alinhados com o conteÃºdo real**, sem jamais alterar a ordem cronolÃ³gica.

---

## ğŸ“‹ ESTRUTURA DE MAPEAMENTO INICIAL (ReferÃªncia):
{estrutura_mapeada}

---

## âœ… O QUE VOCÃŠ DEVE FAZER:
1. **Comparar TÃ­tulos:** Verifique se os tÃ­tulos do documento refletem corretamente os tÃ³picos do mapeamento. Se um tÃ­tulo estiver genÃ©rico mas o mapeamento indicar um tema especÃ­fico, refine-o.
2. **Validar Hierarquia:** Confirme que a estrutura (##, ###, ####) segue uma lÃ³gica consistente (ex: seÃ§Ãµes > subseÃ§Ãµes > detalhes). MÃXIMO 3 nÃ­veis.
3. **Decidir a Melhor Estrutura:** Se houver conflito entre mapeamento e documento, escolha a estrutura que melhor reflete o CONTEÃšDO REAL do texto.
4. **SubtÃ³picos Ã“rfÃ£os:** Se detectar headers como "A.", "B.", "C." isolados como tÃ³picos principais, converta-os em subnÃ­veis do tÃ³pico anterior (ex: ## para ###).
5. **TÃ­tulos Descritivos:** Refine tÃ­tulos genÃ©ricos (ex: "QuestÃ£o 1") para algo que cite o tema tÃ©cnico (ex: "QuestÃ£o 1: Responsabilidade Civil").
6. **Corrigir Sintaxe Markdown:** Tabelas (alinhar colunas), listas (bullets), espaÃ§amento entre seÃ§Ãµes.
7. **Remover Vazios:** TÃ­tulos sem conteÃºdo abaixo.

## âŒ O QUE VOCÃŠ NÃƒO DEVE FAZER:
1. **NÃƒO MOVA** blocos de texto. A ordem deve permanecer 100% cronolÃ³gica.
2. **NÃƒO MESCLE** seÃ§Ãµes que apareÃ§am em momentos diferentes da aula.
3. **NÃƒO RESUMA** nem altere o corpo dos parÃ¡grafos.
4. **NÃƒO ADICIONE** conteÃºdo novo.

## ğŸ“ RELATÃ“RIO ESPERADO:
Ao final do documento, inclua um bloco de comentÃ¡rio (que serÃ¡ removido) indicando:
- Quantos tÃ­tulos foram refinados
- Se a estrutura final segue o mapeamento ou foi adaptada
- DiscrepÃ¢ncias encontradas (se houver)

Formato:
<!-- RELATÃ“RIO: X tÃ­tulos refinados | Estrutura: [MAPEAMENTO/ADAPTADA] | DiscrepÃ¢ncias: [Nenhuma/Lista] -->

---

## ğŸ“„ DOCUMENTO PARA REVISAR:
{documento}

---

## RESPOSTA:
Retorne o documento COMPLETO E INTEGRAL (mesmo tamanho do input) com a formataÃ§Ã£o corrigida e o relatÃ³rio no final. NÃƒO RESUMA."""

def ai_structure_review_lite(client, texto, estrutura_mapeada=None):
    """
    v2.0: RevisÃ£o LEVE de formataÃ§Ã£o Markdown com VALIDAÃ‡ÃƒO CRUZADA.
    Compara o documento processado com a estrutura de mapeamento inicial.
    Refina tÃ­tulos, valida hierarquia, e reporta discrepÃ¢ncias.
    NÃƒO reorganiza nem mescla conteÃºdo.
    """
    logger.info("ğŸ§¹ RevisÃ£o Leve de FormataÃ§Ã£o (IA - Modo Fidelidade v2.0)...")
    
    # Gemini 2.0 Flash suporta 1M tokens (~4M chars) - usar atÃ© 500k chars para documento + 50k para estrutura
    max_doc_chars = 500000
    if len(texto) > max_doc_chars:
        logger.warning(f"   âš ï¸ Documento muito longo ({len(texto)} chars), truncando para {max_doc_chars//1000}k...")
        texto_para_revisao = texto[:max_doc_chars] + "\n\n[... documento truncado para revisÃ£o ...]"
    else:
        texto_para_revisao = texto
    
    # Preparar estrutura mapeada (se disponÃ­vel)
    if estrutura_mapeada:
        estrutura_str = estrutura_mapeada[:50000]  # Limitar estrutura a 50k chars
        logger.info(f"   ğŸ“‹ Usando estrutura de mapeamento inicial ({len(estrutura_mapeada)} chars) para validaÃ§Ã£o cruzada.")
    else:
        estrutura_str = "[Estrutura de mapeamento nÃ£o disponÃ­vel - analisar documento para inferir estrutura ideal]"
        logger.info("   â„¹ï¸  Sem mapeamento inicial, IA irÃ¡ inferir estrutura ideal do prÃ³prio documento.")
    
    try:
        rate_limiter.wait_if_needed()
        
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=PROMPT_STRUCTURE_REVIEW_LITE.format(
                estrutura_mapeada=estrutura_str,
                documento=texto_para_revisao
            ),
            config=types.GenerateContentConfig(
                max_output_tokens=65536,  # MÃ¡ximo permitido
                thinking_config={"include_thoughts": False, "thinking_level": "HIGH"},  # HIGH para anÃ¡lise estrutural profunda
                safety_settings=[
                    types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                ]
            )
        )
        
        resultado = response.text.replace('```markdown', '').replace('```', '').strip()
        
        # Extrair e exibir relatÃ³rio da IA (se presente)
        relatorio_match = re.search(r'<!--\s*RELATÃ“RIO:\s*(.+?)\s*-->', resultado, re.IGNORECASE)
        if relatorio_match:
            relatorio = relatorio_match.group(1)
            logger.info(f"   ğŸ“Š RelatÃ³rio da IA: {relatorio}")
            # Remover o comentÃ¡rio do resultado final
            resultado = re.sub(r'<!--\s*RELATÃ“RIO:.+?-->\s*', '', resultado, flags=re.IGNORECASE).strip()
        
        # ValidaÃ§Ã£o: resultado deve ter pelo menos 85% do tamanho
        if len(resultado) < len(texto) * 0.85:
            logger.warning(f"   âš ï¸ RevisÃ£o retornou texto muito curto ({len(resultado)} vs {len(texto)}). Mantendo original.")
            return texto
        
        # Verificar se a ordem dos headers foi preservada (validaÃ§Ã£o extra)
        headers_original = re.findall(r'^#{1,4}\s+(.+?)$', texto, re.MULTILINE)[:20]
        headers_revisado = re.findall(r'^#{1,4}\s+(.+?)$', resultado, re.MULTILINE)[:20]
        
        if len(headers_original) > 5 and len(headers_revisado) > 5:
            matches = sum(1 for h1, h2 in zip(headers_original[:10], headers_revisado[:10]) 
                         if h1.strip()[:30] == h2.strip()[:30])
            if matches < 6:
                logger.warning(f"   âš ï¸ Ordem dos headers parece alterada. Mantendo original.")
                return texto
        
        # RelatÃ³rio de alteraÃ§Ãµes de tÃ­tulos
        alteracoes = []
        for h_orig, h_rev in zip(headers_original[:15], headers_revisado[:15]):
            if h_orig.strip() != h_rev.strip():
                orig_short = h_orig.strip()[:40] + "..." if len(h_orig.strip()) > 40 else h_orig.strip()
                rev_short = h_rev.strip()[:40] + "..." if len(h_rev.strip()) > 40 else h_rev.strip()
                alteracoes.append(f"   - '{orig_short}' â†’ '{rev_short}'")
        
        if alteracoes:
            logger.info(f"ğŸ“ TÃ­tulos Refinados ({len(alteracoes)}):")
            for alt in alteracoes[:5]:
                logger.info(alt)
            if len(alteracoes) > 5:
                logger.info(f"   ... e mais {len(alteracoes) - 5} alteraÃ§Ãµes")
        else:
            logger.info("   â„¹ï¸  Nenhum tÃ­tulo foi alterado (estrutura jÃ¡ estava OK).")
        
        logger.info(f"   âœ… FormataÃ§Ã£o revisada (modo leve v2.0).")
        return resultado
        
    except Exception as e:
        logger.warning(f"   âš ï¸ Erro na revisÃ£o leve: {e}. Mantendo original.")
        return texto




def deterministic_structure_fix(text):
    """
    v1.1: ReorganizaÃ§Ã£o Estrutural DeterminÃ­stica (Regex).
    Adaptativo: Detecta se o documento usa H1 ou apenas H2 como nÃ­vel principal.
    """
    logger.info("ğŸ§© Executando ReorganizaÃ§Ã£o Estrutural DeterminÃ­stica...")
    
    lines = text.split('\n')
    
    # DetecÃ§Ã£o de Hierarquia
    has_h1 = any(re.match(r'^#\s+', line) for line in lines)
    header_level_regex = r'^#\s+' if has_h1 else r'^##\s+'
    logger.info(f"   â„¹ï¸  NÃ­vel principal detectado: {'H1 (#)' if has_h1 else 'H2 (##)'}")

    # Estruturas de dados (Preserva ordem de inserÃ§Ã£o)
    content_map = {
        "PREAMBULO": [],
        "DISCIPLINAS": {}, 
        "ENCERRAMENTO": []
    }
    
    current_area = "PREAMBULO"
    current_block = []
    disciplinas_order = [] 
    
    # Regex Adaptativo
    # Captura o texto do cabeÃ§alho principal (seja H1 ou H2)
    # Exclui "QuestÃ£o" ou "Q." para nÃ£o quebrar simulados dentro de uma Ã¡rea
    re_disciplina = re.compile(f'{header_level_regex}(?!QuestÃ£o|Q\\.)([^0-9\\.]+.*)', re.IGNORECASE)
    re_encerramento = re.compile(f'{header_level_regex}(?:ENCERRAMENTO|CONSIDERAÃ‡Ã•ES|CONCLUSÃƒO)', re.IGNORECASE)
    
    def flush_block(area, block_lines):
        if not block_lines: return
        block_text = '\n'.join(block_lines)
        
        if area == "PREAMBULO":
            content_map["PREAMBULO"].append(block_text)
        elif area == "ENCERRAMENTO":
            content_map["ENCERRAMENTO"].append(block_text)
        else:
            if area not in content_map["DISCIPLINAS"]:
                content_map["DISCIPLINAS"][area] = []
                disciplinas_order.append(area)
            content_map["DISCIPLINAS"][area].append(block_text)

    for line in lines:
        # 1. Detectar mudanÃ§a de disciplina macro
        match_disc = re_disciplina.match(line)
        if match_disc:
            flush_block(current_area, current_block)
            current_block = []
            
            raw_area = match_disc.group(1).strip().upper()
            
            # NormalizaÃ§Ã£o de nome de Ã¡rea
            if "DIREITO" not in raw_area and len(raw_area) < 50:
                 # Adiciona prefixo se parecer nome de matÃ©ria jurÃ­dica comum
                 if any(x in raw_area for x in ["CIVIL", "PENAL", "TRABALHO", "ADMINISTRATIVO", "CONSTITUCIONAL"]):
                     current_area = f"DIREITO {raw_area}"
                 else:
                     current_area = raw_area
            else:
                 current_area = raw_area
            
            # IMPORTANTE: Se estamos operando em modo H2, essa linha Ã© um Header que queremos manter?
            # Se for modo H1, recriamos "# AREA". 
            # Se for modo H2, recriamos "# AREA" (upcast) ou mantemos "## AREA"?
            # Para padronizar Apostilas, vamos promover tudo a H1 na reconstruÃ§Ã£o.
            continue 
            
        # 2. Detectar Encerramento
        if re_encerramento.match(line):
            flush_block(current_area, current_block)
            current_block = []
            current_area = "ENCERRAMENTO"
            continue

        current_block.append(line)
        
    flush_block(current_area, current_block)
    
    # ReconstruÃ§Ã£o
    final_output = []
    
    # Preambulo
    if content_map["PREAMBULO"]:
        final_output.append("# ORIENTAÃ‡Ã•ES GERAIS / INTRODUÃ‡ÃƒO")
        final_output.extend(content_map["PREAMBULO"])
        final_output.append("")

    # Disciplinas / TÃ³picos Principais
    for area in disciplinas_order:
        area_clean = area.replace("#", "").strip()
        final_output.append(f"# {area_clean}")
        for block in content_map["DISCIPLINAS"][area]:
            final_output.append(block)
        final_output.append("")
        
    # Encerramento
    if content_map["ENCERRAMENTO"]:
        final_output.append("# CONSIDERAÃ‡Ã•ES FINAIS")
        final_output.extend(content_map["ENCERRAMENTO"])
        
    num_identified = len(disciplinas_order)
    logger.info(f"   âœ… Reorganizado: {num_identified} seÃ§Ãµes principais identificadas.")
    
    # Fallback: Se nÃ£o identificou nada (tudo preambulo), retorna original para nÃ£o estragar
    if num_identified == 0 and len(content_map["PREAMBULO"]) > 0:
        logger.warning("   âš ï¸ Nenhuma estrutura detectada. Mantendo original.")
        return text
        
    return '\n'.join(final_output)

def numerar_titulos(texto):
    """Adiciona numeraÃ§Ã£o sequencial aos tÃ­tulos"""
    linhas = texto.split('\n')
    linhas_numeradas = []
    
    contador_h2 = 0
    contador_h3 = 0
    contador_h4 = 0
    
    titulo_pattern = re.compile(r'^(#{2,4})\s+(?:\d+(?:\.\d+)*\.?\s+)?(.+)$')
    
    # Variaveis para rastrear Ãºltimos tÃ­tulos e evitar repetiÃ§Ãµes
    ultimo_h2_texto = ""
    ultimo_h3_texto = ""
    
    for linha in linhas:
        match = titulo_pattern.match(linha)
        
        if match:
            nivel = len(match.group(1))
            texto_titulo = match.group(2).strip()
            
            # NÃ£o numera tÃ­tulos de resumo/quadros
            if any(keyword in texto_titulo.lower() for keyword in ['resumo', 'quadro', 'esquema', 'ğŸ“‹', 'ğŸ“Š', 'ğŸ—‚ï¸']):
                linhas_numeradas.append(linha)
                continue
            
            # MERGE INTELIGENTE DE TÃTULOS REPETIDOS (v2.11)
            # Se o tÃ­tulo atual for muito similar ao anterior do mesmo nÃ­vel ("continuaÃ§Ã£o" de chunk), ignoramos o novo
            # para que o texto flua como um Ãºnico tÃ³pico.
            
            from difflib import SequenceMatcher
            eh_duplicado = False
            
            if nivel == 2:
                # Verifica similaridade com Ãºltimo H2
                ratio = SequenceMatcher(None, texto_titulo.lower(), ultimo_h2_texto.lower()).ratio()
                if ratio > 0.9:
                    eh_duplicado = True
                    logger.info(f"ğŸ”„ TÃ­tulo H2 mesclado: '{texto_titulo}' â‰ˆ '{ultimo_h2_texto}'")
                else:
                    ultimo_h2_texto = texto_titulo
            elif nivel == 3:
                 # Verifica similaridade com Ãºltimo H3
                ratio = SequenceMatcher(None, texto_titulo.lower(), ultimo_h3_texto.lower()).ratio()
                if ratio > 0.9:
                    eh_duplicado = True
                    logger.info(f"ğŸ”„ TÃ­tulo H3 mesclado: '{texto_titulo}' â‰ˆ '{ultimo_h3_texto}'")
                else:
                    ultimo_h3_texto = texto_titulo
            
            if eh_duplicado:
                continue # Pula a linha do tÃ­tulo, fundindo o conteÃºdo
            
            if nivel == 2:
                contador_h2 += 1
                contador_h3 = 0
                contador_h4 = 0
                nova_linha = f"## {contador_h2}. {texto_titulo}"
            elif nivel == 3:
                contador_h3 += 1
                contador_h4 = 0
                nova_linha = f"### {contador_h2}.{contador_h3}. {texto_titulo}"
            elif nivel == 4:
                contador_h4 += 1
                nova_linha = f"#### {contador_h2}.{contador_h3}.{contador_h4}. {texto_titulo}"
            else:
                nova_linha = linha
            
            linhas_numeradas.append(nova_linha)
        else:
            linhas_numeradas.append(linha)
    
    return '\n'.join(linhas_numeradas)

def renumerar_secoes(texto):
    """
    v1.0: RenumeraÃ§Ã£o Sequencial DeterminÃ­stica.
    
    Esta funÃ§Ã£o Ã© uma camada de seguranÃ§a extra, aplicada APÃ“S o AI Review.
    Ela percorre todos os headers numerados e corrige qualquer duplicaÃ§Ã£o ou
    sequÃªncia quebrada, garantindo que os nÃºmeros sejam estritamente sequenciais
    dentro de cada nÃ­vel de hierarquia.
    
    Exemplo de correÃ§Ã£o:
    - Input:  9.20, 9.21, 9.21, 9.35, 9.36  (duplicaÃ§Ã£o e pulo)
    - Output: 9.20, 9.21, 9.22, 9.23, 9.24  (sequencial)
    """
    logger.info("ğŸ”¢ Executando RenumeraÃ§Ã£o Sequencial DeterminÃ­stica...")
    
    linhas = texto.split('\n')
    novas_linhas = []
    
    # Contadores por nÃ­vel de hierarquia (ex: {2: 9, 3: 44} -> ## 9.x, ### 9.44.x)
    # Estrutura: {(header_level, parent_prefix): next_number}
    contadores = {}
    
    # Regex para detectar headers numerados: ### 9.20.1. TÃ­tulo ou ## 5. TÃ­tulo
    header_pattern = re.compile(r'^(#{1,4})\s+([\d.]+\.?)\s*(.*)$')
    
    for linha in linhas:
        match = header_pattern.match(linha)
        
        if match:
            hashes = match.group(1)       # "###"
            numero = match.group(2)       # "9.20.1." ou "9.20.1"
            titulo = match.group(3)       # "TÃ­tulo..."
            
            nivel = len(hashes)
            numero_limpo = numero.rstrip('.')
            partes = numero_limpo.split('.')
            
            # Determina o prefixo pai e o sufixo atual
            if len(partes) == 1:
                # NÃ­vel raiz: ## 1. ou ## 9.
                prefixo_pai = ""
                sufixo_atual = int(partes[0])
            else:
                # SubnÃ­vel: ### 9.20. ou #### 9.20.1.
                prefixo_pai = '.'.join(partes[:-1])
                sufixo_atual = int(partes[-1])
            
            # Inicializa contador se nÃ£o existir
            chave = (nivel, prefixo_pai)
            if chave not in contadores:
                contadores[chave] = sufixo_atual  # ComeÃ§a do nÃºmero encontrado
            else:
                contadores[chave] += 1
            
            novo_sufixo = contadores[chave]
            
            # ReconstrÃ³i o nÃºmero
            if prefixo_pai:
                novo_numero = f"{prefixo_pai}.{novo_sufixo}."
            else:
                novo_numero = f"{novo_sufixo}."
            
            # ReconstrÃ³i a linha
            nova_linha = f"{hashes} {novo_numero} {titulo}"
            novas_linhas.append(nova_linha)
            
            # Log se houve mudanÃ§a
            if numero_limpo != novo_numero.rstrip('.'):
                logger.info(f"   ğŸ”„ {numero_limpo} â†’ {novo_numero.rstrip('.')}")
        else:
            novas_linhas.append(linha)
    
    logger.info("   âœ… RenumeraÃ§Ã£o concluÃ­da.")
    return '\n'.join(novas_linhas)

# =============================================================================
# VERIFICAÃ‡ÃƒO DE COBERTURA E DUPLICAÃ‡Ã•ES
# =============================================================================

def normalizar_fingerprint(texto, tipo):
    """Normaliza texto para comparaÃ§Ã£o (ex: 'Lei 11.100' -> 'lei 11100')"""
    texto = texto.lower().strip()
    
    if tipo == 'leis':
        # MantÃ©m apenas 'lei' e nÃºmeros
        nums = re.findall(r'\d+', texto)
        if nums:
            # ReconstrÃ³i como 'lei 12345'
            # Filtra leis com menos de 4 dÃ­gitos para evitar ruÃ­do (ex: lei 10, lei 13)
            num_full = ''.join(nums)
            if len(num_full) >= 4:
                return f"lei {num_full}"
            return None
            
    elif tipo == 'sumulas':
        nums = re.findall(r'\d+', texto)
        if nums:
            return f"sÃºmula {''.join(nums)}"
            
    elif tipo == 'artigos':
        nums = re.findall(r'\d+', texto)
        if nums:
            return f"artigo {''.join(nums)}"
            
    return re.sub(r'[^\w\s]', '', texto)

def extrair_fingerprints(texto):
    """Extrai 'fingerprints' Ãºnicos e normalizados do texto"""
    fingerprints = {
        'leis': set(),
        'sumulas': set(),
        'artigos': set(),
        'julgados': set()
    }
    
    # Regex melhorado para capturar variaÃ§Ãµes
    lei_pattern = re.compile(r'\b(?:lei|l\.)\s*n?Âº?\s*([\d\.]+)', re.IGNORECASE)
    sumula_pattern = re.compile(r'\bsÃºmula\s*(?:vinculante)?\s*n?Âº?\s*(\d+)', re.IGNORECASE)
    
    # Extrai e normaliza
    for match in lei_pattern.finditer(texto):
        fp = normalizar_fingerprint(f"lei {match.group(1)}", 'leis')
        if fp: fingerprints['leis'].add(fp)
    
    for match in sumula_pattern.finditer(texto):
        fp = normalizar_fingerprint(f"sÃºmula {match.group(1)}", 'sumulas')
        if fp: fingerprints['sumulas'].add(fp)
        
    return fingerprints

def contar_ocorrencias_robust(fingerprints, texto):
    """Conta ocorrÃªncias com suporte a formataÃ§Ã£o jurÃ­dica formal (Lei nÂº X)"""
    contagens = {}
    
    # CORREÃ‡ÃƒO 1: NÃ£o remover pontuaÃ§Ã£o indiscriminadamente, apenas normalizar espaÃ§os
    # Mantemos barras e pontos para evitar fusÃ£o de nÃºmeros (11.101/2005)
    texto_lower = texto.lower()
    
    for categoria, items in fingerprints.items():
        for item in items:
            key = f"{categoria}:{item}"
            
            if categoria == 'leis':
                # item ex: "lei 4320" -> extrai "4320"
                # Remove pontuaÃ§Ã£o do item para garantir match limpo no nÃºmero
                num_bruto = item.split()[-1] 
                num = re.sub(r'[^\d]', '', num_bruto)
                
                # Permite pontos opcionais entre dÃ­gitos (ex: 4.320 match com 4320)
                num_regex = r"\.?".join(list(num))
                
                # CORREÃ‡ÃƒO 2: Regex flexÃ­vel que aceita "n", "nÂº", "no", "num" no meio
                # Aceita: "Lei 4320", "Lei nÂº 4.320", "Lei n. 4320"
                # O \W* permite pontos/barras entre Lei e o nÃºmero
                # Adicionado \b no final para evitar matches parciais (Lei 10 != Lei 100)
                pattern = f"lei(?:\\s+|\\.|\\,|nÂº|n\\.|n\\s|num\\.?)*{num_regex}\\b"
                
                # Usamos findall no texto original (lower) para pegar variaÃ§Ãµes com pontuaÃ§Ã£o
                matches = re.findall(pattern, texto_lower)
                contagens[key] = len(matches)
                
            elif categoria == 'sumulas':
                num = item.split()[-1]
                num_regex = r"\.?".join(list(num)) # SÃºmulas raramente tem ponto, mas por garantia
                
                # Mesma lÃ³gica para sÃºmulas (SÃºmula Vinculante nÂº 10)
                pattern = f"sÃºmula(?:\\s+|\\.|\\,|vinculante|nÂº|n\\.|n\\s)*{num_regex}\\b"
                matches = re.findall(pattern, texto_lower)
                contagens[key] = len(matches)
                
            else:
                # Fallback para outros tipos (busca literal simples)
                contagens[key] = texto_lower.count(item)
                
    return contagens

def verificar_cobertura(texto_original, texto_formatado, arquivo_saida=None):
    """Verifica omissÃµes e duplicaÃ§Ãµes artificiais entre original e formatado"""
    logger.info("ğŸ” Verificando cobertura e duplicaÃ§Ãµes...")
    
    # Extrai fingerprints do original
    fp_original = extrair_fingerprints(texto_original)
    
    # Conta ocorrÃªncias em ambos
    contagem_original = contar_ocorrencias_robust(fp_original, texto_original)
    contagem_formatado = contar_ocorrencias_robust(fp_original, texto_formatado)
    
    omissoes = []
    duplicacoes = []
    
    for key, count_orig in contagem_original.items():
        count_fmt = contagem_formatado.get(key, 0)
        categoria, item = key.split(':', 1)
        
        # OmissÃ£o: estava no original mas sumiu
        if count_orig > 0 and count_fmt == 0:
            omissoes.append({
                'categoria': categoria,
                'item': item,
                'original': count_orig,
                'formatado': count_fmt
            })
        
        # DuplicaÃ§Ã£o (agora considerada positiva em materiais didÃ¡ticos)
        if count_fmt > count_orig:
            duplicacoes.append({
                'categoria': categoria,
                'item': item,
                'original': count_orig,
                'formatado': count_fmt,
                'extra': count_fmt - count_orig
            })
    
    # Gera relatÃ³rio
    total_items = len([k for k, v in contagem_original.items() if v > 0])
    items_preservados = total_items - len(omissoes)
    cobertura = items_preservados / total_items * 100 if total_items > 0 else 100
    
    logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    logger.info(f"ğŸ“Š RELATÃ“RIO DE VERIFICAÃ‡ÃƒO")
    logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    logger.info(f"âœ… Cobertura: {cobertura:.1f}% ({items_preservados}/{total_items} referÃªncias)")
    
    if omissoes:
        logger.warning(f"\nâŒ POSSÃVEIS OMISSÃ•ES ({len(omissoes)}):")
        for o in omissoes[:10]:  # Limita a 10
            logger.warning(f"   - [{o['categoria']}] {o['item']}")
        if len(omissoes) > 10:
            logger.warning(f"   ... e mais {len(omissoes) - 10} omissÃµes")
    else:
        logger.info("âœ… Nenhuma omissÃ£o detectada")
    
    if duplicacoes:
        logger.info(f"\nâ„¹ï¸ CITAÃ‡Ã•ES REFORÃ‡ADAS (Tabelas/Resumos) ({len(duplicacoes)}):")
        for d in duplicacoes[:10]:
            logger.info(f"   - [{d['categoria']}] {d['item']}: {d['original']}x â†’ {d['formatado']}x (+{d['extra']})")
        if len(duplicacoes) > 10:
            logger.info(f"   ... e mais {len(duplicacoes) - 10} citaÃ§Ãµes extras")
    else:
        logger.info("â„¹ï¸ Nenhuma citaÃ§Ã£o extra detectada")
    
    logger.info(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    # Salva relatÃ³rio em arquivo se especificado
    if arquivo_saida:
        relatorio_path = arquivo_saida.replace('.md', '_verificacao.txt')
        with open(relatorio_path, 'w', encoding='utf-8') as f:
            f.write(f"RELATÃ“RIO DE VERIFICAÃ‡ÃƒO\n")
            f.write(f"{'='*50}\n\n")
            f.write(f"Cobertura: {cobertura:.1f}% ({items_preservados}/{total_items})\n\n")
            
            if omissoes:
                f.write(f"OMISSÃ•ES ({len(omissoes)}):\n")
                for o in omissoes:
                    f.write(f"  - [{o['categoria']}] {o['item']}\n")
                f.write("\n")
            
            if duplicacoes:
                f.write(f"DUPLICAÃ‡Ã•ES ARTIFICIAIS ({len(duplicacoes)}):\n")
                for d in duplicacoes:
                    f.write(f"  - [{d['categoria']}] {d['item']}: {d['original']}x original â†’ {d['formatado']}x formatado\n")
        
        logger.info(f"ğŸ“„ RelatÃ³rio salvo: {relatorio_path}")
    
    return {
        'cobertura': cobertura,
        'omissoes': omissoes,
        'duplicacoes': duplicacoes
    }

# =============================================================================
# V2.16: VALIDAÃ‡ÃƒO LLM (Metadata Strategy)
# =============================================================================

PROMPT_VALIDATE_COMPLETENESS = """# TAREFA DE VALIDAÃ‡ÃƒO DE FIDELIDADE (METADATA STRATEGY)

VocÃª Ã© um auditor de qualidade para transcriÃ§Ãµes jurÃ­dicas formatadas.

## SEU OBJETIVO
Compare a ESTRUTURA DO ORIGINAL (Metadata/Skeleton) com o TEXTO FORMATADO FINAL e identifique:

1. **OMISSÃ•ES GRAVES**: Conceitos jurÃ­dicos, leis, sÃºmulas, artigos ou exemplos importantes que estavam no esqueleto original mas foram omitidos no formatado.
2. **DISTORÃ‡Ã•ES**: InformaÃ§Ãµes que foram alteradas de forma que mude o sentido jurÃ­dico.
3. **ESTRUTURA**: Verifique se os tÃ³picos e subtÃ³picos estÃ£o organizados de forma lÃ³gica e se nÃ£o hÃ¡ duplicaÃ§Ãµes.

## REGRAS
- NÃƒO considere como omissÃ£o: hesitaÃ§Ãµes, "nÃ©", "entÃ£o", dados repetitivos, conversas paralelas.
- CONSIDERE como omissÃ£o: qualquer lei, sÃºmula, artigo, jurisprudÃªncia, exemplo prÃ¡tico ou dica de prova.
- O input "TEXTO ORIGINAL" Ã© um RESUMO ESTRUTURAL (Metadata) contendo apenas tÃ­tulos e referÃªncias chave. Use-o para validar se esses elementos aparecem no "TEXTO FORMATADO".

## FORMATO DE RESPOSTA (JSON)
{
    "aprovado": true/false,
    "nota_fidelidade": 0-10,
    "omissoes_graves": ["descriÃ§Ã£o clara do item omitido"],
    "distorcoes": ["descriÃ§Ã£o clara da distorÃ§Ã£o"],
    "problemas_estrutura": ["tÃ­tulos duplicados ou hierarquia quebrada"],
    "observacoes": "comentÃ¡rio geral sobre a qualidade"
}"""

def extract_raw_metadata(texto):
    """
    v2.16.1: Extrai esqueleto robusto do texto original para validaÃ§Ã£o.
    Captura: TÃ­tulos, Leis, SÃºmulas, Artigos, JurisprudÃªncia, Destaques.
    """
    lines = texto.split('\n')
    metadata = []
    metadata.append(f"TOTAL WORDS: {len(texto.split())}")
    metadata.append(f"TOTAL CHARS: {len(texto)}")
    
    # Regex robustas para capturar referÃªncias legais (reutilizando padrÃµes do script)
    patterns = {
        'leis': re.compile(r'\b(?:lei|l\.)\s*n?Âº?\s*([\d\.]+(?:/\d+)?)', re.IGNORECASE),
        'artigos': re.compile(r'\b(?:art\.?|artigo)\s*(\d+)', re.IGNORECASE),
        'sumulas': re.compile(r'\bsÃºmula\s*(?:vinculante)?\s*n?Âº?\s*(\d+)', re.IGNORECASE),
        'jurisprudencia': re.compile(r'\b(?:REsp|RE|ADI|ADPF|HC|MS|AgRg|RMS|Rcl)\s*[\d\.\/\-]+', re.IGNORECASE),
        'informativos': re.compile(r'\b(?:informativo|info\.?)\s*(?:stf|stj)?\s*n?Âº?\s*(\d+)', re.IGNORECASE),
        'temas': re.compile(r'\btema\s*(?:repetitivo)?\s*n?Âº?\s*(\d+)', re.IGNORECASE),
    }
    
    # Contadores para estatÃ­sticas
    refs_encontradas = {k: set() for k in patterns.keys()}
    
    for line in lines:
        line_strip = line.strip()
        if not line_strip: 
            continue
        
        # Detectar possÃ­veis tÃ­tulos (caixa alta, comeÃ§a com nÃºmero, curto)
        if len(line_strip) < 100:
            if line_strip.isupper() and len(line_strip) > 5:
                metadata.append(f"\n[TÃTULO] {line_strip}")
            elif re.match(r'^\d+[\.\)]\s', line_strip):
                metadata.append(f"\n[TÃ“PICO] {line_strip[:80]}")
        
        # Capturar todas as referÃªncias legais
        for categoria, pattern in patterns.items():
            matches = pattern.findall(line_strip)
            for m in matches:
                ref = m if isinstance(m, str) else m[0]
                refs_encontradas[categoria].add(ref.strip())
        
        # Destaques pedagÃ³gicos
        keywords = ['importante', 'atenÃ§Ã£o', 'cuidado', 'exemplo', 'obs:', 'dica', 'pegadinha', 'caiu em prova']
        if any(kw in line_strip.lower() for kw in keywords):
            metadata.append(f"  > DESTAQUE: {line_strip[:120]}...")
    
    # Resumo estatÃ­stico
    metadata.append("\n--- REFERÃŠNCIAS ENCONTRADAS ---")
    for cat, refs in refs_encontradas.items():
        if refs:
            metadata.append(f"[{cat.upper()}] ({len(refs)}): {', '.join(sorted(refs)[:15])}{'...' if len(refs) > 15 else ''}")
    
    return "\n".join(metadata)

# =============================================================================
# AUTO-FIX PASS (v2.18)
# =============================================================================

def aplicar_correcoes_automaticas(texto):
    """
    v2.18: Aplica correÃ§Ãµes automÃ¡ticas baseadas em padrÃµes comuns de erro.
    Retorna (texto_corrigido, lista_de_correcoes).
    """
    correcoes = []
    texto_original = texto
    
    # 1. Remover saudaÃ§Ãµes duplicadas (apenas mantÃ©m a primeira)
    saudacoes_pattern = r'(?:OlÃ¡|Oi),?\s*(?:sejam?\s+)?(?:bem[- ]?vindos?(?:\s+e\s+bem[- ]?vindas?)?)[.,!]?'
    matches = list(re.finditer(saudacoes_pattern, texto, re.IGNORECASE))
    if len(matches) > 1:
        # Remove todas exceto a primeira
        for match in reversed(matches[1:]):
            # Captura a linha inteira onde a saudaÃ§Ã£o aparece
            start = texto.rfind('\n', 0, match.start()) + 1
            end = texto.find('\n', match.end())
            if end == -1: end = len(texto)
            linha = texto[start:end].strip()
            # SÃ³ remove se a linha for majoritariamente saudaÃ§Ã£o
            if len(linha) < 150:
                texto = texto[:start] + texto[end+1:]
                correcoes.append(f"Removida saudaÃ§Ã£o duplicada: '{linha[:50]}...'")
    
    # 2. Remover apresentaÃ§Ãµes repetidas do professor
    apresentacao_pattern = r'Eu sou o professor\s+\w+(?:\s+\w+)?'
    matches = list(re.finditer(apresentacao_pattern, texto, re.IGNORECASE))
    if len(matches) > 1:
        for match in reversed(matches[1:]):
            start = texto.rfind('\n', 0, match.start()) + 1
            end = texto.find('\n', match.end())
            if end == -1: end = len(texto)
            linha = texto[start:end].strip()
            if len(linha) < 200:
                texto = texto[:start] + texto[end+1:]
                correcoes.append(f"Removida apresentaÃ§Ã£o duplicada: '{linha[:50]}...'")
    
    # 3. Padronizar nome do professor (detecta variaÃ§Ãµes e unifica)
    # Extrai primeiro nome mencionado como "professor X"
    nome_match = re.search(r'professor\s+(\w+(?:\s+\w+)?)', texto, re.IGNORECASE)
    if nome_match:
        nome_canonico = nome_match.group(1)
        # Busca variaÃ§Ãµes prÃ³ximas (distÃ¢ncia de Levenshtein simplificada)
        variacoes_pattern = rf'\bprofessor\s+(\w+(?:\s+\w+)?)\b'
        for m in re.finditer(variacoes_pattern, texto, re.IGNORECASE):
            nome_atual = m.group(1)
            if nome_atual.lower() != nome_canonico.lower():
                sim = SequenceMatcher(None, nome_canonico.lower(), nome_atual.lower()).ratio()
                if sim > 0.6 and sim < 1.0:  # Similar mas diferente
                    texto = texto.replace(f"professor {nome_atual}", f"professor {nome_canonico}")
                    texto = texto.replace(f"Professor {nome_atual}", f"Professor {nome_canonico}")
                    correcoes.append(f"Padronizado nome: '{nome_atual}' â†’ '{nome_canonico}'")
    
    # 4. Corrigir itens de lista vazios ou malformados
    # PadrÃ£o: nÃºmero + ponto + espaÃ§os/quebra + prÃ³ximo conteÃºdo
    texto = re.sub(r'(\d+\.)\s*\n\s*((?:Requisitos|Preenchimento|Fundamento|Artigo|Lei))', r'\1 \2', texto)
    if texto != texto_original:
        correcoes.append("Corrigidos itens de lista malformados")
    
    # 5. Remover linhas em branco excessivas (mais de 2 consecutivas)
    texto_limpo = re.sub(r'\n{4,}', '\n\n\n', texto)
    if texto_limpo != texto:
        texto = texto_limpo
        correcoes.append("Removidas linhas em branco excessivas")
    
    logger.info(f"ğŸ”§ Auto-Fix: {len(correcoes)} correÃ§Ãµes aplicadas")
    for c in correcoes:
        logger.info(f"   âœ“ {c}")
    
    return texto, correcoes


def validate_completeness_llm(raw_text, formatted_text, client, output_file=None):
    """
    v2.16.1: ValidaÃ§Ã£o LLM com Metadata Strategy e retorno estruturado.
    """
    logger.info("ğŸ•µï¸ Executando ValidaÃ§Ã£o LLM (Completeness Check) com Gemini 3 Flash...")
    
    # 1. Extrair Metadata do Raw (OtimizaÃ§Ã£o)
    raw_metadata = extract_raw_metadata(raw_text)
    
    # Estimativa de tokens
    input_text = f"{PROMPT_VALIDATE_COMPLETENESS}\n\n## TEXTO ORIGINAL (METADATA/SKELETON):\n{raw_metadata}\n\n## TEXTO FORMATADO:\n{formatted_text}"
    est_tokens = len(input_text) // 4
    logger.info(f"   ğŸ“Š Payload de ValidaÃ§Ã£o: ~{est_tokens:,} tokens")
    
    try:
        if est_tokens > 2_000_000:
             logger.warning("âš ï¸ Payload excede 2M tokens. Pulando validaÃ§Ã£o LLM para evitar erro.")
             return {'aprovado': True, 'nota_fidelidade': 0, 'skipped': True, 'reason': 'payload_too_large'}
             
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=input_text,
            config=types.GenerateContentConfig(
                response_mime_type="application/json",
                max_output_tokens=8000,
                thinking_config=types.ThinkingConfig(
                    include_thoughts=False,
                    thinking_level="HIGH"
                )
            )
        )
        
        result = json.loads(response.text)
        
        # Log GrÃ¡fico
        if result.get('aprovado'):
            logger.info(f"   âœ… APROVADO (Nota {result.get('nota_fidelidade')}/10)")
        else:
            logger.warning(f"   âŒ REPROVADO (Nota {result.get('nota_fidelidade')}/10)")
            
        omissions = result.get('omissoes_graves', [])
        if omissions:
            logger.warning(f"   ğŸš¨ {len(omissions)} OmissÃµes Graves Detectadas:")
            for o in omissions[:5]:  # Limita log a 5
                logger.warning(f"      - {o}")
                
        # Salvar RelatÃ³rio
        if output_file:
            report_path = output_file.replace('.md', '_LLM_VALIDATION.md')
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(f"# RelatÃ³rio de ValidaÃ§Ã£o LLM (Metadata Strategy v2.16.1)\n")
                f.write(f"**Data:** {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"**Modelo:** {GEMINI_MODEL}\n")
                f.write(f"**Nota Fidelidade:** {result.get('nota_fidelidade')}/10\n\n")
                
                f.write("## ğŸ” OmissÃµes Graves\n")
                if omissions:
                    for o in omissions: f.write(f"- ğŸ”´ {o}\n")
                else:
                    f.write("- âœ… Nenhuma omissÃ£o grave detectada.\n")
                    
                f.write("\n## âš ï¸ DistorÃ§Ãµes\n")
                distorcoes = result.get('distorcoes', [])
                if distorcoes:
                    for d in distorcoes: f.write(f"- âš ï¸ {d}\n")
                else:
                    f.write("- Nenhuma distorÃ§Ã£o detectada.\n")
                
                f.write("\n## ğŸ—ï¸ Estrutura\n")
                problemas = result.get('problemas_estrutura', [])
                if problemas:
                    for p in problemas: f.write(f"- ğŸ”§ {p}\n")
                else:
                    f.write("- Estrutura OK.\n")
                
                f.write(f"\n## ğŸ“ ObservaÃ§Ãµes\n{result.get('observacoes', 'N/A')}\n")
                
            logger.info(f"   ğŸ“„ RelatÃ³rio salvo: {report_path}")
        
        return result
            
    except Exception as e:
        logger.error(f"   âŒ Erro na validaÃ§Ã£o LLM: {e}")
        return {'aprovado': True, 'nota_fidelidade': 0, 'error': str(e)}




def auto_fix_smart(raw_text, formatted_text, validation_result, client, estrutura_global=None):
    """
    v2.18 (SAFE MODE): Corretor Estrutural Seguro.
    Foca EXCLUSIVAMENTE em problemas de estrutura (tÃ­tulos, duplicatas, hierarquia).
    NÃƒO altera conteÃºdo jurÃ­dico para evitar alucinaÃ§Ãµes.
    
    Args:
        raw_text: (NÃ£o usado no modo Safe, mantido para compatibilidade)
        formatted_text: Texto formatado atual
        validation_result: Dict com problemas estruturais
        client: Cliente Vertex AI
        estrutura_global: Mapeamento de referÃªncia (opcional)
    """
    # No modo SAFE, ignoramos omissÃµes/distorÃ§Ãµes para nÃ£o correr risco de reescrita
    problemas_estrut = validation_result.get('problemas_estrutura', [])
    
    if not problemas_estrut:
        logger.info("âœ… Nenhum problema estrutural para corrigir.")
        return formatted_text
    
    logger.info(f"ğŸ”§ Auto-Fix Safe: Corrigindo {len(problemas_estrut)} problemas estruturais...")
    
    report = "### PROBLEMAS ESTRUTURAIS:\n" + "\n".join([f"- {p}" for p in problemas_estrut]) + "\n"
        
    PROMPT_FIX = f"""VocÃª Ã© um editor tÃ©cnico de elite.
    
## TAREFA: LIMPEZA ESTRUTURAL (SEM ALTERAR CONTEÃšDO)
VocÃª deve corrigir APENAS a formataÃ§Ã£o e estrutura do documento.

## REGRA DE OURO (SEGURANÃ‡A JURÃDICA):
- **NÃƒO altere o texto dos parÃ¡grafos.**
- **NÃƒO adicione nem remova informaÃ§Ãµes jurÃ­dicas.**
- **NÃƒO reescreva explicaÃ§Ãµes.**
- Sua permissÃ£o Ã© APENAS para TÃ­tulos, Hierarquia e Duplicatas exatas.

## INSTRUÃ‡Ã•ES DE CORREÃ‡ÃƒO:
1. **TÃ­tulos Duplicados**: Se houver tÃ­tulos repetidos (ex: dois "3. IntroduÃ§Ã£o" seguidos), remova o redundante.
2. **Hierarquia**: Ajuste nÃ­veis (H2, H3) para seguir a lÃ³gica do conteÃºdo.
3. **ParÃ¡grafos Repetidos**: Delete duplicaÃ§Ãµes EXATAS de parÃ¡grafos (copia-cola acidental).
4. **RenumeraÃ§Ã£o**: Garanta sequÃªncia lÃ³gica (1, 2, 3...) nos tÃ­tulos.

{f"## ESTRUTURA DE REFERÃŠNCIA (Guia):\n{estrutura_global}" if estrutura_global else ""}

## RELATÃ“RIO DE ERROS:
{report}

## SAÃDA:
Retorne o documento COMPLETO corrigido em Markdown. Sem explicaÃ§Ãµes."""

    try:
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=f"{PROMPT_FIX}\n\n## TEXTO A CORRIGIR:\n{formatted_text}",
            config=types.GenerateContentConfig(
                max_output_tokens=100000,
                thinking_config=types.ThinkingConfig(
                    include_thoughts=False,
                    thinking_level="HIGH" 
                )
            )
        )
        
        resultado = response.text.replace('```markdown', '').replace('```', '').strip()
        
        # ValidaÃ§Ã£o de seguranÃ§a estrita
        if len(resultado) < len(formatted_text) * 0.8: # TolerÃ¢ncia menor no modo safe
            logger.warning("âš ï¸ Auto-Fix Safe cortou muito texto (>20%). Abortando por seguranÃ§a.")
            return formatted_text
            
        logger.info(f"âœ… Auto-Fix Estrutural concluÃ­do. ({len(formatted_text)} -> {len(resultado)} chars)")
        return resultado
        
    except Exception as e:
        logger.error(f"âŒ Falha no Auto-Fix Safe: {e}")
        return formatted_text

# V2.10: FUNÃ‡Ã•ES DE PÃ“S-PROCESSAMENTO ESTRUTURAL (Tabelas e ParÃ¡grafos)
# =============================================================================

def mover_tabelas_para_fim_de_secao(texto):
    """
    v2.11: Reorganiza tabelas movendo-as para o final do BLOCO ATUAL (H2 ou H3).
    Corrige bug de tabelas sumindo ou ficando muito longe do contexto.
    """
    logger.info("ğŸ“Š Reorganizando tabelas (Smart Layout v2.11)...")
    
    linhas = texto.split('\n')
    resultado = []
    tabelas_pendentes = [] 
    
    i = 0
    while i < len(linhas):
        linha = linhas[i]
        linha_strip = linha.strip()
        
        # 1. DETECTAR SE Ã‰ UM TÃTULO (H1, H2, H3...)
        # Se encontrarmos um novo tÃ­tulo, hora de "despejar" as tabelas acumuladas do bloco anterior
        if linha_strip.startswith('#'):
            # Despeja tabelas antes de iniciar o novo tÃ³pico
            if tabelas_pendentes:
                resultado.append('') # EspaÃ§o antes
                for t_info in tabelas_pendentes:
                    if t_info['titulo']:
                        resultado.append(t_info['titulo'])
                    resultado.extend(t_info['linhas'])
                    resultado.append('') # EspaÃ§o depois
                tabelas_pendentes = []
            
            resultado.append(linha)
            i += 1
            continue

        # 2. DETECTAR INÃCIO DE TABELA
        # CritÃ©rio: Linha tem pipe '|' E parece estrutura de tabela (nÃ£o apenas citaÃ§Ã£o)
        eh_inicio_tabela = False
        if '|' in linha_strip:
            # Verifica se Ã© uma linha de markdown table vÃ¡lida (tem pipe e chars)
            # E se a prÃ³xima linha ou a seguinte tem o separador '---'
            has_separator = False
            for lookahead in range(1, 3): # Olha atÃ© 2 linhas pra frente (ignora 1 linha vazia)
                if i + lookahead < len(linhas):
                    prox = linhas[i + lookahead].strip()
                    if set(prox).issubset(set('|- :')): # SÃ³ contem caracteres de estrutura de tabela
                         has_separator = True
                         break
            
            if has_separator or (linha_strip.startswith('|') and linha_strip.endswith('|')):
                eh_inicio_tabela = True

        if eh_inicio_tabela:
            # --- Captura da Tabela ---
            tabela_linhas = []
            titulo_tabela = None
            
            # Tenta recuperar o tÃ­tulo da tabela que ficou na linha anterior (ou resultado)
            # Verifica se a Ãºltima linha adicionada ao resultado parece um tÃ­tulo de tabela
            if resultado and len(resultado) > 0:
                last_line = resultado[-1].strip()
                # PadrÃµes comuns de tÃ­tulo de tabela gerados pela IA
                if (last_line.startswith('###') or last_line.startswith('**')) and \
                   any(x in last_line.lower() for x in ['tabela', 'resumo', 'quadro', 'sÃ­ntese', 'esquema', 'ğŸ“‹']):
                    titulo_tabela = resultado.pop() # Remove do fluxo principal para agrupar com a tabela

            # Captura as linhas da tabela
            j = i
            while j < len(linhas):
                curr = linhas[j].strip()
                # Continua se tiver pipe ou for linha vazia no meio da tabela (mas cuidado com fim)
                if '|' in curr:
                    tabela_linhas.append(linhas[j])
                    j += 1
                elif not curr:
                    # Linha vazia: verifica se a prÃ³xima volta a ter pipe
                    if j + 1 < len(linhas) and '|' in linhas[j+1]:
                        tabela_linhas.append(linhas[j]) # MantÃ©m linha vazia interna
                        j += 1
                    else:
                        break # Fim da tabela
                else:
                    break # Texto normal, fim da tabela

            # Verifica se capturou algo Ãºtil
            if len(tabela_linhas) > 0:
                tabelas_pendentes.append({
                    'titulo': titulo_tabela,
                    'linhas': tabela_linhas
                })
                i = j # Pula as linhas processadas
                continue
            else:
                # Falso positivo? Devolve o tÃ­tulo se tinhamos pego
                if titulo_tabela:
                    resultado.append(titulo_tabela)
        
        # Se nÃ£o for tabela nem tÃ­tulo, adiciona linha normal
        resultado.append(linha)
        i += 1
    
    # 3. FINAL DO DOCUMENTO
    # Se sobraram tabelas no buffer, despeja agora
    if tabelas_pendentes:
        resultado.append('')
        for t_info in tabelas_pendentes:
            if t_info['titulo']:
                resultado.append(t_info['titulo'])
            resultado.extend(t_info['linhas'])
            resultado.append('')
            
    return '\n'.join(resultado)

def quebrar_paragrafos_longos(texto, max_chars=400, max_sentencas=4):
    """
    Quebra parÃ¡grafos que excedem limite de chars OU nÃºmero de sentenÃ§as.
    Preserva listas, tabelas, citaÃ§Ãµes e blocos especiais.
    """
    logger.info(f"âœ‚ï¸ Quebrando parÃ¡grafos > {max_chars} chars ou > {max_sentencas} sentenÃ§as...")
    
    paragrafos = texto.split('\n\n')
    resultado = []
    quebras = 0
    
    for para in paragrafos:
        linha_strip = para.strip()
        
        # PRESERVAR: tÃ­tulos, listas, tabelas, citaÃ§Ãµes, blocos de cÃ³digo
        if (linha_strip.startswith('#') or 
            linha_strip.startswith('-') or 
            linha_strip.startswith('* ') or 
            linha_strip.startswith('|') or
            linha_strip.startswith('>') or
            linha_strip.startswith('```') or
            re.match(r'^\d+\.', linha_strip)):
            resultado.append(para)
            continue
        
        # Verifica se precisa quebrar
        num_chars = len(para)
        sentencas = re.split(r'(?<=[.!?])\s+', para)
        num_sentencas = len(sentencas)
        
        if num_chars <= max_chars and num_sentencas <= max_sentencas:
            resultado.append(para)
            continue
        
        # QUEBRAR: Agrupa em blocos de atÃ© max_sentencas
        quebras += 1
        subparagrafos = []
        bloco_atual = []
        chars_atual = 0
        sentencas_no_bloco = 0
        
        for sentenca in sentencas:
            teste_chars = chars_atual + len(sentenca)
            
            # Se adicionar essa sentenÃ§a ultrapassar AMBOS os limites ou o limite de sentenÃ§as
            # E jÃ¡ temos algo no bloco...
            if (teste_chars > max_chars or sentencas_no_bloco >= max_sentencas) and bloco_atual:
                subparagrafos.append(' '.join(bloco_atual).strip())
                bloco_atual = [sentenca]
                chars_atual = len(sentenca)
                sentencas_no_bloco = 1
            else:
                bloco_atual.append(sentenca)
                chars_atual = teste_chars
                sentencas_no_bloco += 1
        
        # Adiciona Ãºltimo bloco
        if bloco_atual:
            subparagrafos.append(' '.join(bloco_atual).strip())
        
        resultado.append('\n\n'.join(subparagrafos))
        
    if quebras > 0:
        logger.info(f"   âœ… {quebras} parÃ¡grafos foram ajustados.")
        
    return '\n\n'.join(resultado)

# =============================================================================
# FLUXO PRINCIPAL
# =============================================================================

def formatar_transcricao(transcricao_completa, usar_cache=True, input_file=None, custom_prompt=None):
    prompt_ativo = custom_prompt if custom_prompt else PROMPT_FORMATACAO
    if custom_prompt:
        logger.info(f"ğŸ¨ Usando prompt customizado ({len(custom_prompt):,} caracteres)")
    
    estrutura_global = "" # v2.17: Inicializa para evitar UnboundLocalError
    # âš¡ FORÃ‡AR USO DE VERTEX AI - AI Studio desabilitado
    rate_limiter.max_rpm = 60
    # Tenta pegar o projeto do ambiente, se nÃ£o tiver, usa o hardcoded como fallback
    project_id = os.getenv("GOOGLE_CLOUD_PROJECT", "gen-lang-client-0727883752")
    logger.info(f" Usando Vertex AI (Project: {project_id})")
    logger.info(" Rate Limit: 60 RPM")
    logger.info("ğŸ”¥ AI Studio DESABILITADO - Usando apenas Vertex AI")
    
    # Verificar se as credenciais do Vertex AI estÃ£o configuradas
    if os.getenv("GOOGLE_APPLICATION_CREDENTIALS") is None:
        logger.error("âŒ Nenhuma autenticaÃ§Ã£o configurada para Vertex AI.")
        logger.error("Configure a variÃ¡vel de ambiente GOOGLE_APPLICATION_CREDENTIALS:")
        logger.error("  export GOOGLE_APPLICATION_CREDENTIALS='/path/to/service-account.json'")
        sys.exit(1)

    client = genai.Client(
        vertexai=True,
        project=project_id,
        location="global"
    )
    
    # Dry-run mode
    if '--dry-run' in sys.argv:
        logger.info("ğŸ” MODO DRY-RUN: Validando divisÃ£o de chunks")
        chunks = dividir_sequencial(transcricao_completa)
        validar_chunks(chunks, transcricao_completa)
        for i, c in enumerate(chunks):
            print(f"\n  Chunk {i+1}/{len(chunks)}:")
            print(f"    PosiÃ§Ã£o: {c['inicio']:,} â†’ {c['fim']:,} ({c['fim']-c['inicio']:,} chars)")
            inicio_preview = transcricao_completa[c['inicio']:c['inicio']+80].replace('\n', 'â†µ')
            fim_preview = transcricao_completa[max(0, c['fim']-80):c['fim']].replace('\n', 'â†µ')
            print(f"    InÃ­cio: {inicio_preview}...")
            print(f"    Fim: ...{fim_preview}")
        sys.exit(0)
    
    tamanho_total = len(transcricao_completa)
    logger.info(f"ğŸ“Š Tamanho: {tamanho_total:,} caracteres")
    
    # v2.10: Iniciar timer de mÃ©tricas
    metrics.start_timer()
    
    # v2.17: DivisÃ£o Inicial (Estimativa)
    chunks = dividir_sequencial(transcricao_completa)
    num_partes = len(chunks)
    
    # v2.8: Mapeamento estrutural prÃ©vio (se necessÃ¡rio)
    # Se nÃ£o foi passado externamente e temos mÃºltiplas partes, gera agora.
    if num_partes > 1 and not estrutura_global:
        estrutura_global = map_structure(client, transcricao_completa)
        
        # v2.10: Filtros de estrutura
        estrutura_global = filtrar_niveis_execessivos(estrutura_global, max_nivel=3)
        estrutura_global = simplificar_estrutura_se_necessario(estrutura_global)
        
        # v2.17: RE-DIVISÃƒO OTIMIZADA (Anchor-Based Chunking)
        # Agora que temos a estrutura, refazemos os cortes para alinhar com os tÃ³picos
        logger.info("ğŸ”„ Otimizando cortes com base na estrutura (Anchor-Based Chunking)...")
        chunks = dividir_sequencial(transcricao_completa, estrutura_global=estrutura_global)
        num_partes = len(chunks)

    # v2.7: ValidaÃ§Ã£o rigorosa
    if not validar_chunks(chunks, transcricao_completa):
        logger.error("âŒ Chunks invÃ¡lidos! Abortando.")
        sys.exit(1)
    
    chunks_info = [{'inicio': c['inicio'], 'fim': c['fim']} for c in chunks]
    
    estimar_custo(transcricao_completa, usar_cache, num_partes)
    
    if num_partes == 1:
        return processar_simples(client, transcricao_completa, prompt_ativo), None, client
    
    checkpoint = None
    resultados = []
    inicio_secao = 0
    
    if input_file:
        checkpoint = load_checkpoint(input_file)
        if checkpoint:
            logger.info(f"ğŸ“ Checkpoint: seÃ§Ã£o {checkpoint['secao_atual']}/{checkpoint['total_secoes']}")
            resposta = input("   Continuar? (s/n): ").strip().lower()
            if resposta == 's':
                resultados = checkpoint['resultados']
                inicio_secao = checkpoint['secao_atual']
                chunks_info = checkpoint['chunks_info']
            else:
                delete_checkpoint(input_file)
    
    # v2.19: Cache HABILITADO com Estrutura Global (User Request)
    cache = None
    if usar_cache and num_partes > 1:
         cache = criar_cache_contexto(client, transcricao_completa, prompt_ativo, estrutura_global)
    
    try:
        # Prepara o progresso
        iter_chunks = range(inicio_secao, num_partes)
        if tqdm:
            iter_chunks = tqdm(iter_chunks, desc="Processando", unit="seÃ§Ã£o")
        
        for i in iter_chunks:
            chunk = chunks_info[i]
            texto_chunk = transcricao_completa[chunk['inicio']:chunk['fim']]
            
            # v2.10: Contexto estÃ¡tico apenas (instruÃ§Ã£o de estilo)
            contexto_estilo = None
            if resultados:
                 # Pega os Ãºltimos 3000 caracteres como referÃªncia de ESTILO
                 ultimo_texto = resultados[-1]
                 contexto_estilo = ultimo_texto[-CONTEXTO_ESTILO:] if len(ultimo_texto) > CONTEXTO_ESTILO else ultimo_texto
            
            # v2.10: Extrair Ãºltimo tÃ­tulo do chunk anterior para anti-duplicaÃ§Ã£o
            ultimo_titulo = None
            if resultados:
                texto_anterior = resultados[-1]
                for linha in reversed(texto_anterior.split('\n')[-30:]):
                    if linha.strip().startswith('##'):
                        ultimo_titulo = re.sub(r'^#+\s*(?:\d+(?:\.\d+)*\.?)?\s*', '', linha).strip()
                        break
            
            # v2.17: Contexto Localizado (Localized Context)
            # Em vez de passar a estrutura inteira, passa uma janela: [Anterior] + [Atual] + [PrÃ³ximo]
            estrutura_local = estrutura_global
            if estrutura_global and num_partes > 1:
                itens_estrutura = estrutura_global.split('\n')
                # Estimativa simples: mapeia chunk i para itens da estrutura
                ratio = len(itens_estrutura) / num_partes
                center_idx = int(i * ratio)
                window_size = max(4, int(len(itens_estrutura) * 0.15)) # 15% da estrutura ou min 4 itens
                
                start_idx = max(0, center_idx - window_size)
                end_idx = min(len(itens_estrutura), center_idx + window_size + 2)
                
                slice_itens = itens_estrutura[start_idx:end_idx]
                if start_idx > 0: slice_itens.insert(0, "[... TÃ³picos anteriores ...]")
                if end_idx < len(itens_estrutura): slice_itens.append("[... TÃ³picos posteriores ...]")
                
                estrutura_local = '\n'.join(slice_itens)

            resultado = processar_chunk(
                client, cache, prompt_ativo, texto_chunk,
                i + 1, num_partes,
                contexto_estilo=contexto_estilo,
                estrutura_global=estrutura_local, # Passa estrutura localmente fatiada
                ultimo_titulo=ultimo_titulo
            )
            
            # v2.10: Smart Stitching - Remove eco do contexto
            if contexto_estilo:
                resultado = remover_eco_do_contexto(resultado, contexto_estilo)
            
            # v2.10: Smart Stitching - Remove tÃ­tulo duplicado na fronteira
            texto_acumulado = '\n\n'.join(resultados) if resultados else ""
            resultado = limpar_inicio_redundante(resultado, texto_acumulado)
            
            resultados.append(resultado)
            
            if input_file:
                save_checkpoint(input_file, resultados, chunks_info, i + 1)
            
            if not tqdm:
                logger.info(f"âœ… SeÃ§Ã£o {i+1}/{num_partes}")
        
        # v2.7: Post-processing em mÃºltiplas passadas
        logger.info("ğŸ§¹ Iniciando limpeza (v2.7)...")
        
        texto_final = '\n\n'.join(resultados)
        
        # Passada 0: Limpar metadados de mapeamento que vazam para o output
        # Remove linhas como "[TIPO: AULA EXPOSITIVA]" ou "**[TIPO: SIMULADO]**"
        texto_final = re.sub(r'^#?\s*\*?\*?\[TIPO:.*?\]\*?\*?\s*$', '', texto_final, flags=re.MULTILINE)
        # Remove marcadores de bloco [BLOCO 01], [BLOCO 02], etc.
        texto_final = re.sub(r'^\s*\[BLOCO\s*\d+\]\s*$', '', texto_final, flags=re.MULTILINE)
        # Remove timestamps Ã³rfÃ£os [HH:MM] ou [HH:MM:SS] no inÃ­cio de linha
        texto_final = re.sub(r'^\s*\[\d{1,2}:\d{2}(:\d{2})?\]\s*$', '', texto_final, flags=re.MULTILINE)
        texto_final = re.sub(r'\n{3,}', '\n\n', texto_final)  # Remove linhas em branco extras
        
        logger.info("  Passada 1: Removendo duplicaÃ§Ãµes literais...")
        texto_final = remover_duplicacoes_literais(texto_final)
        
        logger.info("  Passada 1.5: DeduplicaÃ§Ã£o robusta (7-DIFF Strategy)...")
        texto_final = remover_overlap_duplicado(texto_final)
        
        logger.info("  Passada 2: Detectando e removendo seÃ§Ãµes duplicadas (v2.15)...")
        texto_final = remover_secoes_duplicadas(texto_final)
        
        logger.info("  Passada 3: Normalizando tÃ­tulos similares...")
        texto_final = normalize_headings(texto_final)
        
        if MODO_NOME != "FIDELIDADE":
            logger.info("  Passada 3.5: ReorganizaÃ§Ã£o Estrutural DeterminÃ­stica...")
            texto_final = deterministic_structure_fix(texto_final)
        else:
            logger.info("  â„¹ï¸  Modo FIDELIDADE: Pulando reorganizaÃ§Ã£o para preservar linearidade.")
            
        
        # v2.10: ReordenaÃ§Ã£o do Pipeline (Tabelas -> NumeraÃ§Ã£o -> ParÃ¡grafos)
        
        logger.info("  Passada 4: Reorganizando tabelas (Smart Layout)...")
        texto_final = mover_tabelas_para_fim_de_secao(texto_final)

        
        logger.info("  Passada 5: Numerando tÃ­tulos...")
        texto_final = numerar_titulos(texto_final)
        
        logger.info("  Passada 6: Ajustando parÃ¡grafos longos...")
        texto_final = quebrar_paragrafos_longos(texto_final, max_chars=400, max_sentencas=4)
        
        if MODO_NOME != "FIDELIDADE":
            logger.info("  Passada 7: RevisÃ£o semÃ¢ntica de estrutura (IA v2.0)...")
            texto_final = ai_structure_review(client, texto_final, estrutura_mapeada=estrutura_global)
        else:
            logger.info("  Passada 7: RevisÃ£o leve de formataÃ§Ã£o (IA - Modo Fidelidade v2.0)...")
            texto_final = ai_structure_review_lite(client, texto_final, estrutura_mapeada=estrutura_global)
        
        # Passada 7.5: RenumeraÃ§Ã£o Sequencial DeterminÃ­stica (camada de seguranÃ§a)
        try:
            texto_final = renumerar_secoes(texto_final)
        except Exception as e:
            logger.warning(f"âš ï¸ Erro na renumeraÃ§Ã£o: {e}. Continuando sem renumerar.")
        
        # ValidaÃ§Ã£o final
        palavras_in = len(transcricao_completa.split())
        palavras_out = len(texto_final.split())
        razao = palavras_out / palavras_in if palavras_in > 0 else 1.0
        
        logger.info(f"âœ… ValidaÃ§Ã£o: {razao:.0%} do original ({palavras_out:,}/{palavras_in:,} palavras)")
        
        if razao < THRESHOLD_CRITICO:
            if FIDELIDADE_MODE:
                logger.error(f"âŒ POSSÃVEL PERDA DE CONTEÃšDO ({razao:.0%})")
                logger.error(f"   Esperado: >{THRESHOLD_CRITICO:.0%} | Obtido: {razao:.0%}")
            elif APOSTILA_MODE:
                logger.warning(f"âš ï¸  Texto condensado: {razao:.0%}")
                logger.info(f"   âœ… Esperado no modo {MODO_NOME}")
            
            if razao < 0.30 or (FIDELIDADE_MODE and razao < THRESHOLD_CRITICO):
                resposta = input("\n   Continuar? (s/n): ").strip().lower()
                if resposta != 's':
                    logger.info("Cancelado.")
                    sys.exit(1)
        
        if input_file:
            delete_checkpoint(input_file)
        
        return texto_final, cache, client
    
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  Interrompido. Checkpoint salvo.")
        sys.exit(1)

# =============================================================================
# EXPORTAÃ‡ÃƒO WORD
# =============================================================================

def create_toc(doc):
    """Adiciona SumÃ¡rio (Table of Contents) nativo do Word"""
    paragraph = doc.add_paragraph()
    run = paragraph.add_run()
    
    fldChar = OxmlElement('w:fldChar')
    fldChar.set(qn('w:fldCharType'), 'begin')
    run._r.append(fldChar)
    
    instrText = OxmlElement('w:instrText')
    instrText.set(qn('xml:space'), 'preserve')
    instrText.text = 'TOC \\o "1-3" \\h \\z \\u'
    run._r.append(instrText)
    
    fldChar = OxmlElement('w:fldChar')
    fldChar.set(qn('w:fldCharType'), 'separate')
    run._r.append(fldChar)
    
    fldChar = OxmlElement('w:fldChar')
    fldChar.set(qn('w:fldCharType'), 'end')
    run._r.append(fldChar)

def _format_inline_markdown(paragraph, text):
    """
    v2.16.2: Formata markdown inline de forma robusta.
    Suporta: ***bold_italic***, **bold**, *italic*, __bold__, _italic_, `code`.
    """
    paragraph.clear()
    
    # Regex robusta:
    # Group 2: ***text*** ou ___text___ (Bold + Italic)
    # Group 3: **text** ou ___text___ (Bold + Italic)
    # Group 4: **text** (Bold)
    # Group 5: __text__ (Bold)
    # Group 6: *text* (Italic)
    # Group 7: _text_ (Italic)
    # Group 8: `text` (Code)
    pattern = r'(\*{3}(.+?)\*{3}|_{3}(.+?)_{3}|\*{2}(.+?)\*{2}|_{2}(.+?)_{2}|(?<!\*)\*(?!\*)(.+?)(?<!\*)\*(?!\*)|(?<!_)_(?!(?:_|\s))(.+?)(?<!(?:_|\s))_(?!_)|`(.+?)`)'
    
    last_end = 0
    for match in re.finditer(pattern, text):
        if match.start() > last_end:
            paragraph.add_run(text[last_end:match.start()])
        
        full_match = match.group(0)
        
        if full_match.startswith('***'):
            content = match.group(2)
            run = paragraph.add_run(content)
            run.bold = True
            run.italic = True
        elif full_match.startswith('___'):
            content = match.group(3)
            run = paragraph.add_run(content)
            run.bold = True
            run.italic = True
        elif full_match.startswith('**'):
            content = match.group(4)
            run = paragraph.add_run(content)
            run.bold = True
            run.font.name = 'Arial'
        elif full_match.startswith('__'):
            content = match.group(5)
            run = paragraph.add_run(content)
            run.bold = True
            run.font.name = 'Arial'
        elif full_match.startswith('*'):
            run = paragraph.add_run(match.group(6))
            run.italic = True
            run.font.name = 'Arial'
        elif full_match.startswith('_'):
            run = paragraph.add_run(match.group(7))
            run.italic = True
            run.font.name = 'Arial'
        elif full_match.startswith('`'):
            run = paragraph.add_run(match.group(8))
            run.font.name = 'Courier New'
            run.font.size = Pt(9)
            run.font.color.rgb = RGBColor(200, 0, 0)
        
        last_end = match.end()
    
    if last_end < len(text):
        run = paragraph.add_run(text[last_end:])
        run.font.name = 'Arial'

def _add_table_to_doc(doc, rows):
    """Adiciona tabela formatada ao documento Word"""
    if len(rows) < 2:
        return
    
    max_cols = max(len(row) for row in rows)
    if max_cols == 0:
        return
    
    table = doc.add_table(rows=len(rows), cols=max_cols)
    table.style = 'Light Grid Accent 1'
    
    for i, row_data in enumerate(rows):
        for j in range(max_cols):
            cell = table.rows[i].cells[j]
            cell_text = row_data[j] if j < len(row_data) else ""
            
            # Formata markdown dentro da cÃ©lula
            _format_inline_markdown(cell.paragraphs[0], cell_text)
            
            # Alinhamento padrÃ£o: Esquerda
            cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.LEFT

            if i == 0:
                for paragraph in cell.paragraphs:
                    for run in paragraph.runs:
                        run.font.bold = True
                        run.font.name = 'Arial'
                        run.font.color.rgb = RGBColor(255, 255, 255)
                    # Alinhamento Ã  esquerda conforme solicitado
                    paragraph.alignment = WD_ALIGN_PARAGRAPH.LEFT
                
                shading_elm = OxmlElement('w:shd')
                shading_elm.set(qn('w:fill'), '336699')
                cell._element.get_or_add_tcPr().append(shading_elm)
            else:
                 # ConteÃºdo normal da tabela: Esquerda
                 for paragraph in cell.paragraphs:
                    paragraph.alignment = WD_ALIGN_PARAGRAPH.LEFT
                    for run in paragraph.runs:
                        run.font.name = 'Arial'
    
    table.alignment = WD_TABLE_ALIGNMENT.CENTER

def save_as_word(formatted_text, video_name, output_file):
    """Salva markdown formatado como documento Word (.docx)"""
    if not DOCX_AVAILABLE:
        logger.warning("python-docx nÃ£o disponÃ­vel. Salvando apenas Markdown.")
        return None
    
    logger.info("ğŸ“„ Gerando documento Word...")
    
    doc = Document()
    
    # Configura Fonte Arial no estilo Normal
    style = doc.styles['Normal']
    font = style.font
    font.name = 'Arial'
    font.size = Pt(12)
    # python-docx precisa desse hack para garantir que o nome da fonte seja aplicado corretamente
    style.element.rPr.rFonts.set(qn('w:ascii'), 'Arial')
    style.element.rPr.rFonts.set(qn('w:hAnsi'), 'Arial')
    
    # Margens
    section = doc.sections[0]
    section.top_margin = Inches(1)
    section.bottom_margin = Inches(1)
    section.left_margin = Inches(1.25)
    section.right_margin = Inches(1.25)
    
    # TÃ­tulo principal
    title = doc.add_heading(video_name, level=0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    for run in title.runs:
        run.font.name = 'Arial'
        run.font.size = Pt(20)
        run.font.color.rgb = RGBColor(0, 51, 102)
    
    # Data de geraÃ§Ã£o
    date_para = doc.add_paragraph()
    date_run = date_para.add_run(f"Gerado em {time.strftime('%d/%m/%Y Ã s %H:%M')} - Modo: {MODO_NOME}")
    date_run.font.name = 'Arial'
    date_run.italic = True
    date_run.font.size = Pt(10)
    date_run.font.color.rgb = RGBColor(128, 128, 128)
    date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    doc.add_paragraph()
    
    # SumÃ¡rio
    doc.add_heading('SumÃ¡rio', level=1)
    create_toc(doc)
    doc.add_page_break()
    
    # Processa conteÃºdo markdown
    lines = formatted_text.split('\n')
    i = 0
    in_table = False
    table_rows = []
    
    while i < len(lines):
        line = lines[i].strip()
        
        if not line:
            i += 1
            continue
        
        # Tabelas
        if '|' in line and not in_table:
            in_table = True
            table_rows = []
        
        if in_table:
            # Ignora linha separadora (ex: |---| ou | :--- | :--- | :--- |)
            is_separator = re.match(r'^\s*\|[\s:|-]+\|[\s:|-]*$', line)
            
            if '|' in line and not is_separator:
                table_rows.append([cell.strip() for cell in line.split('|')[1:-1]])
            
            if '|' not in line or i == len(lines) - 1:
                if len(table_rows) > 0:
                    _add_table_to_doc(doc, table_rows)
                in_table = False
                table_rows = []
                if '|' not in line:
                    continue
            i += 1
            continue
        
        # Headings
        if line.startswith('##### '):
            h = doc.add_heading('', level=5)
            _format_inline_markdown(h.paragraphs[0], line[6:])
        elif h_match := re.match(r'^(####|###|##|#)\s+(.*)', line):
            lvl = len(h_match.group(1))
            h_text = h_match.group(2)
            if lvl == 1 and h_text == video_name:
                i += 1
                continue
            h = doc.add_heading('', level=lvl)
            _format_inline_markdown(h, h_text)  # 'h' jÃ¡ Ã© um Paragraph
        # Separadores
        elif line.strip() in ['---', '***', '___']:
            p = doc.add_paragraph()
            p.add_run('_' * 80).font.color.rgb = RGBColor(192, 192, 192)
        # Quotes
        elif line.startswith('>'):
            p = doc.add_paragraph(style='Quote')
            p.paragraph_format.left_indent = Cm(4.0)
            p.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
            _format_inline_markdown(p, line[1:].strip())
            # ForÃ§ar estilo de quote em todos os runs
            for run in p.runs:
                run.font.name = 'Arial'
                run.italic = True
                run.font.size = Pt(10)
        # Listas nÃ£o-ordenadas
        elif line.startswith('- ') or line.startswith('* '):
            p = doc.add_paragraph(line[2:], style='List Bullet')
            # ForÃ§ar recuo de 1,5cm
            p.paragraph_format.left_indent = Cm(1.5)
            p.paragraph_format.first_line_indent = Cm(-0.63)
            
            _format_inline_markdown(p, line[2:])
            
        # Listas numeradas
        elif len(line) > 2 and line[0].isdigit() and line[1:3] in ['. ', ') ']:
            p = doc.add_paragraph(style='Normal')
            # Recuo padronizado de 1,5cm
            p.paragraph_format.left_indent = Cm(1.5)
            p.paragraph_format.first_line_indent = Cm(-0.63) # MantÃ©m hanging indent para o nÃºmero
            _format_inline_markdown(p, line)
            
        # ParÃ¡grafo normal
        else:
            p = doc.add_paragraph()
            # EspaÃ§amento 1.5 (Line Spacing = 1.5)
            p.paragraph_format.line_spacing_rule = WD_LINE_SPACING.ONE_POINT_FIVE
            # EspaÃ§amento antes e apÃ³s parÃ¡grafo (6pt)
            p.paragraph_format.space_before = Pt(6)
            p.paragraph_format.space_after = Pt(6) 
            
            # Recuo de 1Âª linha (1cm)
            p.paragraph_format.first_line_indent = Cm(1.0)
            
            # Justificado
            p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
            
            _format_inline_markdown(p, line)
            
            # ForÃ§ar fonte 12pt para texto normal e Arial
            for run in p.runs:
                run.font.name = 'Arial'
                run.font.size = Pt(12)
        
        i += 1
    
    # Flush final (caso o arquivo termine com tabela seguida de linhas em branco)
    if in_table and len(table_rows) > 0:
        _add_table_to_doc(doc, table_rows)
        
    doc.save(output_file)
    logger.info(f"âœ… Word salvo: {output_file}")
    return output_file

def salvar_resultado(conteudo, arquivo):
    with open(arquivo, 'w', encoding='utf-8') as f:
        f.write(conteudo)
    logger.info(f"âœ… Markdown salvo: {arquivo}")

# =============================================================================
# MAIN
# =============================================================================

def main():
    if len(sys.argv) < 2 or '--help' in sys.argv:
        print("=" * 70)
        print("FORMATADOR v2.7 - ANTI-DUPLICAÃ‡ÃƒO")
        print("=" * 70)
        print("\nUso: python format_transcription_gemini.py <entrada.txt> [saida] [--prompt <texto_ou_path>]")
        print("\nOpÃ§Ãµes:")
        print("  --dry-run         Valida chunks e mostra preview")
        print("  --prompt <p>      Prompt customizado (texto direto ou caminho .txt)")
        print("  --help            Mostra esta mensagem")
        print("\nğŸ›¡ï¸  CORREÃ‡Ã•ES v2.7:")
        print("  â€¢ DetecÃ§Ã£o agressiva de seÃ§Ãµes duplicadas")
        print("  â€¢ ValidaÃ§Ã£o rigorosa de chunks sequenciais")
        print("  â€¢ Delimitadores de contexto mais visÃ­veis")
        print("  â€¢ Post-processing em mÃºltiplas passadas")
        print("  â€¢ Cache desabilitado (debug)")
        print("\nDependÃªncias:")
        print("  pip install google-genai python-docx tqdm")
        sys.exit(1)
    
    arquivo_entrada = sys.argv[1]
    
    if len(sys.argv) > 2 and not sys.argv[2].startswith('--'):
        arquivo_saida = sys.argv[2]
    else:
        base = arquivo_entrada.replace('.txt', '_formatada_v2.7')
        arquivo_saida = f"{base}.md"
    
    video_name = Path(arquivo_entrada).stem
    
    # v2.21: Parse prompt customizado
    custom_prompt = None
    if '--prompt' in sys.argv:
        try:
            p_idx = sys.argv.index('--prompt')
            p_val = sys.argv[p_idx + 1]
            if os.path.exists(p_val):
                with open(p_val, 'r', encoding='utf-8') as f:
                    custom_prompt = f.read().strip()
                logger.info(f"ğŸ“‚ Prompt carregado de arquivo: {p_val}")
            else:
                custom_prompt = p_val.strip()
                logger.info("âœï¸ Prompt customizado lido diretamente da CLI")
        except Exception as e:
            logger.error(f"âŒ Erro ao ler prompt: {e}")
            sys.exit(1)

    logger.info("=" * 60)
    logger.info(f"FORMATADOR v2.7 ANTI-DUPLICAÃ‡ÃƒO - Modo {MODO_NOME}")
    logger.info("=" * 60)
    logger.info(f"ğŸ“‚ Entrada: {arquivo_entrada}")
    logger.info(f"ğŸ“ SaÃ­da: {arquivo_saida}")
    
    transcricao = carregar_transcricao(arquivo_entrada)
    
    try:
        resultado, cache, client = formatar_transcricao(transcricao, input_file=arquivo_entrada, custom_prompt=custom_prompt)
    except Exception as e:
        logger.error(f"\nâŒ Falha: {e}", exc_info=True)
        sys.exit(1)
    
    # v2.18: Auto-Fix Pass - CorreÃ§Ãµes automÃ¡ticas
    logger.info("ğŸ”§ Aplicando Auto-Fix Pass (v2.18)...")
    resultado, correcoes = aplicar_correcoes_automaticas(resultado)
    
    salvar_resultado(resultado, arquivo_saida)
    
    # v2.8: VerificaÃ§Ã£o de cobertura e duplicaÃ§Ãµes
    verificar_cobertura(transcricao, resultado, arquivo_saida)

    # v2.16: ValidaÃ§Ã£o LLM (Metadata Strategy)
    validation_result = validate_completeness_llm(transcricao, resultado, client, arquivo_saida)
    
    # v2.17: Auto-Fix Loop - Corrige omissÃµes se detectadas
    if validation_result and not validation_result.get('aprovado', True):
        logger.info("ğŸ” Iniciando Auto-Fix Loop...")
        resultado = auto_fix_smart(transcricao, resultado, validation_result, client, estrutura_global)
        
        # Salvar versÃ£o corrigida
        with open(arquivo_saida, 'w', encoding='utf-8') as f:
            f.write(resultado)
        logger.info(f"ğŸ’¾ VersÃ£o corrigida salva: {arquivo_saida}")

    # v2.9: Auditoria Legal PÃ³s-Processamento
    if AUDIT_AVAILABLE:
        report_path = arquivo_saida.replace('.md', '_RELATORIO_AUDITORIA.md')
        auditar_consistencia_legal(client, resultado, report_path)

    if DOCX_AVAILABLE:
        arquivo_docx = arquivo_saida.replace('.md', '.docx')
        save_as_word(resultado, video_name, arquivo_docx)
    
    # v2.10: MÃ©tricas de ExecuÃ§Ã£o (tokens reais)
    metrics.stop_timer()
    logger.info(metrics.get_report())
    logger.info("âœ¨ ConcluÃ­do! (v2.10 com mÃ©tricas)")

    # v2.19: Cleanup manual do cache para economia
    if cache:
        try:
            client.caches.delete(name=cache.name)
            logger.info(f"ğŸ—‘ï¸ Cache {cache.name} deletado manualmente para economizar custos.")
        except Exception as e:
            logger.warning(f"âš ï¸ NÃ£o foi possÃ­vel deletar o cache: {e}")

if __name__ == "__main__":
    main()
