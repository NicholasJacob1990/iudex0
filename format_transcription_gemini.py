#!/usr/bin/env python3
"""
Script v2.11 - Formata√ß√£o de Transcri√ß√µes com Gemini 3 Flash Preview
MELHORIAS: Smart Stitching Anti-Duplica√ß√£o Cir√∫rgica

Mudan√ßas v2.10 vs v2.9:
- remover_eco_do_contexto: Remove eco do contexto na resposta da API
- titulos_sao_similares: Fuzzy matching para detec√ß√£o de t√≠tulos duplicados
- limpar_inicio_redundante: Limpeza na jun√ß√£o de chunks
- Inje√ß√£o din√¢mica de ultimo_titulo no prompt para prevenir repeti√ß√£o

Uso: python format_transcription_gemini.py <entrada.txt> [saida]
"""


import os
import sys
import time
import random
import json
import re
import threading
from pathlib import Path
from time import sleep
from difflib import SequenceMatcher
import hashlib

try:
    from audit_module import auditar_consistencia_legal
    AUDIT_AVAILABLE = True
except ImportError:
    AUDIT_AVAILABLE = False
    logger = logging.getLogger(__name__) if 'logging' in locals() else None
    if logger: logger.warning("‚ö†Ô∏è  M√≥dulo de auditoria n√£o encontrado.")
    else: print("‚ö†Ô∏è  M√≥dulo de auditoria n√£o encontrado.")

try:
    from google import genai
    from google.genai import types
except ImportError:
    print("‚ùå Erro: Biblioteca google-genai n√£o instalada.")
    print("   Instale com: pip install google-genai")
    sys.exit(1)

try:
    from tqdm import tqdm
except ImportError:
    print("‚ö†Ô∏è Aviso: tqdm n√£o instalado. Progress bar desabilitada.")
    tqdm = None

try:
    from docx import Document
    from docx.shared import Pt, RGBColor, Inches, Cm
    from docx.enum.text import WD_ALIGN_PARAGRAPH, WD_LINE_SPACING
    from docx.enum.table import WD_TABLE_ALIGNMENT
    from docx.oxml.ns import qn
    from docx.oxml import OxmlElement
    DOCX_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è Aviso: python-docx n√£o dispon√≠vel. Sa√≠da em Word desabilitada.")
    DOCX_AVAILABLE = False

# =============================================================================
# SETUP CREDENCIAIS (v2.11)
# =============================================================================
CREDENTIALS_PATH = "/Users/nicholasjacob/Documents/Aplicativos/Transcritor/vertex_credentials.json"
if os.path.exists(CREDENTIALS_PATH) and not os.environ.get("GOOGLE_APPLICATION_CREDENTIALS"):
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = CREDENTIALS_PATH
    # print(f"üîë Credenciais carregadas: {CREDENTIALS_PATH}")

import logging

# =============================================================================
# CONFIGURA√á√ïES v2.7
# =============================================================================

CHARS_POR_PARTE = 15000
CONTEXTO_ESTILO = 3000
OUTPUT_TOKEN_LIMIT = 32000
CACHE_TTL = '7200s'
MIN_CHARS_PARA_CACHE = 150000
MAX_RETRIES = 3
MAX_RPM = 60 
# v2.7: FOR√áAR delimitadores vis√≠veis para evitar confus√£o
USE_FANCY_DELIMITERS = True

# Modelo Gemini (centralizado para f√°cil atualiza√ß√£o)
GEMINI_MODEL = 'gemini-3-flash-preview'

# Pre√ßos API Gemini 3 Flash Preview (Estimativa)
PRECO_INPUT_SEM_CACHE = 0.50
PRECO_INPUT_COM_CACHE = 0.05  # Estimado a 10% do input (manter propor√ß√£o anterior)
PRECO_OUTPUT = 3.00

# =============================================================================
# LOGGING
# =============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S',
    handlers=[
        logging.FileHandler('formatacao.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# =============================================================================
# M√âTRICAS DE EXECU√á√ÉO (v2.10)
# =============================================================================
class MetricsCollector:
    """Coleta e reporta m√©tricas de execu√ß√£o para otimiza√ß√£o de custos."""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.api_calls = 0
        self.gemini_calls = 0
        self.openai_calls = 0
        self.cache_hits = 0
        self.adaptive_splits = 0
        self.total_prompt_tokens = 0
        self.total_cached_tokens = 0
        self.total_completion_tokens = 0
        self.total_time_seconds = 0.0
        self.chunks_processed = 0
        self.start_time = None
    
    def start_timer(self):
        self.start_time = time.time()
    
    def stop_timer(self):
        if self.start_time:
            self.total_time_seconds = time.time() - self.start_time
    
    def record_api_call(self, prompt_tokens=0, completion_tokens=0, cached_tokens=0, provider='gemini'):
        # Garantir que s√£o inteiros (v2.17.1: fix NoneType)
        prompt_tokens = prompt_tokens or 0
        completion_tokens = completion_tokens or 0
        cached_tokens = cached_tokens or 0
        
        self.api_calls += 1
        if provider == 'gemini':
            self.gemini_calls += 1
        elif provider == 'openai':
            self.openai_calls += 1
            
        # Se houve tokens cacheados, considera cache hit
        if cached_tokens > 0:
            self.cache_hits += 1
            
        self.total_prompt_tokens += (prompt_tokens - cached_tokens)
        self.total_cached_tokens += cached_tokens
        self.total_completion_tokens += completion_tokens
    
    def record_adaptive_split(self):
        self.adaptive_splits += 1

    def record_cache_hit(self):
        self.cache_hits += 1
    
    def get_cost(self):
        """Calcula custo estimado (Gemini 3 Flash Preview)."""
        # Pre√ßos por 1M tokens ($0.50 Input / $3.00 Output)
        input_price = 0.50       # USD (Standard)
        cached_price = 0.05      # USD (Cached - Est. 10%)
        output_price = 3.00      # USD (Output)
        
        cost = (
            (self.total_prompt_tokens * input_price) + 
            (self.total_cached_tokens * cached_price) +
            (self.total_completion_tokens * output_price)
        ) / 1_000_000
        return cost
    
    def get_report(self):
        avg_time = self.total_time_seconds / self.api_calls if self.api_calls > 0 else 0
        cost = self.get_cost()
        
        return f"""
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìä M√âTRICAS DE EXECU√á√ÉO (v2.10)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
   üì° Total de Chamadas API: {self.api_calls}
      - Gemini: {self.gemini_calls}
      - OpenAI: {self.openai_calls}
      - Cache Hits: {self.cache_hits}
   ‚úÇÔ∏è Divis√µes Adaptativas: {self.adaptive_splits}
   üéØ Tokens Usados:
      - Prompt (Regular): {self.total_prompt_tokens:,}
      - Prompt (Cached):  {self.total_cached_tokens:,}
      - Completion:       {self.total_completion_tokens:,}
      - Total Geral:      {self.total_prompt_tokens + self.total_cached_tokens + self.total_completion_tokens:,}
   ‚è±Ô∏è Tempo Total: {self.total_time_seconds:.1f}s (m√©dia: {avg_time:.2f}s/chamada)
   üí∞ Custo Real: ${cost:.6f} USD
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
"""

# Global metrics instance
metrics = MetricsCollector()

# =============================================================================
# PROMPTS v2.7 - INSTRU√á√ïES ANTI-DUPLICA√á√ÉO REFOR√áADAS
# =============================================================================

PROMPT_FIDELIDADE = """# DIRETRIZES DE FORMATA√á√ÉO E REVIS√ÉO

## PAPEL
VOC√ä √â UM EXCELENTISSIMO REDATOR T√âCNICO E DID√ÅTICO

 **Tom:** did√°tico, como o professor explicando em aula.  
- **Pessoa:** manter a pessoa original da transcri√ß√£o (1¬™ pessoa se for assim na fala).  
- **Estilo:** texto corrido, com par√°grafos curtos, sem ‚Äúinventar‚Äù doutrina nova.  
- **Objetivo:** reproduzir a aula em forma escrita, clara e organizada, mas ainda com a ‚Äúvoz‚Äù do professor.


## OBJETIVO
-Transformar a transcri√ß√£o em um texto claro, leg√≠vel e coeso, em Portugu√™s Padr√£o, MANTENDO A FIDELIDADE TOTAL ao conte√∫do original.
-- **Tamanho:** a sa√≠da deve ficar **entre 95% e 115%** do tamanho do trecho de entrada (salvo remo√ß√£o de muletas e log√≠stica).

## üö´ O QUE N√ÉO FAZER
1. **N√ÉO RESUMA**. O tamanho do texto de sa√≠da deve ser pr√≥ximo ao de entrada.
2. **N√ÉO OMITA** informa√ß√µes, exemplos, casos concretos ou explica√ß√µes.
3. **N√ÉO ALTERE** o significado ou a sequ√™ncia das ideias e das falas do professor.
4. **N√ÉO CRIE MUITOS BULLET POINTS** ou frases curtas demasiadamente. PREFIRA UM FORMATO DE MANUAL DID√ÅTICO, n√£o checklist.
5. **N√ÉO USE NEGRITOS EM EXCESSO**. Use apenas para conceitos-chave realmente importantes.
6. **N√ÉO RESUMA e N√ÉO OMITA**. Voc√™ pode reescrever frases em portugu√™s padr√£o para melhorar a fluidez, preservar a ordem, os detalhes t√©cnicos e os exemplos, mas **REMOVA** pausas excessivas e hesita√ß√µes.


## ‚ùå PRESERVE OBRIGATORIAMENTE
- **N√öMEROS EXATOS**: Artigos, Leis, S√∫mulas, Julgados (REDI/Informativos). **NUNCA OMITA N√öMEROS DE LEIS OU S√öMULAS**.
- **TODO o conte√∫do t√©cnico**: exemplos, explica√ß√µes, analogias, racioc√≠nios
- **Refer√™ncias**: leis, artigos, jurisprud√™ncia, autores, casos citados
- **√änfases intencionais**: "isso √© MUITO importante" (mantenha o destaque)
- **Observa√ß√µes pedag√≥gicas**: "cuidado com isso!", "ponto pol√™mico"

## üéØ PRESERVA√á√ÉO ESPECIAL: DICAS DE PROVA E EXAMINADORES (CR√çTICO)
Aulas presenciais frequentemente cont√™m informa√ß√µes valiosas sobre:
1. **Refer√™ncias a Examinadores**: Nomes de examinadores de concursos, suas prefer√™ncias, posicionamentos ou temas favoritos. **PRESERVE INTEGRALMENTE**.
   - Exemplo: "O examinador Fulano costuma cobrar..." ‚Üí MANTER
   - Exemplo: "Esse tema foi cobrado pelo professor X na prova..." ‚Üí MANTER
2. **Dicas de Prova**: Orienta√ß√µes sobre o que costuma cair em provas, pegadinhas comuns, temas recorrentes.
   - Exemplo: "Isso cai muito em prova..." ‚Üí MANTER
   - Exemplo: "Aten√ß√£o: essa √© uma pegadinha cl√°ssica..." ‚Üí MANTER
3. **Estrat√©gias de Estudo**: Sugest√µes do professor sobre prioriza√ß√£o, macetes, formas de memoriza√ß√£o.
   - Exemplo: "Gravem isso: na d√∫vida, marquem..." ‚Üí MANTER
   - Exemplo: "Para PGM, foquem em..." ‚Üí MANTER
4. **Casos Pr√°ticos e Hist√≥rias Reais**: Exemplos de situa√ß√µes reais, casos julgados, hist√≥rias ilustrativas.
   - **NUNCA RESUMA** hist√≥rias ou exemplos pr√°ticos. Preserve na √≠ntegra.

> ‚ö†Ô∏è **ESSAS INFORMA√á√ïES S√ÉO O DIFERENCIAL DE UMA AULA AO VIVO.** Sua omiss√£o representa perda irrepar√°vel de valor did√°tico.


## ‚úÖ DIRETRIZES DE ESTILO
1. **Corre√ß√£o Gramatical**: Corrija erros gramaticais, reg√™ncias, ortogr√°ficos e de pontua√ß√£o, tornando o texto gramaticalmente correto e claro.
2. **Limpeza Profunda:**
   - **REMOVA** marcadores de oralidade: "n√©", "t√°?", "entende?", "veja bem", "tipo assim".
   - **REMOVA** intera√ß√µes diretas com a turma/alunos e log√≠stica: "Isso mesmo", "A colega perguntou", "J√° est√£o me vendo?", "Est√£o ouvindo?", "Como ele disse ali atr√°s".
   - **REMOVA** redund√¢ncias: "subir para cima", "cria√ß√£o nova".
   - **TRANSFORME** perguntas ret√≥ricas em afirma√ß√µes quando poss√≠vel (ex: "E o que isso significa?" -> "Isso significa que...").
3. **Coes√£o**: Utilize conectivos necess√°rios para tornar o texto mais fluido. Aplique a pontua√ß√£o devida para deixar o texto coeso e coerente.
4. **Legibilidade**:
   - **USE TEXTO CORRIDO NA MEDIDA DO POSS√çVEL**
   - Utilize formata√ß√£o e estrutura com par√°grafos bem definidos, facilitando a leitura e compreens√£o
   - Evite par√°grafos longos (m√°ximo 3-4 linhas visuais)
   - Evite blocos de texto maci√ßos, quebre os blocos de texto em par√°grafos menores
   - Seja did√°tico sem perder detalhes e conte√∫do
5. **Linguagem**: Ajuste a linguagem coloquial para um portugu√™s padr√£o, mantendo o significado original.
6. **Cita√ß√µes**: Use it√°lico para cita√ß√µes curtas e recuo em it√°lico para cita√ß√µes longas.
7. -Use **negrito** para destacar conceitos-chave (sem exagero).
8. **Formata√ß√£o Did√°tica** (use com modera√ß√£o, sem excesso):
   - **Bullet points** para enumerar elementos, requisitos ou caracter√≠sticas
   - **Listas numeradas** (1., 2., 3.) para enumerar itens, etapas, correntes ou exemplos
   - **Marcadores relacionais** como "‚Üí" para indicar rela√ß√µes, transi√ß√µes, ou consequ√™ncias l√≥gicas
   - Exemplo: "Processo entre A e B ‚Üí prova usada contra C"
9. **Quest√µes e Exerc√≠cios**:
   - Se o professor ditar uma quest√£o, exerc√≠cio ou caso hipot√©tico para julgar, **ILHE-O** em um bloco de cita√ß√£o:
   > **Quest√£o:** O prazo para agravo de peti√ß√£o √© de...
   - Separe claramente o enunciado da quest√£o da explica√ß√£o/gabarito subsequente.
10. **Destaques com Emojis** (use com modera√ß√£o para facilitar escaneamento visual):
   - üí° **Dica de Prova** ou **Observa√ß√£o Pedag√≥gica**: Quando o professor der uma dica espec√≠fica para provas ou concursos.
   - ‚ö†Ô∏è **Aten√ß√£o** ou **Cuidado**: Para alertas, pegadinhas ou pontos pol√™micos.
   - üìå **Ponto Importante**: Para conceitos-chave que merecem destaque especial.
   - Exemplo de uso: `> üí° **Dica de Prova:** Esse tema caiu 3 vezes na PGM-Rio.`

## üìù ESTRUTURA
- Mantenha a sequ√™ncia exata das falas.
- Use T√≠tulos Markdown (##, ###) para organizar os t√≥picos, se identific√°veis a partir do contexto.

## üö´ T√çTULOS E SUBT√ìPICOS (IMPORTANTE)
- **N√ÉO cr√≠e subt√≥picos para frases soltas.**
- Use t√≠tulos (##, ###) **APENAS** para mudan√ßas reais de assunto.
- Se uma frase parece um t√≠tulo mas n√£o inicia uma nova se√ß√£o, mantenha como texto normal e use **negrito** se necess√°rio.

## üìä TABELA DE S√çNTESE (FLEX√çVEL)
Ao final de cada **bloco tem√°tico relevante** (ou cap√≠tulo), produza uma tabela de s√≠ntese completa (modelo flex√≠vel).
Exemplo de estrutura (adapte conforme o conte√∫do):

```
### üìã Tabela de s√≠ntese do t√≥pico

| Conceito/Instituto | Defini√ß√£o (conforme a aula) | Fundamento Legal (se citado) | Observa√ß√µes (alertas/exce√ß√µes/juris) |
| :--- | :--- | :--- | :--- |
| ...  | ...  | Art. X, Lei Y / "‚Äî" | ... |
```

***REGRAS CR√çTICAS PARA TABELAS:**
1. **Limite de conte√∫do por c√©lula:** m√°ximo ~50 palavras. Se precisar de mais, divida em m√∫ltiplas linhas da tabela
2. **PROIBIDO usar blocos de c√≥digo (```) dentro de c√©lulas** - use texto simples
3. **NUNCA deixe o t√≠tulo "üìã Resumo do T√≥pico" sozinho** - se n√£o houver dados para tabela, N√ÉO escreva o t√≠tulo
4. **POSICIONAMENTO ESTRITO:**
   - A tabela deve vir **APENAS AO FINAL** de um bloco conclu√≠do.
   - **PROIBIDO** inserir tabela no meio de uma frase ou interrompendo uma explica√ß√£o.
   - Se o texto continuar sobre o mesmo assunto, **termine o texto primeiro** e coloque a tabela depois.


## ‚ö†Ô∏è REGRA ANTI-DUPLICA√á√ÉO (CR√çTICA)
Se voc√™ receber um CONTEXTO de refer√™ncia (entre delimitadores ‚îÅ‚îÅ‚îÅ):
- Este contexto √© APENAS para voc√™ manter o mesmo ESTILO DE ESCRITA
- **NUNCA formate novamente esse contexto**
- **NUNCA inclua esse contexto na sua resposta**
- **NUNCA repita informa√ß√µes que j√° est√£o no contexto**
- Formate APENAS o texto que est√° entre as tags <texto_para_formatar>
- Se o texto_para_formatar come√ßar com algo similar ao fim do contexto, N√ÉO duplique, apenas continue naturalmente
"""

PROMPT_APOSTILA = """# DIRETRIZES DE REDA√á√ÉO: MANUAL JUR√çDICO DID√ÅTICO (MODO APOSTILA)
## PAPEL
VOC√ä √â UM EXCELENTISSIMO REDATOR T√âCNICO E DID√ÅTICO
- **Tom:** doutrin√°rio, impessoal, estilo manual de Direito.  
- **Pessoa:** 3¬™ pessoa ou constru√ß√µes impessoais (‚Äúobserva-se‚Äù, ‚Äúentende-se‚Äù).  
- **Estilo:** prosa mais densa, por√©m com par√°grafos curtos e did√°ticos.  
- **Objetivo:** transformar o conte√∫do da aula em texto de apostila/livro, sem alterar o conte√∫do e sem inventar informa√ß√µes.


## OBJETIVO
Transformar a transcri√ß√£o em um texto claro, leg√≠vel e coeso, em Portugu√™s Padr√£o, em formato de apostila/manual did√°tico

## üö´ O QUE N√ÉO FAZER
1. **N√ÉO RESUMA**. O tamanho do texto de sa√≠da deve ser pr√≥ximo ao de entrada.
2. **N√ÉO OMITA** informa√ß√µes, exemplos, casos concretos ou explica√ß√µes.
3. **N√ÉO ALTERE** o significado ou a sequ√™ncia das ideias 


‚ùå PRESERVE obrigatoriamente:
- **N√öMEROS EXATOS**: Artigos, Leis, Artigos S√∫mulas, Julgados (REDI/Informativos). **NUNCA OMITA N√öMEROS DE LEIS OU S√öMULAS**.
- **TODO o conte√∫do t√©cnico**: exemplos, explica√ß√µes, analogias, racioc√≠nios
- **Refer√™ncias**: leis, artigos, jurisprud√™ncia, autores, casos citados
- **√änfases intencionais**: "isso √© MUITO importante" (mantenha o destaque)
- **Observa√ß√µes pedag√≥gicas**: "cuidado com isso!", "ponto pol√™mico"

## üéØ PRESERVA√á√ÉO ESPECIAL: DICAS DE PROVA E EXAMINADORES (CR√çTICO)
Aulas presenciais frequentemente cont√™m informa√ß√µes valiosas sobre:
1. **Refer√™ncias a Examinadores**: Nomes de examinadores de concursos, suas prefer√™ncias, posicionamentos ou temas favoritos. **PRESERVE INTEGRALMENTE**.
   - Exemplo: "O examinador Fulano costuma cobrar..." ‚Üí MANTER
   - Exemplo: "Esse tema foi cobrado pelo professor X na prova..." ‚Üí MANTER
2. **Dicas de Prova**: Orienta√ß√µes sobre o que costuma cair em provas, pegadinhas comuns, temas recorrentes.
   - Exemplo: "Isso cai muito em prova..." ‚Üí MANTER
   - Exemplo: "Aten√ß√£o: essa √© uma pegadinha cl√°ssica..." ‚Üí MANTER
3. **Estrat√©gias de Estudo**: Sugest√µes do professor sobre prioriza√ß√£o, macetes, formas de memoriza√ß√£o.
   - Exemplo: "Gravem isso: na d√∫vida, marquem..." ‚Üí MANTER
   - Exemplo: "Para PGM, foquem em..." ‚Üí MANTER
4. **Casos Pr√°ticos e Hist√≥rias Reais**: Exemplos de situa√ß√µes reais, casos julgados, hist√≥rias ilustrativas.
   - **NUNCA RESUMA** hist√≥rias ou exemplos pr√°ticos. Preserve na √≠ntegra.

> ‚ö†Ô∏è **ESSAS INFORMA√á√ïES S√ÉO O DIFERENCIAL DE UMA AULA AO VIVO.** Sua omiss√£o representa perda irrepar√°vel de valor did√°tico.


## ‚úÖ DIRETRIZES DE ESTILO
1. **Corre√ß√£o Gramatical**: Ajuste a linguagem coloquial para o padr√£o culto.
2. **Limpeza**: Remova g√≠rias, cacoetes ("n√©", "tipo assim", "ent√£o") e v√≠cios de oralidade.
3. **Coes√£o**: Use conectivos e pontua√ß√£o adequada para tornar o texto fluido.
4. **Legibilidade**:
   - Use par√°grafos bem definidos e curtos (m√°ximo 3-4 linhas visuais).
    - Evite blocos de texto maci√ßos, quebre os blocos de texto em par√°grafos menores
   - Use **negrito** para destacar conceitos-chave (sem exagero).
5. **Formata√ß√£o Did√°tica** (use com modera√ß√£o, sem excesso):
   - **Bullet points** para enumerar elementos, requisitos ou caracter√≠sticas
   - **Listas numeradas** (1., 2., 3.) para enumerar itens, etapas, correntes doutrin√°rias ou exemplos
   - **Marcadores relacionais** como "‚Üí" para indicar rela√ß√µes, transi√ß√µes, ou consequ√™ncias l√≥gicas
   - Exemplo: "Processo entre Pedro e Jos√© ‚Üí prova usada contra Ana"
6. **Destaques com Emojis** (use com modera√ß√£o para facilitar escaneamento visual):
   - üí° **Dica de Prova** ou **Observa√ß√£o Pedag√≥gica**: Quando houver uma dica espec√≠fica para provas ou concursos.
   - ‚ö†Ô∏è **Aten√ß√£o** ou **Cuidado**: Para alertas, pegadinhas ou pontos pol√™micos.
   - üìå **Ponto Importante**: Para conceitos-chave que merecem destaque especial.
   - Exemplo de uso: `> üí° **Dica de Prova:** Esse tema caiu 3 vezes na PGM-Rio.`

## üìù ESTRUTURA
- Mantenha a sequ√™ncia exata das falas.
- Use T√≠tulos Markdown (##, ###) para organizar os t√≥picos, se identific√°veis a partir do contexto.

## üìä TABELA DE S√çNTESE (FLEX√çVEL)
Ao final de cada **bloco tem√°tico relevante** (ou cap√≠tulo), produza uma tabela de s√≠ntese (modelo flex√≠vel).
Exemplo de estrutura (adapte conforme o conte√∫do):

```
### üìã Tabela de s√≠ntese do t√≥pico

| Conceito/Instituto | Defini√ß√£o (conforme a aula) | Fundamento Legal (se citado) | Observa√ß√µes (alertas/exce√ß√µes/juris) |
| :--- | :--- | :--- | :--- |
| ...  | ...  | Art. X, Lei Y / "‚Äî" | ... |
```

***REGRAS CR√çTICAS PARA TABELAS:**
1. **Limite de conte√∫do por c√©lula:** m√°ximo ~50 palavras. Se precisar de mais, divida em m√∫ltiplas linhas da tabela
2. **PROIBIDO usar blocos de c√≥digo (```) dentro de c√©lulas** - use texto simples
3. **NUNCA deixe o t√≠tulo "üìã Resumo do T√≥pico" sozinho** - se n√£o houver dados para tabela, N√ÉO escreva o t√≠tulo
4. **POSICIONAMENTO:** A tabela deve vir **APENAS AO FINAL** da explica√ß√£o completa dos t√≥picos ou blocos tem√°ticos relevantes 
   - **NUNCA** insira a tabela no meio de uma explica√ß√£o.
   - **NUNCA** resuma um t√≥pico que voc√™ ainda n√£o acabou de explicar no texto.
   - A tabela deve ser o **fechamento** l√≥gico da se√ß√£o, antes de iniciar um novo t√≠tulo ou t√≥pico (## ou ###).

## ‚ö†Ô∏è REGRA ANTI-DUPLICA√á√ÉO (CR√çTICA)
Se voc√™ receber um CONTEXTO de refer√™ncia (entre delimitadores ‚îÅ‚îÅ‚îÅ):
- Este contexto √© APENAS para voc√™ manter o mesmo ESTILO DE ESCRITA
- **NUNCA formate novamente esse contexto**
- **NUNCA inclua esse contexto na sua resposta**
- **NUNCA repita informa√ß√µes que j√° est√£o no contexto**
- Formate APENAS o texto que est√° entre as tags <texto_para_formatar>
- Se o texto_para_formatar come√ßar com algo similar ao fim do contexto, N√ÉO duplique, apenas continue naturalmente
"""


# =============================================================================
# ESCOLHA O MODO
# =============================================================================

PROMPT_FORMATACAO = PROMPT_FIDELIDADE
# PROMPT_FORMATACAO = PROMPT_APOSTILA

# =============================================================================
# DETEC√á√ÉO DE MODO
# =============================================================================

FIDELIDADE_MODE = "N√ÉO RESUMA" in PROMPT_FORMATACAO
APOSTILA_MODE = "MANUAL JUR√çDICO" in PROMPT_FORMATACAO

if APOSTILA_MODE:
    THRESHOLD_MINIMO = 0.75
    THRESHOLD_CRITICO = 0.65
    MODO_NOME = "APOSTILA"
elif FIDELIDADE_MODE:
    THRESHOLD_MINIMO = 0.75
    THRESHOLD_CRITICO = 0.70
    MODO_NOME = "FIDELIDADE"
else:
    THRESHOLD_MINIMO = 0.70
    THRESHOLD_CRITICO = 0.60
    MODO_NOME = "PADR√ÉO"

logger.info(f"üéØ Modo: {MODO_NOME} (threshold={THRESHOLD_MINIMO:.0%})")
logger.info(f"üõ°Ô∏è  Anti-duplica√ß√£o: ATIVADA (v2.7)")

# Limiares adaptativos por camada de deduplica√ß√£o
# 7-DIFF (chunk overlaps): pode ser agressivo, overlaps s√£o quase sempre erros
LIMIAR_7DIFF = 0.85 if MODO_NOME == "FIDELIDADE" else 0.80
# Se√ß√µes duplicadas: mais cuidado, professor pode repetir propositalmente
LIMIAR_SECOES = 0.70 if MODO_NOME == "FIDELIDADE" else 0.60

logger.info(f"üìä Limiares: 7-DIFF={LIMIAR_7DIFF:.0%} | Se√ß√µes={LIMIAR_SECOES:.0%}")

# =============================================================================
# RATE LIMITER
# =============================================================================

class RateLimiter:
    def __init__(self, max_requests_per_minute=MAX_RPM):
        self.max_rpm = max_requests_per_minute
        self.requests = []
        self.lock = threading.Lock()
    
    def wait_if_needed(self):
        with self.lock:
            now = time.time()
            self.requests = [t for t in self.requests if now - t < 60]
            
            if len(self.requests) >= self.max_rpm:
                oldest = min(self.requests)
                wait_time = 60 - (now - oldest) + 0.5
                logger.info(f"‚è±Ô∏è  Rate limit: aguardando {wait_time:.1f}s...")
                sleep(wait_time)
                self.requests = [t for t in self.requests if time.time() - t < 60]
            
            self.requests.append(time.time())

rate_limiter = RateLimiter()

# =============================================================================
# CHECKPOINT/RESUME
# =============================================================================

def get_checkpoint_path(input_file):
    return Path(input_file).with_suffix('.checkpoint.json')

def save_checkpoint(input_file, resultados, chunks_info, secao_atual):
    checkpoint_path = get_checkpoint_path(input_file)
    checkpoint_data = {
        'input_file': str(input_file),
        'secao_atual': secao_atual,
        'total_secoes': len(chunks_info),
        'chunks_info': chunks_info,
        'resultados': resultados,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'version': '2.7',
        'modo': MODO_NOME
    }
    with open(checkpoint_path, 'w', encoding='utf-8') as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)

def load_checkpoint(input_file):
    checkpoint_path = get_checkpoint_path(input_file)
    if checkpoint_path.exists():
        try:
            with open(checkpoint_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if data.get('version') not in ('2.6', '2.7'):
                    logger.warning("Checkpoint de vers√£o anterior. Reiniciando...")
                    return None
                return data
        except Exception as e:
            logger.error(f"Erro ao carregar checkpoint: {e}")
    return None

def delete_checkpoint(input_file):
    checkpoint_path = get_checkpoint_path(input_file)
    if checkpoint_path.exists():
        checkpoint_path.unlink()
        logger.info("üßπ Checkpoint removido")

# =============================================================================
# DIVIS√ÉO SEQUENCIAL
# =============================================================================

def dividir_sequencial(transcricao_completa, estrutura_global=None):
    """
    v2.17: Divide documento com Intelig√™ncia de √Çncoras (Anchor-Based Chunking).
    Se estrutura_global for fornecida, tenta alinhar cortes com in√≠cio de t√≥picos.
    """
    chunks = []
    tamanho_total = len(transcricao_completa)
    inicio = 0
    
    # Prepara √¢ncoras (keywords dos t√≠tulos)
    ancoras = []
    if estrutura_global:
        for item in estrutura_global.split('\n'):
            clean_item = re.sub(r'^\d+(\.\d+)*\.?\s*', '', item.strip())
            if len(clean_item) > 10:
                # Pega as primeiras 4 palavras do t√≠tulo como "√¢ncora"
                keywords = ' '.join(clean_item.split()[:4])
                ancoras.append(keywords)
    
    while inicio < tamanho_total:
        target_fim = inicio + CHARS_POR_PARTE
        fim = min(target_fim, tamanho_total)
        
        if fim < tamanho_total:
            # Janela de busca para ajuste fino
            janela_busca = transcricao_completa[max(0, fim - 1000):min(tamanho_total, fim + 1000)]
            offset_janela = max(0, fim - 1000)
            
            melhor_corte = -1
            
            # 1. Tenta encontrar uma √ÇNCORA de T√≥pico (Prioridade Alta)
            if ancoras:
                for ancora in ancoras:
                    # Busca fuzzy ou exata da √¢ncora na janela
                    # Simplifica√ß√£o: busca exata case-insensitive
                    pos_ancora = janela_busca.lower().find(ancora.lower())
                    if pos_ancora != -1:
                        corte_proposto = offset_janela + pos_ancora
                        # Se o corte sugerido pela √¢ncora estiver dentro de um range aceit√°vel
                        if abs(corte_proposto - fim) < 800: # Aceita desvio de at√© 800 chars
                            melhor_corte = corte_proposto
                            logger.info(f"   ‚öì √Çncora encontrada: '{ancora}' (ajustando corte)")
                            break
            
            # 2. Se n√£o achou √¢ncora, busca quebra estrutural forte (## T√≠tulo)
            if melhor_corte == -1:
                titulo_match = re.search(r'\n(#{2,4}\s+.+)\n', janela_busca)
                if titulo_match:
                    melhor_corte = offset_janela + janela_busca.find(titulo_match.group(0))
            
            # 3. Fallback: Quebra de par√°grafo duplo
            if melhor_corte == -1:
                quebra = transcricao_completa.rfind('\n\n', fim - 300, fim + 300)
                if quebra != -1 and quebra > inicio:
                    melhor_corte = quebra
            
            # 4. √öltimo recurso: Ponto final
            if melhor_corte == -1:
                quebra = transcricao_completa.rfind('. ', fim - 150, fim + 150)
                if quebra != -1 and quebra > inicio:
                    melhor_corte = quebra + 1
            
            # Aplica o melhor corte encontrado
            if melhor_corte != -1:
                fim = melhor_corte
        
        chunks.append({'inicio': inicio, 'fim': fim})
        inicio = fim
    
    return chunks

def validar_chunks(chunks, transcricao_completa):
    """v2.7: Valida√ß√£o rigorosa de chunks sequenciais"""
    logger.info("üîç Validando chunks sequenciais...")
    
    for i in range(len(chunks)):
        chunk = chunks[i]
        
        # Verifica se in√≠cio == fim do anterior
        if i > 0:
            anterior = chunks[i-1]
            if chunk['inicio'] != anterior['fim']:
                logger.error(f"‚ùå Gap/Overlap no chunk {i+1}!")
                logger.error(f"   Anterior termina em: {anterior['fim']}")
                logger.error(f"   Atual come√ßa em: {chunk['inicio']}")
                logger.error(f"   Diferen√ßa: {chunk['inicio'] - anterior['fim']} chars")
                
                # Mostra preview
                if chunk['inicio'] < anterior['fim']:
                    overlap_text = transcricao_completa[chunk['inicio']:anterior['fim']]
                    logger.error(f"   OVERLAP: '{overlap_text[:100]}...'")
                
                return False
    
    logger.info(f"‚úÖ {len(chunks)} chunks validados (sequenciais, sem overlap)")
    return True

# =============================================================================
# FUN√á√ïES AUXILIARES
# =============================================================================

def limpar_tags_xml(texto):
    texto = re.sub(r'</?[a-z_][\w\-]*>', '', texto, flags=re.IGNORECASE)
    texto = re.sub(r'<[a-z_][\w\-]*\s+[^>]+>', '', texto, flags=re.IGNORECASE)
    return texto

def carregar_transcricao(arquivo):
    try:
        with open(arquivo, 'r', encoding='utf-8-sig') as f:
            conteudo = f.read()
        
        if not conteudo.strip():
            logger.error("Arquivo est√° vazio.")
            sys.exit(1)
            
        return conteudo
    except FileNotFoundError:
        logger.error(f"Arquivo '{arquivo}' n√£o encontrado.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Erro ao ler arquivo: {e}")
        sys.exit(1)

def estimar_custo(transcricao, usar_cache, num_chunks=1):
    tokens_in = len(transcricao) // 4
    
    if APOSTILA_MODE:
        tokens_out_estimado = int(tokens_in * 0.65)
    elif FIDELIDADE_MODE:
        tokens_out_estimado = int(tokens_in * 1.00)
    else:
        tokens_out_estimado = int(tokens_in * 0.85)
    
    tokens_prompt = len(PROMPT_FORMATACAO) // 4
    tokens_in_total = tokens_in + (tokens_prompt * num_chunks)
    
    if usar_cache:
        custo_input = (tokens_in * PRECO_INPUT_COM_CACHE + tokens_prompt * num_chunks * PRECO_INPUT_SEM_CACHE) / 1_000_000
        custo_output = (tokens_out_estimado * PRECO_OUTPUT) / 1_000_000
        custo = custo_input + custo_output
    else:
        custo = (tokens_in_total * PRECO_INPUT_SEM_CACHE + tokens_out_estimado * PRECO_OUTPUT) / 1_000_000
    
    logger.info(f"üí∞ Custo estimado: ${custo:.4f} USD (modo {MODO_NOME})")

# v2.9: Cache REABILITADO com hash inteligente
def criar_cache_contexto(client, transcricao_completa, system_prompt, estrutura_global=None):
    """Cria cache de contexto com hash est√°vel para reutiliza√ß√£o"""
    
    # Cache s√≥ vale a pena para documentos grandes
    if len(transcricao_completa) < MIN_CHARS_PARA_CACHE:
        logger.info(f"üì¶ Documento pequeno ({len(transcricao_completa):,} chars), cache n√£o necess√°rio")
        return None
    
    try:
        # Hash do prompt + estrutura para garantir unicidade por documento
        combined_content = system_prompt + (estrutura_global or "")
        prompt_hash = hashlib.sha256(combined_content.encode()).hexdigest()[:16]
        cache_name = f"fmt_{prompt_hash}"
        
        # v2.9: Tenta encontrar cache existente v√°lido
        try:
            for c in client.caches.list(page_size=100):
                if c.display_name == cache_name:
                    logger.info(f"‚ôªÔ∏è  Reusando cache existente: {cache_name} ({c.name})")
                    return c
        except Exception as e:
            logger.warning(f"Cache lookup warning: {e}")

        # Adiciona a estrutura global se dispon√≠vel
        estrutura_text = f"\n\n## ESTRUTURA GLOBAL:\n{estrutura_global}" if estrutura_global else ""
        
        cache_content = f"""{system_prompt}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìö CONTEXTO GLOBAL
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Modo: {MODO_NOME}{estrutura_text}
"""
        
        # v2.19: TTL Din√¢mico (User Request)
        # Estimativa: 1 hora a cada 500k chars + 1h margem
        # Isso evita que o cache expire no meio de um documento longo
        tempo_estimado_segundos = int((len(transcricao_completa) / 500000) * 3600) + 3600
        dinamico_ttl = f"{max(3600, tempo_estimado_segundos)}s"
        
        # Cria cache usando a API do google-genai
        cache = client.caches.create(
            model=GEMINI_MODEL,
            config=types.CreateCachedContentConfig(
                contents=[cache_content],
                ttl=dinamico_ttl,
                display_name=cache_name
            )
        )
        
        logger.info(f"‚úÖ Cache criado: {cache_name} (hash: {prompt_hash}, TTL: {CACHE_TTL})")
        return cache
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Falha ao criar cache: {e}. Continuando sem cache.")
        return None

# =============================================================================
# MAPEAMENTO ESTRUTURAL (v2.8)
# =============================================================================

PROMPT_MAPEAMENTO = """Voc√™ √© um especialista em organiza√ß√£o de conte√∫do educacional acad√™mico.

## ETAPA 1: IDENTIFICAR O TIPO DE CONTE√öDO
Analise a transcri√ß√£o e determine qual √© a **natureza predominante** do material:

| Tipo | Pistas no Texto | Estrutura Ideal |
|------|-----------------|-----------------|
| **SIMULADO** | "quest√£o 1", "quest√£o 2", "espelho de corre√ß√£o", "corre√ß√£o do simulado", "vamos corrigir" | Organizar por QUEST√ïES numeradas |
| **AULA EXPOSITIVA** | explica√ß√µes cont√≠nuas de um tema, teoria, doutrina, sem quest√µes espec√≠ficas | Organizar por TEMAS/MAT√âRIAS |
| **REVIS√ÉO** | "revis√£o", "resumo", m√∫ltiplos temas curtos, "pontos importantes" | Organizar por T√ìPICOS de revis√£o |
| **CORRE√á√ÉO DE PROVA** | "gabarito", "alternativa correta", "item certo/errado" | Organizar por QUEST√ïES com gabarito |

## ETAPA 2: EXTRAIR A ESTRUTURA

### Se for SIMULADO ou CORRE√á√ÉO DE PROVA:
```
1. Orienta√ß√µes Gerais / Introdu√ß√£o
2. Quest√£o 1: [T√≠tulo descritivo] ‚Äî [√Årea do Direito]
   2.1. Enunciado e Contexto
   2.2. Fundamenta√ß√£o (Doutrina/Jurisprud√™ncia)
   2.3. Pontos do Espelho / Resposta
3. Quest√£o 2: [T√≠tulo descritivo] ‚Äî [√Årea do Direito]
   3.1. ...
[Continue para cada quest√£o]
N. Considera√ß√µes Finais / D√∫vidas
```

### Se for AULA EXPOSITIVA:
```
1. Introdu√ß√£o
2. [Mat√©ria 1: ex. Direito Administrativo]
   2.1. [Subtema]
      2.1.1. [Detalhamento]
3. [Mat√©ria 2: ex. Direito Civil]
   3.1. ...
```

### Se for REVIS√ÉO:
```
1. [Tema 1]
   1.1. Pontos-chave
   1.2. Jurisprud√™ncia/S√∫mulas
2. [Tema 2]
   2.1. ...
```

## REGRAS GERAIS:
1. **M√ÅXIMO 3 N√çVEIS** de hierarquia (1., 1.1., 1.1.1.)
2. **Seja descritivo** nos t√≠tulos ‚Äî inclua o assunto real, n√£o apenas "Quest√£o 1"
3. **Mantenha a ORDEM** cronol√≥gica da transcri√ß√£o
4. **Mapeie do IN√çCIO ao FIM** ‚Äî n√£o omita partes
5. **Identifique a √ÅREA DO DIREITO** de cada bloco quando poss√≠vel

## üèõÔ∏è REGRA ESPECIAL: MARCOS LEGAIS (v2.17)
Quando identificar marcos legais importantes, crie subt√≥picos espec√≠ficos:
- **S√∫mulas** (STF, STJ, Vinculantes): Criar subt√≥pico "X.Y. S√∫mula [N√∫mero] do [Tribunal]"
- **Teses (Repercuss√£o Geral/Repetitivos)**: Criar subt√≥pico "X.Y. Tese/Tema [N√∫mero] do STJ/STF"
- **Artigos de Lei Central**: Se um artigo √© explicado em profundidade, criar subt√≥pico "X.Y. Art. [N√∫mero] da [Lei]"

Exemplo:
```
2. Execu√ß√£o Fiscal
   2.1. Procedimento da LEF (Lei 6.830/80)
   2.2. S√∫mula 314 do STJ (Cita√ß√£o por Hora Certa)
   2.3. Tema 444 do STJ (Redirecionamento)
```

## RESPOSTA:
Primeiro, indique em uma linha: `[TIPO: SIMULADO/EXPOSITIVA/REVIS√ÉO/CORRE√á√ÉO]`
Depois, retorne APENAS a estrutura hier√°rquica (m√°x 3 n√≠veis).
"""

# ---------------------------------------------------------------------------
# Sanitiza√ß√£o de t√≠tulos na estrutura mapeada (v2.47)
# ---------------------------------------------------------------------------

_CONVERSATIONAL_TITLE_PREFIXES = (
    "j√° ", "na prova", "para quem", "minha proposta",
    "bom dia", "gente ", "pessoal ", "vamos ", "ent√£o ",
    "logo ", "eu ", "n√≥s ", "aqui ", "olha ", "vejam ",
    "como eu ", "antes de ", "boa tarde", "boa noite",
    "obrigado", "obrigada", "com licen√ßa",
)

_GREETING_PREFIXES = (
    "bom dia", "boa tarde", "boa noite", "j√° ", "pessoal ",
    "gente ", "olha ", "obrigado", "obrigada",
)

# R√≥tulos can√¥nicos por n√≠vel
_CANONICAL_LABEL_L1 = "Introdu√ß√£o e Contextualiza√ß√£o"
_CANONICAL_LABEL_SUB = "Abertura"

_MAX_TITLE_WORDS = 8
_MAX_TITLE_CHARS = 70


def _sanitize_structure_titles(estrutura: str) -> str:
    """Valida e corrige t√≠tulos de estrutura que s√£o trechos literais de fala.

    Regras (alinhadas com PROMPT_MAPEAMENTO regra 8):
    - T√≠tulos > 8 palavras ou > 70 chars ‚Üí mapear para r√≥tulo can√¥nico
    - Prefixos conversacionais (sauda√ß√µes, log√≠stica) ‚Üí r√≥tulo can√¥nico
    - Preserva √¢ncoras ABRE/FECHA intactas (incluindo aspas)

    Fun√ß√£o pura, sem depend√™ncias externas (logger opcional).
    """
    if not estrutura:
        return estrutura

    lines = estrutura.split('\n')
    fixed_lines: list[str] = []
    sanitized_count = 0

    for line in lines:
        stripped = line.strip()
        # Detecta linhas numeradas: "1. T√≠tulo", "   1.1. Subt√≠tulo"
        m = re.match(r'^(\s*\d+(?:\.\d+)*\.?\s+)(.*)', stripped)
        if not m:
            fixed_lines.append(line)
            continue

        prefix_num = m.group(1)
        rest = m.group(2).strip()

        # Separa √¢ncoras ABRE/FECHA se existirem (preserva literalmente)
        anchor_part = ""
        title = rest
        anchor_idx = rest.find("| ABRE:")
        if anchor_idx >= 0:
            title = rest[:anchor_idx].strip()
            anchor_part = " " + rest[anchor_idx:]

        title_lower = title.lower()
        words = re.findall(r'[A-Za-z√Ä-√ø0-9]+', title)

        needs_fix = False
        if len(title) > _MAX_TITLE_CHARS:
            needs_fix = True
        elif len(words) > _MAX_TITLE_WORDS:
            needs_fix = True
        elif any(title_lower.startswith(pfx) for pfx in _CONVERSATIONAL_TITLE_PREFIXES):
            needs_fix = True

        if needs_fix:
            # Determinar n√≠vel: "1." ‚Üí n√≠vel 1, "1.1." ‚Üí n√≠vel 2+
            is_level1 = re.match(r'^\s*\d+\.\s', stripped) and not re.match(r'^\s*\d+\.\d+', stripped)

            if any(title_lower.startswith(pfx) for pfx in _GREETING_PREFIXES):
                canonical = _CANONICAL_LABEL_L1 if is_level1 else _CANONICAL_LABEL_SUB
            elif is_level1:
                canonical = _CANONICAL_LABEL_L1
            else:
                canonical = _CANONICAL_LABEL_SUB

            sanitized_count += 1
            try:
                logger.warning(f"‚ö†Ô∏è  T√≠tulo sanitizado: '{title[:60]}' ‚Üí '{canonical}'")
            except Exception:
                pass  # logger pode n√£o existir em contexto de teste
            fixed_lines.append(f"{prefix_num}{canonical}{anchor_part}")
        else:
            fixed_lines.append(line)

    if sanitized_count:
        try:
            logger.info(f"üîß {sanitized_count} t√≠tulo(s) de estrutura sanitizado(s)")
        except Exception:
            pass

    return '\n'.join(fixed_lines)


def mapear_estrutura(client, transcricao_completa):
    """Analisa o documento completo e extrai a estrutura de t√≥picos"""
    logger.info("üó∫Ô∏è  Mapeando estrutura do documento...")
    
    rate_limiter.wait_if_needed()
    
    # Gemini 2.5 Flash suporta 1M tokens (~4M chars)
    # Limite de 3.5M chars para deixar margem para output (20k tokens)
    max_chars_mapeamento = 3_500_000
    
    if len(transcricao_completa) > max_chars_mapeamento:
        logger.warning(f"‚ö†Ô∏è  Documento EXTREMAMENTE grande ({len(transcricao_completa):,} chars). Cortando final para caber no contexto.")
        # √â melhor cortar o final do que picotar o meio para estrutura
        texto_para_mapear = transcricao_completa[:max_chars_mapeamento]
    else:
        texto_para_mapear = transcricao_completa
        logger.info(f"   Mapeando documento completo ({len(transcricao_completa):,} chars)")
    
    prompt = PROMPT_MAPEAMENTO.format(transcricao=texto_para_mapear)
    
    try:
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=prompt,
            config=types.GenerateContentConfig(
                max_output_tokens=20000,  # Aumentado para documentos grandes
                thinking_config={"include_thoughts": False, "thinking_level": "HIGH"}, # Mapeamento: HIGH
                safety_settings=[
                    types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                ]
            )
        )
        
        
        if not response.text:
            logger.warning("‚ö†Ô∏è  Resposta vazia do mapeamento.")
            logger.warning(f"   Response object: {response}")
            if hasattr(response, 'candidates') and response.candidates:
                logger.warning(f"   Finish reason: {response.candidates[0].finish_reason}")
                if hasattr(response.candidates[0], 'safety_ratings'):
                    logger.warning(f"   Safety ratings: {response.candidates[0].safety_ratings}")
            logger.warning("   Continuando sem estrutura pr√©via.")
            return None
        
        estrutura = response.text.strip()
        
        # Remove markdown code blocks se presentes
        if estrutura.startswith('```'):
            linhas = estrutura.split('\n')
            estrutura = '\n'.join(linhas[1:-1]) if len(linhas) > 2 else estrutura
        
        linhas = [l for l in estrutura.split('\n') if l.strip()]
        
        # Valida√ß√£o da estrutura
        if len(linhas) < 3:
            logger.warning(f"‚ö†Ô∏è  Estrutura muito curta ({len(linhas)} linhas). Pode estar incompleta.")
            return None
        
        tem_numeracao = any(re.match(r'^\d+\.', l.strip()) for l in linhas)
        if not tem_numeracao:
            logger.warning("‚ö†Ô∏è  Estrutura sem numera√ß√£o hier√°rquica. Pode estar mal formatada.")
            return None
        
        logger.info(f"‚úÖ Estrutura mapeada: {len(linhas)} t√≥picos identificados")
        
        # Log preview (primeiras 10 linhas + total)
        for linha in linhas[:10]:
            logger.info(f"   {linha}")
        if len(linhas) > 10:
            logger.info(f"   ... e mais {len(linhas) - 10} t√≥picos")

        # v2.47: Sanitiza t√≠tulos que s√£o frases literais de fala
        estrutura = _sanitize_structure_titles(estrutura)

        return estrutura
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Falha no mapeamento: {e}. Continuando sem estrutura pr√©via.")
        return None

def simplificar_estrutura_se_necessario(estrutura, max_linhas=60):
    """
    Se a estrutura for muito longa (> max_linhas), mant√©m apenas:
    - N√≠vel 1: 1. Assunto
    - N√≠vel 2: 1.1. Subassunto

    Sempre inclui todos os n√≠veis 1, e todos os n√≠veis 2,
    e depois corta no m√°ximo max_linhas, preservando a ordem original.
    """
    if not estrutura:
        return estrutura

    linhas = [l for l in estrutura.strip().split("\n") if l.strip()]
    if len(linhas) <= max_linhas:
        # J√° est√° razo√°vel, mant√©m at√© n√≠vel 3 (ser√° filtrado depois por filtrar_niveis_excessivos)
        return estrutura

    logger.info(f"üìâ Estrutura muito longa ({len(linhas)} itens). Simplificando para n√≠veis 1 e 2, m√°x {max_linhas} linhas...")

    nivel1 = []
    nivel2 = []

    for l in linhas:
        s = l.strip()
        # 1. Processo do Trabalho
        if re.match(r"^\d+\.\s", s):
            nivel1.append(l)
        # 1.1. Recursos Trabalhistas
        elif re.match(r"^\d+\.\d+\.\s", s):
            nivel2.append(l)

    # Se por algum motivo n√£o identificou quase nada, devolve original para n√£o quebrar
    if len(nivel1) + len(nivel2) < 5:
        logger.warning("‚ö†Ô∏è Simplifica√ß√£o deixou poucos t√≥picos. Mantendo estrutura original.")
        return estrutura

    # Monta nova estrutura: primeiro todos os n√≠veis 1, depois os n√≠veis 2, respeitando ordem de aparecimento
    nova = []
    vistos = set()

    for l in linhas:
        if l in vistos:
            continue
        if l in nivel1 or l in nivel2:
            nova.append(l)
            vistos.add(l)

    if len(nova) > max_linhas:
        nova = nova[:max_linhas]

    logger.info(f"‚úÖ Estrutura simplificada: {len(linhas)} -> {len(nova)} linhas (n√≠veis 1 e 2).")
    return "\n".join(nova)

def filtrar_niveis_execessivos(estrutura, max_nivel=3):
    """
    Remove itens da estrutura que sejam mais profundos que max_nivel.
    Ex: se max_nivel=3, remove 1.1.1.1
    """
    if not estrutura:
        return estrutura
        
    linhas = estrutura.strip().split('\n')
    linhas_filtradas = []
    itens_removidos = 0
    
    # Regex para validar n√≠vel. 
    # N√≠vel 1: \d+\.
    # N√≠vel 2: \d+\.\d+
    # N√≠vel 3: \d+\.\d+\.\d+
    # O regex verifica se tem no m√°ximo (max_nivel-1) pontos internos entre n√∫meros
    
    for linha in linhas:
        # Conta quantos grupos de n√∫meros existem
        match = re.match(r'^(\d+(?:\.\d+)*)', linha.strip())
        if match:
            numeracao = match.group(1)
            nivel = numeracao.count('.') + 1
            if linha.strip().endswith('.'): # Se terminar com ponto (1.1.), n√£o conta como n√≠vel extra
                 nivel = numeracao.count('.')
            
            # Ajuste robusto: contar n√∫meros separados por ponto
            partes = [p for p in numeracao.split('.') if p.isdigit()]
            nivel_real = len(partes)
            
            if nivel_real <= max_nivel:
                linhas_filtradas.append(linha)
            else:
                itens_removidos += 1
        else:
            # Linhas sem numera√ß√£o (t√≠tulos soltos?) mant√©m por seguran√ßa ou remove?
            # Vamos manter para n√£o quebrar formata√ß√£o estranha
            linhas_filtradas.append(linha)
            
    if itens_removidos > 0:
        logger.info(f"‚úÇÔ∏è  Filtrados {itens_removidos} itens com n√≠vel > {max_nivel}")
    
    return '\n'.join(linhas_filtradas)

# =============================================================================
# PROCESSAMENTO
# =============================================================================

def processar_simples(client, transcricao_bruta, system_prompt):
    logger.info("üìÑ Documento pequeno - processando em requisi√ß√£o √∫nica...")
    
    prompt = f"""{system_prompt}

<texto_para_formatar>
{transcricao_bruta}
</texto_para_formatar>

Retorne APENAS o Markdown formatado."""
    
    for tentativa in range(MAX_RETRIES):
        try:
            response = client.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt,
                config=types.GenerateContentConfig(
                    max_output_tokens=OUTPUT_TOKEN_LIMIT,
                    temperature=0,
                    thinking_config={"include_thoughts": False, "thinking_level": "LOW"}, # Formata√ß√£o: LOW
                    safety_settings=[
                        types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                        types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                    ]
                )
            )
            resultado = response.text
            return limpar_tags_xml(resultado)
        except Exception as e:
            if tentativa < MAX_RETRIES - 1:
                wait = (2 ** tentativa) + random.uniform(0, 1)
                logger.warning(f"Erro, retry em {wait:.1f}s...")
                sleep(wait)
            else:
                raise

def processar_chunk(client, cache, system_prompt, texto_chunk, numero, total, contexto_estilo="", estrutura_global=None, ultimo_titulo=None, profundidade=0):
    rate_limiter.wait_if_needed()
    
    # Recurs√£o infinita protection e limite m√≠nimo
    MIN_CHUNK_CHARS = 4000
    if len(texto_chunk) < MIN_CHUNK_CHARS:
        logger.warning(f"‚ö†Ô∏è Chunk {numero} muito pequeno ({len(texto_chunk)} chars). Processando sem dividir.")
    elif profundidade > 2:
        logger.warning(f"‚ö†Ô∏è Chunk {numero}: Profundidade de recurs√£o {profundidade} atingida. Processando sem dividir.")

    # v2.8: Se√ß√£o de estrutura global
    secao_estrutura = ""
    if estrutura_global:
        secao_estrutura = f"""
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìã ESTRUTURA GLOBAL DA AULA (GUIA)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

{estrutura_global}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚ö†Ô∏è REGRA DE OURO - PRIORIDADE DO CONTE√öDO REAL:
O Mapeamento acima √© apenas um guia inicial. SE houver diverg√™ncia 
entre o Mapeamento e a Transcri√ß√£o Real (ex: o professor mudou de 
assunto, ou o t√≠tulo n√£o existe na fala), SIGA A TRANSCRI√á√ÉO REAL.
A fidelidade ao que foi *falado* √© mais importante que seguir 
cegamente a estrutura.
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
"""
    
    # v2.10: Aviso sobre √∫ltimo t√≠tulo (anti-duplica√ß√£o)
    aviso_titulo = ""
    if ultimo_titulo:
        aviso_titulo = f"""
üö´ O bloco anterior TERMINOU no t√≥pico: "{ultimo_titulo}"
   N√ÉO inicie sua resposta repetindo este t√≠tulo.
   Continue o conte√∫do ou inicie o PR√ìXIMO subt√≥pico.
"""
    
    # v2.7: Delimitadores MUITO vis√≠veis e instru√ß√µes refor√ßadas
    secao_contexto = ""
    if contexto_estilo:
        secao_contexto = f"""
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üîí CONTEXTO ANTERIOR (SOMENTE REFER√äNCIA DE ESTILO)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

{contexto_estilo}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚ö†Ô∏è ATEN√á√ÉO: O bloco acima J√Å FOI FORMATADO anteriormente.
- N√ÉO formate novamente esse conte√∫do
- N√ÉO inclua esse conte√∫do na sua resposta
- Use APENAS como refer√™ncia de estilo de escrita
{aviso_titulo}‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìù NOVO TEXTO PARA FORMATAR (comece aqui):
"""
    
    # v2.9: Cache Support - Se usar cache, n√£o repete PROMPT_FORMATACAO
    instructions_body = f"""
{secao_estrutura}
{secao_contexto}

<texto_para_formatar>
{texto_chunk}
</texto_para_formatar>

**INSTRU√á√ïES FINAIS**:
- Esta √© a parte {numero} de {total} (Profundidade: {profundidade})
- Formate APENAS o texto entre <texto_para_formatar>
- Se houver ESTRUTURA GLOBAL acima, use os mesmos nomes de t√≥picos
- Se houver contexto acima, N√ÉO o reprocesse
- **ANTI-REPETI√á√ÉO DE T√çTULOS**: Se o contexto anterior termina com um t√≠tulo (ex: "## Homologa√ß√£o"), N√ÉO repita esse t√≠tulo no in√≠cio da sua resposta. Continue o conte√∫do diretamente ou inicie o PR√ìXIMO t√≥pico diferente.
- Retorne APENAS o Markdown formatado do NOVO texto
"""

    if cache:
        prompt = instructions_body
    else:
        prompt = f"{system_prompt}\n{instructions_body}"

    for tentativa in range(MAX_RETRIES):
        try:
            safety_config = [
                types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
            ]
            
            # Configura√ß√£o din√¢mica para suportar cache
            gen_config_args = {
                "max_output_tokens": OUTPUT_TOKEN_LIMIT,
                "temperature": 0.1,
                "thinking_config": {"include_thoughts": False, "thinking_level": "LOW"}, # Formata√ß√£o: LOW
                "safety_settings": safety_config
            }
            if cache:
                gen_config_args['cached_content'] = cache.name

            response = client.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt,
                config=types.GenerateContentConfig(**gen_config_args)
            )
            
            # --- Diagn√≥stico finishReason e Usage ---
            finish_reason = "UNKNOWN"
            usage_tokens = 0
            
            try:
                if hasattr(response, 'usage_metadata'):
                    usage = response.usage_metadata
                    # Tenta acessar atributos (pode variar entre SDKs/Vertex)
                    # v2.17.1: Garantir inteiros (getattr pode retornar None se o atributo existe mas √© None)
                    candidates_token_count = getattr(usage, 'candidates_token_count', 0) or 0
                    prompt_token_count = getattr(usage, 'prompt_token_count', 0) or 0
                    cached_content_token_count = getattr(usage, 'cached_content_token_count', 0) or 0
                    
                    usage_tokens = candidates_token_count
                    
                    logger.info(f"üìä Usage: Prompt={prompt_token_count} (Cached: {cached_content_token_count}) | Candidates={candidates_token_count}")
                    
                    # Acumular m√©tricas globais (v2.10)
                    metrics.record_api_call(
                        prompt_tokens=prompt_token_count, 
                        completion_tokens=candidates_token_count, 
                        cached_tokens=cached_content_token_count,
                        provider='gemini'
                    )
                
                if hasattr(response, 'candidates') and response.candidates:
                    cand = response.candidates[0]
                    if hasattr(cand, 'finish_reason'):
                         finish_reason = str(cand.finish_reason) # ex: FinishReason.STOP ou "STOP"
            except Exception as ex_usage:
                logger.warning(f"‚ö†Ô∏è Erro ao ler metadados: {ex_usage}")

            # Captura texto (lidando com caso de .text vazio mas content presente)
            resultado = ""
            try:
                resultado = response.text
            except ValueError:
                # O SDK levanta ValueError se finish_reason for SAFETY ou se n√£o houver text field padr√£o
                pass
            
            if not resultado and hasattr(response, 'candidates') and response.candidates:
                # Tenta extrair parts[0].text manualmente se .text falhou
                try:
                    parts = response.candidates[0].content.parts
                    if parts:
                        resultado = parts[0].text
                except:
                    pass
            
            if not resultado:
                logger.warning(f"‚ö†Ô∏è  Resposta vazia na tentativa {tentativa+1}. Reason: {finish_reason}")
                if tentativa < MAX_RETRIES - 1:
                    sleep(2 * (tentativa + 1))
                    continue
                else:
                    return f"[ERRO SE√á√ÉO {numero}: RESPOSTA VAZIA]"

            # Valida√ß√£o b√°sica de tamanho (compress√£o)
            razao = len(resultado) / len(texto_chunk) if len(texto_chunk) > 0 else 0
            
            problema_detectado = False
            msg_problema = ""
            
            compressao_excessiva_severa = False
            
            if razao < THRESHOLD_CRITICO: # < 0.70 por padr√£o
                problema_detectado = True
                msg_problema = f"Compress√£o excessiva ({razao:.0%})"
                # Flag para chunking imediato se for muito baixo (ex < 0.65)
                # O usu√°rio pediu < THRESHOLD_CRITICO, vamos ser assertivos.
                compressao_excessiva_severa = True
                
            if problema_detectado:
                logger.warning(f"‚ö†Ô∏è Se√ß√£o {numero}: {msg_problema}. Reason: {finish_reason}. (Tentativa {tentativa+1}/{MAX_RETRIES})")
                
                # SE√á√ÉO CR√çTICA: Decis√£o de Chunking Adaptativo
                
                # L√≥gica antiga: s√≥ no final ou MAX_TOKENS
                # L√≥gica nova: divide cedo se compress√£o for severa
                
                deve_dividir = (
                    len(texto_chunk) > MIN_CHUNK_CHARS 
                    and profundidade < 2
                    and (
                        ("MAX_TOKENS" in str(finish_reason)) or 
                        (tentativa == MAX_RETRIES - 1) or
                        (compressao_excessiva_severa) # NOVO: Divide j√°!
                    )
                )
                
                if deve_dividir:
                    motivo = "MAX_TOKENS" if "MAX_TOKENS" in str(finish_reason) else "COMPRESS√ÉO"
                    logger.info(f"‚úÇÔ∏è  ATIVANDO CHUNKING ADAPTATIVO para Se√ß√£o {numero} (Motivo: {motivo} | Profundidade {profundidade} -> {profundidade+1})")
                    return dividir_e_reprocessar(client, cache, system_prompt, texto_chunk, numero, total, contexto_estilo, estrutura_global, ultimo_titulo, profundidade)
                
                if tentativa < MAX_RETRIES - 1:
                    continue  # Tenta de novo (se n√£o foi severo o suficiente para dividir)
                else:
                    logger.error(f"Se√ß√£o {numero}: Falha ap√≥s {MAX_RETRIES} tentativas. Retornando melhor esfor√ßo.")
            
            return resultado
            
        except Exception as e:
            erro_msg = str(e)
            is_recoverable = any(code in erro_msg for code in ['503', '429', '500', 'RESOURCE_EXHAUSTED', 'InternalServerError']) or "Resposta vazia" in erro_msg
            
            if tentativa < MAX_RETRIES - 1 and is_recoverable:
                wait = (2 ** (tentativa + 2)) + random.uniform(1, 3)
                if '429' in erro_msg or 'RATE_LIMIT' in erro_msg:
                    wait = 30 + random.uniform(0, 5)
                    logger.warning(f"üõë Rate Limit (429) detectado na se√ß√£o {numero}. Pausa longa de {wait:.1f}s...")
                else:
                    logger.warning(f"Erro se√ß√£o {numero}: {erro_msg}. Retry {tentativa+2}/{MAX_RETRIES} em {wait:.1f}s")
                sleep(wait)
            else:
                logger.error(f"Falha se√ß√£o {numero}: {erro_msg}")
                return f"\n\n> [!WARNING]\n> Falha ao processar se√ß√£o {numero}. Texto original:\n\n{texto_chunk}"
    
    return texto_chunk

def dividir_e_reprocessar(client, cache, system_prompt, texto_chunk, numero, total, contexto_estilo, estrutura_global, ultimo_titulo, profundidade):
    """
    Divide um chunk grande em dois menores e processa recursivamente.
    Tenta dividir em quebras de par√°grafo (\n\n) pr√≥ximas ao meio.
    """
    # v2.10: Registrar divis√£o adaptativa
    metrics.record_adaptive_split()
    
    meio = len(texto_chunk) // 2
    
    # Procura quebra ideal (\n\n) num raio de 20% do meio
    margem = int(len(texto_chunk) * 0.2)
    inicio_busca = max(0, meio - margem)
    fim_busca = min(len(texto_chunk), meio + margem)
    
    janela_busca = texto_chunk[inicio_busca:fim_busca]
    pos_relativa = janela_busca.find('\n\n')
    
    if pos_relativa != -1:
        ponto_corte = inicio_busca + pos_relativa + 2 # +2 para incluir os \n\n no primeiro bloco ou pular? Vamos cortar DEPOIS dos \n\n
    else:
        # Tenta quebra simples \n
        pos_relativa_n = janela_busca.find('\n')
        if pos_relativa_n != -1:
            ponto_corte = inicio_busca + pos_relativa_n + 1
        else:
            # Corte seco no espa√ßo mais pr√≥ximo
            pos_relativa_espaco = janela_busca.find(' ')
            if pos_relativa_espaco != -1:
                ponto_corte = inicio_busca + pos_relativa_espaco + 1
            else:
                ponto_corte = meio # Corte arbitr√°rio
    
    parte_a = texto_chunk[:ponto_corte]
    parte_b = texto_chunk[ponto_corte:]
    
    logger.info(f"   Splitting chunk {numero}: Part A ({len(parte_a)} chars) + Part B ({len(parte_b)} chars)")
    
    # Processa Parte A
    resultado_a = processar_chunk(
        client, cache, system_prompt, parte_a, f"{numero}.A", total, 
        contexto_estilo, estrutura_global, ultimo_titulo, profundidade + 1
    )
    
    # Usa o final de A como contexto para B? Talvez seja excessivo e caro.
    # Vamos manter o contexto original para B por seguran√ßa, 
    # ou usar resultado_a[-1000:] como novo contexto_estilo.
    # Usar resultado_a √© melhor para continuidade.
    
    novo_contexto = resultado_a[-2000:] if len(resultado_a) > 2000 else resultado_a
    
    # Processa Parte B
    resultado_b = processar_chunk(
        client, cache, system_prompt, parte_b, f"{numero}.B", total, 
        novo_contexto, estrutura_global, None, profundidade + 1 # ultimo_titulo None pois A j√° tratou disso
    )
    
    return f"{resultado_a}\n\n{resultado_b}"

def extrair_titulos_h2(texto):
    """Extrai todos os t√≠tulos de n√≠vel 2 (##) do texto"""
    titulos = []
    for linha in texto.split('\n'):
        if linha.strip().startswith('##') and not linha.strip().startswith('###'):
            titulo_limpo = re.sub(r'^##\s*\d+\.\s*', '', linha.strip())
            titulos.append(titulo_limpo.lower())
    return titulos

# =============================================================================
# SMART STITCHING (v2.10) - Anti-Duplica√ß√£o Cir√∫rgica
# =============================================================================

def remover_eco_do_contexto(resposta_api, contexto_enviado):
    """
    Remove o in√≠cio da resposta se for apenas um 'eco' do final do contexto.
    """
    if not contexto_enviado or not resposta_api:
        return resposta_api

    final_contexto = contexto_enviado.strip()[-300:]
    inicio_resposta = resposta_api.strip()[:300]

    matcher = SequenceMatcher(None, final_contexto, inicio_resposta)
    match = matcher.find_longest_match(0, len(final_contexto), 0, len(inicio_resposta))

    if match.size > 50:
        logger.info(f"‚úÇÔ∏è Eco detectado! Removendo {match.size} chars repetidos no in√≠cio.")
        return resposta_api.strip()[match.size:].strip()
    
    return resposta_api

def titulos_sao_similares(t1, t2, threshold=None):
    """Verifica se dois t√≠tulos s√£o semanticamente iguais (fuzzy matching).
    Usa LIMIAR_SECOES global se threshold n√£o for especificado.
    """
    if threshold is None:
        threshold = LIMIAR_SECOES  # Usa limiar de se√ß√µes (0.70 Fidelidade / 0.60 Apostila)
        
    def normalizar(t):
        # Remove apenas caracteres n√£o alfanum√©ricos, mas MANT√âM tamanho relativo
        return re.sub(r'[^a-z0-9 ]', '', t.lower())
    
    nt1 = normalizar(t1)
    nt2 = normalizar(t2)
    
    if not nt1 or not nt2:
        return False
    
    # PROTE√á√ÉO 1: Se um t√≠tulo for muito maior que o outro, n√£o s√£o duplicatas
    nt1_compact = nt1.replace(' ', '')
    nt2_compact = nt2.replace(' ', '')
    len_ratio = min(len(nt1_compact), len(nt2_compact)) / max(len(nt1_compact), len(nt2_compact))
    if len_ratio < 0.8:  # Se a diferen√ßa de tamanho for > 20%, assume que s√£o diferentes
        return False
    
    # PROTE√á√ÉO 2: Verifica√ß√£o por palavras - se houver palavras exclusivas significativas
    palavras1 = set(nt1.split())
    palavras2 = set(nt2.split())
    diferenca = palavras1.symmetric_difference(palavras2)
    
    # Se as palavras diferentes forem longas (n√£o apenas 'e', 'do', 'da'), assume diferen√ßa real
    if any(len(w) > 3 for w in diferenca):
        return False
        
    return SequenceMatcher(None, nt1_compact, nt2_compact).ratio() > threshold

def limpar_inicio_redundante(texto_novo, texto_acumulado):
    """
    Remove t√≠tulo no in√≠cio do novo chunk se similar ao √∫ltimo t√≠tulo do texto acumulado.
    """
    if not texto_acumulado.strip():
        return texto_novo

    ultimas_linhas = texto_acumulado.strip().split('\n')[-30:]
    ultimo_titulo = None
    for linha in reversed(ultimas_linhas):
        if linha.strip().startswith('##'):
            ultimo_titulo = re.sub(r'^#+\s*(?:\d+(?:\.\d+)*\.?)?\s*', '', linha).strip()
            break
    
    if not ultimo_titulo:
        return texto_novo

    linhas_novas = texto_novo.strip().split('\n')
    
    for i, linha in enumerate(linhas_novas[:10]):  # v2.11: Busca mais profunda (era 5)
        if linha.strip().startswith('##'):
            novo_titulo = re.sub(r'^#+\s*(?:\d+(?:\.\d+)*\.?)?\s*', '', linha).strip()
            
            if titulos_sao_similares(ultimo_titulo, novo_titulo):
                logger.info(f"‚úÇÔ∏è T√≠tulo duplicado na jun√ß√£o: '{novo_titulo}' ‚âà '{ultimo_titulo}'")
                return '\n'.join(linhas_novas[i+1:])
    
    return texto_novo

def detectar_secoes_duplicadas(texto):
    """v2.15: Detecta se√ß√µes duplicadas por t√≠tulos em ## e ### (Fuzzy Matching)"""
    logger.info("üîç Detectando se√ß√µes duplicadas (fuzzy, H2+H3)...")
    
    linhas = texto.split('\n')
    titulos_vistos = []  # (titulo_normalizado, linha_idx)
    secoes_duplicadas = []
    
    for i, linha in enumerate(linhas):
        linha_strip = linha.strip()
        # Match both ## (H2) and ### (H3)
        if linha_strip.startswith('##'):
            # Extract level
            match_nivel = re.match(r'^(#+)', linha_strip)
            nivel = len(match_nivel.group(1)) if match_nivel else 2
            
            # Normalize title: remove ## prefix, numbers, emojis, and "(Continua√ß√£o)"
            titulo_normalizado = re.sub(r'^#{2,4}\s*\d+(?:\.\d+)*\.?\s*', '', linha_strip)
            titulo_normalizado = re.sub(r'[üìãüìäüóÇÔ∏è]', '', titulo_normalizado).strip()
            titulo_normalizado = re.sub(r'\s*\(Continua√ß√£o\)\s*$', '', titulo_normalizado, flags=re.IGNORECASE).strip()
            
            duplicado = False
            for t_visto, linha_visto in titulos_vistos:
                if titulos_sao_similares(titulo_normalizado, t_visto):
                    logger.warning(f"‚ö†Ô∏è  Duplicado (fuzzy): '{linha_strip}' ‚âà '{t_visto}'")
                    secoes_duplicadas.append({
                        'titulo': titulo_normalizado,
                        'primeira_linha': linha_visto,
                        'duplicada_linha': i
                    })
                    duplicado = True
                    break
            
            if not duplicado:
                titulos_vistos.append((titulo_normalizado, i))
    
    if secoes_duplicadas:
        logger.error(f"‚ùå {len(secoes_duplicadas)} se√ß√µes duplicadas detectadas!")
    else:
        logger.info("‚úÖ Nenhuma se√ß√£o duplicada detectada")
    
    return secoes_duplicadas

def remover_secoes_duplicadas(texto):
    """v2.14: Remove se√ß√µes duplicadas com COMPARA√á√ÉO JANELADA (Fix Dilui√ß√£o)"""
    from difflib import SequenceMatcher
    
    secoes_dup = detectar_secoes_duplicadas(texto)
    if not secoes_dup: return texto
    
    print("üßπ Removendo se√ß√µes duplicadas (Smart Dedupe v2.14)...")
    linhas = texto.split('\n')
    
    # Rastreia o √öLTIMO segmento adicionado para cada t√≠tulo (para evitar dilui√ß√£o na compara√ß√£o)
    ultimo_segmento_visto = {}  # titulo_normalizado -> √∫ltimo texto adicionado
    linhas_para_remover = set()
    linhas_para_adicionar_separador = set()
    
    for dup in secoes_dup:
        # --- 1. Extrair Conte√∫do Original ---
        idx_orig = dup['primeira_linha']
        header_orig = linhas[idx_orig].strip()
        match_orig = re.match(r'^(#+)', header_orig)
        nivel_orig = len(match_orig.group(1)) if match_orig else 2
        
        # Extrai conte√∫do da se√ß√£o original
        content_orig = []
        for i in range(idx_orig + 1, len(linhas)):
            line = linhas[i].strip()
            if line.startswith('#'):
                match_now = re.match(r'^(#+)', line)
                if match_now and len(match_now.group(1)) <= nivel_orig: break
            content_orig.append(linhas[i])
        text_orig = "\n".join(content_orig)
        
        titulo_key = re.sub(r'^#{2,4}\s*\d+(?:\.\d+)*\.?\s*', '', header_orig)
        titulo_key = re.sub(r'\s*\(Continua√ß√£o\)\s*$', '', titulo_key, flags=re.IGNORECASE).strip()
        
        # Inicializa o rastreamento se for a primeira vez
        if titulo_key not in ultimo_segmento_visto:
            ultimo_segmento_visto[titulo_key] = text_orig

        # --- 2. Extrair Conte√∫do Duplicado ---
        idx_dup = dup['duplicada_linha']
        header_dup = linhas[idx_dup].strip()
        match_dup = re.match(r'^(#+)', header_dup)
        nivel_dup = len(match_dup.group(1)) if match_dup else 2
        
        fim_dup_idx = len(linhas)
        content_dup = []
        for i in range(idx_dup + 1, len(linhas)):
            line = linhas[i].strip()
            if line.startswith('#'):
                match_now = re.match(r'^(#+)', line)
                if match_now and len(match_now.group(1)) <= nivel_dup:
                    fim_dup_idx = i
                    break
            content_dup.append(linhas[i])
        text_dup = "\n".join(content_dup)
        
        # --- 3. Comparar Conte√∫do (L√≥gica Janelada v2.15) ---
        # Compara APENAS com o √∫ltimo segmento conhecido dessa se√ß√£o
        texto_referencia = ultimo_segmento_visto.get(titulo_key, text_orig)
        
        len_dup = len(text_dup.strip())
        len_ref = len(texto_referencia.strip())
        
        # L√≥gica de decis√£o baseada em tamanho
        if len_dup < 50:
            # Duplicado curto demais = lixo, deletar
            sim = 1.0
            print(f"   ‚ö†Ô∏è  Se√ß√£o duplicada muito curta ({len_dup} chars) - marcando para remo√ß√£o")
        elif len_ref < 50 and len_dup >= 200:
            # Original curto, mas duplicado substancial = original estava incompleto
            # Manter o duplicado como novo conte√∫do
            sim = 0.0
            print(f"   ‚ÑπÔ∏è  Original curto ({len_ref}c), duplicado substancial ({len_dup}c) - mantendo novo conte√∫do")
        else:
            sim = SequenceMatcher(None, texto_referencia, text_dup).ratio()
            
        print(f"   Similaridade: {sim:.1%} | Linha {idx_dup} | '{titulo_key[:40]}...'")
        
        if sim > LIMIAR_SECOES:  # Usa limiar de se√ß√µes (0.70 Fidelidade / 0.60 Apostila) 
            print(f"   üóëÔ∏è  Removendo SE√á√ÉO INTEIRA (Duplicata confirmada)")
            for i in range(idx_dup, fim_dup_idx):
                linhas_para_remover.add(i)
        else:
            print(f"   üîó Mesclando conte√∫do (Nova informa√ß√£o detectada)")
            linhas_para_remover.add(idx_dup)
            if idx_dup + 1 < len(linhas):
                linhas_para_adicionar_separador.add(idx_dup + 1)
            
            # ATUALIZA o √∫ltimo segmento visto para a pr√≥xima compara√ß√£o
            ultimo_segmento_visto[titulo_key] = text_dup

    # --- 4. Reconstru√ß√£o ---
    linhas_limpas = []
    for i, linha in enumerate(linhas):
        if i in linhas_para_remover:
            continue
        if i in linhas_para_adicionar_separador:
            linhas_limpas.append("") 
        linhas_limpas.append(linha)
        
    print(f"‚úÖ {len(linhas_para_remover)} linhas removidas")
    return '\n'.join(linhas_limpas)

def remover_duplicacoes_literais(texto):
    """Remove par√°grafos individuais duplicados"""
    paragrafos = texto.split('\n\n')
    paragrafos_limpos = []
    dup_count = 0
    
    for i, para in enumerate(paragrafos):
        if i == 0:
            paragrafos_limpos.append(para)
            continue
        
        if len(para.strip()) < 80 or para.strip().startswith('#'):
            paragrafos_limpos.append(para)
            continue
        
        is_duplicate = False
        para_norm = ' '.join(para.split()).lower()
        
        for j in range(max(0, len(paragrafos_limpos) - 3), len(paragrafos_limpos)):
            para_ant = paragrafos_limpos[j]
            para_ant_norm = ' '.join(para_ant.split()).lower()
            
            ratio = SequenceMatcher(None, para_norm, para_ant_norm).ratio()
            
            if ratio > 0.95:
                is_duplicate = True
                dup_count += 1
                break
        
        if not is_duplicate:
            paragrafos_limpos.append(para)
    
    if dup_count > 5:
        logger.warning(f"‚ö†Ô∏è  {dup_count} par√°grafos duplicados removidos")
    
    return '\n\n'.join(paragrafos_limpos)

# =============================================================================
# V2.17: DEDUPLICA√á√ÉO ROBUSTA (7-DIFF Strategy) - Portado de mlx_vomo.py
# =============================================================================

def remover_overlap_duplicado(resultados):
    """
    v2.17: Remove duplica√ß√£o entre chunks usando detec√ß√£o ROBUSTA de conte√∫do.
    Estrat√©gia 7-DIFF: Compara t√≠tulo + conte√∫do com janela deslizante de 20 se√ß√µes.
    """
    if isinstance(resultados, str):
        resultados = [resultados]
    if len(resultados) <= 1:
        return resultados[0] if resultados else ""
    
    from difflib import SequenceMatcher
    
    def normalize_text(text):
        if not text: return ""
        text = re.sub(r'[#*-]', ' ', text)
        text = re.sub(r'[^\w\s]', '', text)
        return ' '.join(text.lower().split())

    def calculate_similarity(text1, text2):
        if not text1 or not text2:
            return 0.0
        if len(text1) < 50:
            return 1.0 if text1 in text2 or text2 in text1 else 0.0
        return SequenceMatcher(None, text1, text2).quick_ratio()

    def extract_unique_paragraphs(sec_curr_content, sec_prev_content):
        if not sec_curr_content: return []
        unique = []
        paras_curr = sec_curr_content.split('\n\n')
        paras_prev_norm = [normalize_text(p) for p in sec_prev_content.split('\n\n')]
        
        for p in paras_curr:
            p_clean = p.strip()
            if not p_clean or len(p_clean) < 20: continue
            
            p_norm = normalize_text(p_clean)
            is_present = False
            for pp_norm in paras_prev_norm:
                if calculate_similarity(p_norm, pp_norm) > 0.85:
                    is_present = True
                    break
            
            if not is_present:
                unique.append(p_clean)
        return unique

    logger.info("üßπ Iniciando deduplica√ß√£o robusta (7-DIFF Strategy)...")

    # 1. Junta e Parseia
    texto_bruto = '\n\n'.join(resultados)
    lines = texto_bruto.split('\n')
    
    sections = []
    current_section = None
    header_pattern = re.compile(r'^(#{1,6})\s+(.+)$')
    
    captured_lines = []
    intro_lines = []
    has_started = False
    
    for line in lines:
        match = header_pattern.match(line)
        if match:
            has_started = True
            if current_section:
                current_section['content'] = '\n'.join(captured_lines).strip()
                sections.append(current_section)
                captured_lines = []
            
            title_text = match.group(2).strip()
            title_clean = re.sub(r'^\d+(?:\.\d+)*\.?\s*', '', title_text)
            
            current_section = {
                'title_clean': title_clean,
                'level': len(match.group(1)),
                'full_header': line,
                'content': ""
            }
        else:
            if has_started:
                captured_lines.append(line)
            else:
                intro_lines.append(line)
    
    if current_section:
        current_section['content'] = '\n'.join(captured_lines).strip()
        sections.append(current_section)
        
    logger.info(f"   üìä Analisando {len(sections)} se√ß√µes...")

    # 2. Detec√ß√£o e Remo√ß√£o
    indices_to_remove = set()
    MAX_WINDOW = 20
    
    for i in range(len(sections)):
        if i in indices_to_remove: continue
        sec_curr = sections[i]
        
        start_check = max(0, i - MAX_WINDOW)
        for j in range(start_check, i):
            if j in indices_to_remove: continue
            sec_prev = sections[j]
            
            if sec_curr['level'] != sec_prev['level']: continue
            
            sim_title = calculate_similarity(normalize_text(sec_curr['title_clean']), normalize_text(sec_prev['title_clean']))
            sim_content = calculate_similarity(normalize_text(sec_curr['content']), normalize_text(sec_prev['content']))
            
            is_duplicate = False
            
            if sim_title > 0.9 and sim_content > 0.6:
                is_duplicate = True
            elif sim_content > LIMIAR_7DIFF:  # Usa limiar 7-DIFF (0.85 Fidelidade / 0.80 Apostila)
                 is_duplicate = True
            elif sim_title > 0.95 and len(sec_curr['content']) < 100:
                 is_duplicate = True
            
            if is_duplicate:
                logger.info(f"   üóëÔ∏è  Duplicata detectada: '{sec_curr['title_clean'][:40]}...'")
                
                unique_paras = extract_unique_paragraphs(sec_curr['content'], sec_prev['content'])
                if unique_paras:
                    sections[j]['content'] += '\n\n' + '\n\n'.join(unique_paras)
                
                indices_to_remove.add(i)
                break
    
    # 3. Reconstru√ß√£o
    final_lines = list(intro_lines)
    for i, sec in enumerate(sections):
        if i in indices_to_remove: continue
        final_lines.append(sec['full_header'])
        if sec['content']:
            final_lines.append(sec['content'])
        final_lines.append("")
    
    logger.info(f"   ‚úÖ {len(indices_to_remove)} se√ß√µes duplicadas removidas/mescladas")
    return '\n'.join(final_lines)

def deterministic_structure_fix(text):
    """
    v2.17: Reorganiza√ß√£o Estrutural Determin√≠stica.
    Detecta se usa H1 ou H2 como n√≠vel principal e reorganiza o documento.
    """
    logger.info("üß© Executando Reorganiza√ß√£o Estrutural Determin√≠stica...")
    
    lines = text.split('\n')
    
    # Detec√ß√£o de Hierarquia
    has_h1 = any(re.match(r'^#\s+', line) for line in lines)
    header_level_regex = r'^#\s+' if has_h1 else r'^##\s+'
    logger.info(f"   ‚ÑπÔ∏è  N√≠vel principal detectado: {'H1 (#)' if has_h1 else 'H2 (##)'}")

    content_map = {
        "PREAMBULO": [],
        "DISCIPLINAS": {}, 
        "ENCERRAMENTO": []
    }
    
    current_area = "PREAMBULO"
    current_block = []
    disciplinas_order = [] 
    
    re_disciplina = re.compile(rf'{header_level_regex}(?!Quest√£o|Q\.)([^0-9\.]+.*)', re.IGNORECASE)
    re_encerramento = re.compile(rf'{header_level_regex}(?:ENCERRAMENTO|CONSIDERA√á√ïES|CONCLUS√ÉO)', re.IGNORECASE)
    
    def flush_block(area, block_lines):
        if not block_lines: return
        block_text = '\n'.join(block_lines)
        
        if area == "PREAMBULO":
            content_map["PREAMBULO"].append(block_text)
        elif area == "ENCERRAMENTO":
            content_map["ENCERRAMENTO"].append(block_text)
        else:
            if area not in content_map["DISCIPLINAS"]:
                content_map["DISCIPLINAS"][area] = []
                disciplinas_order.append(area)
            content_map["DISCIPLINAS"][area].append(block_text)

    for line in lines:
        match_disc = re_disciplina.match(line)
        if match_disc:
            flush_block(current_area, current_block)
            current_block = []
            
            raw_area = match_disc.group(1).strip().upper()
            
            if "DIREITO" not in raw_area and len(raw_area) < 50:
                 if any(x in raw_area for x in ["CIVIL", "PENAL", "TRABALHO", "ADMINISTRATIVO", "CONSTITUCIONAL"]):
                     current_area = f"DIREITO {raw_area}"
                 else:
                     current_area = raw_area
            else:
                 current_area = raw_area
            continue 
            
        if re_encerramento.match(line):
            flush_block(current_area, current_block)
            current_block = []
            current_area = "ENCERRAMENTO"
            continue

        current_block.append(line)
        
    flush_block(current_area, current_block)
    
    # Reconstru√ß√£o
    final_output = []
    
    if content_map["PREAMBULO"]:
        final_output.append("# ORIENTA√á√ïES GERAIS / INTRODU√á√ÉO")
        final_output.extend(content_map["PREAMBULO"])
        final_output.append("")

    for area in disciplinas_order:
        area_clean = area.replace("#", "").strip()
        final_output.append(f"# {area_clean}")
        for block in content_map["DISCIPLINAS"][area]:
            final_output.append(block)
        final_output.append("")
        
    if content_map["ENCERRAMENTO"]:
        final_output.append("# CONSIDERA√á√ïES FINAIS")
        final_output.extend(content_map["ENCERRAMENTO"])
        
    num_identified = len(disciplinas_order)
    logger.info(f"   ‚úÖ Reorganizado: {num_identified} se√ß√µes principais identificadas.")
    
    if num_identified == 0 and len(content_map["PREAMBULO"]) > 0:
        logger.warning("   ‚ö†Ô∏è Nenhuma estrutura detectada. Mantendo original.")
        return text
        
    return '\n'.join(final_output)

def normalize_headings(texto):
    """
    v1.0: Normaliza t√≠tulos semanticamente similares para uma vers√£o √∫nica.
    - Agrupa t√≠tulos por similaridade
    - Escolhe o t√≠tulo mais descritivo de cada grupo
    - Remove sufixos como "(Continua√ß√£o)"
    """
    from difflib import SequenceMatcher
    
    print("üî§ Normalizando t√≠tulos similares...")
    linhas = texto.split('\n')
    
    # 1. Extrair todos os t√≠tulos com info de n√≠vel e posi√ß√£o
    titulos = []
    for i, linha in enumerate(linhas):
        stripped = linha.strip()
        if stripped.startswith('##'):
            match = re.match(r'^(#+)\s*', stripped)
            nivel = len(match.group(1)) if match else 2
            # Extrai t√≠tulo limpo (sem # e sem numera√ß√£o)
            titulo_limpo = re.sub(r'^#{2,4}\s*\d+(?:\.\d+)*\.?\s*', '', stripped).strip()
            # Remove "(Continua√ß√£o)" para compara√ß√£o
            titulo_base = re.sub(r'\s*\(Continua√ß√£o\)\s*$', '', titulo_limpo, flags=re.IGNORECASE).strip()
            titulos.append({
                'linha': i,
                'nivel': nivel,
                'original': stripped,
                'limpo': titulo_limpo,
                'base': titulo_base
            })
    
    if not titulos:
        return texto
    
    # 2. Agrupar t√≠tulos similares (mesmo n√≠vel + similaridade > LIMIAR)
    grupos = []
    usados = set()
    
    for i, t1 in enumerate(titulos):
        if i in usados:
            continue
        grupo = [t1]
        usados.add(i)
        
        for j, t2 in enumerate(titulos):
            if j in usados or j <= i:
                continue
            if t1['nivel'] == t2['nivel']:
                sim = SequenceMatcher(None, t1['base'].lower(), t2['base'].lower()).ratio()
                if sim > LIMIAR_SECOES:
                    grupo.append(t2)
                    usados.add(j)
        
        if len(grupo) > 1:
            grupos.append(grupo)
    
    if not grupos:
        # Nada para normalizar, mas ainda remove "(Continua√ß√£o)"
        texto_limpo = re.sub(r'\s*\(Continua√ß√£o\)\s*(?=\n|$)', '', texto, flags=re.IGNORECASE)
        return texto_limpo
    
    # 3. Para cada grupo, escolher o "melhor" t√≠tulo (mais curto entre os mais longos)
    #    L√≥gica: preferir t√≠tulos sem "(Continua√ß√£o)" e com descri√ß√£o completa
    substituicoes = {}
    for grupo in grupos:
        # Ordenar por: n√£o ter "(Continua√ß√£o)" primeiro, depois por comprimento (prefer m√©dio)
        candidatos = sorted(grupo, key=lambda x: (
            '(Continua√ß√£o)' in x['limpo'],  # False (0) vem antes de True (1)
            abs(len(x['limpo']) - 40)  # Preferir t√≠tulos de ~40 chars (nem muito curto nem muito longo)
        ))
        
        melhor = candidatos[0]['limpo']
        print(f"   üìù Grupo de {len(grupo)} t√≠tulos similares ‚Üí padronizando para: '{melhor[:50]}...'")
        
        for t in grupo:
            if t['limpo'] != melhor:
                substituicoes[t['linha']] = (t['nivel'], melhor)
    
    # 4. Aplicar substitui√ß√µes
    novas_linhas = []
    for i, linha in enumerate(linhas):
        if i in substituicoes:
            nivel, novo_titulo = substituicoes[i]
            # Preservar numera√ß√£o existente se houver
            match_num = re.match(r'^(#+\s*\d+(?:\.\d+)*\.?\s*)', linhas[i].strip())
            if match_num:
                prefixo = match_num.group(1)
                # Remove a numera√ß√£o do novo t√≠tulo se ele tiver uma
                novo_titulo = re.sub(r'^\d+(?:\.\d+)*\.?\s*', '', novo_titulo)
                novas_linhas.append(f"{prefixo}{novo_titulo}")
            else:
                novas_linhas.append(f"{'#' * nivel} {novo_titulo}")
        else:
            # Remove "(Continua√ß√£o)" de qualquer t√≠tulo restante
            if linha.strip().startswith('##'):
                linha = re.sub(r'\s*\(Continua√ß√£o\)\s*(?=\n|$)', '', linha, flags=re.IGNORECASE)
            novas_linhas.append(linha)
    
    print(f"   ‚úÖ {len(substituicoes)} t√≠tulos normalizados, '(Continua√ß√£o)' removidos")
    return '\n'.join(novas_linhas)

# =============================================================================
# REVIS√ÉO SEM√ÇNTICA DE ESTRUTURA POR IA (v2.16)
# =============================================================================

# =============================================================================
# REVIS√ÉO SEM√ÇNTICA, MAPEAMENTO E ESTRUTURA (v2.16)
# =============================================================================

PROMPT_MAPEAMENTO = """Voc√™ √© um especialista em organiza√ß√£o de conte√∫do educacional acad√™mico (v2.17).

## ETAPA 1: IDENTIFICAR O TIPO DE CONTE√öDO
Analise a transcri√ß√£o e determine qual √© a **natureza predominante** do material:

| Tipo | Pistas no Texto | Estrutura Ideal |
|------|-----------------|-----------------|
| **SIMULADO** | "quest√£o 1", "quest√£o 2", "espelho de corre√ß√£o", "corre√ß√£o do simulado", "vamos corrigir" | Organizar por QUEST√ïES numeradas |
| **AULA EXPOSITIVA** | explica√ß√µes cont√≠nuas de um tema, teoria, doutrina, sem quest√µes espec√≠ficas | Organizar por TEMAS/MAT√âRIAS e MARCOS LEGAIS |
| **REVIS√ÉO** | "revis√£o", "resumo", m√∫ltiplos temas curtos, "pontos importantes" | Organizar por T√ìPICOS de revis√£o |
| **CORRE√á√ÉO DE PROVA** | "gabarito", "alternativa correta", "item certo/errado" | Organizar por QUEST√ïES com gabarito |

## ETAPA 2: EXTRAIR A ESTRUTURA

### Se for SIMULADO ou CORRE√á√ÉO DE PROVA:
```
1. Orienta√ß√µes Gerais / Introdu√ß√£o
2. Quest√£o 1: [T√≠tulo descritivo] ‚Äî [√Årea do Direito]
   2.1. Enunciado e Contexto
   2.2. Fundamenta√ß√£o (Doutrina/Jurisprud√™ncia)
   2.3. Pontos do Espelho / Resposta
3. Quest√£o 2: [T√≠tulo descritivo] ‚Äî [√Årea do Direito]
   3.1. ...
[Continue para cada quest√£o]
N. Considera√ß√µes Finais / D√∫vidas
```

### Se for AULA EXPOSITIVA (ATEN√á√ÉO AOS MARCOS LEGAIS):
Voc√™ DEVE identificar **Marcos Legais e Jurisprudenciais** importantes e elev√°-los √† categoria de SUBT√ìPICOS.
Exemplos de marcos: "S√∫mula X", "Artigo Y do CC", "Tese de Repercuss√£o Geral Z", "Julgado X do STF".

```
1. Introdu√ß√£o
2. [Mat√©ria 1: ex. Direito Administrativo]
   2.1. [Subtema]
      2.1.1. [Detalhamento]
3. [Mat√©ria 2: ex. Direito Civil]
   3.1. ...
```

### Se for REVIS√ÉO:
```
1. [Tema 1]
   1.1. Pontos-chave
   1.2. Jurisprud√™ncia/S√∫mulas
2. [Tema 2]
   2.1. ...
```

## REGRAS GERAIS:
1. **M√ÅXIMO 3 N√çVEIS** de hierarquia (1., 1.1., 1.1.1.)
2. **Seja descritivo** nos t√≠tulos ‚Äî inclua o assunto real, n√£o apenas "Quest√£o 1"
3. **Mantenha a ORDEM** cronol√≥gica da transcri√ß√£o
4. **Mapeie do IN√çCIO ao FIM** ‚Äî n√£o omita partes
5. **Identifique a √ÅREA DO DIREITO** de cada bloco quando poss√≠vel
6. **PREFIRA SUBT√ìPICOS (1.1.) a novos t√≥picos (2.)**: Abra novo t√≥pico de n√≠vel 1 SOMENTE quando o macroassunto mudar de verdade (ex.: de Direito Administrativo para Direito Civil). Aspectos, institutos e marcos legais DENTRO do mesmo macroassunto devem ser subt√≥picos (1.1., 1.2., etc.), NUNCA t√≥picos de n√≠vel 1 separados.
7. **ANTI-FRAGMENTA√á√ÉO**: Se o professor trata 4+ aspectos de um tema, todos devem ser subt√≥picos de um √∫nico tema-m√£e. Exemplo correto: `2. Execu√ß√£o Fiscal` com `2.1. Procedimento`, `2.2. Cita√ß√£o`, `2.3. Exce√ß√£o de Pr√©-Executividade`. Exemplo ERRADO: `2. Execu√ß√£o Fiscal`, `3. Procedimento`, `4. Cita√ß√£o`.
8. **T√çTULOS S√ÉO R√ìTULOS, N√ÉO FALAS**: Os t√≠tulos devem ser r√≥tulos descritivos curtos (m√°x 8 palavras), NUNCA trechos literais da fala do professor.
   - ERRADO: "1. J√° est√°vamos conversando aqui antes de come√ßar a transmiss√£o"
   - CORRETO: "1. Introdu√ß√£o e Apresenta√ß√£o"
   - ERRADO: "2.1. Bom dia pessoal vamos come√ßar a aula de hoje sobre licita√ß√µes"
   - CORRETO: "2.1. Abertura ‚Äî Licita√ß√µes e Contratos"
9. **SAUDA√á√ïES E LOG√çSTICA ‚Üí "Introdu√ß√£o"**: Trechos de boas-vindas, ajustes t√©cnicos, apresenta√ß√£o pessoal ou log√≠stica devem ser agrupados sob "1. Introdu√ß√£o" ou "1. Apresenta√ß√£o e Contextualiza√ß√£o", nunca com a fala literal como t√≠tulo.

## TRANSCRI√á√ÉO:
{transcricao}

## RESPOSTA:
Primeiro, indique em uma linha: `[TIPO: SIMULADO/EXPOSITIVA/REVIS√ÉO/CORRE√á√ÉO]`
Depois, retorne APENAS a estrutura hier√°rquica (m√°x 3 n√≠veis)."""

PROMPT_STRUCTURE_REVIEW = """Voc√™ √© um revisor especializado em estrutura de documentos jur√≠dicos educacionais.

## ESTRUTURA DE MAPEAMENTO INICIAL (Refer√™ncia - se dispon√≠vel):
{estrutura_mapeada}

## TAREFA
Revise a ESTRUTURA (headers/t√≠tulos) do documento abaixo, COMPARANDO com o mapeamento acima (se dispon√≠vel). Sua miss√£o √© harmonizar o mapeamento planejado com o CONTE√öDO REAL da aula.

## ‚úÖ O QUE VOC√ä DEVE FAZER:

### 1. COMPARAR E REFINAR T√çTULOS (CR√çTICO)
Verifique se los t√≠tulos refletem os t√≥picos do mapeamento. Se um t√≠tulo for gen√©rico, torne-o descritivo.
- ERRADO: "### Quest√£o" ou "### T√≥pico"
- CORRETO: "### Quest√£o 1: Responsabilidade Civil" (Descritivo conforme mapa/conte√∫do)
- **CR√çTICO:** Evite t√≠tulos id√™nticos em se√ß√µes "irm√£s". Diferencie-os pelo conte√∫do espec√≠fico de cada uma.

### 2. VALIDAR HIERARQUIA E PROMO√á√ÉO DE T√ìPICOS
Confirme se a estrutura segue l√≥gica consistente (##, ###, ####). 
- **PROMO√á√ÉO:** Se um sub-subt√≥pico (ex: 9.19.5) for extenso e tratar de um tema central (ex: Execu√ß√£o Fiscal), PROMOVA-O a um n√≠vel superior (ex: ### 9.20) para evitar fragmenta√ß√£o excessiva e respeitar o limite de n√≠veis.

### 3. RENUMERA√á√ÉO SEQUENCIAL OBRIGAT√ìRIA
Se voc√™ criar, deletar ou promover uma se√ß√£o, voc√™ DEVE renumerar TODAS as se√ß√µes subsequentes daquela mesma hierarquia para manter a sequ√™ncia l√≥gica (ex: se 9.20 foi criado, o antigo 9.20 vira 9.21, e assim por diante).

### 4. MESCLAR QUEST√ïES DUPLICADAS
Se duas se√ß√µes t√™m o mesmo n√∫mero de quest√£o na mesma √°rea, MESCLE-AS.
- ERRADO: "2.1. Quest√£o 1: TAC" + "2.2. Quest√£o 1: TAC" 
- CORRETO: "2.1. Quest√£o 1: TAC" (√önica, com todo o conte√∫do unificado)

### 5. PRIORIDADE DO CONTE√öDO REAL (DECIDIR ESTRUTURA)
Se houver conflito entre mapeamento e documento, escolha a estrutura que melhor reflete o CONTE√öDO REAL. O mapeamento √© apenas um guia.

### 6. LIMPEZA T√âCNICA (SINTAXE MARKDOWN)
Corrija problemas detalhados de formata√ß√£o:
- **Tabelas:** Alinhar colunas e adicionar separadores faltantes.
- **Listas:** Corrigir bullets ou numera√ß√£o mal formatada.
- **Espa√ßamento:** Padronizar linhas em branco entre se√ß√µes (m√≠nimo uma linha).
- **Headers Vazios:** Remover t√≠tulos sem conte√∫do abaixo.
- **Metadados:** Remover headers ou tags inline como "[TIPO: ...]", "**[TIPO: ...]**", ou "[BLOCO 0X]".

## ‚ùå O QUE VOC√ä N√ÉO DEVE FAZER:
1. **N√ÉO ALTERE O CONTE√öDO** dos par√°grafos - apenas os t√≠tulos e a organiza√ß√£o.
2. **NUNCA RESUMA** ou encurte o texto (mesmo tamanho de entrada/sa√≠da √© obrigat√≥rio).
3. **N√ÉO INVENTE** fatos jur√≠dicos.
4. **N√ÉO REMOVA** trechos t√©cnicos ou exemplos.

## REGRAS CR√çTICAS DE HIERARQUIA:
- Use **M√ÅXIMO 3** n√≠veis de hierarquia (##, ###, ####).
- Nunca use # (H1) para subt√≥picos (apenas para o t√≠tulo principal do documento).
- Preserve a ordem cronol√≥gica geral.

## DOCUMENTO PARA REVISAR:
{documento}

## üìù RELAT√ìRIO ESPERADO:
Ao final do documento, inclua um bloco de coment√°rio indicando:
- Quantos t√≠tulos foram refinados
- Promo√ß√µes de se√ß√µes e renumera√ß√µes realizadas
- Discrep√¢ncias com o mapeamento (se houver)

Formato:
<!-- RELAT√ìRIO: X t√≠tulos refinados | Y se√ß√µes promovidas/renumeradas | Discrep√¢ncias: [Nenhuma/Lista] -->

## RESPOSTA:
Retorne o documento COMPLETO E INTEGRAL (mesmo tamanho do original) com os t√≠tulos/headers corrigidos e o relat√≥rio no final. N√ÉO RESUMA."""


def map_structure(client, full_text):
    """Creates a global structure skeleton to guide the formatting."""
    logger.info("üó∫Ô∏è  Mapeando estrutura global do documento...")
    
    # Limit input to avoid context overflow (200k chars is plenty for structure)
    input_sample = full_text[:200000] 
    
    try:
        rate_limiter.wait_if_needed()
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=PROMPT_MAPEAMENTO.format(transcricao=input_sample),
            config=types.GenerateContentConfig(
                max_output_tokens=20000,
                thinking_config={"include_thoughts": False, "thinking_level": "HIGH"}
            )
        )
        content = response.text.replace('```markdown', '').replace('```', '')
        content = _sanitize_structure_titles(content)
        logger.info("   ‚úÖ Estrutura mapeada com sucesso.")
        return content

    except Exception as e:
        logger.warning(f"   ‚ö†Ô∏è  Falha no mapeamento via Gemini: {e}")
        return None

def ai_structure_review(client, texto, estrutura_mapeada=None):
    """
    v2.0: Revis√£o sem√¢ntica de estrutura usando IA com VALIDA√á√ÉO CRUZADA.
    Compara o documento com a estrutura de mapeamento inicial.
    Corrige: quest√µes duplicadas, subt√≥picos √≥rf√£os, fragmenta√ß√£o excessiva.
    """
    logger.info("üß† Revis√£o Sem√¢ntica de Estrutura (IA v2.0)...")
    
    # Gemini 3 Flash suporta 1M tokens (~4M chars) - usar at√© 500k chars
    max_doc_chars = 500000
    if len(texto) > max_doc_chars:
        logger.warning(f"   ‚ö†Ô∏è Documento muito longo ({len(texto)} chars), truncando para {max_doc_chars//1000}k...")
        texto_para_revisao = texto[:max_doc_chars] + "\n\n[... documento truncado para revis√£o estrutural ...]"
    else:
        texto_para_revisao = texto
    
    # Preparar estrutura mapeada (se dispon√≠vel)
    if estrutura_mapeada:
        estrutura_str = estrutura_mapeada[:50000]  # Limitar estrutura a 50k chars
        logger.info(f"   üìã Usando estrutura de mapeamento inicial ({len(estrutura_mapeada)} chars) para valida√ß√£o cruzada.")
    else:
        estrutura_str = "[Estrutura de mapeamento n√£o dispon√≠vel - analisar documento autonomamente]"
        logger.info("   ‚ÑπÔ∏è  Sem mapeamento inicial, IA revisar√° estrutura autonomamente.")
    
    try:
        rate_limiter.wait_if_needed()
        
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=PROMPT_STRUCTURE_REVIEW.format(
                estrutura_mapeada=estrutura_str,
                documento=texto_para_revisao
            ),
            config=types.GenerateContentConfig(
                max_output_tokens=65536,  # M√°ximo permitido
                thinking_config={"include_thoughts": False, "thinking_level": "HIGH"},
                safety_settings=[
                    types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                ]
            )
        )
        
        resultado = response.text.replace('```markdown', '').replace('```', '').strip()
        
        # Extrair e exibir relat√≥rio da IA (se presente)
        relatorio_match = re.search(r'<!--\s*RELAT√ìRIO:\s*(.+?)\s*-->', resultado, re.IGNORECASE)
        if relatorio_match:
            relatorio = relatorio_match.group(1)
            logger.info(f"   üìä Relat√≥rio da IA: {relatorio}")
            # Remover o coment√°rio do resultado final
            resultado = re.sub(r'<!--\s*RELAT√ìRIO:.+?-->\s*', '', resultado, flags=re.IGNORECASE).strip()
        
        # Valida√ß√£o b√°sica: o resultado deve ter pelo menos 70% do tamanho original
        if len(resultado) < len(texto) * 0.7:
            logger.warning(f"   ‚ö†Ô∏è Revis√£o retornou texto muito curto ({len(resultado)} vs {len(texto)}). Mantendo original.")
            return texto
        
        # Contar quantos headers foram alterados
        headers_original = len(re.findall(r'^#{2,4}\s', texto, re.MULTILINE))
        headers_revisado = len(re.findall(r'^#{2,4}\s', resultado, re.MULTILINE))
        diff = abs(headers_original - headers_revisado)
        
        logger.info(f"   ‚úÖ Estrutura revisada: {headers_original} ‚Üí {headers_revisado} headers (Œî{diff})")
        return resultado
        
    except Exception as e:
        logger.warning(f"   ‚ö†Ô∏è Erro na revis√£o por IA: {e}. Mantendo estrutura original.")
        return texto

# =============================================================================
# REVIS√ÉO LEVE DE FORMATA√á√ÉO (MODO FIDELIDADE) v2.0
# =============================================================================

PROMPT_STRUCTURE_REVIEW_LITE = """Voc√™ √© um revisor editorial especializado em ESTRUTURA e FORMATA√á√ÉO de documentos educacionais. Voc√™ receber√°:
1. Uma **Estrutura de Mapeamento Inicial** (planejada antes da formata√ß√£o)
2. O **Documento Processado** (resultado da formata√ß√£o por chunks)

Sua tarefa √© analisar ambos e garantir que os t√≠tulos estejam **descritivos, hierarquicamente corretos e alinhados com o conte√∫do real**, sem jamais alterar a ordem cronol√≥gica.

---

## üìã ESTRUTURA DE MAPEAMENTO INICIAL (Refer√™ncia):
{estrutura_mapeada}

---

## ‚úÖ O QUE VOC√ä DEVE FAZER:
1. **Comparar T√≠tulos:** Verifique se os t√≠tulos do documento refletem corretamente os t√≥picos do mapeamento. Se um t√≠tulo estiver gen√©rico mas o mapeamento indicar um tema espec√≠fico, refine-o.
2. **Validar Hierarquia:** Confirme que a estrutura (##, ###, ####) segue uma l√≥gica consistente (ex: se√ß√µes > subse√ß√µes > detalhes). M√ÅXIMO 3 n√≠veis.
3. **Decidir a Melhor Estrutura:** Se houver conflito entre mapeamento e documento, escolha a estrutura que melhor reflete o CONTE√öDO REAL do texto.
4. **Subt√≥picos √ìrf√£os:** Se detectar headers como "A.", "B.", "C." isolados como t√≥picos principais, converta-os em subn√≠veis do t√≥pico anterior (ex: ## para ###).
5. **T√≠tulos Descritivos:** Refine t√≠tulos gen√©ricos (ex: "Quest√£o 1") para algo que cite o tema t√©cnico (ex: "Quest√£o 1: Responsabilidade Civil").
6. **Corrigir Sintaxe Markdown:** Tabelas (alinhar colunas), listas (bullets), espa√ßamento entre se√ß√µes.
7. **Remover Vazios:** T√≠tulos sem conte√∫do abaixo.

## ‚ùå O QUE VOC√ä N√ÉO DEVE FAZER:
1. **N√ÉO MOVA** blocos de texto. A ordem deve permanecer 100% cronol√≥gica.
2. **N√ÉO MESCLE** se√ß√µes que apare√ßam em momentos diferentes da aula.
3. **N√ÉO RESUMA** nem altere o corpo dos par√°grafos.
4. **N√ÉO ADICIONE** conte√∫do novo.

## üìù RELAT√ìRIO ESPERADO:
Ao final do documento, inclua um bloco de coment√°rio (que ser√° removido) indicando:
- Quantos t√≠tulos foram refinados
- Se a estrutura final segue o mapeamento ou foi adaptada
- Discrep√¢ncias encontradas (se houver)

Formato:
<!-- RELAT√ìRIO: X t√≠tulos refinados | Estrutura: [MAPEAMENTO/ADAPTADA] | Discrep√¢ncias: [Nenhuma/Lista] -->

---

## üìÑ DOCUMENTO PARA REVISAR:
{documento}

---

## RESPOSTA:
Retorne o documento COMPLETO E INTEGRAL (mesmo tamanho do input) com a formata√ß√£o corrigida e o relat√≥rio no final. N√ÉO RESUMA."""

def ai_structure_review_lite(client, texto, estrutura_mapeada=None):
    """
    v2.0: Revis√£o LEVE de formata√ß√£o Markdown com VALIDA√á√ÉO CRUZADA.
    Compara o documento processado com a estrutura de mapeamento inicial.
    Refina t√≠tulos, valida hierarquia, e reporta discrep√¢ncias.
    N√ÉO reorganiza nem mescla conte√∫do.
    """
    logger.info("üßπ Revis√£o Leve de Formata√ß√£o (IA - Modo Fidelidade v2.0)...")
    
    # Gemini 2.0 Flash suporta 1M tokens (~4M chars) - usar at√© 500k chars para documento + 50k para estrutura
    max_doc_chars = 500000
    if len(texto) > max_doc_chars:
        logger.warning(f"   ‚ö†Ô∏è Documento muito longo ({len(texto)} chars), truncando para {max_doc_chars//1000}k...")
        texto_para_revisao = texto[:max_doc_chars] + "\n\n[... documento truncado para revis√£o ...]"
    else:
        texto_para_revisao = texto
    
    # Preparar estrutura mapeada (se dispon√≠vel)
    if estrutura_mapeada:
        estrutura_str = estrutura_mapeada[:50000]  # Limitar estrutura a 50k chars
        logger.info(f"   üìã Usando estrutura de mapeamento inicial ({len(estrutura_mapeada)} chars) para valida√ß√£o cruzada.")
    else:
        estrutura_str = "[Estrutura de mapeamento n√£o dispon√≠vel - analisar documento para inferir estrutura ideal]"
        logger.info("   ‚ÑπÔ∏è  Sem mapeamento inicial, IA ir√° inferir estrutura ideal do pr√≥prio documento.")
    
    try:
        rate_limiter.wait_if_needed()
        
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=PROMPT_STRUCTURE_REVIEW_LITE.format(
                estrutura_mapeada=estrutura_str,
                documento=texto_para_revisao
            ),
            config=types.GenerateContentConfig(
                max_output_tokens=65536,  # M√°ximo permitido
                thinking_config={"include_thoughts": False, "thinking_level": "HIGH"},  # HIGH para an√°lise estrutural profunda
                safety_settings=[
                    types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                ]
            )
        )
        
        resultado = response.text.replace('```markdown', '').replace('```', '').strip()
        
        # Extrair e exibir relat√≥rio da IA (se presente)
        relatorio_match = re.search(r'<!--\s*RELAT√ìRIO:\s*(.+?)\s*-->', resultado, re.IGNORECASE)
        if relatorio_match:
            relatorio = relatorio_match.group(1)
            logger.info(f"   üìä Relat√≥rio da IA: {relatorio}")
            # Remover o coment√°rio do resultado final
            resultado = re.sub(r'<!--\s*RELAT√ìRIO:.+?-->\s*', '', resultado, flags=re.IGNORECASE).strip()
        
        # Valida√ß√£o: resultado deve ter pelo menos 85% do tamanho
        if len(resultado) < len(texto) * 0.85:
            logger.warning(f"   ‚ö†Ô∏è Revis√£o retornou texto muito curto ({len(resultado)} vs {len(texto)}). Mantendo original.")
            return texto
        
        # Verificar se a ordem dos headers foi preservada (valida√ß√£o extra)
        headers_original = re.findall(r'^#{1,4}\s+(.+?)$', texto, re.MULTILINE)[:20]
        headers_revisado = re.findall(r'^#{1,4}\s+(.+?)$', resultado, re.MULTILINE)[:20]
        
        if len(headers_original) > 5 and len(headers_revisado) > 5:
            matches = sum(1 for h1, h2 in zip(headers_original[:10], headers_revisado[:10]) 
                         if h1.strip()[:30] == h2.strip()[:30])
            if matches < 6:
                logger.warning(f"   ‚ö†Ô∏è Ordem dos headers parece alterada. Mantendo original.")
                return texto
        
        # Relat√≥rio de altera√ß√µes de t√≠tulos
        alteracoes = []
        for h_orig, h_rev in zip(headers_original[:15], headers_revisado[:15]):
            if h_orig.strip() != h_rev.strip():
                orig_short = h_orig.strip()[:40] + "..." if len(h_orig.strip()) > 40 else h_orig.strip()
                rev_short = h_rev.strip()[:40] + "..." if len(h_rev.strip()) > 40 else h_rev.strip()
                alteracoes.append(f"   - '{orig_short}' ‚Üí '{rev_short}'")
        
        if alteracoes:
            logger.info(f"üìù T√≠tulos Refinados ({len(alteracoes)}):")
            for alt in alteracoes[:5]:
                logger.info(alt)
            if len(alteracoes) > 5:
                logger.info(f"   ... e mais {len(alteracoes) - 5} altera√ß√µes")
        else:
            logger.info("   ‚ÑπÔ∏è  Nenhum t√≠tulo foi alterado (estrutura j√° estava OK).")
        
        logger.info(f"   ‚úÖ Formata√ß√£o revisada (modo leve v2.0).")
        return resultado
        
    except Exception as e:
        logger.warning(f"   ‚ö†Ô∏è Erro na revis√£o leve: {e}. Mantendo original.")
        return texto




def deterministic_structure_fix(text):
    """
    v1.1: Reorganiza√ß√£o Estrutural Determin√≠stica (Regex).
    Adaptativo: Detecta se o documento usa H1 ou apenas H2 como n√≠vel principal.
    """
    logger.info("üß© Executando Reorganiza√ß√£o Estrutural Determin√≠stica...")
    
    lines = text.split('\n')
    
    # Detec√ß√£o de Hierarquia
    has_h1 = any(re.match(r'^#\s+', line) for line in lines)
    header_level_regex = r'^#\s+' if has_h1 else r'^##\s+'
    logger.info(f"   ‚ÑπÔ∏è  N√≠vel principal detectado: {'H1 (#)' if has_h1 else 'H2 (##)'}")

    # Estruturas de dados (Preserva ordem de inser√ß√£o)
    content_map = {
        "PREAMBULO": [],
        "DISCIPLINAS": {}, 
        "ENCERRAMENTO": []
    }
    
    current_area = "PREAMBULO"
    current_block = []
    disciplinas_order = [] 
    
    # Regex Adaptativo
    # Captura o texto do cabe√ßalho principal (seja H1 ou H2)
    # Exclui "Quest√£o" ou "Q." para n√£o quebrar simulados dentro de uma √°rea
    re_disciplina = re.compile(f'{header_level_regex}(?!Quest√£o|Q\\.)([^0-9\\.]+.*)', re.IGNORECASE)
    re_encerramento = re.compile(f'{header_level_regex}(?:ENCERRAMENTO|CONSIDERA√á√ïES|CONCLUS√ÉO)', re.IGNORECASE)
    
    def flush_block(area, block_lines):
        if not block_lines: return
        block_text = '\n'.join(block_lines)
        
        if area == "PREAMBULO":
            content_map["PREAMBULO"].append(block_text)
        elif area == "ENCERRAMENTO":
            content_map["ENCERRAMENTO"].append(block_text)
        else:
            if area not in content_map["DISCIPLINAS"]:
                content_map["DISCIPLINAS"][area] = []
                disciplinas_order.append(area)
            content_map["DISCIPLINAS"][area].append(block_text)

    for line in lines:
        # 1. Detectar mudan√ßa de disciplina macro
        match_disc = re_disciplina.match(line)
        if match_disc:
            flush_block(current_area, current_block)
            current_block = []
            
            raw_area = match_disc.group(1).strip().upper()
            
            # Normaliza√ß√£o de nome de √°rea
            if "DIREITO" not in raw_area and len(raw_area) < 50:
                 # Adiciona prefixo se parecer nome de mat√©ria jur√≠dica comum
                 if any(x in raw_area for x in ["CIVIL", "PENAL", "TRABALHO", "ADMINISTRATIVO", "CONSTITUCIONAL"]):
                     current_area = f"DIREITO {raw_area}"
                 else:
                     current_area = raw_area
            else:
                 current_area = raw_area
            
            # IMPORTANTE: Se estamos operando em modo H2, essa linha √© um Header que queremos manter?
            # Se for modo H1, recriamos "# AREA". 
            # Se for modo H2, recriamos "# AREA" (upcast) ou mantemos "## AREA"?
            # Para padronizar Apostilas, vamos promover tudo a H1 na reconstru√ß√£o.
            continue 
            
        # 2. Detectar Encerramento
        if re_encerramento.match(line):
            flush_block(current_area, current_block)
            current_block = []
            current_area = "ENCERRAMENTO"
            continue

        current_block.append(line)
        
    flush_block(current_area, current_block)
    
    # Reconstru√ß√£o
    final_output = []
    
    # Preambulo
    if content_map["PREAMBULO"]:
        final_output.append("# ORIENTA√á√ïES GERAIS / INTRODU√á√ÉO")
        final_output.extend(content_map["PREAMBULO"])
        final_output.append("")

    # Disciplinas / T√≥picos Principais
    for area in disciplinas_order:
        area_clean = area.replace("#", "").strip()
        final_output.append(f"# {area_clean}")
        for block in content_map["DISCIPLINAS"][area]:
            final_output.append(block)
        final_output.append("")
        
    # Encerramento
    if content_map["ENCERRAMENTO"]:
        final_output.append("# CONSIDERA√á√ïES FINAIS")
        final_output.extend(content_map["ENCERRAMENTO"])
        
    num_identified = len(disciplinas_order)
    logger.info(f"   ‚úÖ Reorganizado: {num_identified} se√ß√µes principais identificadas.")
    
    # Fallback: Se n√£o identificou nada (tudo preambulo), retorna original para n√£o estragar
    if num_identified == 0 and len(content_map["PREAMBULO"]) > 0:
        logger.warning("   ‚ö†Ô∏è Nenhuma estrutura detectada. Mantendo original.")
        return text
        
    return '\n'.join(final_output)

def numerar_titulos(texto):
    """Adiciona numera√ß√£o sequencial aos t√≠tulos"""
    linhas = texto.split('\n')
    linhas_numeradas = []
    
    contador_h2 = 0
    contador_h3 = 0
    contador_h4 = 0
    
    titulo_pattern = re.compile(r'^(#{2,4})\s+(?:\d+(?:\.\d+)*\.?\s+)?(.+)$')
    
    # Variaveis para rastrear √∫ltimos t√≠tulos e evitar repeti√ß√µes
    ultimo_h2_texto = ""
    ultimo_h3_texto = ""
    
    for linha in linhas:
        match = titulo_pattern.match(linha)
        
        if match:
            nivel = len(match.group(1))
            texto_titulo = match.group(2).strip()
            
            # N√£o numera t√≠tulos de resumo/quadros
            if any(keyword in texto_titulo.lower() for keyword in ['resumo', 'quadro', 'esquema', 'üìã', 'üìä', 'üóÇÔ∏è']):
                linhas_numeradas.append(linha)
                continue
            
            # MERGE INTELIGENTE DE T√çTULOS REPETIDOS (v2.11)
            # Se o t√≠tulo atual for muito similar ao anterior do mesmo n√≠vel ("continua√ß√£o" de chunk), ignoramos o novo
            # para que o texto flua como um √∫nico t√≥pico.
            
            from difflib import SequenceMatcher
            eh_duplicado = False
            
            if nivel == 2:
                # Verifica similaridade com √∫ltimo H2
                ratio = SequenceMatcher(None, texto_titulo.lower(), ultimo_h2_texto.lower()).ratio()
                if ratio > 0.9:
                    eh_duplicado = True
                    logger.info(f"üîÑ T√≠tulo H2 mesclado: '{texto_titulo}' ‚âà '{ultimo_h2_texto}'")
                else:
                    ultimo_h2_texto = texto_titulo
            elif nivel == 3:
                 # Verifica similaridade com √∫ltimo H3
                ratio = SequenceMatcher(None, texto_titulo.lower(), ultimo_h3_texto.lower()).ratio()
                if ratio > 0.9:
                    eh_duplicado = True
                    logger.info(f"üîÑ T√≠tulo H3 mesclado: '{texto_titulo}' ‚âà '{ultimo_h3_texto}'")
                else:
                    ultimo_h3_texto = texto_titulo
            
            if eh_duplicado:
                continue # Pula a linha do t√≠tulo, fundindo o conte√∫do
            
            if nivel == 2:
                contador_h2 += 1
                contador_h3 = 0
                contador_h4 = 0
                nova_linha = f"## {contador_h2}. {texto_titulo}"
            elif nivel == 3:
                contador_h3 += 1
                contador_h4 = 0
                nova_linha = f"### {contador_h2}.{contador_h3}. {texto_titulo}"
            elif nivel == 4:
                contador_h4 += 1
                nova_linha = f"#### {contador_h2}.{contador_h3}.{contador_h4}. {texto_titulo}"
            else:
                nova_linha = linha
            
            linhas_numeradas.append(nova_linha)
        else:
            linhas_numeradas.append(linha)
    
    return '\n'.join(linhas_numeradas)

def renumerar_secoes(texto):
    """
    v1.0: Renumera√ß√£o Sequencial Determin√≠stica.
    
    Esta fun√ß√£o √© uma camada de seguran√ßa extra, aplicada AP√ìS o AI Review.
    Ela percorre todos os headers numerados e corrige qualquer duplica√ß√£o ou
    sequ√™ncia quebrada, garantindo que os n√∫meros sejam estritamente sequenciais
    dentro de cada n√≠vel de hierarquia.
    
    Exemplo de corre√ß√£o:
    - Input:  9.20, 9.21, 9.21, 9.35, 9.36  (duplica√ß√£o e pulo)
    - Output: 9.20, 9.21, 9.22, 9.23, 9.24  (sequencial)
    """
    logger.info("üî¢ Executando Renumera√ß√£o Sequencial Determin√≠stica...")
    
    linhas = texto.split('\n')
    novas_linhas = []
    
    # Contadores por n√≠vel de hierarquia (ex: {2: 9, 3: 44} -> ## 9.x, ### 9.44.x)
    # Estrutura: {(header_level, parent_prefix): next_number}
    contadores = {}
    
    # Regex para detectar headers numerados: ### 9.20.1. T√≠tulo ou ## 5. T√≠tulo
    header_pattern = re.compile(r'^(#{1,4})\s+([\d.]+\.?)\s*(.*)$')
    
    for linha in linhas:
        match = header_pattern.match(linha)
        
        if match:
            hashes = match.group(1)       # "###"
            numero = match.group(2)       # "9.20.1." ou "9.20.1"
            titulo = match.group(3)       # "T√≠tulo..."
            
            nivel = len(hashes)
            numero_limpo = numero.rstrip('.')
            partes = numero_limpo.split('.')
            
            # Determina o prefixo pai e o sufixo atual
            if len(partes) == 1:
                # N√≠vel raiz: ## 1. ou ## 9.
                prefixo_pai = ""
                sufixo_atual = int(partes[0])
            else:
                # Subn√≠vel: ### 9.20. ou #### 9.20.1.
                prefixo_pai = '.'.join(partes[:-1])
                sufixo_atual = int(partes[-1])
            
            # Inicializa contador se n√£o existir
            chave = (nivel, prefixo_pai)
            if chave not in contadores:
                contadores[chave] = sufixo_atual  # Come√ßa do n√∫mero encontrado
            else:
                contadores[chave] += 1
            
            novo_sufixo = contadores[chave]
            
            # Reconstr√≥i o n√∫mero
            if prefixo_pai:
                novo_numero = f"{prefixo_pai}.{novo_sufixo}."
            else:
                novo_numero = f"{novo_sufixo}."
            
            # Reconstr√≥i a linha
            nova_linha = f"{hashes} {novo_numero} {titulo}"
            novas_linhas.append(nova_linha)
            
            # Log se houve mudan√ßa
            if numero_limpo != novo_numero.rstrip('.'):
                logger.info(f"   üîÑ {numero_limpo} ‚Üí {novo_numero.rstrip('.')}")
        else:
            novas_linhas.append(linha)
    
    logger.info("   ‚úÖ Renumera√ß√£o conclu√≠da.")
    return '\n'.join(novas_linhas)

# =============================================================================
# VERIFICA√á√ÉO DE COBERTURA E DUPLICA√á√ïES
# =============================================================================

def normalizar_fingerprint(texto, tipo):
    """Normaliza texto para compara√ß√£o (ex: 'Lei 11.100' -> 'lei 11100')"""
    texto = texto.lower().strip()
    
    if tipo == 'leis':
        # Mant√©m apenas 'lei' e n√∫meros
        nums = re.findall(r'\d+', texto)
        if nums:
            # Reconstr√≥i como 'lei 12345'
            # Filtra leis com menos de 4 d√≠gitos para evitar ru√≠do (ex: lei 10, lei 13)
            num_full = ''.join(nums)
            if len(num_full) >= 4:
                return f"lei {num_full}"
            return None
            
    elif tipo == 'sumulas':
        nums = re.findall(r'\d+', texto)
        if nums:
            return f"s√∫mula {''.join(nums)}"
            
    elif tipo == 'artigos':
        nums = re.findall(r'\d+', texto)
        if nums:
            return f"artigo {''.join(nums)}"
            
    return re.sub(r'[^\w\s]', '', texto)

def extrair_fingerprints(texto):
    """Extrai 'fingerprints' √∫nicos e normalizados do texto"""
    fingerprints = {
        'leis': set(),
        'sumulas': set(),
        'artigos': set(),
        'julgados': set()
    }
    
    # Regex melhorado para capturar varia√ß√µes
    lei_pattern = re.compile(r'\b(?:lei|l\.)\s*n?¬∫?\s*([\d\.]+)', re.IGNORECASE)
    sumula_pattern = re.compile(r'\bs√∫mula\s*(?:vinculante)?\s*n?¬∫?\s*(\d+)', re.IGNORECASE)
    
    # Extrai e normaliza
    for match in lei_pattern.finditer(texto):
        fp = normalizar_fingerprint(f"lei {match.group(1)}", 'leis')
        if fp: fingerprints['leis'].add(fp)
    
    for match in sumula_pattern.finditer(texto):
        fp = normalizar_fingerprint(f"s√∫mula {match.group(1)}", 'sumulas')
        if fp: fingerprints['sumulas'].add(fp)
        
    return fingerprints

def contar_ocorrencias_robust(fingerprints, texto):
    """Conta ocorr√™ncias com suporte a formata√ß√£o jur√≠dica formal (Lei n¬∫ X)"""
    contagens = {}
    
    # CORRE√á√ÉO 1: N√£o remover pontua√ß√£o indiscriminadamente, apenas normalizar espa√ßos
    # Mantemos barras e pontos para evitar fus√£o de n√∫meros (11.101/2005)
    texto_lower = texto.lower()
    
    for categoria, items in fingerprints.items():
        for item in items:
            key = f"{categoria}:{item}"
            
            if categoria == 'leis':
                # item ex: "lei 4320" -> extrai "4320"
                # Remove pontua√ß√£o do item para garantir match limpo no n√∫mero
                num_bruto = item.split()[-1] 
                num = re.sub(r'[^\d]', '', num_bruto)
                
                # Permite pontos opcionais entre d√≠gitos (ex: 4.320 match com 4320)
                num_regex = r"\.?".join(list(num))
                
                # CORRE√á√ÉO 2: Regex flex√≠vel que aceita "n", "n¬∫", "no", "num" no meio
                # Aceita: "Lei 4320", "Lei n¬∫ 4.320", "Lei n. 4320"
                # O \W* permite pontos/barras entre Lei e o n√∫mero
                # Adicionado \b no final para evitar matches parciais (Lei 10 != Lei 100)
                pattern = f"lei(?:\\s+|\\.|\\,|n¬∫|n\\.|n\\s|num\\.?)*{num_regex}\\b"
                
                # Usamos findall no texto original (lower) para pegar varia√ß√µes com pontua√ß√£o
                matches = re.findall(pattern, texto_lower)
                contagens[key] = len(matches)
                
            elif categoria == 'sumulas':
                num = item.split()[-1]
                num_regex = r"\.?".join(list(num)) # S√∫mulas raramente tem ponto, mas por garantia
                
                # Mesma l√≥gica para s√∫mulas (S√∫mula Vinculante n¬∫ 10)
                pattern = f"s√∫mula(?:\\s+|\\.|\\,|vinculante|n¬∫|n\\.|n\\s)*{num_regex}\\b"
                matches = re.findall(pattern, texto_lower)
                contagens[key] = len(matches)
                
            else:
                # Fallback para outros tipos (busca literal simples)
                contagens[key] = texto_lower.count(item)
                
    return contagens

def verificar_cobertura(texto_original, texto_formatado, arquivo_saida=None):
    """Verifica omiss√µes e duplica√ß√µes artificiais entre original e formatado"""
    logger.info("üîç Verificando cobertura e duplica√ß√µes...")
    
    # Extrai fingerprints do original
    fp_original = extrair_fingerprints(texto_original)
    
    # Conta ocorr√™ncias em ambos
    contagem_original = contar_ocorrencias_robust(fp_original, texto_original)
    contagem_formatado = contar_ocorrencias_robust(fp_original, texto_formatado)
    
    omissoes = []
    duplicacoes = []
    
    for key, count_orig in contagem_original.items():
        count_fmt = contagem_formatado.get(key, 0)
        categoria, item = key.split(':', 1)
        
        # Omiss√£o: estava no original mas sumiu
        if count_orig > 0 and count_fmt == 0:
            omissoes.append({
                'categoria': categoria,
                'item': item,
                'original': count_orig,
                'formatado': count_fmt
            })
        
        # Duplica√ß√£o (agora considerada positiva em materiais did√°ticos)
        if count_fmt > count_orig:
            duplicacoes.append({
                'categoria': categoria,
                'item': item,
                'original': count_orig,
                'formatado': count_fmt,
                'extra': count_fmt - count_orig
            })
    
    # Gera relat√≥rio
    total_items = len([k for k, v in contagem_original.items() if v > 0])
    items_preservados = total_items - len(omissoes)
    cobertura = items_preservados / total_items * 100 if total_items > 0 else 100
    
    logger.info(f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    logger.info(f"üìä RELAT√ìRIO DE VERIFICA√á√ÉO")
    logger.info(f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    logger.info(f"‚úÖ Cobertura: {cobertura:.1f}% ({items_preservados}/{total_items} refer√™ncias)")
    
    if omissoes:
        logger.warning(f"\n‚ùå POSS√çVEIS OMISS√ïES ({len(omissoes)}):")
        for o in omissoes[:10]:  # Limita a 10
            logger.warning(f"   - [{o['categoria']}] {o['item']}")
        if len(omissoes) > 10:
            logger.warning(f"   ... e mais {len(omissoes) - 10} omiss√µes")
    else:
        logger.info("‚úÖ Nenhuma omiss√£o detectada")
    
    if duplicacoes:
        logger.info(f"\n‚ÑπÔ∏è CITA√á√ïES REFOR√áADAS (Tabelas/Resumos) ({len(duplicacoes)}):")
        for d in duplicacoes[:10]:
            logger.info(f"   - [{d['categoria']}] {d['item']}: {d['original']}x ‚Üí {d['formatado']}x (+{d['extra']})")
        if len(duplicacoes) > 10:
            logger.info(f"   ... e mais {len(duplicacoes) - 10} cita√ß√µes extras")
    else:
        logger.info("‚ÑπÔ∏è Nenhuma cita√ß√£o extra detectada")
    
    logger.info(f"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    
    # Salva relat√≥rio em arquivo se especificado
    if arquivo_saida:
        relatorio_path = arquivo_saida.replace('.md', '_verificacao.txt')
        with open(relatorio_path, 'w', encoding='utf-8') as f:
            f.write(f"RELAT√ìRIO DE VERIFICA√á√ÉO\n")
            f.write(f"{'='*50}\n\n")
            f.write(f"Cobertura: {cobertura:.1f}% ({items_preservados}/{total_items})\n\n")
            
            if omissoes:
                f.write(f"OMISS√ïES ({len(omissoes)}):\n")
                for o in omissoes:
                    f.write(f"  - [{o['categoria']}] {o['item']}\n")
                f.write("\n")
            
            if duplicacoes:
                f.write(f"DUPLICA√á√ïES ARTIFICIAIS ({len(duplicacoes)}):\n")
                for d in duplicacoes:
                    f.write(f"  - [{d['categoria']}] {d['item']}: {d['original']}x original ‚Üí {d['formatado']}x formatado\n")
        
        logger.info(f"üìÑ Relat√≥rio salvo: {relatorio_path}")
    
    return {
        'cobertura': cobertura,
        'omissoes': omissoes,
        'duplicacoes': duplicacoes
    }

# =============================================================================
# V2.16: VALIDA√á√ÉO LLM (Metadata Strategy)
# =============================================================================

PROMPT_VALIDATE_COMPLETENESS = """# TAREFA DE VALIDA√á√ÉO DE FIDELIDADE (METADATA STRATEGY)

Voc√™ √© um auditor de qualidade para transcri√ß√µes jur√≠dicas formatadas.

## SEU OBJETIVO
Compare a ESTRUTURA DO ORIGINAL (Metadata/Skeleton) com o TEXTO FORMATADO FINAL e identifique:

1. **OMISS√ïES GRAVES**: Conceitos jur√≠dicos, leis, s√∫mulas, artigos ou exemplos importantes que estavam no esqueleto original mas foram omitidos no formatado.
2. **DISTOR√á√ïES**: Informa√ß√µes que foram alteradas de forma que mude o sentido jur√≠dico.
3. **ESTRUTURA**: Verifique se os t√≥picos e subt√≥picos est√£o organizados de forma l√≥gica e se n√£o h√° duplica√ß√µes.

## REGRAS
- N√ÉO considere como omiss√£o: hesita√ß√µes, "n√©", "ent√£o", dados repetitivos, conversas paralelas.
- CONSIDERE como omiss√£o: qualquer lei, s√∫mula, artigo, jurisprud√™ncia, exemplo pr√°tico ou dica de prova.
- O input "TEXTO ORIGINAL" √© um RESUMO ESTRUTURAL (Metadata) contendo apenas t√≠tulos e refer√™ncias chave. Use-o para validar se esses elementos aparecem no "TEXTO FORMATADO".

## FORMATO DE RESPOSTA (JSON)
{
    "aprovado": true/false,
    "nota_fidelidade": 0-10,
    "omissoes_graves": ["descri√ß√£o clara do item omitido"],
    "distorcoes": ["descri√ß√£o clara da distor√ß√£o"],
    "problemas_estrutura": ["t√≠tulos duplicados ou hierarquia quebrada"],
    "observacoes": "coment√°rio geral sobre a qualidade"
}"""

def extract_raw_metadata(texto):
    """
    v2.16.1: Extrai esqueleto robusto do texto original para valida√ß√£o.
    Captura: T√≠tulos, Leis, S√∫mulas, Artigos, Jurisprud√™ncia, Destaques.
    """
    lines = texto.split('\n')
    metadata = []
    metadata.append(f"TOTAL WORDS: {len(texto.split())}")
    metadata.append(f"TOTAL CHARS: {len(texto)}")
    
    # Regex robustas para capturar refer√™ncias legais (reutilizando padr√µes do script)
    patterns = {
        'leis': re.compile(r'\b(?:lei|l\.)\s*n?¬∫?\s*([\d\.]+(?:/\d+)?)', re.IGNORECASE),
        'artigos': re.compile(r'\b(?:art\.?|artigo)\s*(\d+)', re.IGNORECASE),
        'sumulas': re.compile(r'\bs√∫mula\s*(?:vinculante)?\s*n?¬∫?\s*(\d+)', re.IGNORECASE),
        'jurisprudencia': re.compile(r'\b(?:REsp|RE|ADI|ADPF|HC|MS|AgRg|RMS|Rcl)\s*[\d\.\/\-]+', re.IGNORECASE),
        'informativos': re.compile(r'\b(?:informativo|info\.?)\s*(?:stf|stj)?\s*n?¬∫?\s*(\d+)', re.IGNORECASE),
        'temas': re.compile(r'\btema\s*(?:repetitivo)?\s*n?¬∫?\s*(\d+)', re.IGNORECASE),
    }
    
    # Contadores para estat√≠sticas
    refs_encontradas = {k: set() for k in patterns.keys()}
    
    for line in lines:
        line_strip = line.strip()
        if not line_strip: 
            continue
        
        # Detectar poss√≠veis t√≠tulos (caixa alta, come√ßa com n√∫mero, curto)
        if len(line_strip) < 100:
            if line_strip.isupper() and len(line_strip) > 5:
                metadata.append(f"\n[T√çTULO] {line_strip}")
            elif re.match(r'^\d+[\.\)]\s', line_strip):
                metadata.append(f"\n[T√ìPICO] {line_strip[:80]}")
        
        # Capturar todas as refer√™ncias legais
        for categoria, pattern in patterns.items():
            matches = pattern.findall(line_strip)
            for m in matches:
                ref = m if isinstance(m, str) else m[0]
                refs_encontradas[categoria].add(ref.strip())
        
        # Destaques pedag√≥gicos
        keywords = ['importante', 'aten√ß√£o', 'cuidado', 'exemplo', 'obs:', 'dica', 'pegadinha', 'caiu em prova']
        if any(kw in line_strip.lower() for kw in keywords):
            metadata.append(f"  > DESTAQUE: {line_strip[:120]}...")
    
    # Resumo estat√≠stico
    metadata.append("\n--- REFER√äNCIAS ENCONTRADAS ---")
    for cat, refs in refs_encontradas.items():
        if refs:
            metadata.append(f"[{cat.upper()}] ({len(refs)}): {', '.join(sorted(refs)[:15])}{'...' if len(refs) > 15 else ''}")
    
    return "\n".join(metadata)

# =============================================================================
# AUTO-FIX PASS (v2.18)
# =============================================================================

def aplicar_correcoes_automaticas(texto):
    """
    v2.18: Aplica corre√ß√µes autom√°ticas baseadas em padr√µes comuns de erro.
    Retorna (texto_corrigido, lista_de_correcoes).
    """
    correcoes = []
    texto_original = texto
    
    # 1. Remover sauda√ß√µes duplicadas (apenas mant√©m a primeira)
    saudacoes_pattern = r'(?:Ol√°|Oi),?\s*(?:sejam?\s+)?(?:bem[- ]?vindos?(?:\s+e\s+bem[- ]?vindas?)?)[.,!]?'
    matches = list(re.finditer(saudacoes_pattern, texto, re.IGNORECASE))
    if len(matches) > 1:
        # Remove todas exceto a primeira
        for match in reversed(matches[1:]):
            # Captura a linha inteira onde a sauda√ß√£o aparece
            start = texto.rfind('\n', 0, match.start()) + 1
            end = texto.find('\n', match.end())
            if end == -1: end = len(texto)
            linha = texto[start:end].strip()
            # S√≥ remove se a linha for majoritariamente sauda√ß√£o
            if len(linha) < 150:
                texto = texto[:start] + texto[end+1:]
                correcoes.append(f"Removida sauda√ß√£o duplicada: '{linha[:50]}...'")
    
    # 2. Remover apresenta√ß√µes repetidas do professor
    apresentacao_pattern = r'Eu sou o professor\s+\w+(?:\s+\w+)?'
    matches = list(re.finditer(apresentacao_pattern, texto, re.IGNORECASE))
    if len(matches) > 1:
        for match in reversed(matches[1:]):
            start = texto.rfind('\n', 0, match.start()) + 1
            end = texto.find('\n', match.end())
            if end == -1: end = len(texto)
            linha = texto[start:end].strip()
            if len(linha) < 200:
                texto = texto[:start] + texto[end+1:]
                correcoes.append(f"Removida apresenta√ß√£o duplicada: '{linha[:50]}...'")
    
    # 3. Padronizar nome do professor (detecta varia√ß√µes e unifica)
    # Extrai primeiro nome mencionado como "professor X"
    nome_match = re.search(r'professor\s+(\w+(?:\s+\w+)?)', texto, re.IGNORECASE)
    if nome_match:
        nome_canonico = nome_match.group(1)
        # Busca varia√ß√µes pr√≥ximas (dist√¢ncia de Levenshtein simplificada)
        variacoes_pattern = rf'\bprofessor\s+(\w+(?:\s+\w+)?)\b'
        for m in re.finditer(variacoes_pattern, texto, re.IGNORECASE):
            nome_atual = m.group(1)
            if nome_atual.lower() != nome_canonico.lower():
                sim = SequenceMatcher(None, nome_canonico.lower(), nome_atual.lower()).ratio()
                if sim > 0.6 and sim < 1.0:  # Similar mas diferente
                    texto = texto.replace(f"professor {nome_atual}", f"professor {nome_canonico}")
                    texto = texto.replace(f"Professor {nome_atual}", f"Professor {nome_canonico}")
                    correcoes.append(f"Padronizado nome: '{nome_atual}' ‚Üí '{nome_canonico}'")
    
    # 4. Corrigir itens de lista vazios ou malformados
    # Padr√£o: n√∫mero + ponto + espa√ßos/quebra + pr√≥ximo conte√∫do
    texto = re.sub(r'(\d+\.)\s*\n\s*((?:Requisitos|Preenchimento|Fundamento|Artigo|Lei))', r'\1 \2', texto)
    if texto != texto_original:
        correcoes.append("Corrigidos itens de lista malformados")
    
    # 5. Remover linhas em branco excessivas (mais de 2 consecutivas)
    texto_limpo = re.sub(r'\n{4,}', '\n\n\n', texto)
    if texto_limpo != texto:
        texto = texto_limpo
        correcoes.append("Removidas linhas em branco excessivas")
    
    logger.info(f"üîß Auto-Fix: {len(correcoes)} corre√ß√µes aplicadas")
    for c in correcoes:
        logger.info(f"   ‚úì {c}")
    
    return texto, correcoes


def validate_completeness_llm(raw_text, formatted_text, client, output_file=None):
    """
    v2.16.1: Valida√ß√£o LLM com Metadata Strategy e retorno estruturado.
    """
    logger.info("üïµÔ∏è Executando Valida√ß√£o LLM (Completeness Check) com Gemini 3 Flash...")
    
    # 1. Extrair Metadata do Raw (Otimiza√ß√£o)
    raw_metadata = extract_raw_metadata(raw_text)
    
    # Estimativa de tokens
    input_text = f"{PROMPT_VALIDATE_COMPLETENESS}\n\n## TEXTO ORIGINAL (METADATA/SKELETON):\n{raw_metadata}\n\n## TEXTO FORMATADO:\n{formatted_text}"
    est_tokens = len(input_text) // 4
    logger.info(f"   üìä Payload de Valida√ß√£o: ~{est_tokens:,} tokens")
    
    try:
        if est_tokens > 2_000_000:
             logger.warning("‚ö†Ô∏è Payload excede 2M tokens. Pulando valida√ß√£o LLM para evitar erro.")
             return {'aprovado': True, 'nota_fidelidade': 0, 'skipped': True, 'reason': 'payload_too_large'}
             
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=input_text,
            config=types.GenerateContentConfig(
                response_mime_type="application/json",
                max_output_tokens=8000,
                thinking_config=types.ThinkingConfig(
                    include_thoughts=False,
                    thinking_level="HIGH"
                )
            )
        )
        
        result = json.loads(response.text)
        
        # Log Gr√°fico
        if result.get('aprovado'):
            logger.info(f"   ‚úÖ APROVADO (Nota {result.get('nota_fidelidade')}/10)")
        else:
            logger.warning(f"   ‚ùå REPROVADO (Nota {result.get('nota_fidelidade')}/10)")
            
        omissions = result.get('omissoes_graves', [])
        if omissions:
            logger.warning(f"   üö® {len(omissions)} Omiss√µes Graves Detectadas:")
            for o in omissions[:5]:  # Limita log a 5
                logger.warning(f"      - {o}")
                
        # Salvar Relat√≥rio
        if output_file:
            report_path = output_file.replace('.md', '_LLM_VALIDATION.md')
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(f"# Relat√≥rio de Valida√ß√£o LLM (Metadata Strategy v2.16.1)\n")
                f.write(f"**Data:** {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"**Modelo:** {GEMINI_MODEL}\n")
                f.write(f"**Nota Fidelidade:** {result.get('nota_fidelidade')}/10\n\n")
                
                f.write("## üîç Omiss√µes Graves\n")
                if omissions:
                    for o in omissions: f.write(f"- üî¥ {o}\n")
                else:
                    f.write("- ‚úÖ Nenhuma omiss√£o grave detectada.\n")
                    
                f.write("\n## ‚ö†Ô∏è Distor√ß√µes\n")
                distorcoes = result.get('distorcoes', [])
                if distorcoes:
                    for d in distorcoes: f.write(f"- ‚ö†Ô∏è {d}\n")
                else:
                    f.write("- Nenhuma distor√ß√£o detectada.\n")
                
                f.write("\n## üèóÔ∏è Estrutura\n")
                problemas = result.get('problemas_estrutura', [])
                if problemas:
                    for p in problemas: f.write(f"- üîß {p}\n")
                else:
                    f.write("- Estrutura OK.\n")
                
                f.write(f"\n## üìù Observa√ß√µes\n{result.get('observacoes', 'N/A')}\n")
                
            logger.info(f"   üìÑ Relat√≥rio salvo: {report_path}")
        
        return result
            
    except Exception as e:
        logger.error(f"   ‚ùå Erro na valida√ß√£o LLM: {e}")
        return {'aprovado': True, 'nota_fidelidade': 0, 'error': str(e)}




def auto_fix_smart(raw_text, formatted_text, validation_result, client, estrutura_global=None):
    """
    v2.18 (SAFE MODE): Corretor Estrutural Seguro.
    Foca EXCLUSIVAMENTE em problemas de estrutura (t√≠tulos, duplicatas, hierarquia).
    N√ÉO altera conte√∫do jur√≠dico para evitar alucina√ß√µes.
    
    Args:
        raw_text: (N√£o usado no modo Safe, mantido para compatibilidade)
        formatted_text: Texto formatado atual
        validation_result: Dict com problemas estruturais
        client: Cliente Vertex AI
        estrutura_global: Mapeamento de refer√™ncia (opcional)
    """
    # No modo SAFE, ignoramos omiss√µes/distor√ß√µes para n√£o correr risco de reescrita
    problemas_estrut = validation_result.get('problemas_estrutura', [])
    
    if not problemas_estrut:
        logger.info("‚úÖ Nenhum problema estrutural para corrigir.")
        return formatted_text
    
    logger.info(f"üîß Auto-Fix Safe: Corrigindo {len(problemas_estrut)} problemas estruturais...")
    
    report = "### PROBLEMAS ESTRUTURAIS:\n" + "\n".join([f"- {p}" for p in problemas_estrut]) + "\n"
        
    PROMPT_FIX = f"""Voc√™ √© um editor t√©cnico de elite.
    
## TAREFA: LIMPEZA ESTRUTURAL (SEM ALTERAR CONTE√öDO)
Voc√™ deve corrigir APENAS a formata√ß√£o e estrutura do documento.

## REGRA DE OURO (SEGURAN√áA JUR√çDICA):
- **N√ÉO altere o texto dos par√°grafos.**
- **N√ÉO adicione nem remova informa√ß√µes jur√≠dicas.**
- **N√ÉO reescreva explica√ß√µes.**
- Sua permiss√£o √© APENAS para T√≠tulos, Hierarquia e Duplicatas exatas.

## INSTRU√á√ïES DE CORRE√á√ÉO:
1. **T√≠tulos Duplicados**: Se houver t√≠tulos repetidos (ex: dois "3. Introdu√ß√£o" seguidos), remova o redundante.
2. **Hierarquia**: Ajuste n√≠veis (H2, H3) para seguir a l√≥gica do conte√∫do.
3. **Par√°grafos Repetidos**: Delete duplica√ß√µes EXATAS de par√°grafos (copia-cola acidental).
4. **Renumera√ß√£o**: Garanta sequ√™ncia l√≥gica (1, 2, 3...) nos t√≠tulos.

{f"## ESTRUTURA DE REFER√äNCIA (Guia):\n{estrutura_global}" if estrutura_global else ""}

## RELAT√ìRIO DE ERROS:
{report}

## SA√çDA:
Retorne o documento COMPLETO corrigido em Markdown. Sem explica√ß√µes."""

    try:
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=f"{PROMPT_FIX}\n\n## TEXTO A CORRIGIR:\n{formatted_text}",
            config=types.GenerateContentConfig(
                max_output_tokens=100000,
                thinking_config=types.ThinkingConfig(
                    include_thoughts=False,
                    thinking_level="HIGH" 
                )
            )
        )
        
        resultado = response.text.replace('```markdown', '').replace('```', '').strip()
        
        # Valida√ß√£o de seguran√ßa estrita
        if len(resultado) < len(formatted_text) * 0.8: # Toler√¢ncia menor no modo safe
            logger.warning("‚ö†Ô∏è Auto-Fix Safe cortou muito texto (>20%). Abortando por seguran√ßa.")
            return formatted_text
            
        logger.info(f"‚úÖ Auto-Fix Estrutural conclu√≠do. ({len(formatted_text)} -> {len(resultado)} chars)")
        return resultado
        
    except Exception as e:
        logger.error(f"‚ùå Falha no Auto-Fix Safe: {e}")
        return formatted_text

# V2.10: FUN√á√ïES DE P√ìS-PROCESSAMENTO ESTRUTURAL (Tabelas e Par√°grafos)
# =============================================================================

def mover_tabelas_para_fim_de_secao(texto):
    """
    v2.11: Reorganiza tabelas movendo-as para o final do BLOCO ATUAL (H2 ou H3).
    Corrige bug de tabelas sumindo ou ficando muito longe do contexto.
    """
    logger.info("üìä Reorganizando tabelas (Smart Layout v2.11)...")
    
    linhas = texto.split('\n')
    resultado = []
    tabelas_pendentes = [] 
    
    i = 0
    while i < len(linhas):
        linha = linhas[i]
        linha_strip = linha.strip()
        
        # 1. DETECTAR SE √â UM T√çTULO (H1, H2, H3...)
        # Se encontrarmos um novo t√≠tulo, hora de "despejar" as tabelas acumuladas do bloco anterior
        if linha_strip.startswith('#'):
            # Despeja tabelas antes de iniciar o novo t√≥pico
            if tabelas_pendentes:
                resultado.append('') # Espa√ßo antes
                for t_info in tabelas_pendentes:
                    if t_info['titulo']:
                        resultado.append(t_info['titulo'])
                    resultado.extend(t_info['linhas'])
                    resultado.append('') # Espa√ßo depois
                tabelas_pendentes = []
            
            resultado.append(linha)
            i += 1
            continue

        # 2. DETECTAR IN√çCIO DE TABELA
        # Crit√©rio: Linha tem pipe '|' E parece estrutura de tabela (n√£o apenas cita√ß√£o)
        eh_inicio_tabela = False
        if '|' in linha_strip:
            # Verifica se √© uma linha de markdown table v√°lida (tem pipe e chars)
            # E se a pr√≥xima linha ou a seguinte tem o separador '---'
            has_separator = False
            for lookahead in range(1, 3): # Olha at√© 2 linhas pra frente (ignora 1 linha vazia)
                if i + lookahead < len(linhas):
                    prox = linhas[i + lookahead].strip()
                    if set(prox).issubset(set('|- :')): # S√≥ contem caracteres de estrutura de tabela
                         has_separator = True
                         break
            
            if has_separator or (linha_strip.startswith('|') and linha_strip.endswith('|')):
                eh_inicio_tabela = True

        if eh_inicio_tabela:
            # --- Captura da Tabela ---
            tabela_linhas = []
            titulo_tabela = None
            
            # Tenta recuperar o t√≠tulo da tabela que ficou na linha anterior (ou resultado)
            # Verifica se a √∫ltima linha adicionada ao resultado parece um t√≠tulo de tabela
            if resultado and len(resultado) > 0:
                last_line = resultado[-1].strip()
                # Padr√µes comuns de t√≠tulo de tabela gerados pela IA
                if (last_line.startswith('###') or last_line.startswith('**')) and \
                   any(x in last_line.lower() for x in ['tabela', 'resumo', 'quadro', 's√≠ntese', 'esquema', 'üìã']):
                    titulo_tabela = resultado.pop() # Remove do fluxo principal para agrupar com a tabela

            # Captura as linhas da tabela
            j = i
            while j < len(linhas):
                curr = linhas[j].strip()
                # Continua se tiver pipe ou for linha vazia no meio da tabela (mas cuidado com fim)
                if '|' in curr:
                    tabela_linhas.append(linhas[j])
                    j += 1
                elif not curr:
                    # Linha vazia: verifica se a pr√≥xima volta a ter pipe
                    if j + 1 < len(linhas) and '|' in linhas[j+1]:
                        tabela_linhas.append(linhas[j]) # Mant√©m linha vazia interna
                        j += 1
                    else:
                        break # Fim da tabela
                else:
                    break # Texto normal, fim da tabela

            # Verifica se capturou algo √∫til
            if len(tabela_linhas) > 0:
                tabelas_pendentes.append({
                    'titulo': titulo_tabela,
                    'linhas': tabela_linhas
                })
                i = j # Pula as linhas processadas
                continue
            else:
                # Falso positivo? Devolve o t√≠tulo se tinhamos pego
                if titulo_tabela:
                    resultado.append(titulo_tabela)
        
        # Se n√£o for tabela nem t√≠tulo, adiciona linha normal
        resultado.append(linha)
        i += 1
    
    # 3. FINAL DO DOCUMENTO
    # Se sobraram tabelas no buffer, despeja agora
    if tabelas_pendentes:
        resultado.append('')
        for t_info in tabelas_pendentes:
            if t_info['titulo']:
                resultado.append(t_info['titulo'])
            resultado.extend(t_info['linhas'])
            resultado.append('')
            
    return '\n'.join(resultado)

def quebrar_paragrafos_longos(texto, max_chars=400, max_sentencas=4):
    """
    Quebra par√°grafos que excedem limite de chars OU n√∫mero de senten√ßas.
    Preserva listas, tabelas, cita√ß√µes e blocos especiais.
    """
    logger.info(f"‚úÇÔ∏è Quebrando par√°grafos > {max_chars} chars ou > {max_sentencas} senten√ßas...")
    
    paragrafos = texto.split('\n\n')
    resultado = []
    quebras = 0
    
    for para in paragrafos:
        linha_strip = para.strip()
        
        # PRESERVAR: t√≠tulos, listas, tabelas, cita√ß√µes, blocos de c√≥digo
        if (linha_strip.startswith('#') or 
            linha_strip.startswith('-') or 
            linha_strip.startswith('* ') or 
            linha_strip.startswith('|') or
            linha_strip.startswith('>') or
            linha_strip.startswith('```') or
            re.match(r'^\d+\.', linha_strip)):
            resultado.append(para)
            continue
        
        # Verifica se precisa quebrar
        num_chars = len(para)
        sentencas = re.split(r'(?<=[.!?])\s+', para)
        num_sentencas = len(sentencas)
        
        if num_chars <= max_chars and num_sentencas <= max_sentencas:
            resultado.append(para)
            continue
        
        # QUEBRAR: Agrupa em blocos de at√© max_sentencas
        quebras += 1
        subparagrafos = []
        bloco_atual = []
        chars_atual = 0
        sentencas_no_bloco = 0
        
        for sentenca in sentencas:
            teste_chars = chars_atual + len(sentenca)
            
            # Se adicionar essa senten√ßa ultrapassar AMBOS os limites ou o limite de senten√ßas
            # E j√° temos algo no bloco...
            if (teste_chars > max_chars or sentencas_no_bloco >= max_sentencas) and bloco_atual:
                subparagrafos.append(' '.join(bloco_atual).strip())
                bloco_atual = [sentenca]
                chars_atual = len(sentenca)
                sentencas_no_bloco = 1
            else:
                bloco_atual.append(sentenca)
                chars_atual = teste_chars
                sentencas_no_bloco += 1
        
        # Adiciona √∫ltimo bloco
        if bloco_atual:
            subparagrafos.append(' '.join(bloco_atual).strip())
        
        resultado.append('\n\n'.join(subparagrafos))
        
    if quebras > 0:
        logger.info(f"   ‚úÖ {quebras} par√°grafos foram ajustados.")
        
    return '\n\n'.join(resultado)

# =============================================================================
# FLUXO PRINCIPAL
# =============================================================================

def formatar_transcricao(transcricao_completa, usar_cache=True, input_file=None, custom_prompt=None):
    prompt_ativo = custom_prompt if custom_prompt else PROMPT_FORMATACAO
    if custom_prompt:
        logger.info(f"üé® Usando prompt customizado ({len(custom_prompt):,} caracteres)")
    
    estrutura_global = "" # v2.17: Inicializa para evitar UnboundLocalError
    # ‚ö° FOR√áAR USO DE VERTEX AI - AI Studio desabilitado
    rate_limiter.max_rpm = 60
    # Tenta pegar o projeto do ambiente, se n√£o tiver, usa o hardcoded como fallback
    project_id = os.getenv("GOOGLE_CLOUD_PROJECT", "gen-lang-client-0727883752")
    logger.info(f" Usando Vertex AI (Project: {project_id})")
    logger.info(" Rate Limit: 60 RPM")
    logger.info("üî• AI Studio DESABILITADO - Usando apenas Vertex AI")
    
    # Verificar se as credenciais do Vertex AI est√£o configuradas
    if os.getenv("GOOGLE_APPLICATION_CREDENTIALS") is None:
        logger.error("‚ùå Nenhuma autentica√ß√£o configurada para Vertex AI.")
        logger.error("Configure a vari√°vel de ambiente GOOGLE_APPLICATION_CREDENTIALS:")
        logger.error("  export GOOGLE_APPLICATION_CREDENTIALS='/path/to/service-account.json'")
        sys.exit(1)

    client = genai.Client(
        vertexai=True,
        project=project_id,
        location="global"
    )
    
    # Dry-run mode
    if '--dry-run' in sys.argv:
        logger.info("üîç MODO DRY-RUN: Validando divis√£o de chunks")
        chunks = dividir_sequencial(transcricao_completa)
        validar_chunks(chunks, transcricao_completa)
        for i, c in enumerate(chunks):
            print(f"\n  Chunk {i+1}/{len(chunks)}:")
            print(f"    Posi√ß√£o: {c['inicio']:,} ‚Üí {c['fim']:,} ({c['fim']-c['inicio']:,} chars)")
            inicio_preview = transcricao_completa[c['inicio']:c['inicio']+80].replace('\n', '‚Üµ')
            fim_preview = transcricao_completa[max(0, c['fim']-80):c['fim']].replace('\n', '‚Üµ')
            print(f"    In√≠cio: {inicio_preview}...")
            print(f"    Fim: ...{fim_preview}")
        sys.exit(0)
    
    tamanho_total = len(transcricao_completa)
    logger.info(f"üìä Tamanho: {tamanho_total:,} caracteres")
    
    # v2.10: Iniciar timer de m√©tricas
    metrics.start_timer()
    
    # v2.17: Divis√£o Inicial (Estimativa)
    chunks = dividir_sequencial(transcricao_completa)
    num_partes = len(chunks)
    
    # v2.8: Mapeamento estrutural pr√©vio (se necess√°rio)
    # Se n√£o foi passado externamente e temos m√∫ltiplas partes, gera agora.
    if num_partes > 1 and not estrutura_global:
        estrutura_global = map_structure(client, transcricao_completa)
        
        # v2.10: Filtros de estrutura
        estrutura_global = filtrar_niveis_execessivos(estrutura_global, max_nivel=3)
        estrutura_global = simplificar_estrutura_se_necessario(estrutura_global)
        
        # v2.17: RE-DIVIS√ÉO OTIMIZADA (Anchor-Based Chunking)
        # Agora que temos a estrutura, refazemos os cortes para alinhar com os t√≥picos
        logger.info("üîÑ Otimizando cortes com base na estrutura (Anchor-Based Chunking)...")
        chunks = dividir_sequencial(transcricao_completa, estrutura_global=estrutura_global)
        num_partes = len(chunks)

    # v2.7: Valida√ß√£o rigorosa
    if not validar_chunks(chunks, transcricao_completa):
        logger.error("‚ùå Chunks inv√°lidos! Abortando.")
        sys.exit(1)
    
    chunks_info = [{'inicio': c['inicio'], 'fim': c['fim']} for c in chunks]
    
    estimar_custo(transcricao_completa, usar_cache, num_partes)
    
    if num_partes == 1:
        return processar_simples(client, transcricao_completa, prompt_ativo), None, client
    
    checkpoint = None
    resultados = []
    inicio_secao = 0
    
    if input_file:
        checkpoint = load_checkpoint(input_file)
        if checkpoint:
            logger.info(f"üìÅ Checkpoint: se√ß√£o {checkpoint['secao_atual']}/{checkpoint['total_secoes']}")
            resposta = input("   Continuar? (s/n): ").strip().lower()
            if resposta == 's':
                resultados = checkpoint['resultados']
                inicio_secao = checkpoint['secao_atual']
                chunks_info = checkpoint['chunks_info']
            else:
                delete_checkpoint(input_file)
    
    # v2.19: Cache HABILITADO com Estrutura Global (User Request)
    cache = None
    if usar_cache and num_partes > 1:
         cache = criar_cache_contexto(client, transcricao_completa, prompt_ativo, estrutura_global)
    
    try:
        # Prepara o progresso
        iter_chunks = range(inicio_secao, num_partes)
        if tqdm:
            iter_chunks = tqdm(iter_chunks, desc="Processando", unit="se√ß√£o")
        
        for i in iter_chunks:
            chunk = chunks_info[i]
            texto_chunk = transcricao_completa[chunk['inicio']:chunk['fim']]
            
            # v2.10: Contexto est√°tico apenas (instru√ß√£o de estilo)
            contexto_estilo = None
            if resultados:
                 # Pega os √∫ltimos 3000 caracteres como refer√™ncia de ESTILO
                 ultimo_texto = resultados[-1]
                 contexto_estilo = ultimo_texto[-CONTEXTO_ESTILO:] if len(ultimo_texto) > CONTEXTO_ESTILO else ultimo_texto
            
            # v2.10: Extrair √∫ltimo t√≠tulo do chunk anterior para anti-duplica√ß√£o
            ultimo_titulo = None
            if resultados:
                texto_anterior = resultados[-1]
                for linha in reversed(texto_anterior.split('\n')[-30:]):
                    if linha.strip().startswith('##'):
                        ultimo_titulo = re.sub(r'^#+\s*(?:\d+(?:\.\d+)*\.?)?\s*', '', linha).strip()
                        break
            
            # v2.17: Contexto Localizado (Localized Context)
            # Em vez de passar a estrutura inteira, passa uma janela: [Anterior] + [Atual] + [Pr√≥ximo]
            estrutura_local = estrutura_global
            if estrutura_global and num_partes > 1:
                itens_estrutura = estrutura_global.split('\n')
                # Estimativa simples: mapeia chunk i para itens da estrutura
                ratio = len(itens_estrutura) / num_partes
                center_idx = int(i * ratio)
                window_size = max(4, int(len(itens_estrutura) * 0.15)) # 15% da estrutura ou min 4 itens
                
                start_idx = max(0, center_idx - window_size)
                end_idx = min(len(itens_estrutura), center_idx + window_size + 2)
                
                slice_itens = itens_estrutura[start_idx:end_idx]
                if start_idx > 0: slice_itens.insert(0, "[... T√≥picos anteriores ...]")
                if end_idx < len(itens_estrutura): slice_itens.append("[... T√≥picos posteriores ...]")
                
                estrutura_local = '\n'.join(slice_itens)

            resultado = processar_chunk(
                client, cache, prompt_ativo, texto_chunk,
                i + 1, num_partes,
                contexto_estilo=contexto_estilo,
                estrutura_global=estrutura_local, # Passa estrutura localmente fatiada
                ultimo_titulo=ultimo_titulo
            )
            
            # v2.10: Smart Stitching - Remove eco do contexto
            if contexto_estilo:
                resultado = remover_eco_do_contexto(resultado, contexto_estilo)
            
            # v2.10: Smart Stitching - Remove t√≠tulo duplicado na fronteira
            texto_acumulado = '\n\n'.join(resultados) if resultados else ""
            resultado = limpar_inicio_redundante(resultado, texto_acumulado)
            
            resultados.append(resultado)
            
            if input_file:
                save_checkpoint(input_file, resultados, chunks_info, i + 1)
            
            if not tqdm:
                logger.info(f"‚úÖ Se√ß√£o {i+1}/{num_partes}")
        
        # v2.7: Post-processing em m√∫ltiplas passadas
        logger.info("üßπ Iniciando limpeza (v2.7)...")
        
        texto_final = '\n\n'.join(resultados)
        
        # Passada 0: Limpar metadados de mapeamento que vazam para o output
        # Remove linhas como "[TIPO: AULA EXPOSITIVA]" ou "**[TIPO: SIMULADO]**"
        texto_final = re.sub(r'^#?\s*\*?\*?\[TIPO:.*?\]\*?\*?\s*$', '', texto_final, flags=re.MULTILINE)
        # Remove marcadores de bloco [BLOCO 01], [BLOCO 02], etc.
        texto_final = re.sub(r'^\s*\[BLOCO\s*\d+\]\s*$', '', texto_final, flags=re.MULTILINE)
        # Remove timestamps √≥rf√£os [HH:MM] ou [HH:MM:SS] no in√≠cio de linha
        texto_final = re.sub(r'^\s*\[\d{1,2}:\d{2}(:\d{2})?\]\s*$', '', texto_final, flags=re.MULTILINE)
        texto_final = re.sub(r'\n{3,}', '\n\n', texto_final)  # Remove linhas em branco extras
        
        logger.info("  Passada 1: Removendo duplica√ß√µes literais...")
        texto_final = remover_duplicacoes_literais(texto_final)
        
        logger.info("  Passada 1.5: Deduplica√ß√£o robusta (7-DIFF Strategy)...")
        texto_final = remover_overlap_duplicado(texto_final)
        
        logger.info("  Passada 2: Detectando e removendo se√ß√µes duplicadas (v2.15)...")
        texto_final = remover_secoes_duplicadas(texto_final)
        
        logger.info("  Passada 3: Normalizando t√≠tulos similares...")
        texto_final = normalize_headings(texto_final)
        
        if MODO_NOME != "FIDELIDADE":
            logger.info("  Passada 3.5: Reorganiza√ß√£o Estrutural Determin√≠stica...")
            texto_final = deterministic_structure_fix(texto_final)
        else:
            logger.info("  ‚ÑπÔ∏è  Modo FIDELIDADE: Pulando reorganiza√ß√£o para preservar linearidade.")
            
        
        # v2.10: Reordena√ß√£o do Pipeline (Tabelas -> Numera√ß√£o -> Par√°grafos)
        
        logger.info("  Passada 4: Reorganizando tabelas (Smart Layout)...")
        texto_final = mover_tabelas_para_fim_de_secao(texto_final)

        
        logger.info("  Passada 5: Numerando t√≠tulos...")
        texto_final = numerar_titulos(texto_final)
        
        logger.info("  Passada 6: Ajustando par√°grafos longos...")
        texto_final = quebrar_paragrafos_longos(texto_final, max_chars=400, max_sentencas=4)
        
        if MODO_NOME != "FIDELIDADE":
            logger.info("  Passada 7: Revis√£o sem√¢ntica de estrutura (IA v2.0)...")
            texto_final = ai_structure_review(client, texto_final, estrutura_mapeada=estrutura_global)
        else:
            logger.info("  Passada 7: Revis√£o leve de formata√ß√£o (IA - Modo Fidelidade v2.0)...")
            texto_final = ai_structure_review_lite(client, texto_final, estrutura_mapeada=estrutura_global)
        
        # Passada 7.5: Renumera√ß√£o Sequencial Determin√≠stica (camada de seguran√ßa)
        try:
            texto_final = renumerar_secoes(texto_final)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro na renumera√ß√£o: {e}. Continuando sem renumerar.")
        
        # Valida√ß√£o final
        palavras_in = len(transcricao_completa.split())
        palavras_out = len(texto_final.split())
        razao = palavras_out / palavras_in if palavras_in > 0 else 1.0
        
        logger.info(f"‚úÖ Valida√ß√£o: {razao:.0%} do original ({palavras_out:,}/{palavras_in:,} palavras)")
        
        if razao < THRESHOLD_CRITICO:
            if FIDELIDADE_MODE:
                logger.error(f"‚ùå POSS√çVEL PERDA DE CONTE√öDO ({razao:.0%})")
                logger.error(f"   Esperado: >{THRESHOLD_CRITICO:.0%} | Obtido: {razao:.0%}")
            elif APOSTILA_MODE:
                logger.warning(f"‚ö†Ô∏è  Texto condensado: {razao:.0%}")
                logger.info(f"   ‚úÖ Esperado no modo {MODO_NOME}")
            
            if razao < 0.30 or (FIDELIDADE_MODE and razao < THRESHOLD_CRITICO):
                resposta = input("\n   Continuar? (s/n): ").strip().lower()
                if resposta != 's':
                    logger.info("Cancelado.")
                    sys.exit(1)
        
        if input_file:
            delete_checkpoint(input_file)
        
        return texto_final, cache, client
    
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  Interrompido. Checkpoint salvo.")
        sys.exit(1)

# =============================================================================
# EXPORTA√á√ÉO WORD
# =============================================================================

def create_toc(doc):
    """Adiciona Sum√°rio (Table of Contents) nativo do Word"""
    paragraph = doc.add_paragraph()
    run = paragraph.add_run()
    
    fldChar = OxmlElement('w:fldChar')
    fldChar.set(qn('w:fldCharType'), 'begin')
    run._r.append(fldChar)
    
    instrText = OxmlElement('w:instrText')
    instrText.set(qn('xml:space'), 'preserve')
    instrText.text = 'TOC \\o "1-3" \\h \\z \\u'
    run._r.append(instrText)
    
    fldChar = OxmlElement('w:fldChar')
    fldChar.set(qn('w:fldCharType'), 'separate')
    run._r.append(fldChar)
    
    fldChar = OxmlElement('w:fldChar')
    fldChar.set(qn('w:fldCharType'), 'end')
    run._r.append(fldChar)

def _format_inline_markdown(paragraph, text):
    """
    v2.16.2: Formata markdown inline de forma robusta.
    Suporta: ***bold_italic***, **bold**, *italic*, __bold__, _italic_, `code`.
    """
    paragraph.clear()
    
    # Regex robusta:
    # Group 2: ***text*** ou ___text___ (Bold + Italic)
    # Group 3: **text** ou ___text___ (Bold + Italic)
    # Group 4: **text** (Bold)
    # Group 5: __text__ (Bold)
    # Group 6: *text* (Italic)
    # Group 7: _text_ (Italic)
    # Group 8: `text` (Code)
    pattern = r'(\*{3}(.+?)\*{3}|_{3}(.+?)_{3}|\*{2}(.+?)\*{2}|_{2}(.+?)_{2}|(?<!\*)\*(?!\*)(.+?)(?<!\*)\*(?!\*)|(?<!_)_(?!(?:_|\s))(.+?)(?<!(?:_|\s))_(?!_)|`(.+?)`)'
    
    last_end = 0
    for match in re.finditer(pattern, text):
        if match.start() > last_end:
            paragraph.add_run(text[last_end:match.start()])
        
        full_match = match.group(0)
        
        if full_match.startswith('***'):
            content = match.group(2)
            run = paragraph.add_run(content)
            run.bold = True
            run.italic = True
        elif full_match.startswith('___'):
            content = match.group(3)
            run = paragraph.add_run(content)
            run.bold = True
            run.italic = True
        elif full_match.startswith('**'):
            content = match.group(4)
            run = paragraph.add_run(content)
            run.bold = True
            run.font.name = 'Arial'
        elif full_match.startswith('__'):
            content = match.group(5)
            run = paragraph.add_run(content)
            run.bold = True
            run.font.name = 'Arial'
        elif full_match.startswith('*'):
            run = paragraph.add_run(match.group(6))
            run.italic = True
            run.font.name = 'Arial'
        elif full_match.startswith('_'):
            run = paragraph.add_run(match.group(7))
            run.italic = True
            run.font.name = 'Arial'
        elif full_match.startswith('`'):
            run = paragraph.add_run(match.group(8))
            run.font.name = 'Courier New'
            run.font.size = Pt(9)
            run.font.color.rgb = RGBColor(200, 0, 0)
        
        last_end = match.end()
    
    if last_end < len(text):
        run = paragraph.add_run(text[last_end:])
        run.font.name = 'Arial'

def _add_table_to_doc(doc, rows):
    """Adiciona tabela formatada ao documento Word"""
    if len(rows) < 2:
        return
    
    max_cols = max(len(row) for row in rows)
    if max_cols == 0:
        return
    
    table = doc.add_table(rows=len(rows), cols=max_cols)
    table.style = 'Light Grid Accent 1'
    
    for i, row_data in enumerate(rows):
        for j in range(max_cols):
            cell = table.rows[i].cells[j]
            cell_text = row_data[j] if j < len(row_data) else ""
            
            # Formata markdown dentro da c√©lula
            _format_inline_markdown(cell.paragraphs[0], cell_text)
            
            # Alinhamento padr√£o: Esquerda
            cell.paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.LEFT

            if i == 0:
                for paragraph in cell.paragraphs:
                    for run in paragraph.runs:
                        run.font.bold = True
                        run.font.name = 'Arial'
                        run.font.color.rgb = RGBColor(255, 255, 255)
                    # Alinhamento √† esquerda conforme solicitado
                    paragraph.alignment = WD_ALIGN_PARAGRAPH.LEFT
                
                shading_elm = OxmlElement('w:shd')
                shading_elm.set(qn('w:fill'), '336699')
                cell._element.get_or_add_tcPr().append(shading_elm)
            else:
                 # Conte√∫do normal da tabela: Esquerda
                 for paragraph in cell.paragraphs:
                    paragraph.alignment = WD_ALIGN_PARAGRAPH.LEFT
                    for run in paragraph.runs:
                        run.font.name = 'Arial'
    
    table.alignment = WD_TABLE_ALIGNMENT.CENTER

def save_as_word(formatted_text, video_name, output_file):
    """Salva markdown formatado como documento Word (.docx)"""
    if not DOCX_AVAILABLE:
        logger.warning("python-docx n√£o dispon√≠vel. Salvando apenas Markdown.")
        return None
    
    logger.info("üìÑ Gerando documento Word...")
    
    doc = Document()
    
    # Configura Fonte Arial no estilo Normal
    style = doc.styles['Normal']
    font = style.font
    font.name = 'Arial'
    font.size = Pt(12)
    # python-docx precisa desse hack para garantir que o nome da fonte seja aplicado corretamente
    style.element.rPr.rFonts.set(qn('w:ascii'), 'Arial')
    style.element.rPr.rFonts.set(qn('w:hAnsi'), 'Arial')
    
    # Margens
    section = doc.sections[0]
    section.top_margin = Inches(1)
    section.bottom_margin = Inches(1)
    section.left_margin = Inches(1.25)
    section.right_margin = Inches(1.25)
    
    # T√≠tulo principal
    title = doc.add_heading(video_name, level=0)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER
    for run in title.runs:
        run.font.name = 'Arial'
        run.font.size = Pt(20)
        run.font.color.rgb = RGBColor(0, 51, 102)
    
    # Data de gera√ß√£o
    date_para = doc.add_paragraph()
    date_run = date_para.add_run(f"Gerado em {time.strftime('%d/%m/%Y √†s %H:%M')} - Modo: {MODO_NOME}")
    date_run.font.name = 'Arial'
    date_run.italic = True
    date_run.font.size = Pt(10)
    date_run.font.color.rgb = RGBColor(128, 128, 128)
    date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
    
    doc.add_paragraph()
    
    # Sum√°rio
    doc.add_heading('Sum√°rio', level=1)
    create_toc(doc)
    doc.add_page_break()
    
    # Processa conte√∫do markdown
    lines = formatted_text.split('\n')
    i = 0
    in_table = False
    table_rows = []
    
    while i < len(lines):
        line = lines[i].strip()
        
        if not line:
            i += 1
            continue
        
        # Tabelas
        if '|' in line and not in_table:
            in_table = True
            table_rows = []
        
        if in_table:
            # Ignora linha separadora (ex: |---| ou | :--- | :--- | :--- |)
            is_separator = re.match(r'^\s*\|[\s:|-]+\|[\s:|-]*$', line)
            
            if '|' in line and not is_separator:
                table_rows.append([cell.strip() for cell in line.split('|')[1:-1]])
            
            if '|' not in line or i == len(lines) - 1:
                if len(table_rows) > 0:
                    _add_table_to_doc(doc, table_rows)
                in_table = False
                table_rows = []
                if '|' not in line:
                    continue
            i += 1
            continue
        
        # Headings
        if line.startswith('##### '):
            h = doc.add_heading('', level=5)
            _format_inline_markdown(h.paragraphs[0], line[6:])
        elif h_match := re.match(r'^(####|###|##|#)\s+(.*)', line):
            lvl = len(h_match.group(1))
            h_text = h_match.group(2)
            if lvl == 1 and h_text == video_name:
                i += 1
                continue
            h = doc.add_heading('', level=lvl)
            _format_inline_markdown(h, h_text)  # 'h' j√° √© um Paragraph
        # Separadores
        elif line.strip() in ['---', '***', '___']:
            p = doc.add_paragraph()
            p.add_run('_' * 80).font.color.rgb = RGBColor(192, 192, 192)
        # Quotes
        elif line.startswith('>'):
            p = doc.add_paragraph(style='Quote')
            p.paragraph_format.left_indent = Cm(4.0)
            p.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
            _format_inline_markdown(p, line[1:].strip())
            # For√ßar estilo de quote em todos os runs
            for run in p.runs:
                run.font.name = 'Arial'
                run.italic = True
                run.font.size = Pt(10)
        # Listas n√£o-ordenadas
        elif line.startswith('- ') or line.startswith('* '):
            p = doc.add_paragraph(line[2:], style='List Bullet')
            # For√ßar recuo de 1,5cm
            p.paragraph_format.left_indent = Cm(1.5)
            p.paragraph_format.first_line_indent = Cm(-0.63)
            
            _format_inline_markdown(p, line[2:])
            
        # Listas numeradas
        elif len(line) > 2 and line[0].isdigit() and line[1:3] in ['. ', ') ']:
            p = doc.add_paragraph(style='Normal')
            # Recuo padronizado de 1,5cm
            p.paragraph_format.left_indent = Cm(1.5)
            p.paragraph_format.first_line_indent = Cm(-0.63) # Mant√©m hanging indent para o n√∫mero
            _format_inline_markdown(p, line)
            
        # Par√°grafo normal
        else:
            p = doc.add_paragraph()
            # Espa√ßamento 1.5 (Line Spacing = 1.5)
            p.paragraph_format.line_spacing_rule = WD_LINE_SPACING.ONE_POINT_FIVE
            # Espa√ßamento antes e ap√≥s par√°grafo (6pt)
            p.paragraph_format.space_before = Pt(6)
            p.paragraph_format.space_after = Pt(6) 
            
            # Recuo de 1¬™ linha (1cm)
            p.paragraph_format.first_line_indent = Cm(1.0)
            
            # Justificado
            p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY
            
            _format_inline_markdown(p, line)
            
            # For√ßar fonte 12pt para texto normal e Arial
            for run in p.runs:
                run.font.name = 'Arial'
                run.font.size = Pt(12)
        
        i += 1
    
    # Flush final (caso o arquivo termine com tabela seguida de linhas em branco)
    if in_table and len(table_rows) > 0:
        _add_table_to_doc(doc, table_rows)
        
    doc.save(output_file)
    logger.info(f"‚úÖ Word salvo: {output_file}")
    return output_file

def salvar_resultado(conteudo, arquivo):
    with open(arquivo, 'w', encoding='utf-8') as f:
        f.write(conteudo)
    logger.info(f"‚úÖ Markdown salvo: {arquivo}")

# =============================================================================
# MAIN
# =============================================================================

def main():
    if len(sys.argv) < 2 or '--help' in sys.argv:
        print("=" * 70)
        print("FORMATADOR v2.7 - ANTI-DUPLICA√á√ÉO")
        print("=" * 70)
        print("\nUso: python format_transcription_gemini.py <entrada.txt> [saida] [--prompt <texto_ou_path>]")
        print("\nOp√ß√µes:")
        print("  --dry-run         Valida chunks e mostra preview")
        print("  --prompt <p>      Prompt customizado (texto direto ou caminho .txt)")
        print("  --help            Mostra esta mensagem")
        print("\nüõ°Ô∏è  CORRE√á√ïES v2.7:")
        print("  ‚Ä¢ Detec√ß√£o agressiva de se√ß√µes duplicadas")
        print("  ‚Ä¢ Valida√ß√£o rigorosa de chunks sequenciais")
        print("  ‚Ä¢ Delimitadores de contexto mais vis√≠veis")
        print("  ‚Ä¢ Post-processing em m√∫ltiplas passadas")
        print("  ‚Ä¢ Cache desabilitado (debug)")
        print("\nDepend√™ncias:")
        print("  pip install google-genai python-docx tqdm")
        sys.exit(1)
    
    arquivo_entrada = sys.argv[1]
    
    if len(sys.argv) > 2 and not sys.argv[2].startswith('--'):
        arquivo_saida = sys.argv[2]
    else:
        base = arquivo_entrada.replace('.txt', '_formatada_v2.7')
        arquivo_saida = f"{base}.md"
    
    video_name = Path(arquivo_entrada).stem
    
    # v2.21: Parse prompt customizado
    custom_prompt = None
    if '--prompt' in sys.argv:
        try:
            p_idx = sys.argv.index('--prompt')
            p_val = sys.argv[p_idx + 1]
            if os.path.exists(p_val):
                with open(p_val, 'r', encoding='utf-8') as f:
                    custom_prompt = f.read().strip()
                logger.info(f"üìÇ Prompt carregado de arquivo: {p_val}")
            else:
                custom_prompt = p_val.strip()
                logger.info("‚úçÔ∏è Prompt customizado lido diretamente da CLI")
        except Exception as e:
            logger.error(f"‚ùå Erro ao ler prompt: {e}")
            sys.exit(1)

    logger.info("=" * 60)
    logger.info(f"FORMATADOR v2.7 ANTI-DUPLICA√á√ÉO - Modo {MODO_NOME}")
    logger.info("=" * 60)
    logger.info(f"üìÇ Entrada: {arquivo_entrada}")
    logger.info(f"üìù Sa√≠da: {arquivo_saida}")
    
    transcricao = carregar_transcricao(arquivo_entrada)
    
    try:
        resultado, cache, client = formatar_transcricao(transcricao, input_file=arquivo_entrada, custom_prompt=custom_prompt)
    except Exception as e:
        logger.error(f"\n‚ùå Falha: {e}", exc_info=True)
        sys.exit(1)
    
    # v2.18: Auto-Fix Pass - Corre√ß√µes autom√°ticas
    logger.info("üîß Aplicando Auto-Fix Pass (v2.18)...")
    resultado, correcoes = aplicar_correcoes_automaticas(resultado)
    
    salvar_resultado(resultado, arquivo_saida)
    
    # v2.8: Verifica√ß√£o de cobertura e duplica√ß√µes
    verificar_cobertura(transcricao, resultado, arquivo_saida)

    # v2.16: Valida√ß√£o LLM (Metadata Strategy)
    validation_result = validate_completeness_llm(transcricao, resultado, client, arquivo_saida)
    
    # v2.17: Auto-Fix Loop - Corrige omiss√µes se detectadas
    if validation_result and not validation_result.get('aprovado', True):
        logger.info("üîÅ Iniciando Auto-Fix Loop...")
        resultado = auto_fix_smart(transcricao, resultado, validation_result, client, estrutura_global)
        
        # Salvar vers√£o corrigida
        with open(arquivo_saida, 'w', encoding='utf-8') as f:
            f.write(resultado)
        logger.info(f"üíæ Vers√£o corrigida salva: {arquivo_saida}")

    # v2.9: Auditoria Legal P√≥s-Processamento
    if AUDIT_AVAILABLE:
        report_path = arquivo_saida.replace('.md', '_RELATORIO_AUDITORIA.md')
        auditar_consistencia_legal(client, resultado, report_path)

    if DOCX_AVAILABLE:
        arquivo_docx = arquivo_saida.replace('.md', '.docx')
        save_as_word(resultado, video_name, arquivo_docx)
    
    # v2.10: M√©tricas de Execu√ß√£o (tokens reais)
    metrics.stop_timer()
    logger.info(metrics.get_report())
    logger.info("‚ú® Conclu√≠do! (v2.10 com m√©tricas)")

    # v2.19: Cleanup manual do cache para economia
    if cache:
        try:
            client.caches.delete(name=cache.name)
            logger.info(f"üóëÔ∏è Cache {cache.name} deletado manualmente para economizar custos.")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel deletar o cache: {e}")

if __name__ == "__main__":
    main()
